<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>tests.test_basic API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_basic</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import time
import common
import subprocess
import pytest

from common import clients, random_labels, volume_name  # NOQA
from common import core_api, apps_api, pod   # NOQA
from common import SIZE, DEV_PATH, EXPAND_SIZE
from common import check_device_data, write_device_random_data
from common import check_volume_data, write_volume_random_data
from common import get_self_host_id, volume_valid
from common import iscsi_login, iscsi_logout
from common import wait_for_volume_status
from common import wait_for_volume_delete
from common import wait_for_snapshot_purge
from common import generate_volume_name
from common import get_volume_endpoint, get_volume_engine
from common import get_random_client
from common import activate_standby_volume, check_volume_last_backup
from common import create_pv_for_volume, create_pvc_for_volume
from common import create_and_wait_pod, delete_and_wait_pod
from common import delete_and_wait_pvc, delete_and_wait_pv
from common import CONDITION_STATUS_FALSE, CONDITION_STATUS_TRUE
from common import RETRY_COUNTS, RETRY_INTERVAL, RETRY_COMMAND_COUNT
from common import cleanup_volume, create_and_check_volume, create_backup
from common import DEFAULT_VOLUME_SIZE
from common import Gi
from common import wait_for_volume_detached
from common import create_pvc_spec
from common import generate_random_data, write_volume_data
from common import VOLUME_RWTEST_SIZE
from common import write_pod_volume_data
from common import find_backup
from common import wait_for_backup_completion
from common import create_storage_class
from common import wait_for_volume_restoration_completed
from common import read_volume_data
from common import delete_backup
from common import pvc_name # NOQA
from common import storage_class # NOQA
from common import pod_make # NOQA
from common import set_random_backupstore
from common import create_snapshot
from common import expand_attached_volume
from common import wait_for_dr_volume_expansion
from common import check_block_device_size
from common import wait_for_rebuild_complete


@pytest.mark.coretest   # NOQA
def test_hosts(clients):  # NOQA
    &#34;&#34;&#34;
    Check node name and IP
    &#34;&#34;&#34;
    hosts = next(iter(clients.values())).list_node()
    for host in hosts:
        assert host.name is not None
        assert host.address is not None

    host_id = []
    for i in range(0, len(hosts)):
        host_id.append(hosts.data[i].name)

    host0_from_i = {}
    for i in range(0, len(hosts)):
        if len(host0_from_i) == 0:
            host0_from_i = clients[host_id[0]].by_id_node(host_id[0])
        else:
            assert host0_from_i.name == \
                clients[host_id[i]].by_id_node(host_id[0]).name
            assert host0_from_i.address == \
                clients[host_id[i]].by_id_node(host_id[0]).address


@pytest.mark.coretest   # NOQA
def test_settings(clients):  # NOQA
    &#34;&#34;&#34;
    Check input for settings
    &#34;&#34;&#34;
    client = get_random_client(clients)

    setting_names = [common.SETTING_BACKUP_TARGET,
                     common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET,
                     common.SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE,
                     common.SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE,
                     common.SETTING_DEFAULT_REPLICA_COUNT]
    settings = client.list_setting()

    settingMap = {}
    for setting in settings:
        settingMap[setting.name] = setting

    for name in setting_names:
        assert settingMap[name] is not None
        assert settingMap[name].definition.description is not None

    for name in setting_names:
        setting = client.by_id_setting(name)
        assert settingMap[name].value == setting.value

        old_value = setting.value

        if name == common.SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-100&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            setting = client.update(setting, value=&#34;200&#34;)
            assert setting.value == &#34;200&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;200&#34;
        elif name == common.SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;300&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-30&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            setting = client.update(setting, value=&#34;30&#34;)
            assert setting.value == &#34;30&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;30&#34;
        elif name == common.SETTING_BACKUP_TARGET:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue$test&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            setting = client.update(setting, value=&#34;nfs://test&#34;)
            assert setting.value == &#34;nfs://test&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;nfs://test&#34;
        elif name == common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET:
            setting = client.update(setting, value=&#34;testvalue&#34;)
            assert setting.value == &#34;testvalue&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;testvalue&#34;
        elif name == common.SETTING_DEFAULT_REPLICA_COUNT:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-1&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;21&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            setting = client.update(setting, value=&#34;2&#34;)
            assert setting.value == &#34;2&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;2&#34;

        setting = client.update(setting, value=old_value)
        assert setting.value == old_value


def volume_rw_test(dev):
    assert volume_valid(dev)
    data = write_device_random_data(dev)
    check_device_data(dev, data)


@pytest.mark.coretest   # NOQA
def test_volume_basic(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic volume operations:

    1. Check volume name and parameter
    2. Create a volume and attach to the current node, then check volume states
    3. Check soft anti-affinity rule
    4. Write then read back to check volume data
    &#34;&#34;&#34;
    volume_basic_test(clients, volume_name)


def volume_basic_test(clients, volume_name, base_image=&#34;&#34;):  # NOQA
    num_hosts = len(clients)
    num_replicas = 3

    # get a random client
    for host_id, client in iter(clients.items()):
        break

    with pytest.raises(Exception):
        volume = client.create_volume(name=&#34;wrong_volume-name-1.0&#34;, size=SIZE,
                                      numberOfReplicas=2)
        volume = client.create_volume(name=&#34;wrong_volume-name&#34;, size=SIZE,
                                      numberOfReplicas=2)
        volume = client.create_volume(name=&#34;wrong_volume-name&#34;, size=SIZE,
                                      numberOfReplicas=2,
                                      frontend=&#34;invalid_frontend&#34;)

    volume = create_and_check_volume(client, volume_name, num_replicas, SIZE,
                                     base_image)
    assert volume.initialRestorationRequired is False

    def validate_volume_basic(expected, actual):
        assert actual.name == expected.name
        assert actual.size == expected.size
        assert actual.numberOfReplicas == expected.numberOfReplicas
        assert actual.frontend == &#34;blockdev&#34;
        assert actual.baseImage == base_image
        assert actual.state == expected.state
        assert actual.created == expected.created

    volumes = client.list_volume().data
    assert len(volumes) == 1
    validate_volume_basic(volume, volumes[0])

    volumeByName = client.by_id_volume(volume_name)
    validate_volume_basic(volume, volumeByName)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert volume.initialRestorationRequired is False

    volumeByName = client.by_id_volume(volume_name)
    validate_volume_basic(volume, volumeByName)
    assert get_volume_endpoint(volumeByName) == DEV_PATH + volume_name

    # validate soft anti-affinity
    hosts = {}
    for replica in volume.replicas:
        id = replica.hostId
        assert id != &#34;&#34;
        hosts[id] = True
    if num_hosts &gt;= num_replicas:
        assert len(hosts) == num_replicas
    else:
        assert len(hosts) == num_hosts

    volumes = client.list_volume().data
    assert len(volumes) == 1
    assert volumes[0].name == volume.name
    assert volumes[0].size == volume.size
    assert volumes[0].numberOfReplicas == volume.numberOfReplicas
    assert volumes[0].state == volume.state
    assert volumes[0].created == volume.created
    assert get_volume_endpoint(volumes[0]) == DEV_PATH + volume_name

    volume = client.by_id_volume(volume_name)
    assert get_volume_endpoint(volume) == DEV_PATH + volume_name

    volume_rw_test(get_volume_endpoint(volume))

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)
    assert volume.initialRestorationRequired is False

    cleanup_volume(client, volume)


def test_volume_iscsi_basic(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic volume operations with iscsi frontend

    1. Create and attach a volume with iscsi frontend
    2. Check the volume endpoint and connect it using the iscsi
    initator on the node.
    3. Write then read back volume data for validation

    &#34;&#34;&#34;
    volume_iscsi_basic_test(clients, volume_name)


def volume_iscsi_basic_test(clients, volume_name, base_image=&#34;&#34;):  # NOQA
    # get a random client
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name, 3, SIZE, base_image,
                                     &#34;iscsi&#34;)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volumes = client.list_volume().data
    assert len(volumes) == 1
    assert volumes[0].name == volume.name
    assert volumes[0].size == volume.size
    assert volumes[0].numberOfReplicas == volume.numberOfReplicas
    assert volumes[0].state == volume.state
    assert volumes[0].created == volume.created
    assert volumes[0].frontend == &#34;iscsi&#34;
    endpoint = get_volume_endpoint(volumes[0])
    assert endpoint.startswith(&#34;iscsi://&#34;)

    try:
        dev = iscsi_login(endpoint)
        volume_rw_test(dev)
    finally:
        iscsi_logout(endpoint)

    cleanup_volume(client, volume)


@pytest.mark.coretest   # NOQA
def test_snapshot(clients, volume_name, base_image=&#34;&#34;):  # NOQA
    &#34;&#34;&#34;
    Test snapshot operations

    1. Create a volume and attach to the node
    2. Create the empty snapshot `snap1`
    3. Generate and write data `snap2_data`, then create `snap2`
    4. Generate and write data `snap3_data`, then create `snap3`
    5. List snapshot. Validate the snapshot chain relationship
    6. Mark `snap3` as removed. Make sure volume&#39;s data didn&#39;t change
    7. List snapshot. Make sure `snap3` is marked as removed
    8. Detach and reattach the volume in maintenance mode.
    9. Make sure the volume frontend is still `blockdev` but disabled
    10. Revert to `snap2`
    11. Detach and reattach the volume with frontend enabled
    12. Make sure volume&#39;s data is `snap2_data`
    13. List snapshot. Make sure `volume-head` is now `snap2`&#39;s child
    14. Delete `snap1` and `snap2`
    15. Purge the snapshot.
    16. List the snapshot, make sure `snap1` and `snap3`
    are gone. `snap2` is marked as removed.
    17. Check volume data, make sure it&#39;s still `snap2_data`.
    &#34;&#34;&#34;
    snapshot_test(clients, volume_name, base_image)


def snapshot_test(clients, volume_name, base_image):  # NOQA
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name,
                                     base_image=base_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    positions = {}

    snap1 = create_snapshot(client, volume_name)

    snap2_data = write_volume_random_data(volume, positions)
    snap2 = create_snapshot(client, volume_name)

    snap3_data = write_volume_random_data(volume, positions)
    snap3 = create_snapshot(client, volume_name)

    snapshots = volume.snapshotList()
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert snapMap[snap3.name].removed is False

    volume.snapshotDelete(name=snap3.name)
    check_volume_data(volume, snap3_data)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert len(snapMap[snap3.name].children) == 1
    assert &#34;volume-head&#34; in snapMap[snap3.name].children.keys()
    assert snapMap[snap3.name].removed is True

    snap = volume.snapshotGet(name=snap3.name)
    assert snap.name == snap3.name
    assert snap.parent == snap3.parent
    assert len(snap3.children) == 1
    assert len(snap.children) == 1
    assert &#34;volume-head&#34; in snap3.children.keys()
    assert &#34;volume-head&#34; in snap.children.keys()
    assert snap.removed is True

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    common.wait_for_volume_healthy_no_frontend(client, volume_name)

    volume = client.by_id_volume(volume_name)
    engine = get_volume_engine(volume)
    assert volume.disableFrontend is True
    assert volume.frontend == &#34;blockdev&#34;
    assert engine.endpoint == &#34;&#34;

    volume.snapshotRevert(name=snap2.name)

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == &#34;blockdev&#34;

    check_volume_data(volume, snap2_data)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert &#34;volume-head&#34; in snapMap[snap2.name].children.keys()
    assert snap3.name in snapMap[snap2.name].children.keys()
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert len(snapMap[snap3.name].children) == 0
    assert snapMap[snap3.name].removed is True

    volume.snapshotDelete(name=snap1.name)
    volume.snapshotDelete(name=snap2.name)

    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap1.name,
                                     snap3.name)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap
    assert snap1.name not in snapMap
    assert snap3.name not in snapMap

    # it&#39;s the parent of volume-head, so it cannot be purged at this time
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == &#34;&#34;
    assert &#34;volume-head&#34; in snapMap[snap2.name].children.keys()
    assert snapMap[snap2.name].removed is True
    check_volume_data(volume, snap2_data)

    cleanup_volume(client, volume)


@pytest.mark.coretest   # NOQA
def test_backup(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic backup

    Setup:

    1. Create a volume and attach to the current node
    2. Run the test for all the available backupstores.

    Steps:

    1. Create a backup of volume
    2. Restore the backup to a new volume
    3. Attach the new volume and make sure the data is the same as the old one
    4. Detach the volume and delete the backup.
    5. Wait for the restored volume&#39;s `lastBackup` to be cleaned (due to remove
    the backup)
    6. Delete the volume
    &#34;&#34;&#34;
    backup_test(clients, volume_name, SIZE)


def backup_test(clients, volume_name, size, base_image=&#34;&#34;):  # NOQA
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name, 2, size, base_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    # test backupTarget for multiple settings
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                    common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                    common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;

        backupstore_test(client, lht_hostId, volume_name, size)

    cleanup_volume(client, volume)


def backupstore_test(client, host_id, volname, size):
    bv, b, snap2, data = create_backup(client, volname)

    # test restore
    restoreName = generate_volume_name()
    volume = client.create_volume(name=restoreName, size=size,
                                  numberOfReplicas=2,
                                  fromBackup=b.url)

    volume = common.wait_for_volume_restoration_completed(client, restoreName)
    volume = common.wait_for_volume_detached(client, restoreName)
    assert volume.name == restoreName
    assert volume.size == size
    assert volume.numberOfReplicas == 2
    assert volume.state == &#34;detached&#34;
    assert volume.initialRestorationRequired is False

    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, restoreName)
    check_volume_data(volume, data)
    volume = volume.detach()
    volume = common.wait_for_volume_detached(client, restoreName)

    delete_backup(bv, b.name)

    volume = wait_for_volume_status(client, volume.name,
                                    &#34;lastBackup&#34;, &#34;&#34;)
    assert volume.lastBackupAt == &#34;&#34;

    client.delete(volume)

    volume = wait_for_volume_delete(client, restoreName)


@pytest.mark.coretest
def test_backup_labels(clients, random_labels, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the proper Labels are applied when creating a Backup manually.

    1. Create a volume
    2. Run the following steps on all backupstores
    3. Create a backup with some random labels
    4. Get backup from backupstore, verify the labels are set on the backups
    &#34;&#34;&#34;
    backup_labels_test(clients, random_labels, volume_name)


def backup_labels_test(clients, random_labels, volume_name, size=SIZE, base_image=&#34;&#34;):  # NOQA
    for _, client in iter(clients.items()):
        break
    host_id = get_self_host_id()

    volume = create_and_check_volume(client, volume_name, 2, size, base_image)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    # test backupTarget for multiple settings
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;

        bv, b, _, _ = create_backup(client, volume_name, labels=random_labels)
        # If we&#39;re running the test with a BaseImage, check that this Label is
        # set properly.
        backup = bv.backupGet(name=b.name)
        if base_image:
            assert backup.labels.get(common.BASE_IMAGE_LABEL) == base_image
            # One extra Label from the BaseImage being set.
            assert len(backup.labels) == len(random_labels) + 1
        else:
            assert len(backup.labels) == len(random_labels)

    cleanup_volume(client, volume)


@pytest.mark.coretest   # NOQA
def test_restore_inc(clients, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test restore from disaster recovery volume (incremental restore)

    Run test against all the backupstores

    1. Create a volume and attach to the current node
    2. Generate `data0`, write to the volume, make a backup `backup0`
    3. Create three DR(standby) volumes from the backup: `sb_volume0/1/2`
    4. Wait for all three DR volumes to finish the initial restoration
    5. Verify DR volumes&#39;s `lastBackup` is `backup0`
    6. Verify snapshot/pv/pvc/change backup target are not allowed as long
    as the DR volume exists
    7. Activate standby `sb_volume0` and attach it to check the volume data
    8. Generate `data1` and write to the original volume and create `backup1`
    9. Make sure `sb_volume1`&#39;s `lastBackup` field has been updated to
    `backup1`
    10. Wait for `sb_volume1` to finish incremental restoration then activate
    11. Attach and check `sb_volume1`&#39;s data
    12. Generate `data2` and write to the original volume and create `backup2`
    13. Make sure `sb_volume2`&#39;s `lastBackup` field has been updated to
    `backup1`
    14. Wait for `sb_volume2` to finish incremental restoration then activate
    15. Attach and check `sb_volume2`&#39;s data
    16. Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good

    FIXME: Step 16 works because the disk will be treated as a unformatted disk
    &#34;&#34;&#34;
    for _, client in iter(clients.items()):
        break

    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    # test backupTarget for multiple settings
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;

        restore_inc_test(client, core_api, volume_name, pod)


def restore_inc_test(client, core_api, volume_name, pod):  # NOQA
    std_volume = create_and_check_volume(client, volume_name, 2, SIZE)
    lht_host_id = get_self_host_id()
    std_volume.attach(hostId=lht_host_id)
    std_volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        std_volume.activate(frontend=&#34;blockdev&#34;)
        assert &#34;already in active mode&#34; in str(e.value)

    data0 = {&#39;len&#39;: 4 * 1024, &#39;pos&#39;: 0}
    data0[&#39;content&#39;] = common.generate_random_data(data0[&#39;len&#39;])
    bv, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    sb_volume0_name = &#34;sb-0-&#34; + volume_name
    sb_volume1_name = &#34;sb-1-&#34; + volume_name
    sb_volume2_name = &#34;sb-2-&#34; + volume_name
    client.create_volume(name=sb_volume0_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=sb_volume1_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=sb_volume2_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    common.wait_for_volume_restoration_completed(client, sb_volume0_name)
    common.wait_for_volume_restoration_completed(client, sb_volume1_name)
    common.wait_for_volume_restoration_completed(client, sb_volume2_name)

    sb_volume0 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume0_name)
    sb_volume1 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume1_name)
    sb_volume2 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume2_name)

    for i in range(RETRY_COUNTS):
        sb_volume0 = client.by_id_volume(sb_volume0_name)
        sb_volume1 = client.by_id_volume(sb_volume1_name)
        sb_volume2 = client.by_id_volume(sb_volume2_name)
        sb_engine0 = get_volume_engine(sb_volume0)
        sb_engine1 = get_volume_engine(sb_volume1)
        sb_engine2 = get_volume_engine(sb_volume2)
        if sb_volume0.initialRestorationRequired is True or \
           sb_volume1.initialRestorationRequired is True or \
           sb_volume2.initialRestorationRequired is True:
            time.sleep(RETRY_INTERVAL)
        else:
            break
    assert sb_volume0.standby is True
    assert sb_volume0.lastBackup == backup0.name
    assert sb_volume0.frontend == &#34;&#34;
    assert sb_volume0.initialRestorationRequired is False
    sb_engine0 = get_volume_engine(sb_volume0)
    assert sb_engine0.lastRestoredBackup == backup0.name
    assert sb_engine0.requestedBackupRestore == backup0.name
    assert sb_volume1.standby is True
    assert sb_volume1.lastBackup == backup0.name
    assert sb_volume1.frontend == &#34;&#34;
    assert sb_volume1.initialRestorationRequired is False
    sb_engine1 = get_volume_engine(sb_volume1)
    assert sb_engine1.lastRestoredBackup == backup0.name
    assert sb_engine1.requestedBackupRestore == backup0.name
    assert sb_volume2.standby is True
    assert sb_volume2.lastBackup == backup0.name
    assert sb_volume2.frontend == &#34;&#34;
    assert sb_volume2.initialRestorationRequired is False
    sb_engine2 = get_volume_engine(sb_volume2)
    assert sb_engine2.lastRestoredBackup == backup0.name
    assert sb_engine2.requestedBackupRestore == backup0.name

    sb0_snaps = sb_volume0.snapshotList()
    assert len(sb0_snaps) == 2
    for s in sb0_snaps:
        if s.name != &#34;volume-head&#34;:
            sb0_snap = s
    assert sb0_snaps
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotCreate()
        assert &#34;cannot create snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotRevert(name=sb0_snap.name)
        assert &#34;cannot revert snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotDelete(name=sb0_snap.name)
        assert &#34;cannot delete snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotBackup(name=sb0_snap.name)
        assert &#34;cannot create backup for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.pvCreate(pvName=sb_volume0_name)
        assert &#34;cannot create PV for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.pvcCreate(pvcName=sb_volume0_name)
        assert &#34;cannot create PVC for standby volume&#34; in str(e.value)
    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    with pytest.raises(Exception) as e:
        client.update(setting, value=&#34;random.backup.target&#34;)
        assert &#34;cannot modify BackupTarget &#34; \
               &#34;since there are existing standby volumes&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.activate(frontend=&#34;wrong_frontend&#34;)
        assert &#34;invalid frontend&#34; in str(e.value)

    activate_standby_volume(client, sb_volume0_name)
    sb_volume0 = client.by_id_volume(sb_volume0_name)
    sb_volume0.attach(hostId=lht_host_id)
    sb_volume0 = common.wait_for_volume_healthy(client, sb_volume0_name)
    check_volume_data(sb_volume0, data0, False)

    zero_string = b&#39;\x00&#39;.decode(&#39;utf-8&#39;)
    _, backup1, _, data1 = create_backup(
        client, volume_name,
        {&#39;len&#39;: 2 * 1024, &#39;pos&#39;: 0, &#39;content&#39;: zero_string * 2 * 1024})
    # use this api to update field `last backup`
    client.list_backupVolume()
    check_volume_last_backup(client, sb_volume1_name, backup1.name)
    activate_standby_volume(client, sb_volume1_name)
    sb_volume1 = client.by_id_volume(sb_volume1_name)
    sb_volume1.attach(hostId=lht_host_id)
    sb_volume1 = common.wait_for_volume_healthy(client, sb_volume1_name)
    data0_modified = {
        &#39;len&#39;: data0[&#39;len&#39;] - data1[&#39;len&#39;],
        &#39;pos&#39;: data1[&#39;len&#39;],
        &#39;content&#39;: data0[&#39;content&#39;][data1[&#39;len&#39;]:],
    }
    check_volume_data(sb_volume1, data0_modified, False)
    check_volume_data(sb_volume1, data1)

    data2 = {&#39;len&#39;: 1 * 1024 * 1024, &#39;pos&#39;: 0}
    data2[&#39;content&#39;] = common.generate_random_data(data2[&#39;len&#39;])
    _, backup2, _, data2 = create_backup(client, volume_name, data2)
    client.list_backupVolume()
    check_volume_last_backup(client, sb_volume2_name, backup2.name)
    activate_standby_volume(client, sb_volume2_name)
    sb_volume2 = client.by_id_volume(sb_volume2_name)
    sb_volume2.attach(hostId=lht_host_id)
    sb_volume2 = common.wait_for_volume_healthy(client, sb_volume2_name)
    check_volume_data(sb_volume2, data2)

    # allocated this active volume to a pod
    sb_volume2.detach()
    sb_volume2 = common.wait_for_volume_detached(client, sb_volume2_name)

    create_pv_for_volume(client, core_api, sb_volume2, sb_volume2_name)
    create_pvc_for_volume(client, core_api, sb_volume2, sb_volume2_name)

    sb_volume2_pod_name = &#34;pod-&#34; + sb_volume2_name
    pod[&#39;metadata&#39;][&#39;name&#39;] = sb_volume2_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: sb_volume2_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    sb_volume2 = client.by_id_volume(sb_volume2_name)
    k_status = sb_volume2.kubernetesStatus
    workloads = k_status.workloadsStatus
    assert k_status.pvName == sb_volume2_name
    assert k_status.pvStatus == &#39;Bound&#39;
    assert len(workloads) == 1
    for i in range(RETRY_COUNTS):
        if workloads[0].podStatus == &#39;Running&#39;:
            break
        time.sleep(RETRY_INTERVAL)
        sb_volume2 = client.by_id_volume(sb_volume2_name)
        k_status = sb_volume2.kubernetesStatus
        workloads = k_status.workloadsStatus
        assert len(workloads) == 1
    assert workloads[0].podName == sb_volume2_pod_name
    assert workloads[0].podStatus == &#39;Running&#39;
    assert not workloads[0].workloadName
    assert not workloads[0].workloadType
    assert k_status.namespace == &#39;default&#39;
    assert k_status.pvcName == sb_volume2_name
    assert not k_status.lastPVCRefAt
    assert not k_status.lastPodRefAt

    delete_and_wait_pod(core_api, sb_volume2_pod_name)
    delete_and_wait_pvc(core_api, sb_volume2_name)
    delete_and_wait_pv(core_api, sb_volume2_name)

    # cleanup
    std_volume.detach()
    sb_volume0.detach()
    sb_volume1.detach()
    std_volume = common.wait_for_volume_detached(client, volume_name)
    sb_volume0 = common.wait_for_volume_detached(client, sb_volume0_name)
    sb_volume1 = common.wait_for_volume_detached(client, sb_volume1_name)
    sb_volume2 = common.wait_for_volume_detached(client, sb_volume2_name)

    bv.backupDelete(name=backup2.name)
    bv.backupDelete(name=backup1.name)
    bv.backupDelete(name=backup0.name)

    client.delete(std_volume)
    client.delete(sb_volume0)
    client.delete(sb_volume1)
    client.delete(sb_volume2)

    wait_for_volume_delete(client, volume_name)
    wait_for_volume_delete(client, sb_volume0_name)
    wait_for_volume_delete(client, sb_volume1_name)
    wait_for_volume_delete(client, sb_volume2_name)

    volumes = client.list_volume()
    assert len(volumes) == 0


def test_deleting_backup_volume(clients):  # NOQA
    &#34;&#34;&#34;
    Test deleting backup volumes

    1. Create volume and create backup
    2. Delete the backup and make sure it&#39;s gone in the backupstore
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break
    lht_hostId = get_self_host_id()

    volName = generate_volume_name()
    volume = create_and_check_volume(client, volName)

    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volName)

    bv, _, snap1, _ = create_backup(client, volName)
    _, _, snap2, _ = create_backup(client, volName)

    bv = client.by_id_backupVolume(volName)
    client.delete(bv)
    common.wait_for_backup_volume_delete(client, volName)
    cleanup_volume(client, volume)


@pytest.mark.coretest   # NOQA
def test_listing_backup_volume(clients, base_image=&#34;&#34;):   # NOQA
    &#34;&#34;&#34;
    Test listing backup volumes

    1. Create three volumes: `volume1/2/3`
    2. Setup NFS backupstore since we can manipulate the content easily
    3. Create snapshots for all three volumes
    4. Rename `volume1`&#39;s `volume.cfg` to `volume.cfg.tmp` in backupstore
    5. List backup volumes. Make sure `volume1` errors out but found other two
    6. Restore `volume1`&#39;s `volume.cfg`.
    7. Make sure now backup volume `volume1` can be found and deleted
    8. Delete backups for `volume2/3`, make sure they cannot be found later
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break
    lht_hostId = get_self_host_id()

    # create 3 volumes.
    volume1_name = generate_volume_name()
    volume2_name = generate_volume_name()
    volume3_name = generate_volume_name()

    volume1 = create_and_check_volume(client, volume1_name)
    volume2 = create_and_check_volume(client, volume2_name)
    volume3 = create_and_check_volume(client, volume3_name)

    volume1.attach(hostId=lht_hostId)
    volume1 = common.wait_for_volume_healthy(client, volume1_name)
    volume2.attach(hostId=lht_hostId)
    volume2 = common.wait_for_volume_healthy(client, volume2_name)
    volume3.attach(hostId=lht_hostId)
    volume3 = common.wait_for_volume_healthy(client, volume3_name)

    # we only test NFS here.
    # Since it is difficult to directly remove volume.cfg from s3 buckets
    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_nfs(backupstore):
            updated = False
            for i in range(RETRY_COMMAND_COUNT):
                nfs_url = backupstore.strip(&#34;nfs://&#34;)
                setting = client.update(setting, value=backupstore)
                assert setting.value == backupstore
                setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
                if &#34;nfs&#34; in setting.value:
                    updated = True
                    break
            assert updated

    _, _, snap1, _ = create_backup(client, volume1_name)
    _, _, snap2, _ = create_backup(client, volume2_name)
    _, _, snap3, _ = create_backup(client, volume3_name)

    # invalidate backup volume 1 by renaming volume.cfg to volume.cfg.tmp
    cmd = [&#34;mkdir&#34;, &#34;-p&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;mount&#34;, &#34;-t&#34;, &#34;nfs4&#34;, nfs_url, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;find&#34;, &#34;/mnt/nfs&#34;, &#34;-type&#34;, &#34;d&#34;, &#34;-name&#34;, volume1_name]
    volume1_backup_volume_path = \
        subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)

    cmd = [&#34;find&#34;, volume1_backup_volume_path, &#34;-name&#34;, &#34;volume.cfg&#34;]
    volume1_backup_volume_cfg_path = \
        subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)
    cmd = [&#34;mv&#34;, volume1_backup_volume_cfg_path,
           volume1_backup_volume_cfg_path + &#34;.tmp&#34;]
    subprocess.check_output(cmd)
    subprocess.check_output([&#34;sync&#34;])

    found1 = found2 = found3 = False
    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()
        for bv in bvs:
            if bv.name == volume1_name:
                if &#34;error&#34; in bv.messages:
                    assert &#34;volume.cfg&#34; in bv.messages.error.lower()
                    found1 = True
            elif bv.name == volume2_name:
                assert not bv.messages
                found2 = True
            elif bv.name == volume3_name:
                assert not bv.messages
                found3 = True
        if found1 &amp; found2 &amp; found3:
            break
        time.sleep(RETRY_INTERVAL)
    assert found1 &amp; found2 &amp; found3

    cmd = [&#34;mv&#34;, volume1_backup_volume_cfg_path + &#34;.tmp&#34;,
           volume1_backup_volume_cfg_path]
    subprocess.check_output(cmd)
    subprocess.check_output([&#34;sync&#34;])

    found = False
    for i in range(RETRY_COMMAND_COUNT):
        try:
            bv1, b1 = common.find_backup(client, volume1_name, snap1.name)
            found = True
            break
        except Exception:
            time.sleep(1)
    assert found
    bv1.backupDelete(name=b1.name)
    for i in range(RETRY_COMMAND_COUNT):
        found = False
        backups1 = bv1.backupList().data
        for b in backups1:
            if b.snapshotName == snap1.name:
                found = True
                break
    assert not found

    bv2, b2 = common.find_backup(client, volume2_name, snap2.name)
    bv2.backupDelete(name=b2.name)
    for i in range(RETRY_COMMAND_COUNT):
        found = False
        backups2 = bv2.backupList().data
        for b in backups2:
            if b.snapshotName == snap2.name:
                found = True
                break
    assert not found

    bv3, b3 = common.find_backup(client, volume3_name, snap3.name)
    bv3.backupDelete(name=b3.name)
    for i in range(RETRY_COMMAND_COUNT):
        found = False
        backups3 = bv3.backupList().data
        for b in backups3:
            if b.snapshotName == snap3.name:
                found = True
                break
    assert not found

    volume1.detach()
    volume1 = common.wait_for_volume_detached(client, volume1_name)
    client.delete(volume1)
    wait_for_volume_delete(client, volume1_name)

    volume2.detach()
    volume2 = common.wait_for_volume_detached(client, volume2_name)
    client.delete(volume2)
    wait_for_volume_delete(client, volume2_name)

    volume3.detach()
    volume3 = common.wait_for_volume_detached(client, volume3_name)
    client.delete(volume3)
    wait_for_volume_delete(client, volume3_name)

    volumes = client.list_volume()
    assert len(volumes) == 0


@pytest.mark.coretest   # NOQA
def test_volume_multinode(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test the volume can be attached on multiple nodes

    1. Create one volume
    2. Attach it on every node once, verify the state, then detach it
    &#34;&#34;&#34;
    hosts = clients.keys()

    volume = get_random_client(clients).create_volume(name=volume_name,
                                                      size=SIZE,
                                                      numberOfReplicas=2)
    volume = common.wait_for_volume_detached(get_random_client(clients),
                                             volume_name)

    for host_id in hosts:
        volume = volume.attach(hostId=host_id)
        volume = common.wait_for_volume_healthy(get_random_client(clients),
                                                volume_name)
        engine = get_volume_engine(volume)
        assert engine.hostId == host_id
        volume = volume.detach()
        volume = common.wait_for_volume_detached(get_random_client(clients),
                                                 volume_name)

    get_random_client(clients).delete(volume)
    wait_for_volume_delete(get_random_client(clients), volume_name)

    volumes = get_random_client(clients).list_volume()
    assert len(volumes) == 0


@pytest.mark.coretest  # NOQA
def test_volume_scheduling_failure(clients, volume_name):  # NOQA
    &#39;&#39;&#39;
    Test fail to schedule by disable scheduling for all the nodes

    Also test cannot attach a scheduling failed volume

    1. Disable `allowScheduling` for all nodes
    2. Create a volume.
    3. Verify the volume condition `Scheduled` is false
    4. Verify attaching the volume will result in error
    5. Enable `allowScheduling` for all nodes
    6. Volume should be automatically scheduled (condition become true)
    7. Volume can be attached now
    &#39;&#39;&#39;
    client = get_random_client(clients)
    nodes = client.list_node()
    assert len(nodes) &gt; 0

    for node in nodes:
        node = client.update(node, allowScheduling=False)
        node = common.wait_for_node_update(client, node.id,
                                           &#34;allowScheduling&#34;, False)

    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=3)

    volume = common.wait_for_volume_condition_scheduled(client, volume_name,
                                                        &#34;status&#34;,
                                                        CONDITION_STATUS_FALSE)
    volume = common.wait_for_volume_detached(client, volume_name)
    self_node = get_self_host_id()
    with pytest.raises(Exception) as e:
        volume.attach(hostId=self_node)
    assert &#34;not scheduled&#34; in str(e.value)

    for node in nodes:
        node = client.update(node, allowScheduling=True)
        node = common.wait_for_node_update(client, node.id,
                                           &#34;allowScheduling&#34;, True)

    volume = common.wait_for_volume_condition_scheduled(client, volume_name,
                                                        &#34;status&#34;,
                                                        CONDITION_STATUS_TRUE)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume = volume.attach(hostId=self_node)
    volume = common.wait_for_volume_healthy(client, volume_name)
    endpoint = get_volume_endpoint(volume)
    assert endpoint != &#34;&#34;
    volume_rw_test(endpoint)

    volume = volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest   # NOQA
def test_setting_default_replica_count(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test `Default Replica Count` setting

    1. Set default replica count in the global settings to 5
    2. Create a volume without specify the replica count
    3. The volume should have 5 replicas (instead of the previous default 3)
    &#34;&#34;&#34;
    client = get_random_client(clients)
    setting = client.by_id_setting(common.SETTING_DEFAULT_REPLICA_COUNT)
    old_value = setting.value
    setting = client.update(setting, value=&#34;5&#34;)

    volume = client.create_volume(name=volume_name, size=SIZE)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert len(volume.replicas) == int(setting.value)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    setting = client.update(setting, value=old_value)


@pytest.mark.coretest   # NOQA
def test_volume_update_replica_count(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test updating volume&#39;s replica count

    1. Create a volume with 3 replicas
    2. Attach the volume
    3. Increase the replica to 5.
    4. Volume will become degraded and start rebuilding
    5. Wait for rebuilding to complete
    6. Update the replica count to 2. Volume should remain healthy
    7. Remove 3 replicas, so there will be 2 replicas in the volume
    8. Verify the volume is still healthy

    FIXME: Don&#39;t need to wait for volume to rebuild and healthy before step 8.
    Volume should always be healthy even only with 2 replicas.
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break

    replica_count = 3
    volume = create_and_check_volume(client, volume_name, replica_count)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    replica_count = 5
    volume = volume.updateReplicaCount(replicaCount=replica_count)
    volume = common.wait_for_volume_degraded(client, volume_name)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == replica_count

    old_replica_count = replica_count
    replica_count = 2
    volume = volume.updateReplicaCount(replicaCount=replica_count)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == old_replica_count

    volume.replicaRemove(name=volume.replicas[0].name)
    volume.replicaRemove(name=volume.replicas[1].name)
    volume.replicaRemove(name=volume.replicas[2].name)

    volume = common.wait_for_volume_replica_count(client,
                                                  volume_name, replica_count)
    wait_for_rebuild_complete(client, volume_name)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == replica_count

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest   # NOQA
def test_attach_without_frontend(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test attach in maintenance mode (without frontend)

    1. Create a volume and attach to the current node with enabled frontend
    2. Check volume has `blockdev`
    3. Write `snap1_data` into volume and create snapshot `snap1`
    4. Write more random data into volume and create another anspshot
    5. Detach the volume and reattach with disabled frontend
    6. Check volume still has `blockdev` as frontend but no endpoint
    7. Revert back to `snap1`
    8. Detach and reattach the volume with enabled frontend
    9. Check volume contains data `snap1_data`
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == &#34;blockdev&#34;

    snap1_data = write_volume_random_data(volume)
    snap1 = create_snapshot(client, volume_name)

    write_volume_random_data(volume)
    create_snapshot(client, volume_name)

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    common.wait_for_volume_healthy_no_frontend(client, volume_name)

    volume = client.by_id_volume(volume_name)
    engine = get_volume_engine(volume)
    assert volume.disableFrontend is True
    assert volume.frontend == &#34;blockdev&#34;
    assert engine.endpoint == &#34;&#34;

    volume.snapshotRevert(name=snap1.name)

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == &#34;blockdev&#34;

    check_volume_data(volume, snap1_data)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest
def test_storage_class_from_backup(volume_name, pvc_name, storage_class, clients, core_api, pod_make): # NOQA
    &#34;&#34;&#34;
    Test restore backup using StorageClass

    1. Create volume and PV/PVC/POD
    2. Write `test_data` into pod
    3. Create a snapshot and back it up. Get the backup URL
    4. Create a new StorageClass `longhorn-from-backup` and set backup URL.
    5. Use `longhorn-from-backup` to create a new PVC
    6. Wait for the volume to be created and complete the restoration.
    7. Create the pod using the PVC. Verify the data
    &#34;&#34;&#34;
    VOLUME_SIZE = str(DEFAULT_VOLUME_SIZE * Gi)

    for _, client in iter(clients.items()):
        break

    set_random_backupstore(client)

    pv_name = pvc_name

    volume = create_and_check_volume(
        client,
        volume_name,
        size=VOLUME_SIZE
    )

    wait_for_volume_detached(client, volume_name)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)

    pod_manifest = pod_make()
    pod_manifest[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    pod_name = pod_manifest[&#39;metadata&#39;][&#39;name&#39;]
    create_and_wait_pod(core_api, pod_manifest)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)
    write_pod_volume_data(core_api, pod_name, test_data)

    volume_id = client.by_id_volume(volume_name)
    snapshot = volume_id.snapshotCreate()

    volume_id.snapshotBackup(name=snapshot.name)

    bv, b = find_backup(client, volume_name, snapshot.name)

    wait_for_backup_completion(client, volume_name, snapshot.name)

    backup_url = b.url

    storage_class[&#39;metadata&#39;][&#39;name&#39;] = &#34;longhorn-from-backup&#34;
    storage_class[&#39;parameters&#39;][&#39;fromBackup&#39;] = backup_url

    create_storage_class(storage_class)

    backup_pvc_name = generate_volume_name()

    backup_pvc_spec = {
        &#34;apiVersion&#34;: &#34;v1&#34;,
        &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;,
        &#34;metadata&#34;: {
                &#34;name&#34;: backup_pvc_name,
        },
        &#34;spec&#34;: {
                &#34;accessModes&#34;: [
                        &#34;ReadWriteOnce&#34;
                ],
                &#34;storageClassName&#34;: storage_class[&#39;metadata&#39;][&#39;name&#39;],
                &#34;resources&#34;: {
                        &#34;requests&#34;: {
                                &#34;storage&#34;: VOLUME_SIZE
                        }
                }
        }
    }

    volume_count = len(client.list_volume())

    core_api.create_namespaced_persistent_volume_claim(
        &#39;default&#39;,
        backup_pvc_spec
    )

    backup_volume_created = False

    for i in range(RETRY_COUNTS):
        if len(client.list_volume()) == volume_count + 1:
            backup_volume_created = True
            break
        time.sleep(RETRY_INTERVAL)

    assert backup_volume_created

    for i in range(RETRY_COUNTS):
        pvc_status = core_api.read_namespaced_persistent_volume_claim_status(
            name=backup_pvc_name,
            namespace=&#39;default&#39;
        )

        if pvc_status.status.phase == &#39;Bound&#39;:
            break
        time.sleep(RETRY_INTERVAL)

    found = False
    for i in range(RETRY_COUNTS):
        volumes = client.list_volume()
        for volume in volumes:
            if volume.kubernetesStatus.pvcName == backup_pvc_name:
                backup_volume_name = volume.name
                found = True
                break
        if found:
            break
        time.sleep(RETRY_INTERVAL)
    assert found

    wait_for_volume_restoration_completed(client, backup_volume_name)
    wait_for_volume_detached(client, backup_volume_name)

    backup_pod_manifest = pod_make(name=&#34;backup-pod&#34;)
    backup_pod_manifest[&#39;spec&#39;][&#39;volumes&#39;] = \
        [create_pvc_spec(backup_pvc_name)]
    backup_pod_name = backup_pod_manifest[&#39;metadata&#39;][&#39;name&#39;]
    create_and_wait_pod(core_api, backup_pod_manifest)

    restored_data = read_volume_data(core_api, backup_pod_name)
    assert test_data == restored_data


@pytest.mark.coretest   # NOQA
def test_expansion_basic(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test volume expansion using Longhorn API

    1. Create volume and attach to the current node
    2. Generate data `snap1_data` and write it to the volume
    3. Create snapshot `snap1`
    4. Expand the volume (volume will be detached, expanded, then attached)
    5. Verify the volume has been expanded
    6. Generate data `snap2_data` and write it to the volume
    7. Create snapshot `snap2`
    8. Gerneate data `snap3_data` and write it after the original size
    9. Create snapshot `snap3` and verify the `snap3_data` with location
    10. Detach and reattach the volume.
    11. Verify the volume is still expanded, and `snap3_data` remain valid
    12. Detach the volume.
    13. Reattach the volume in maintence mode
    14. Revert to `snap2` and detach.
    15. Attach the volume and check data `snap2_data`
    16. Generate `snap4_data` and write it after the original size
    17. Create snapshot `snap4` and verify `snap4_data`.
    18. Detach the volume and revert to `snap1`
    19. Validate `snap1_data`
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == &#34;blockdev&#34;

    snap1_data = write_volume_random_data(volume)
    snap1 = create_snapshot(client, volume_name)

    expand_attached_volume(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))

    snap2_data = write_volume_random_data(volume)
    snap2 = create_snapshot(client, volume_name)

    snap3_data = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    snap3_data = write_volume_data(volume, snap3_data)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap3_data)

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))
    check_volume_data(volume, snap3_data)
    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    engine = get_volume_engine(volume)
    assert volume.disableFrontend is True
    assert volume.frontend == &#34;blockdev&#34;
    assert engine.endpoint == &#34;&#34;
    volume.snapshotRevert(name=snap2.name)
    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_volume_data(volume, snap2_data, False)
    snap4_data = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    snap4_data = write_volume_data(volume, snap4_data)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap4_data)
    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    volume.snapshotRevert(name=snap1.name)
    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_volume_data(volume, snap1_data, False)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest   # NOQA
def test_restore_inc_with_expansion(clients, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test restore from disaster recovery volume with volume expansion

    Run test against a random backupstores

    1. Create a volume and attach to the current node
    2. Generate `data0`, write to the volume, make a backup `backup0`
    3. Create three DR(standby) volumes from the backup: `dr_volume0/1/2`
    4. Wait for all three DR volumes to finish the initial restoration
    5. Verify DR volumes&#39;s `lastBackup` is `backup0`
    6. Verify snapshot/pv/pvc/change backup target are not allowed as long
    as the DR volume exists
    7. Activate standby `dr_volume0` and attach it to check the volume data
    8. Expand the original volume. Make sure the expansion is successful.
    8. Generate `data1` and write to the original volume and create `backup1`
    9. Make sure `dr_volume1`&#39;s `lastBackup` field has been updated to
    `backup1`
    10. Activate `dr_volume1` and check data `data0` and `data1`
    11. Generate `data2` and write to the original volume after original SIZE
    12. Create `backup2`
    13. Wait for `dr_volume2` to finish expansion, show `backup2` as latest
    14. Activate `dr_volume2` and verify `data2`
    15. Detach `dr_volume2`
    16. Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good

    FIXME: Step 16 works because the disk will be treated as a unformatted disk
    &#34;&#34;&#34;
    client = get_random_client(clients)
    lht_host_id = get_self_host_id()

    set_random_backupstore(client)

    std_volume = create_and_check_volume(client, volume_name, 2, SIZE)
    std_volume.attach(hostId=lht_host_id)
    std_volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        std_volume.activate(frontend=&#34;blockdev&#34;)
        assert &#34;already in active mode&#34; in str(e.value)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    dr_volume0_name = &#34;dr-expand-0-&#34; + volume_name
    dr_volume1_name = &#34;dr-expand-1-&#34; + volume_name
    dr_volume2_name = &#34;dr-expand-2-&#34; + volume_name
    client.create_volume(name=dr_volume0_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=dr_volume1_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=dr_volume2_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    common.wait_for_volume_restoration_completed(client, dr_volume0_name)
    common.wait_for_volume_restoration_completed(client, dr_volume1_name)
    common.wait_for_volume_restoration_completed(client, dr_volume2_name)

    dr_volume0 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume0_name)
    dr_volume1 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume1_name)
    dr_volume2 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume2_name)

    for i in range(RETRY_COUNTS):
        dr_volume0 = client.by_id_volume(dr_volume0_name)
        dr_volume1 = client.by_id_volume(dr_volume1_name)
        dr_volume2 = client.by_id_volume(dr_volume2_name)
        get_volume_engine(dr_volume0)
        get_volume_engine(dr_volume1)
        get_volume_engine(dr_volume2)
        if dr_volume0.initialRestorationRequired is True or \
                dr_volume1.initialRestorationRequired is True or \
                dr_volume2.initialRestorationRequired is True:
            time.sleep(RETRY_INTERVAL)
        else:
            break
    assert dr_volume0.standby is True
    assert dr_volume0.lastBackup == backup0.name
    assert dr_volume0.frontend == &#34;&#34;
    assert dr_volume0.initialRestorationRequired is False
    dr_engine0 = get_volume_engine(dr_volume0)
    assert dr_engine0.lastRestoredBackup == backup0.name
    assert dr_engine0.requestedBackupRestore == backup0.name
    assert dr_volume1.standby is True
    assert dr_volume1.lastBackup == backup0.name
    assert dr_volume1.frontend == &#34;&#34;
    assert dr_volume1.initialRestorationRequired is False
    dr_engine1 = get_volume_engine(dr_volume1)
    assert dr_engine1.lastRestoredBackup == backup0.name
    assert dr_engine1.requestedBackupRestore == backup0.name
    assert dr_volume2.standby is True
    assert dr_volume2.lastBackup == backup0.name
    assert dr_volume2.frontend == &#34;&#34;
    assert dr_volume2.initialRestorationRequired is False
    dr_engine2 = get_volume_engine(dr_volume2)
    assert dr_engine2.lastRestoredBackup == backup0.name
    assert dr_engine2.requestedBackupRestore == backup0.name

    dr0_snaps = dr_volume0.snapshotList()
    assert len(dr0_snaps) == 2

    activate_standby_volume(client, dr_volume0_name)
    dr_volume0 = client.by_id_volume(dr_volume0_name)
    dr_volume0.attach(hostId=lht_host_id)
    dr_volume0 = common.wait_for_volume_healthy(client, dr_volume0_name)
    check_volume_data(dr_volume0, data0, False)

    expand_attached_volume(client, volume_name)
    std_volume = client.by_id_volume(volume_name)
    check_block_device_size(std_volume, int(EXPAND_SIZE))

    data1 = {&#39;pos&#39;: VOLUME_RWTEST_SIZE, &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup1, _, data1 = create_backup(
        client, volume_name, data1)

    client.list_backupVolume()
    check_volume_last_backup(client, dr_volume1_name, backup1.name)
    activate_standby_volume(client, dr_volume1_name)
    dr_volume1 = client.by_id_volume(dr_volume1_name)
    dr_volume1.attach(hostId=lht_host_id)
    dr_volume1 = common.wait_for_volume_healthy(client, dr_volume1_name)
    check_volume_data(dr_volume1, data0, False)
    check_volume_data(dr_volume1, data1, False)

    data2 = {&#39;pos&#39;: int(SIZE), &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup2, _, data2 = create_backup(
        client, volume_name, data2)
    assert backup2.volumeSize == EXPAND_SIZE

    client.list_backupVolume()
    wait_for_dr_volume_expansion(client, dr_volume2_name, EXPAND_SIZE)
    check_volume_last_backup(client, dr_volume2_name, backup2.name)
    activate_standby_volume(client, dr_volume2_name)
    dr_volume2 = client.by_id_volume(dr_volume2_name)
    dr_volume2.attach(hostId=lht_host_id)
    dr_volume2 = common.wait_for_volume_healthy(client, dr_volume2_name)
    check_volume_data(dr_volume2, data2)

    # allocated this active volume to a pod
    dr_volume2.detach()
    dr_volume2 = common.wait_for_volume_detached(client, dr_volume2_name)

    create_pv_for_volume(client, core_api, dr_volume2, dr_volume2_name)
    create_pvc_for_volume(client, core_api, dr_volume2, dr_volume2_name)

    dr_volume2_pod_name = &#34;pod-&#34; + dr_volume2_name
    pod[&#39;metadata&#39;][&#39;name&#39;] = dr_volume2_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: dr_volume2_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    dr_volume2 = client.by_id_volume(dr_volume2_name)
    k_status = dr_volume2.kubernetesStatus
    workloads = k_status.workloadsStatus
    assert k_status.pvName == dr_volume2_name
    assert k_status.pvStatus == &#39;Bound&#39;
    assert len(workloads) == 1
    for i in range(RETRY_COUNTS):
        if workloads[0].podStatus == &#39;Running&#39;:
            break
        time.sleep(RETRY_INTERVAL)
        dr_volume2 = client.by_id_volume(dr_volume2_name)
        k_status = dr_volume2.kubernetesStatus
        workloads = k_status.workloadsStatus
        assert len(workloads) == 1
    assert workloads[0].podName == dr_volume2_pod_name
    assert workloads[0].podStatus == &#39;Running&#39;
    assert not workloads[0].workloadName
    assert not workloads[0].workloadType
    assert k_status.namespace == &#39;default&#39;
    assert k_status.pvcName == dr_volume2_name
    assert not k_status.lastPVCRefAt
    assert not k_status.lastPodRefAt

    delete_and_wait_pod(core_api, dr_volume2_pod_name)
    delete_and_wait_pvc(core_api, dr_volume2_name)
    delete_and_wait_pv(core_api, dr_volume2_name)

    # cleanup
    std_volume.detach()
    dr_volume0.detach()
    dr_volume1.detach()
    std_volume = common.wait_for_volume_detached(client, volume_name)
    dr_volume0 = common.wait_for_volume_detached(client, dr_volume0_name)
    dr_volume1 = common.wait_for_volume_detached(client, dr_volume1_name)
    dr_volume2 = common.wait_for_volume_detached(client, dr_volume2_name)

    bv.backupDelete(name=backup2.name)
    bv.backupDelete(name=backup1.name)
    bv.backupDelete(name=backup0.name)

    client.delete(std_volume)
    client.delete(dr_volume0)
    client.delete(dr_volume1)
    client.delete(dr_volume2)

    wait_for_volume_delete(client, volume_name)
    wait_for_volume_delete(client, dr_volume0_name)
    wait_for_volume_delete(client, dr_volume1_name)
    wait_for_volume_delete(client, dr_volume2_name)

    volumes = client.list_volume().data
    assert len(volumes) == 0


def test_engine_image_daemonset_restart(clients, apps_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test restarting engine image daemonset

    1. Get the default engine image
    2. Create a volume and attach to the current node
    3. Write random data to the volume and create a snapshot
    4. Delete the engine image daemonset
    5. Engine image daemonset should be recreated
    6. In the meantime, validate the volume data to prove it&#39;s still functional
    7. Wait for the engine image to become `ready` again
    8. Check the volume data again.
    9. Write some data and create a new snapshot.
        1. Since create snapshot will use engine image binary.
    10. Check the volume data again
    &#34;&#34;&#34;
    client = get_random_client(clients)
    default_img = common.get_default_engine_image(client)
    ds_name = &#34;engine-image-&#34; + default_img.name

    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    volume = common.wait_for_volume_healthy(client, volume_name)
    snap1_data = write_volume_random_data(volume)
    create_snapshot(client, volume_name)

    # The engine image DaemonSet will be recreated/restarted automatically
    apps_api.delete_namespaced_daemon_set(ds_name, common.LONGHORN_NAMESPACE)

    # The Longhorn volume is still available
    # during the engine image DaemonSet restarting
    check_volume_data(volume, snap1_data)

    # Wait for the restart complete
    common.wait_for_engine_image_state(client, default_img.name, &#34;ready&#34;)

    # Longhorn is still able to use the corresponding engine binary to
    # operate snapshot
    check_volume_data(volume, snap1_data)
    snap2_data = write_volume_random_data(volume)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap2_data)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_basic.backup_labels_test"><code class="name flex">
<span>def <span class="ident">backup_labels_test</span></span>(<span>clients, random_labels, volume_name, size='16777216', base_image='')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup_labels_test(clients, random_labels, volume_name, size=SIZE, base_image=&#34;&#34;):  # NOQA
    for _, client in iter(clients.items()):
        break
    host_id = get_self_host_id()

    volume = create_and_check_volume(client, volume_name, 2, size, base_image)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    # test backupTarget for multiple settings
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;

        bv, b, _, _ = create_backup(client, volume_name, labels=random_labels)
        # If we&#39;re running the test with a BaseImage, check that this Label is
        # set properly.
        backup = bv.backupGet(name=b.name)
        if base_image:
            assert backup.labels.get(common.BASE_IMAGE_LABEL) == base_image
            # One extra Label from the BaseImage being set.
            assert len(backup.labels) == len(random_labels) + 1
        else:
            assert len(backup.labels) == len(random_labels)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.backup_test"><code class="name flex">
<span>def <span class="ident">backup_test</span></span>(<span>clients, volume_name, size, base_image='')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup_test(clients, volume_name, size, base_image=&#34;&#34;):  # NOQA
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name, 2, size, base_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    # test backupTarget for multiple settings
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                    common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                    common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;

        backupstore_test(client, lht_hostId, volume_name, size)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.backupstore_test"><code class="name flex">
<span>def <span class="ident">backupstore_test</span></span>(<span>client, host_id, volname, size)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backupstore_test(client, host_id, volname, size):
    bv, b, snap2, data = create_backup(client, volname)

    # test restore
    restoreName = generate_volume_name()
    volume = client.create_volume(name=restoreName, size=size,
                                  numberOfReplicas=2,
                                  fromBackup=b.url)

    volume = common.wait_for_volume_restoration_completed(client, restoreName)
    volume = common.wait_for_volume_detached(client, restoreName)
    assert volume.name == restoreName
    assert volume.size == size
    assert volume.numberOfReplicas == 2
    assert volume.state == &#34;detached&#34;
    assert volume.initialRestorationRequired is False

    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, restoreName)
    check_volume_data(volume, data)
    volume = volume.detach()
    volume = common.wait_for_volume_detached(client, restoreName)

    delete_backup(bv, b.name)

    volume = wait_for_volume_status(client, volume.name,
                                    &#34;lastBackup&#34;, &#34;&#34;)
    assert volume.lastBackupAt == &#34;&#34;

    client.delete(volume)

    volume = wait_for_volume_delete(client, restoreName)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.restore_inc_test"><code class="name flex">
<span>def <span class="ident">restore_inc_test</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restore_inc_test(client, core_api, volume_name, pod):  # NOQA
    std_volume = create_and_check_volume(client, volume_name, 2, SIZE)
    lht_host_id = get_self_host_id()
    std_volume.attach(hostId=lht_host_id)
    std_volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        std_volume.activate(frontend=&#34;blockdev&#34;)
        assert &#34;already in active mode&#34; in str(e.value)

    data0 = {&#39;len&#39;: 4 * 1024, &#39;pos&#39;: 0}
    data0[&#39;content&#39;] = common.generate_random_data(data0[&#39;len&#39;])
    bv, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    sb_volume0_name = &#34;sb-0-&#34; + volume_name
    sb_volume1_name = &#34;sb-1-&#34; + volume_name
    sb_volume2_name = &#34;sb-2-&#34; + volume_name
    client.create_volume(name=sb_volume0_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=sb_volume1_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=sb_volume2_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    common.wait_for_volume_restoration_completed(client, sb_volume0_name)
    common.wait_for_volume_restoration_completed(client, sb_volume1_name)
    common.wait_for_volume_restoration_completed(client, sb_volume2_name)

    sb_volume0 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume0_name)
    sb_volume1 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume1_name)
    sb_volume2 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume2_name)

    for i in range(RETRY_COUNTS):
        sb_volume0 = client.by_id_volume(sb_volume0_name)
        sb_volume1 = client.by_id_volume(sb_volume1_name)
        sb_volume2 = client.by_id_volume(sb_volume2_name)
        sb_engine0 = get_volume_engine(sb_volume0)
        sb_engine1 = get_volume_engine(sb_volume1)
        sb_engine2 = get_volume_engine(sb_volume2)
        if sb_volume0.initialRestorationRequired is True or \
           sb_volume1.initialRestorationRequired is True or \
           sb_volume2.initialRestorationRequired is True:
            time.sleep(RETRY_INTERVAL)
        else:
            break
    assert sb_volume0.standby is True
    assert sb_volume0.lastBackup == backup0.name
    assert sb_volume0.frontend == &#34;&#34;
    assert sb_volume0.initialRestorationRequired is False
    sb_engine0 = get_volume_engine(sb_volume0)
    assert sb_engine0.lastRestoredBackup == backup0.name
    assert sb_engine0.requestedBackupRestore == backup0.name
    assert sb_volume1.standby is True
    assert sb_volume1.lastBackup == backup0.name
    assert sb_volume1.frontend == &#34;&#34;
    assert sb_volume1.initialRestorationRequired is False
    sb_engine1 = get_volume_engine(sb_volume1)
    assert sb_engine1.lastRestoredBackup == backup0.name
    assert sb_engine1.requestedBackupRestore == backup0.name
    assert sb_volume2.standby is True
    assert sb_volume2.lastBackup == backup0.name
    assert sb_volume2.frontend == &#34;&#34;
    assert sb_volume2.initialRestorationRequired is False
    sb_engine2 = get_volume_engine(sb_volume2)
    assert sb_engine2.lastRestoredBackup == backup0.name
    assert sb_engine2.requestedBackupRestore == backup0.name

    sb0_snaps = sb_volume0.snapshotList()
    assert len(sb0_snaps) == 2
    for s in sb0_snaps:
        if s.name != &#34;volume-head&#34;:
            sb0_snap = s
    assert sb0_snaps
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotCreate()
        assert &#34;cannot create snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotRevert(name=sb0_snap.name)
        assert &#34;cannot revert snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotDelete(name=sb0_snap.name)
        assert &#34;cannot delete snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotBackup(name=sb0_snap.name)
        assert &#34;cannot create backup for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.pvCreate(pvName=sb_volume0_name)
        assert &#34;cannot create PV for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.pvcCreate(pvcName=sb_volume0_name)
        assert &#34;cannot create PVC for standby volume&#34; in str(e.value)
    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    with pytest.raises(Exception) as e:
        client.update(setting, value=&#34;random.backup.target&#34;)
        assert &#34;cannot modify BackupTarget &#34; \
               &#34;since there are existing standby volumes&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.activate(frontend=&#34;wrong_frontend&#34;)
        assert &#34;invalid frontend&#34; in str(e.value)

    activate_standby_volume(client, sb_volume0_name)
    sb_volume0 = client.by_id_volume(sb_volume0_name)
    sb_volume0.attach(hostId=lht_host_id)
    sb_volume0 = common.wait_for_volume_healthy(client, sb_volume0_name)
    check_volume_data(sb_volume0, data0, False)

    zero_string = b&#39;\x00&#39;.decode(&#39;utf-8&#39;)
    _, backup1, _, data1 = create_backup(
        client, volume_name,
        {&#39;len&#39;: 2 * 1024, &#39;pos&#39;: 0, &#39;content&#39;: zero_string * 2 * 1024})
    # use this api to update field `last backup`
    client.list_backupVolume()
    check_volume_last_backup(client, sb_volume1_name, backup1.name)
    activate_standby_volume(client, sb_volume1_name)
    sb_volume1 = client.by_id_volume(sb_volume1_name)
    sb_volume1.attach(hostId=lht_host_id)
    sb_volume1 = common.wait_for_volume_healthy(client, sb_volume1_name)
    data0_modified = {
        &#39;len&#39;: data0[&#39;len&#39;] - data1[&#39;len&#39;],
        &#39;pos&#39;: data1[&#39;len&#39;],
        &#39;content&#39;: data0[&#39;content&#39;][data1[&#39;len&#39;]:],
    }
    check_volume_data(sb_volume1, data0_modified, False)
    check_volume_data(sb_volume1, data1)

    data2 = {&#39;len&#39;: 1 * 1024 * 1024, &#39;pos&#39;: 0}
    data2[&#39;content&#39;] = common.generate_random_data(data2[&#39;len&#39;])
    _, backup2, _, data2 = create_backup(client, volume_name, data2)
    client.list_backupVolume()
    check_volume_last_backup(client, sb_volume2_name, backup2.name)
    activate_standby_volume(client, sb_volume2_name)
    sb_volume2 = client.by_id_volume(sb_volume2_name)
    sb_volume2.attach(hostId=lht_host_id)
    sb_volume2 = common.wait_for_volume_healthy(client, sb_volume2_name)
    check_volume_data(sb_volume2, data2)

    # allocated this active volume to a pod
    sb_volume2.detach()
    sb_volume2 = common.wait_for_volume_detached(client, sb_volume2_name)

    create_pv_for_volume(client, core_api, sb_volume2, sb_volume2_name)
    create_pvc_for_volume(client, core_api, sb_volume2, sb_volume2_name)

    sb_volume2_pod_name = &#34;pod-&#34; + sb_volume2_name
    pod[&#39;metadata&#39;][&#39;name&#39;] = sb_volume2_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: sb_volume2_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    sb_volume2 = client.by_id_volume(sb_volume2_name)
    k_status = sb_volume2.kubernetesStatus
    workloads = k_status.workloadsStatus
    assert k_status.pvName == sb_volume2_name
    assert k_status.pvStatus == &#39;Bound&#39;
    assert len(workloads) == 1
    for i in range(RETRY_COUNTS):
        if workloads[0].podStatus == &#39;Running&#39;:
            break
        time.sleep(RETRY_INTERVAL)
        sb_volume2 = client.by_id_volume(sb_volume2_name)
        k_status = sb_volume2.kubernetesStatus
        workloads = k_status.workloadsStatus
        assert len(workloads) == 1
    assert workloads[0].podName == sb_volume2_pod_name
    assert workloads[0].podStatus == &#39;Running&#39;
    assert not workloads[0].workloadName
    assert not workloads[0].workloadType
    assert k_status.namespace == &#39;default&#39;
    assert k_status.pvcName == sb_volume2_name
    assert not k_status.lastPVCRefAt
    assert not k_status.lastPodRefAt

    delete_and_wait_pod(core_api, sb_volume2_pod_name)
    delete_and_wait_pvc(core_api, sb_volume2_name)
    delete_and_wait_pv(core_api, sb_volume2_name)

    # cleanup
    std_volume.detach()
    sb_volume0.detach()
    sb_volume1.detach()
    std_volume = common.wait_for_volume_detached(client, volume_name)
    sb_volume0 = common.wait_for_volume_detached(client, sb_volume0_name)
    sb_volume1 = common.wait_for_volume_detached(client, sb_volume1_name)
    sb_volume2 = common.wait_for_volume_detached(client, sb_volume2_name)

    bv.backupDelete(name=backup2.name)
    bv.backupDelete(name=backup1.name)
    bv.backupDelete(name=backup0.name)

    client.delete(std_volume)
    client.delete(sb_volume0)
    client.delete(sb_volume1)
    client.delete(sb_volume2)

    wait_for_volume_delete(client, volume_name)
    wait_for_volume_delete(client, sb_volume0_name)
    wait_for_volume_delete(client, sb_volume1_name)
    wait_for_volume_delete(client, sb_volume2_name)

    volumes = client.list_volume()
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_basic.snapshot_test"><code class="name flex">
<span>def <span class="ident">snapshot_test</span></span>(<span>clients, volume_name, base_image)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def snapshot_test(clients, volume_name, base_image):  # NOQA
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name,
                                     base_image=base_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    positions = {}

    snap1 = create_snapshot(client, volume_name)

    snap2_data = write_volume_random_data(volume, positions)
    snap2 = create_snapshot(client, volume_name)

    snap3_data = write_volume_random_data(volume, positions)
    snap3 = create_snapshot(client, volume_name)

    snapshots = volume.snapshotList()
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert snapMap[snap3.name].removed is False

    volume.snapshotDelete(name=snap3.name)
    check_volume_data(volume, snap3_data)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert len(snapMap[snap3.name].children) == 1
    assert &#34;volume-head&#34; in snapMap[snap3.name].children.keys()
    assert snapMap[snap3.name].removed is True

    snap = volume.snapshotGet(name=snap3.name)
    assert snap.name == snap3.name
    assert snap.parent == snap3.parent
    assert len(snap3.children) == 1
    assert len(snap.children) == 1
    assert &#34;volume-head&#34; in snap3.children.keys()
    assert &#34;volume-head&#34; in snap.children.keys()
    assert snap.removed is True

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    common.wait_for_volume_healthy_no_frontend(client, volume_name)

    volume = client.by_id_volume(volume_name)
    engine = get_volume_engine(volume)
    assert volume.disableFrontend is True
    assert volume.frontend == &#34;blockdev&#34;
    assert engine.endpoint == &#34;&#34;

    volume.snapshotRevert(name=snap2.name)

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == &#34;blockdev&#34;

    check_volume_data(volume, snap2_data)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert &#34;volume-head&#34; in snapMap[snap2.name].children.keys()
    assert snap3.name in snapMap[snap2.name].children.keys()
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert len(snapMap[snap3.name].children) == 0
    assert snapMap[snap3.name].removed is True

    volume.snapshotDelete(name=snap1.name)
    volume.snapshotDelete(name=snap2.name)

    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap1.name,
                                     snap3.name)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap
    assert snap1.name not in snapMap
    assert snap3.name not in snapMap

    # it&#39;s the parent of volume-head, so it cannot be purged at this time
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == &#34;&#34;
    assert &#34;volume-head&#34; in snapMap[snap2.name].children.keys()
    assert snapMap[snap2.name].removed is True
    check_volume_data(volume, snap2_data)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_attach_without_frontend"><code class="name flex">
<span>def <span class="ident">test_attach_without_frontend</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test attach in maintenance mode (without frontend)</p>
<ol>
<li>Create a volume and attach to the current node with enabled frontend</li>
<li>Check volume has <code>blockdev</code></li>
<li>Write <code>snap1_data</code> into volume and create snapshot <code>snap1</code></li>
<li>Write more random data into volume and create another anspshot</li>
<li>Detach the volume and reattach with disabled frontend</li>
<li>Check volume still has <code>blockdev</code> as frontend but no endpoint</li>
<li>Revert back to <code>snap1</code></li>
<li>Detach and reattach the volume with enabled frontend</li>
<li>Check volume contains data <code>snap1_data</code></li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_attach_without_frontend(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test attach in maintenance mode (without frontend)

    1. Create a volume and attach to the current node with enabled frontend
    2. Check volume has `blockdev`
    3. Write `snap1_data` into volume and create snapshot `snap1`
    4. Write more random data into volume and create another anspshot
    5. Detach the volume and reattach with disabled frontend
    6. Check volume still has `blockdev` as frontend but no endpoint
    7. Revert back to `snap1`
    8. Detach and reattach the volume with enabled frontend
    9. Check volume contains data `snap1_data`
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == &#34;blockdev&#34;

    snap1_data = write_volume_random_data(volume)
    snap1 = create_snapshot(client, volume_name)

    write_volume_random_data(volume)
    create_snapshot(client, volume_name)

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    common.wait_for_volume_healthy_no_frontend(client, volume_name)

    volume = client.by_id_volume(volume_name)
    engine = get_volume_engine(volume)
    assert volume.disableFrontend is True
    assert volume.frontend == &#34;blockdev&#34;
    assert engine.endpoint == &#34;&#34;

    volume.snapshotRevert(name=snap1.name)

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == &#34;blockdev&#34;

    check_volume_data(volume, snap1_data)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup"><code class="name flex">
<span>def <span class="ident">test_backup</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test basic backup</p>
<p>Setup:</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Run the test for all the available backupstores.</li>
</ol>
<p>Steps:</p>
<ol>
<li>Create a backup of volume</li>
<li>Restore the backup to a new volume</li>
<li>Attach the new volume and make sure the data is the same as the old one</li>
<li>Detach the volume and delete the backup.</li>
<li>Wait for the restored volume's <code>lastBackup</code> to be cleaned (due to remove
the backup)</li>
<li>Delete the volume</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_backup(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic backup

    Setup:

    1. Create a volume and attach to the current node
    2. Run the test for all the available backupstores.

    Steps:

    1. Create a backup of volume
    2. Restore the backup to a new volume
    3. Attach the new volume and make sure the data is the same as the old one
    4. Detach the volume and delete the backup.
    5. Wait for the restored volume&#39;s `lastBackup` to be cleaned (due to remove
    the backup)
    6. Delete the volume
    &#34;&#34;&#34;
    backup_test(clients, volume_name, SIZE)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_labels"><code class="name flex">
<span>def <span class="ident">test_backup_labels</span></span>(<span>clients, random_labels, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test that the proper Labels are applied when creating a Backup manually.</p>
<ol>
<li>Create a volume</li>
<li>Run the following steps on all backupstores</li>
<li>Create a backup with some random labels</li>
<li>Get backup from backupstore, verify the labels are set on the backups</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest
def test_backup_labels(clients, random_labels, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the proper Labels are applied when creating a Backup manually.

    1. Create a volume
    2. Run the following steps on all backupstores
    3. Create a backup with some random labels
    4. Get backup from backupstore, verify the labels are set on the backups
    &#34;&#34;&#34;
    backup_labels_test(clients, random_labels, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_deleting_backup_volume"><code class="name flex">
<span>def <span class="ident">test_deleting_backup_volume</span></span>(<span>clients)</span>
</code></dt>
<dd>
<section class="desc"><p>Test deleting backup volumes</p>
<ol>
<li>Create volume and create backup</li>
<li>Delete the backup and make sure it's gone in the backupstore</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_deleting_backup_volume(clients):  # NOQA
    &#34;&#34;&#34;
    Test deleting backup volumes

    1. Create volume and create backup
    2. Delete the backup and make sure it&#39;s gone in the backupstore
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break
    lht_hostId = get_self_host_id()

    volName = generate_volume_name()
    volume = create_and_check_volume(client, volName)

    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volName)

    bv, _, snap1, _ = create_backup(client, volName)
    _, _, snap2, _ = create_backup(client, volName)

    bv = client.by_id_backupVolume(volName)
    client.delete(bv)
    common.wait_for_backup_volume_delete(client, volName)
    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_engine_image_daemonset_restart"><code class="name flex">
<span>def <span class="ident">test_engine_image_daemonset_restart</span></span>(<span>clients, apps_api, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test restarting engine image daemonset</p>
<ol>
<li>Get the default engine image</li>
<li>Create a volume and attach to the current node</li>
<li>Write random data to the volume and create a snapshot</li>
<li>Delete the engine image daemonset</li>
<li>Engine image daemonset should be recreated</li>
<li>In the meantime, validate the volume data to prove it's still functional</li>
<li>Wait for the engine image to become <code>ready</code> again</li>
<li>Check the volume data again.</li>
<li>Write some data and create a new snapshot.<ol>
<li>Since create snapshot will use engine image binary.</li>
</ol>
</li>
<li>Check the volume data again</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_engine_image_daemonset_restart(clients, apps_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test restarting engine image daemonset

    1. Get the default engine image
    2. Create a volume and attach to the current node
    3. Write random data to the volume and create a snapshot
    4. Delete the engine image daemonset
    5. Engine image daemonset should be recreated
    6. In the meantime, validate the volume data to prove it&#39;s still functional
    7. Wait for the engine image to become `ready` again
    8. Check the volume data again.
    9. Write some data and create a new snapshot.
        1. Since create snapshot will use engine image binary.
    10. Check the volume data again
    &#34;&#34;&#34;
    client = get_random_client(clients)
    default_img = common.get_default_engine_image(client)
    ds_name = &#34;engine-image-&#34; + default_img.name

    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    volume = common.wait_for_volume_healthy(client, volume_name)
    snap1_data = write_volume_random_data(volume)
    create_snapshot(client, volume_name)

    # The engine image DaemonSet will be recreated/restarted automatically
    apps_api.delete_namespaced_daemon_set(ds_name, common.LONGHORN_NAMESPACE)

    # The Longhorn volume is still available
    # during the engine image DaemonSet restarting
    check_volume_data(volume, snap1_data)

    # Wait for the restart complete
    common.wait_for_engine_image_state(client, default_img.name, &#34;ready&#34;)

    # Longhorn is still able to use the corresponding engine binary to
    # operate snapshot
    check_volume_data(volume, snap1_data)
    snap2_data = write_volume_random_data(volume)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap2_data)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_expansion_basic"><code class="name flex">
<span>def <span class="ident">test_expansion_basic</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test volume expansion using Longhorn API</p>
<ol>
<li>Create volume and attach to the current node</li>
<li>Generate data <code>snap1_data</code> and write it to the volume</li>
<li>Create snapshot <code>snap1</code></li>
<li>Expand the volume (volume will be detached, expanded, then attached)</li>
<li>Verify the volume has been expanded</li>
<li>Generate data <code>snap2_data</code> and write it to the volume</li>
<li>Create snapshot <code>snap2</code></li>
<li>Gerneate data <code>snap3_data</code> and write it after the original size</li>
<li>Create snapshot <code>snap3</code> and verify the <code>snap3_data</code> with location</li>
<li>Detach and reattach the volume.</li>
<li>Verify the volume is still expanded, and <code>snap3_data</code> remain valid</li>
<li>Detach the volume.</li>
<li>Reattach the volume in maintence mode</li>
<li>Revert to <code>snap2</code> and detach.</li>
<li>Attach the volume and check data <code>snap2_data</code></li>
<li>Generate <code>snap4_data</code> and write it after the original size</li>
<li>Create snapshot <code>snap4</code> and verify <code>snap4_data</code>.</li>
<li>Detach the volume and revert to <code>snap1</code></li>
<li>Validate <code>snap1_data</code></li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_expansion_basic(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test volume expansion using Longhorn API

    1. Create volume and attach to the current node
    2. Generate data `snap1_data` and write it to the volume
    3. Create snapshot `snap1`
    4. Expand the volume (volume will be detached, expanded, then attached)
    5. Verify the volume has been expanded
    6. Generate data `snap2_data` and write it to the volume
    7. Create snapshot `snap2`
    8. Gerneate data `snap3_data` and write it after the original size
    9. Create snapshot `snap3` and verify the `snap3_data` with location
    10. Detach and reattach the volume.
    11. Verify the volume is still expanded, and `snap3_data` remain valid
    12. Detach the volume.
    13. Reattach the volume in maintence mode
    14. Revert to `snap2` and detach.
    15. Attach the volume and check data `snap2_data`
    16. Generate `snap4_data` and write it after the original size
    17. Create snapshot `snap4` and verify `snap4_data`.
    18. Detach the volume and revert to `snap1`
    19. Validate `snap1_data`
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == &#34;blockdev&#34;

    snap1_data = write_volume_random_data(volume)
    snap1 = create_snapshot(client, volume_name)

    expand_attached_volume(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))

    snap2_data = write_volume_random_data(volume)
    snap2 = create_snapshot(client, volume_name)

    snap3_data = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    snap3_data = write_volume_data(volume, snap3_data)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap3_data)

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))
    check_volume_data(volume, snap3_data)
    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    engine = get_volume_engine(volume)
    assert volume.disableFrontend is True
    assert volume.frontend == &#34;blockdev&#34;
    assert engine.endpoint == &#34;&#34;
    volume.snapshotRevert(name=snap2.name)
    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_volume_data(volume, snap2_data, False)
    snap4_data = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    snap4_data = write_volume_data(volume, snap4_data)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap4_data)
    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    volume.snapshotRevert(name=snap1.name)
    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_volume_data(volume, snap1_data, False)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_hosts"><code class="name flex">
<span>def <span class="ident">test_hosts</span></span>(<span>clients)</span>
</code></dt>
<dd>
<section class="desc"><p>Check node name and IP</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_hosts(clients):  # NOQA
    &#34;&#34;&#34;
    Check node name and IP
    &#34;&#34;&#34;
    hosts = next(iter(clients.values())).list_node()
    for host in hosts:
        assert host.name is not None
        assert host.address is not None

    host_id = []
    for i in range(0, len(hosts)):
        host_id.append(hosts.data[i].name)

    host0_from_i = {}
    for i in range(0, len(hosts)):
        if len(host0_from_i) == 0:
            host0_from_i = clients[host_id[0]].by_id_node(host_id[0])
        else:
            assert host0_from_i.name == \
                clients[host_id[i]].by_id_node(host_id[0]).name
            assert host0_from_i.address == \
                clients[host_id[i]].by_id_node(host_id[0]).address</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_listing_backup_volume"><code class="name flex">
<span>def <span class="ident">test_listing_backup_volume</span></span>(<span>clients, base_image='')</span>
</code></dt>
<dd>
<section class="desc"><p>Test listing backup volumes</p>
<ol>
<li>Create three volumes: <code>volume1/2/3</code></li>
<li>Setup NFS backupstore since we can manipulate the content easily</li>
<li>Create snapshots for all three volumes</li>
<li>Rename <code>volume1</code>'s <code>volume.cfg</code> to <code>volume.cfg.tmp</code> in backupstore</li>
<li>List backup volumes. Make sure <code>volume1</code> errors out but found other two</li>
<li>Restore <code>volume1</code>'s <code>volume.cfg</code>.</li>
<li>Make sure now backup volume <code>volume1</code> can be found and deleted</li>
<li>Delete backups for <code>volume2/3</code>, make sure they cannot be found later</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_listing_backup_volume(clients, base_image=&#34;&#34;):   # NOQA
    &#34;&#34;&#34;
    Test listing backup volumes

    1. Create three volumes: `volume1/2/3`
    2. Setup NFS backupstore since we can manipulate the content easily
    3. Create snapshots for all three volumes
    4. Rename `volume1`&#39;s `volume.cfg` to `volume.cfg.tmp` in backupstore
    5. List backup volumes. Make sure `volume1` errors out but found other two
    6. Restore `volume1`&#39;s `volume.cfg`.
    7. Make sure now backup volume `volume1` can be found and deleted
    8. Delete backups for `volume2/3`, make sure they cannot be found later
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break
    lht_hostId = get_self_host_id()

    # create 3 volumes.
    volume1_name = generate_volume_name()
    volume2_name = generate_volume_name()
    volume3_name = generate_volume_name()

    volume1 = create_and_check_volume(client, volume1_name)
    volume2 = create_and_check_volume(client, volume2_name)
    volume3 = create_and_check_volume(client, volume3_name)

    volume1.attach(hostId=lht_hostId)
    volume1 = common.wait_for_volume_healthy(client, volume1_name)
    volume2.attach(hostId=lht_hostId)
    volume2 = common.wait_for_volume_healthy(client, volume2_name)
    volume3.attach(hostId=lht_hostId)
    volume3 = common.wait_for_volume_healthy(client, volume3_name)

    # we only test NFS here.
    # Since it is difficult to directly remove volume.cfg from s3 buckets
    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_nfs(backupstore):
            updated = False
            for i in range(RETRY_COMMAND_COUNT):
                nfs_url = backupstore.strip(&#34;nfs://&#34;)
                setting = client.update(setting, value=backupstore)
                assert setting.value == backupstore
                setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
                if &#34;nfs&#34; in setting.value:
                    updated = True
                    break
            assert updated

    _, _, snap1, _ = create_backup(client, volume1_name)
    _, _, snap2, _ = create_backup(client, volume2_name)
    _, _, snap3, _ = create_backup(client, volume3_name)

    # invalidate backup volume 1 by renaming volume.cfg to volume.cfg.tmp
    cmd = [&#34;mkdir&#34;, &#34;-p&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;mount&#34;, &#34;-t&#34;, &#34;nfs4&#34;, nfs_url, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;find&#34;, &#34;/mnt/nfs&#34;, &#34;-type&#34;, &#34;d&#34;, &#34;-name&#34;, volume1_name]
    volume1_backup_volume_path = \
        subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)

    cmd = [&#34;find&#34;, volume1_backup_volume_path, &#34;-name&#34;, &#34;volume.cfg&#34;]
    volume1_backup_volume_cfg_path = \
        subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)
    cmd = [&#34;mv&#34;, volume1_backup_volume_cfg_path,
           volume1_backup_volume_cfg_path + &#34;.tmp&#34;]
    subprocess.check_output(cmd)
    subprocess.check_output([&#34;sync&#34;])

    found1 = found2 = found3 = False
    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()
        for bv in bvs:
            if bv.name == volume1_name:
                if &#34;error&#34; in bv.messages:
                    assert &#34;volume.cfg&#34; in bv.messages.error.lower()
                    found1 = True
            elif bv.name == volume2_name:
                assert not bv.messages
                found2 = True
            elif bv.name == volume3_name:
                assert not bv.messages
                found3 = True
        if found1 &amp; found2 &amp; found3:
            break
        time.sleep(RETRY_INTERVAL)
    assert found1 &amp; found2 &amp; found3

    cmd = [&#34;mv&#34;, volume1_backup_volume_cfg_path + &#34;.tmp&#34;,
           volume1_backup_volume_cfg_path]
    subprocess.check_output(cmd)
    subprocess.check_output([&#34;sync&#34;])

    found = False
    for i in range(RETRY_COMMAND_COUNT):
        try:
            bv1, b1 = common.find_backup(client, volume1_name, snap1.name)
            found = True
            break
        except Exception:
            time.sleep(1)
    assert found
    bv1.backupDelete(name=b1.name)
    for i in range(RETRY_COMMAND_COUNT):
        found = False
        backups1 = bv1.backupList().data
        for b in backups1:
            if b.snapshotName == snap1.name:
                found = True
                break
    assert not found

    bv2, b2 = common.find_backup(client, volume2_name, snap2.name)
    bv2.backupDelete(name=b2.name)
    for i in range(RETRY_COMMAND_COUNT):
        found = False
        backups2 = bv2.backupList().data
        for b in backups2:
            if b.snapshotName == snap2.name:
                found = True
                break
    assert not found

    bv3, b3 = common.find_backup(client, volume3_name, snap3.name)
    bv3.backupDelete(name=b3.name)
    for i in range(RETRY_COMMAND_COUNT):
        found = False
        backups3 = bv3.backupList().data
        for b in backups3:
            if b.snapshotName == snap3.name:
                found = True
                break
    assert not found

    volume1.detach()
    volume1 = common.wait_for_volume_detached(client, volume1_name)
    client.delete(volume1)
    wait_for_volume_delete(client, volume1_name)

    volume2.detach()
    volume2 = common.wait_for_volume_detached(client, volume2_name)
    client.delete(volume2)
    wait_for_volume_delete(client, volume2_name)

    volume3.detach()
    volume3 = common.wait_for_volume_detached(client, volume3_name)
    client.delete(volume3)
    wait_for_volume_delete(client, volume3_name)

    volumes = client.list_volume()
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_restore_inc"><code class="name flex">
<span>def <span class="ident">test_restore_inc</span></span>(<span>clients, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<section class="desc"><p>Test restore from disaster recovery volume (incremental restore)</p>
<p>Run test against all the backupstores</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate <code>data0</code>, write to the volume, make a backup <code>backup0</code></li>
<li>Create three DR(standby) volumes from the backup: <code>sb_volume0/1/2</code></li>
<li>Wait for all three DR volumes to finish the initial restoration</li>
<li>Verify DR volumes's <code>lastBackup</code> is <code>backup0</code></li>
<li>Verify snapshot/pv/pvc/change backup target are not allowed as long
as the DR volume exists</li>
<li>Activate standby <code>sb_volume0</code> and attach it to check the volume data</li>
<li>Generate <code>data1</code> and write to the original volume and create <code>backup1</code></li>
<li>Make sure <code>sb_volume1</code>'s <code>lastBackup</code> field has been updated to
<code>backup1</code></li>
<li>Wait for <code>sb_volume1</code> to finish incremental restoration then activate</li>
<li>Attach and check <code>sb_volume1</code>'s data</li>
<li>Generate <code>data2</code> and write to the original volume and create <code>backup2</code></li>
<li>Make sure <code>sb_volume2</code>'s <code>lastBackup</code> field has been updated to
<code>backup1</code></li>
<li>Wait for <code>sb_volume2</code> to finish incremental restoration then activate</li>
<li>Attach and check <code>sb_volume2</code>'s data</li>
<li>Create PV, PVC and Pod to use <code>sb_volume2</code>, check PV/PVC/POD are good</li>
</ol>
<p>FIXME: Step 16 works because the disk will be treated as a unformatted disk</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_restore_inc(clients, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test restore from disaster recovery volume (incremental restore)

    Run test against all the backupstores

    1. Create a volume and attach to the current node
    2. Generate `data0`, write to the volume, make a backup `backup0`
    3. Create three DR(standby) volumes from the backup: `sb_volume0/1/2`
    4. Wait for all three DR volumes to finish the initial restoration
    5. Verify DR volumes&#39;s `lastBackup` is `backup0`
    6. Verify snapshot/pv/pvc/change backup target are not allowed as long
    as the DR volume exists
    7. Activate standby `sb_volume0` and attach it to check the volume data
    8. Generate `data1` and write to the original volume and create `backup1`
    9. Make sure `sb_volume1`&#39;s `lastBackup` field has been updated to
    `backup1`
    10. Wait for `sb_volume1` to finish incremental restoration then activate
    11. Attach and check `sb_volume1`&#39;s data
    12. Generate `data2` and write to the original volume and create `backup2`
    13. Make sure `sb_volume2`&#39;s `lastBackup` field has been updated to
    `backup1`
    14. Wait for `sb_volume2` to finish incremental restoration then activate
    15. Attach and check `sb_volume2`&#39;s data
    16. Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good

    FIXME: Step 16 works because the disk will be treated as a unformatted disk
    &#34;&#34;&#34;
    for _, client in iter(clients.items()):
        break

    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    # test backupTarget for multiple settings
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;

        restore_inc_test(client, core_api, volume_name, pod)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_restore_inc_with_expansion"><code class="name flex">
<span>def <span class="ident">test_restore_inc_with_expansion</span></span>(<span>clients, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<section class="desc"><p>Test restore from disaster recovery volume with volume expansion</p>
<p>Run test against a random backupstores</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate <code>data0</code>, write to the volume, make a backup <code>backup0</code></li>
<li>Create three DR(standby) volumes from the backup: <code>dr_volume0/1/2</code></li>
<li>Wait for all three DR volumes to finish the initial restoration</li>
<li>Verify DR volumes's <code>lastBackup</code> is <code>backup0</code></li>
<li>Verify snapshot/pv/pvc/change backup target are not allowed as long
as the DR volume exists</li>
<li>Activate standby <code>dr_volume0</code> and attach it to check the volume data</li>
<li>Expand the original volume. Make sure the expansion is successful.</li>
<li>Generate <code>data1</code> and write to the original volume and create <code>backup1</code></li>
<li>Make sure <code>dr_volume1</code>'s <code>lastBackup</code> field has been updated to
<code>backup1</code></li>
<li>Activate <code>dr_volume1</code> and check data <code>data0</code> and <code>data1</code></li>
<li>Generate <code>data2</code> and write to the original volume after original SIZE</li>
<li>Create <code>backup2</code></li>
<li>Wait for <code>dr_volume2</code> to finish expansion, show <code>backup2</code> as latest</li>
<li>Activate <code>dr_volume2</code> and verify <code>data2</code></li>
<li>Detach <code>dr_volume2</code></li>
<li>Create PV, PVC and Pod to use <code>sb_volume2</code>, check PV/PVC/POD are good</li>
</ol>
<p>FIXME: Step 16 works because the disk will be treated as a unformatted disk</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_restore_inc_with_expansion(clients, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test restore from disaster recovery volume with volume expansion

    Run test against a random backupstores

    1. Create a volume and attach to the current node
    2. Generate `data0`, write to the volume, make a backup `backup0`
    3. Create three DR(standby) volumes from the backup: `dr_volume0/1/2`
    4. Wait for all three DR volumes to finish the initial restoration
    5. Verify DR volumes&#39;s `lastBackup` is `backup0`
    6. Verify snapshot/pv/pvc/change backup target are not allowed as long
    as the DR volume exists
    7. Activate standby `dr_volume0` and attach it to check the volume data
    8. Expand the original volume. Make sure the expansion is successful.
    8. Generate `data1` and write to the original volume and create `backup1`
    9. Make sure `dr_volume1`&#39;s `lastBackup` field has been updated to
    `backup1`
    10. Activate `dr_volume1` and check data `data0` and `data1`
    11. Generate `data2` and write to the original volume after original SIZE
    12. Create `backup2`
    13. Wait for `dr_volume2` to finish expansion, show `backup2` as latest
    14. Activate `dr_volume2` and verify `data2`
    15. Detach `dr_volume2`
    16. Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good

    FIXME: Step 16 works because the disk will be treated as a unformatted disk
    &#34;&#34;&#34;
    client = get_random_client(clients)
    lht_host_id = get_self_host_id()

    set_random_backupstore(client)

    std_volume = create_and_check_volume(client, volume_name, 2, SIZE)
    std_volume.attach(hostId=lht_host_id)
    std_volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        std_volume.activate(frontend=&#34;blockdev&#34;)
        assert &#34;already in active mode&#34; in str(e.value)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    dr_volume0_name = &#34;dr-expand-0-&#34; + volume_name
    dr_volume1_name = &#34;dr-expand-1-&#34; + volume_name
    dr_volume2_name = &#34;dr-expand-2-&#34; + volume_name
    client.create_volume(name=dr_volume0_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=dr_volume1_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=dr_volume2_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    common.wait_for_volume_restoration_completed(client, dr_volume0_name)
    common.wait_for_volume_restoration_completed(client, dr_volume1_name)
    common.wait_for_volume_restoration_completed(client, dr_volume2_name)

    dr_volume0 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume0_name)
    dr_volume1 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume1_name)
    dr_volume2 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume2_name)

    for i in range(RETRY_COUNTS):
        dr_volume0 = client.by_id_volume(dr_volume0_name)
        dr_volume1 = client.by_id_volume(dr_volume1_name)
        dr_volume2 = client.by_id_volume(dr_volume2_name)
        get_volume_engine(dr_volume0)
        get_volume_engine(dr_volume1)
        get_volume_engine(dr_volume2)
        if dr_volume0.initialRestorationRequired is True or \
                dr_volume1.initialRestorationRequired is True or \
                dr_volume2.initialRestorationRequired is True:
            time.sleep(RETRY_INTERVAL)
        else:
            break
    assert dr_volume0.standby is True
    assert dr_volume0.lastBackup == backup0.name
    assert dr_volume0.frontend == &#34;&#34;
    assert dr_volume0.initialRestorationRequired is False
    dr_engine0 = get_volume_engine(dr_volume0)
    assert dr_engine0.lastRestoredBackup == backup0.name
    assert dr_engine0.requestedBackupRestore == backup0.name
    assert dr_volume1.standby is True
    assert dr_volume1.lastBackup == backup0.name
    assert dr_volume1.frontend == &#34;&#34;
    assert dr_volume1.initialRestorationRequired is False
    dr_engine1 = get_volume_engine(dr_volume1)
    assert dr_engine1.lastRestoredBackup == backup0.name
    assert dr_engine1.requestedBackupRestore == backup0.name
    assert dr_volume2.standby is True
    assert dr_volume2.lastBackup == backup0.name
    assert dr_volume2.frontend == &#34;&#34;
    assert dr_volume2.initialRestorationRequired is False
    dr_engine2 = get_volume_engine(dr_volume2)
    assert dr_engine2.lastRestoredBackup == backup0.name
    assert dr_engine2.requestedBackupRestore == backup0.name

    dr0_snaps = dr_volume0.snapshotList()
    assert len(dr0_snaps) == 2

    activate_standby_volume(client, dr_volume0_name)
    dr_volume0 = client.by_id_volume(dr_volume0_name)
    dr_volume0.attach(hostId=lht_host_id)
    dr_volume0 = common.wait_for_volume_healthy(client, dr_volume0_name)
    check_volume_data(dr_volume0, data0, False)

    expand_attached_volume(client, volume_name)
    std_volume = client.by_id_volume(volume_name)
    check_block_device_size(std_volume, int(EXPAND_SIZE))

    data1 = {&#39;pos&#39;: VOLUME_RWTEST_SIZE, &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup1, _, data1 = create_backup(
        client, volume_name, data1)

    client.list_backupVolume()
    check_volume_last_backup(client, dr_volume1_name, backup1.name)
    activate_standby_volume(client, dr_volume1_name)
    dr_volume1 = client.by_id_volume(dr_volume1_name)
    dr_volume1.attach(hostId=lht_host_id)
    dr_volume1 = common.wait_for_volume_healthy(client, dr_volume1_name)
    check_volume_data(dr_volume1, data0, False)
    check_volume_data(dr_volume1, data1, False)

    data2 = {&#39;pos&#39;: int(SIZE), &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup2, _, data2 = create_backup(
        client, volume_name, data2)
    assert backup2.volumeSize == EXPAND_SIZE

    client.list_backupVolume()
    wait_for_dr_volume_expansion(client, dr_volume2_name, EXPAND_SIZE)
    check_volume_last_backup(client, dr_volume2_name, backup2.name)
    activate_standby_volume(client, dr_volume2_name)
    dr_volume2 = client.by_id_volume(dr_volume2_name)
    dr_volume2.attach(hostId=lht_host_id)
    dr_volume2 = common.wait_for_volume_healthy(client, dr_volume2_name)
    check_volume_data(dr_volume2, data2)

    # allocated this active volume to a pod
    dr_volume2.detach()
    dr_volume2 = common.wait_for_volume_detached(client, dr_volume2_name)

    create_pv_for_volume(client, core_api, dr_volume2, dr_volume2_name)
    create_pvc_for_volume(client, core_api, dr_volume2, dr_volume2_name)

    dr_volume2_pod_name = &#34;pod-&#34; + dr_volume2_name
    pod[&#39;metadata&#39;][&#39;name&#39;] = dr_volume2_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: dr_volume2_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    dr_volume2 = client.by_id_volume(dr_volume2_name)
    k_status = dr_volume2.kubernetesStatus
    workloads = k_status.workloadsStatus
    assert k_status.pvName == dr_volume2_name
    assert k_status.pvStatus == &#39;Bound&#39;
    assert len(workloads) == 1
    for i in range(RETRY_COUNTS):
        if workloads[0].podStatus == &#39;Running&#39;:
            break
        time.sleep(RETRY_INTERVAL)
        dr_volume2 = client.by_id_volume(dr_volume2_name)
        k_status = dr_volume2.kubernetesStatus
        workloads = k_status.workloadsStatus
        assert len(workloads) == 1
    assert workloads[0].podName == dr_volume2_pod_name
    assert workloads[0].podStatus == &#39;Running&#39;
    assert not workloads[0].workloadName
    assert not workloads[0].workloadType
    assert k_status.namespace == &#39;default&#39;
    assert k_status.pvcName == dr_volume2_name
    assert not k_status.lastPVCRefAt
    assert not k_status.lastPodRefAt

    delete_and_wait_pod(core_api, dr_volume2_pod_name)
    delete_and_wait_pvc(core_api, dr_volume2_name)
    delete_and_wait_pv(core_api, dr_volume2_name)

    # cleanup
    std_volume.detach()
    dr_volume0.detach()
    dr_volume1.detach()
    std_volume = common.wait_for_volume_detached(client, volume_name)
    dr_volume0 = common.wait_for_volume_detached(client, dr_volume0_name)
    dr_volume1 = common.wait_for_volume_detached(client, dr_volume1_name)
    dr_volume2 = common.wait_for_volume_detached(client, dr_volume2_name)

    bv.backupDelete(name=backup2.name)
    bv.backupDelete(name=backup1.name)
    bv.backupDelete(name=backup0.name)

    client.delete(std_volume)
    client.delete(dr_volume0)
    client.delete(dr_volume1)
    client.delete(dr_volume2)

    wait_for_volume_delete(client, volume_name)
    wait_for_volume_delete(client, dr_volume0_name)
    wait_for_volume_delete(client, dr_volume1_name)
    wait_for_volume_delete(client, dr_volume2_name)

    volumes = client.list_volume().data
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_setting_default_replica_count"><code class="name flex">
<span>def <span class="ident">test_setting_default_replica_count</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test <code>Default Replica Count</code> setting</p>
<ol>
<li>Set default replica count in the global settings to 5</li>
<li>Create a volume without specify the replica count</li>
<li>The volume should have 5 replicas (instead of the previous default 3)</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_setting_default_replica_count(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test `Default Replica Count` setting

    1. Set default replica count in the global settings to 5
    2. Create a volume without specify the replica count
    3. The volume should have 5 replicas (instead of the previous default 3)
    &#34;&#34;&#34;
    client = get_random_client(clients)
    setting = client.by_id_setting(common.SETTING_DEFAULT_REPLICA_COUNT)
    old_value = setting.value
    setting = client.update(setting, value=&#34;5&#34;)

    volume = client.create_volume(name=volume_name, size=SIZE)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert len(volume.replicas) == int(setting.value)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    setting = client.update(setting, value=old_value)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_settings"><code class="name flex">
<span>def <span class="ident">test_settings</span></span>(<span>clients)</span>
</code></dt>
<dd>
<section class="desc"><p>Check input for settings</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_settings(clients):  # NOQA
    &#34;&#34;&#34;
    Check input for settings
    &#34;&#34;&#34;
    client = get_random_client(clients)

    setting_names = [common.SETTING_BACKUP_TARGET,
                     common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET,
                     common.SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE,
                     common.SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE,
                     common.SETTING_DEFAULT_REPLICA_COUNT]
    settings = client.list_setting()

    settingMap = {}
    for setting in settings:
        settingMap[setting.name] = setting

    for name in setting_names:
        assert settingMap[name] is not None
        assert settingMap[name].definition.description is not None

    for name in setting_names:
        setting = client.by_id_setting(name)
        assert settingMap[name].value == setting.value

        old_value = setting.value

        if name == common.SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-100&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            setting = client.update(setting, value=&#34;200&#34;)
            assert setting.value == &#34;200&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;200&#34;
        elif name == common.SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;300&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-30&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            setting = client.update(setting, value=&#34;30&#34;)
            assert setting.value == &#34;30&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;30&#34;
        elif name == common.SETTING_BACKUP_TARGET:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue$test&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            setting = client.update(setting, value=&#34;nfs://test&#34;)
            assert setting.value == &#34;nfs://test&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;nfs://test&#34;
        elif name == common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET:
            setting = client.update(setting, value=&#34;testvalue&#34;)
            assert setting.value == &#34;testvalue&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;testvalue&#34;
        elif name == common.SETTING_DEFAULT_REPLICA_COUNT:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-1&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;21&#34;)
            assert &#34;with invalid &#34;+name in \
                   str(e.value)
            setting = client.update(setting, value=&#34;2&#34;)
            assert setting.value == &#34;2&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;2&#34;

        setting = client.update(setting, value=old_value)
        assert setting.value == old_value</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_snapshot"><code class="name flex">
<span>def <span class="ident">test_snapshot</span></span>(<span>clients, volume_name, base_image='')</span>
</code></dt>
<dd>
<section class="desc"><p>Test snapshot operations</p>
<ol>
<li>Create a volume and attach to the node</li>
<li>Create the empty snapshot <code>snap1</code></li>
<li>Generate and write data <code>snap2_data</code>, then create <code>snap2</code></li>
<li>Generate and write data <code>snap3_data</code>, then create <code>snap3</code></li>
<li>List snapshot. Validate the snapshot chain relationship</li>
<li>Mark <code>snap3</code> as removed. Make sure volume's data didn't change</li>
<li>List snapshot. Make sure <code>snap3</code> is marked as removed</li>
<li>Detach and reattach the volume in maintenance mode.</li>
<li>Make sure the volume frontend is still <code>blockdev</code> but disabled</li>
<li>Revert to <code>snap2</code></li>
<li>Detach and reattach the volume with frontend enabled</li>
<li>Make sure volume's data is <code>snap2_data</code></li>
<li>List snapshot. Make sure <code>volume-head</code> is now <code>snap2</code>'s child</li>
<li>Delete <code>snap1</code> and <code>snap2</code></li>
<li>Purge the snapshot.</li>
<li>List the snapshot, make sure <code>snap1</code> and <code>snap3</code>
are gone. <code>snap2</code> is marked as removed.</li>
<li>Check volume data, make sure it's still <code>snap2_data</code>.</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_snapshot(clients, volume_name, base_image=&#34;&#34;):  # NOQA
    &#34;&#34;&#34;
    Test snapshot operations

    1. Create a volume and attach to the node
    2. Create the empty snapshot `snap1`
    3. Generate and write data `snap2_data`, then create `snap2`
    4. Generate and write data `snap3_data`, then create `snap3`
    5. List snapshot. Validate the snapshot chain relationship
    6. Mark `snap3` as removed. Make sure volume&#39;s data didn&#39;t change
    7. List snapshot. Make sure `snap3` is marked as removed
    8. Detach and reattach the volume in maintenance mode.
    9. Make sure the volume frontend is still `blockdev` but disabled
    10. Revert to `snap2`
    11. Detach and reattach the volume with frontend enabled
    12. Make sure volume&#39;s data is `snap2_data`
    13. List snapshot. Make sure `volume-head` is now `snap2`&#39;s child
    14. Delete `snap1` and `snap2`
    15. Purge the snapshot.
    16. List the snapshot, make sure `snap1` and `snap3`
    are gone. `snap2` is marked as removed.
    17. Check volume data, make sure it&#39;s still `snap2_data`.
    &#34;&#34;&#34;
    snapshot_test(clients, volume_name, base_image)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_storage_class_from_backup"><code class="name flex">
<span>def <span class="ident">test_storage_class_from_backup</span></span>(<span>volume_name, pvc_name, storage_class, clients, core_api, pod_make)</span>
</code></dt>
<dd>
<section class="desc"><p>Test restore backup using StorageClass</p>
<ol>
<li>Create volume and PV/PVC/POD</li>
<li>Write <code>test_data</code> into pod</li>
<li>Create a snapshot and back it up. Get the backup URL</li>
<li>Create a new StorageClass <code>longhorn-from-backup</code> and set backup URL.</li>
<li>Use <code>longhorn-from-backup</code> to create a new PVC</li>
<li>Wait for the volume to be created and complete the restoration.</li>
<li>Create the pod using the PVC. Verify the data</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest
def test_storage_class_from_backup(volume_name, pvc_name, storage_class, clients, core_api, pod_make): # NOQA
    &#34;&#34;&#34;
    Test restore backup using StorageClass

    1. Create volume and PV/PVC/POD
    2. Write `test_data` into pod
    3. Create a snapshot and back it up. Get the backup URL
    4. Create a new StorageClass `longhorn-from-backup` and set backup URL.
    5. Use `longhorn-from-backup` to create a new PVC
    6. Wait for the volume to be created and complete the restoration.
    7. Create the pod using the PVC. Verify the data
    &#34;&#34;&#34;
    VOLUME_SIZE = str(DEFAULT_VOLUME_SIZE * Gi)

    for _, client in iter(clients.items()):
        break

    set_random_backupstore(client)

    pv_name = pvc_name

    volume = create_and_check_volume(
        client,
        volume_name,
        size=VOLUME_SIZE
    )

    wait_for_volume_detached(client, volume_name)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)

    pod_manifest = pod_make()
    pod_manifest[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    pod_name = pod_manifest[&#39;metadata&#39;][&#39;name&#39;]
    create_and_wait_pod(core_api, pod_manifest)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)
    write_pod_volume_data(core_api, pod_name, test_data)

    volume_id = client.by_id_volume(volume_name)
    snapshot = volume_id.snapshotCreate()

    volume_id.snapshotBackup(name=snapshot.name)

    bv, b = find_backup(client, volume_name, snapshot.name)

    wait_for_backup_completion(client, volume_name, snapshot.name)

    backup_url = b.url

    storage_class[&#39;metadata&#39;][&#39;name&#39;] = &#34;longhorn-from-backup&#34;
    storage_class[&#39;parameters&#39;][&#39;fromBackup&#39;] = backup_url

    create_storage_class(storage_class)

    backup_pvc_name = generate_volume_name()

    backup_pvc_spec = {
        &#34;apiVersion&#34;: &#34;v1&#34;,
        &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;,
        &#34;metadata&#34;: {
                &#34;name&#34;: backup_pvc_name,
        },
        &#34;spec&#34;: {
                &#34;accessModes&#34;: [
                        &#34;ReadWriteOnce&#34;
                ],
                &#34;storageClassName&#34;: storage_class[&#39;metadata&#39;][&#39;name&#39;],
                &#34;resources&#34;: {
                        &#34;requests&#34;: {
                                &#34;storage&#34;: VOLUME_SIZE
                        }
                }
        }
    }

    volume_count = len(client.list_volume())

    core_api.create_namespaced_persistent_volume_claim(
        &#39;default&#39;,
        backup_pvc_spec
    )

    backup_volume_created = False

    for i in range(RETRY_COUNTS):
        if len(client.list_volume()) == volume_count + 1:
            backup_volume_created = True
            break
        time.sleep(RETRY_INTERVAL)

    assert backup_volume_created

    for i in range(RETRY_COUNTS):
        pvc_status = core_api.read_namespaced_persistent_volume_claim_status(
            name=backup_pvc_name,
            namespace=&#39;default&#39;
        )

        if pvc_status.status.phase == &#39;Bound&#39;:
            break
        time.sleep(RETRY_INTERVAL)

    found = False
    for i in range(RETRY_COUNTS):
        volumes = client.list_volume()
        for volume in volumes:
            if volume.kubernetesStatus.pvcName == backup_pvc_name:
                backup_volume_name = volume.name
                found = True
                break
        if found:
            break
        time.sleep(RETRY_INTERVAL)
    assert found

    wait_for_volume_restoration_completed(client, backup_volume_name)
    wait_for_volume_detached(client, backup_volume_name)

    backup_pod_manifest = pod_make(name=&#34;backup-pod&#34;)
    backup_pod_manifest[&#39;spec&#39;][&#39;volumes&#39;] = \
        [create_pvc_spec(backup_pvc_name)]
    backup_pod_name = backup_pod_manifest[&#39;metadata&#39;][&#39;name&#39;]
    create_and_wait_pod(core_api, backup_pod_manifest)

    restored_data = read_volume_data(core_api, backup_pod_name)
    assert test_data == restored_data</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_basic"><code class="name flex">
<span>def <span class="ident">test_volume_basic</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test basic volume operations:</p>
<ol>
<li>Check volume name and parameter</li>
<li>Create a volume and attach to the current node, then check volume states</li>
<li>Check soft anti-affinity rule</li>
<li>Write then read back to check volume data</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_volume_basic(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic volume operations:

    1. Check volume name and parameter
    2. Create a volume and attach to the current node, then check volume states
    3. Check soft anti-affinity rule
    4. Write then read back to check volume data
    &#34;&#34;&#34;
    volume_basic_test(clients, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_iscsi_basic"><code class="name flex">
<span>def <span class="ident">test_volume_iscsi_basic</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test basic volume operations with iscsi frontend</p>
<ol>
<li>Create and attach a volume with iscsi frontend</li>
<li>Check the volume endpoint and connect it using the iscsi
initator on the node.</li>
<li>Write then read back volume data for validation</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_volume_iscsi_basic(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic volume operations with iscsi frontend

    1. Create and attach a volume with iscsi frontend
    2. Check the volume endpoint and connect it using the iscsi
    initator on the node.
    3. Write then read back volume data for validation

    &#34;&#34;&#34;
    volume_iscsi_basic_test(clients, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_multinode"><code class="name flex">
<span>def <span class="ident">test_volume_multinode</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test the volume can be attached on multiple nodes</p>
<ol>
<li>Create one volume</li>
<li>Attach it on every node once, verify the state, then detach it</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_volume_multinode(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test the volume can be attached on multiple nodes

    1. Create one volume
    2. Attach it on every node once, verify the state, then detach it
    &#34;&#34;&#34;
    hosts = clients.keys()

    volume = get_random_client(clients).create_volume(name=volume_name,
                                                      size=SIZE,
                                                      numberOfReplicas=2)
    volume = common.wait_for_volume_detached(get_random_client(clients),
                                             volume_name)

    for host_id in hosts:
        volume = volume.attach(hostId=host_id)
        volume = common.wait_for_volume_healthy(get_random_client(clients),
                                                volume_name)
        engine = get_volume_engine(volume)
        assert engine.hostId == host_id
        volume = volume.detach()
        volume = common.wait_for_volume_detached(get_random_client(clients),
                                                 volume_name)

    get_random_client(clients).delete(volume)
    wait_for_volume_delete(get_random_client(clients), volume_name)

    volumes = get_random_client(clients).list_volume()
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_scheduling_failure"><code class="name flex">
<span>def <span class="ident">test_volume_scheduling_failure</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test fail to schedule by disable scheduling for all the nodes</p>
<p>Also test cannot attach a scheduling failed volume</p>
<ol>
<li>Disable <code>allowScheduling</code> for all nodes</li>
<li>Create a volume.</li>
<li>Verify the volume condition <code>Scheduled</code> is false</li>
<li>Verify attaching the volume will result in error</li>
<li>Enable <code>allowScheduling</code> for all nodes</li>
<li>Volume should be automatically scheduled (condition become true)</li>
<li>Volume can be attached now</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_volume_scheduling_failure(clients, volume_name):  # NOQA
    &#39;&#39;&#39;
    Test fail to schedule by disable scheduling for all the nodes

    Also test cannot attach a scheduling failed volume

    1. Disable `allowScheduling` for all nodes
    2. Create a volume.
    3. Verify the volume condition `Scheduled` is false
    4. Verify attaching the volume will result in error
    5. Enable `allowScheduling` for all nodes
    6. Volume should be automatically scheduled (condition become true)
    7. Volume can be attached now
    &#39;&#39;&#39;
    client = get_random_client(clients)
    nodes = client.list_node()
    assert len(nodes) &gt; 0

    for node in nodes:
        node = client.update(node, allowScheduling=False)
        node = common.wait_for_node_update(client, node.id,
                                           &#34;allowScheduling&#34;, False)

    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=3)

    volume = common.wait_for_volume_condition_scheduled(client, volume_name,
                                                        &#34;status&#34;,
                                                        CONDITION_STATUS_FALSE)
    volume = common.wait_for_volume_detached(client, volume_name)
    self_node = get_self_host_id()
    with pytest.raises(Exception) as e:
        volume.attach(hostId=self_node)
    assert &#34;not scheduled&#34; in str(e.value)

    for node in nodes:
        node = client.update(node, allowScheduling=True)
        node = common.wait_for_node_update(client, node.id,
                                           &#34;allowScheduling&#34;, True)

    volume = common.wait_for_volume_condition_scheduled(client, volume_name,
                                                        &#34;status&#34;,
                                                        CONDITION_STATUS_TRUE)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume = volume.attach(hostId=self_node)
    volume = common.wait_for_volume_healthy(client, volume_name)
    endpoint = get_volume_endpoint(volume)
    assert endpoint != &#34;&#34;
    volume_rw_test(endpoint)

    volume = volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_update_replica_count"><code class="name flex">
<span>def <span class="ident">test_volume_update_replica_count</span></span>(<span>clients, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test updating volume's replica count</p>
<ol>
<li>Create a volume with 3 replicas</li>
<li>Attach the volume</li>
<li>Increase the replica to 5.</li>
<li>Volume will become degraded and start rebuilding</li>
<li>Wait for rebuilding to complete</li>
<li>Update the replica count to 2. Volume should remain healthy</li>
<li>Remove 3 replicas, so there will be 2 replicas in the volume</li>
<li>Verify the volume is still healthy</li>
</ol>
<p>FIXME: Don't need to wait for volume to rebuild and healthy before step 8.
Volume should always be healthy even only with 2 replicas.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_volume_update_replica_count(clients, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test updating volume&#39;s replica count

    1. Create a volume with 3 replicas
    2. Attach the volume
    3. Increase the replica to 5.
    4. Volume will become degraded and start rebuilding
    5. Wait for rebuilding to complete
    6. Update the replica count to 2. Volume should remain healthy
    7. Remove 3 replicas, so there will be 2 replicas in the volume
    8. Verify the volume is still healthy

    FIXME: Don&#39;t need to wait for volume to rebuild and healthy before step 8.
    Volume should always be healthy even only with 2 replicas.
    &#34;&#34;&#34;
    for host_id, client in iter(clients.items()):
        break

    replica_count = 3
    volume = create_and_check_volume(client, volume_name, replica_count)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    replica_count = 5
    volume = volume.updateReplicaCount(replicaCount=replica_count)
    volume = common.wait_for_volume_degraded(client, volume_name)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == replica_count

    old_replica_count = replica_count
    replica_count = 2
    volume = volume.updateReplicaCount(replicaCount=replica_count)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == old_replica_count

    volume.replicaRemove(name=volume.replicas[0].name)
    volume.replicaRemove(name=volume.replicas[1].name)
    volume.replicaRemove(name=volume.replicas[2].name)

    volume = common.wait_for_volume_replica_count(client,
                                                  volume_name, replica_count)
    wait_for_rebuild_complete(client, volume_name)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == replica_count

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.volume_basic_test"><code class="name flex">
<span>def <span class="ident">volume_basic_test</span></span>(<span>clients, volume_name, base_image='')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_basic_test(clients, volume_name, base_image=&#34;&#34;):  # NOQA
    num_hosts = len(clients)
    num_replicas = 3

    # get a random client
    for host_id, client in iter(clients.items()):
        break

    with pytest.raises(Exception):
        volume = client.create_volume(name=&#34;wrong_volume-name-1.0&#34;, size=SIZE,
                                      numberOfReplicas=2)
        volume = client.create_volume(name=&#34;wrong_volume-name&#34;, size=SIZE,
                                      numberOfReplicas=2)
        volume = client.create_volume(name=&#34;wrong_volume-name&#34;, size=SIZE,
                                      numberOfReplicas=2,
                                      frontend=&#34;invalid_frontend&#34;)

    volume = create_and_check_volume(client, volume_name, num_replicas, SIZE,
                                     base_image)
    assert volume.initialRestorationRequired is False

    def validate_volume_basic(expected, actual):
        assert actual.name == expected.name
        assert actual.size == expected.size
        assert actual.numberOfReplicas == expected.numberOfReplicas
        assert actual.frontend == &#34;blockdev&#34;
        assert actual.baseImage == base_image
        assert actual.state == expected.state
        assert actual.created == expected.created

    volumes = client.list_volume().data
    assert len(volumes) == 1
    validate_volume_basic(volume, volumes[0])

    volumeByName = client.by_id_volume(volume_name)
    validate_volume_basic(volume, volumeByName)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert volume.initialRestorationRequired is False

    volumeByName = client.by_id_volume(volume_name)
    validate_volume_basic(volume, volumeByName)
    assert get_volume_endpoint(volumeByName) == DEV_PATH + volume_name

    # validate soft anti-affinity
    hosts = {}
    for replica in volume.replicas:
        id = replica.hostId
        assert id != &#34;&#34;
        hosts[id] = True
    if num_hosts &gt;= num_replicas:
        assert len(hosts) == num_replicas
    else:
        assert len(hosts) == num_hosts

    volumes = client.list_volume().data
    assert len(volumes) == 1
    assert volumes[0].name == volume.name
    assert volumes[0].size == volume.size
    assert volumes[0].numberOfReplicas == volume.numberOfReplicas
    assert volumes[0].state == volume.state
    assert volumes[0].created == volume.created
    assert get_volume_endpoint(volumes[0]) == DEV_PATH + volume_name

    volume = client.by_id_volume(volume_name)
    assert get_volume_endpoint(volume) == DEV_PATH + volume_name

    volume_rw_test(get_volume_endpoint(volume))

    volume.detach()
    volume = common.wait_for_volume_detached(client, volume_name)
    assert volume.initialRestorationRequired is False

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.volume_iscsi_basic_test"><code class="name flex">
<span>def <span class="ident">volume_iscsi_basic_test</span></span>(<span>clients, volume_name, base_image='')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_iscsi_basic_test(clients, volume_name, base_image=&#34;&#34;):  # NOQA
    # get a random client
    for host_id, client in iter(clients.items()):
        break

    volume = create_and_check_volume(client, volume_name, 3, SIZE, base_image,
                                     &#34;iscsi&#34;)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volumes = client.list_volume().data
    assert len(volumes) == 1
    assert volumes[0].name == volume.name
    assert volumes[0].size == volume.size
    assert volumes[0].numberOfReplicas == volume.numberOfReplicas
    assert volumes[0].state == volume.state
    assert volumes[0].created == volume.created
    assert volumes[0].frontend == &#34;iscsi&#34;
    endpoint = get_volume_endpoint(volumes[0])
    assert endpoint.startswith(&#34;iscsi://&#34;)

    try:
        dev = iscsi_login(endpoint)
        volume_rw_test(dev)
    finally:
        iscsi_logout(endpoint)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.volume_rw_test"><code class="name flex">
<span>def <span class="ident">volume_rw_test</span></span>(<span>dev)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_rw_test(dev):
    assert volume_valid(dev)
    data = write_device_random_data(dev)
    check_device_data(dev, data)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_basic.backup_labels_test" href="#tests.test_basic.backup_labels_test">backup_labels_test</a></code></li>
<li><code><a title="tests.test_basic.backup_test" href="#tests.test_basic.backup_test">backup_test</a></code></li>
<li><code><a title="tests.test_basic.backupstore_test" href="#tests.test_basic.backupstore_test">backupstore_test</a></code></li>
<li><code><a title="tests.test_basic.restore_inc_test" href="#tests.test_basic.restore_inc_test">restore_inc_test</a></code></li>
<li><code><a title="tests.test_basic.snapshot_test" href="#tests.test_basic.snapshot_test">snapshot_test</a></code></li>
<li><code><a title="tests.test_basic.test_attach_without_frontend" href="#tests.test_basic.test_attach_without_frontend">test_attach_without_frontend</a></code></li>
<li><code><a title="tests.test_basic.test_backup" href="#tests.test_basic.test_backup">test_backup</a></code></li>
<li><code><a title="tests.test_basic.test_backup_labels" href="#tests.test_basic.test_backup_labels">test_backup_labels</a></code></li>
<li><code><a title="tests.test_basic.test_deleting_backup_volume" href="#tests.test_basic.test_deleting_backup_volume">test_deleting_backup_volume</a></code></li>
<li><code><a title="tests.test_basic.test_engine_image_daemonset_restart" href="#tests.test_basic.test_engine_image_daemonset_restart">test_engine_image_daemonset_restart</a></code></li>
<li><code><a title="tests.test_basic.test_expansion_basic" href="#tests.test_basic.test_expansion_basic">test_expansion_basic</a></code></li>
<li><code><a title="tests.test_basic.test_hosts" href="#tests.test_basic.test_hosts">test_hosts</a></code></li>
<li><code><a title="tests.test_basic.test_listing_backup_volume" href="#tests.test_basic.test_listing_backup_volume">test_listing_backup_volume</a></code></li>
<li><code><a title="tests.test_basic.test_restore_inc" href="#tests.test_basic.test_restore_inc">test_restore_inc</a></code></li>
<li><code><a title="tests.test_basic.test_restore_inc_with_expansion" href="#tests.test_basic.test_restore_inc_with_expansion">test_restore_inc_with_expansion</a></code></li>
<li><code><a title="tests.test_basic.test_setting_default_replica_count" href="#tests.test_basic.test_setting_default_replica_count">test_setting_default_replica_count</a></code></li>
<li><code><a title="tests.test_basic.test_settings" href="#tests.test_basic.test_settings">test_settings</a></code></li>
<li><code><a title="tests.test_basic.test_snapshot" href="#tests.test_basic.test_snapshot">test_snapshot</a></code></li>
<li><code><a title="tests.test_basic.test_storage_class_from_backup" href="#tests.test_basic.test_storage_class_from_backup">test_storage_class_from_backup</a></code></li>
<li><code><a title="tests.test_basic.test_volume_basic" href="#tests.test_basic.test_volume_basic">test_volume_basic</a></code></li>
<li><code><a title="tests.test_basic.test_volume_iscsi_basic" href="#tests.test_basic.test_volume_iscsi_basic">test_volume_iscsi_basic</a></code></li>
<li><code><a title="tests.test_basic.test_volume_multinode" href="#tests.test_basic.test_volume_multinode">test_volume_multinode</a></code></li>
<li><code><a title="tests.test_basic.test_volume_scheduling_failure" href="#tests.test_basic.test_volume_scheduling_failure">test_volume_scheduling_failure</a></code></li>
<li><code><a title="tests.test_basic.test_volume_update_replica_count" href="#tests.test_basic.test_volume_update_replica_count">test_volume_update_replica_count</a></code></li>
<li><code><a title="tests.test_basic.volume_basic_test" href="#tests.test_basic.volume_basic_test">volume_basic_test</a></code></li>
<li><code><a title="tests.test_basic.volume_iscsi_basic_test" href="#tests.test_basic.volume_iscsi_basic_test">volume_iscsi_basic_test</a></code></li>
<li><code><a title="tests.test_basic.volume_rw_test" href="#tests.test_basic.volume_rw_test">volume_rw_test</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>