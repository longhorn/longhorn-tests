<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>tests.test_orphan API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_orphan</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_orphan.create_disks_on_host"><code class="name flex">
<span>def <span class="ident">create_disks_on_host</span></span>(<span>client, disk_names, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_disks_on_host(client, disk_names, request):  # NOQA
    disk_paths = []

    lht_hostId = get_self_host_id()
    node = client.by_id_node(lht_hostId)
    update_disks = get_update_disks(node.disks)

    for name in disk_names:
        disk_path = create_host_disk(client,
                                     name,
                                     str(Gi),
                                     lht_hostId)
        disk = {&#34;path&#34;: disk_path, &#34;allowScheduling&#34;: True}
        update_disks[name] = disk
        disk_paths.append(disk_path)

    node = update_node_disks(client, node.name, disks=update_disks,
                             retry=True)
    node = wait_for_disk_update(client, node.name, len(update_disks))

    def finalizer():
        delete_extra_disks_on_host(client, disk_names)
        for disk_name in disk_names:
            cleanup_host_disks(client, disk_name)

    request.addfinalizer(finalizer)

    return disk_paths</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.create_orphaned_directories_on_host"><code class="name flex">
<span>def <span class="ident">create_orphaned_directories_on_host</span></span>(<span>volume, disk_paths, num_orphans)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_orphaned_directories_on_host(volume, disk_paths, num_orphans):  # NOQA
    lht_hostId = get_self_host_id()
    paths = []
    for replica in volume.replicas:
        if replica.hostId != lht_hostId:
            continue
        for _ in range(num_orphans):
            for i, disk_path in enumerate(disk_paths):
                replica_dir_name = volume.name + &#34;-&#34; + generate_random_id(8)
                path = os.path.join(disk_path, &#34;replicas&#34;, replica_dir_name)
                paths.append(path)
                exec_local(&#34;cp -a {} {}&#34;.format(replica.dataPath, path))

    return paths</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.create_volume_with_replica_on_host"><code class="name flex">
<span>def <span class="ident">create_volume_with_replica_on_host</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_volume_with_replica_on_host(client, volume_name):  # NOQA
    lht_hostId = get_self_host_id()

    nodes = client.list_node()

    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=len(nodes),
                                     size=SIZE)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    wait_for_volume_healthy(client, volume_name)

    return volume</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.delete_extra_disks_on_host"><code class="name flex">
<span>def <span class="ident">delete_extra_disks_on_host</span></span>(<span>client, disk_names)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_extra_disks_on_host(client, disk_names):  # NOQA
    lht_hostId = get_self_host_id()

    node = client.by_id_node(lht_hostId)
    update_disk = get_update_disks(node.disks)

    for disk_name in disk_names:
        update_disk[disk_name].allowScheduling = False
        update_disk[disk_name].evictionRequested = True

    node = node.diskUpdate(disks=update_disk)

    for disk_name in disk_names:
        wait_for_disk_status(client, lht_hostId,
                             disk_name,
                             &#34;storageScheduled&#34;, 0)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.delete_orphan"><code class="name flex">
<span>def <span class="ident">delete_orphan</span></span>(<span>client, orphan_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_orphan(client, orphan_name):  # NOQA
    for _ in range(RETRY_COUNTS):
        found = False
        try:
            orphan = client.by_id_orphan(orphan_name)
            client.delete(orphan)
            time.sleep(RETRY_INTERVAL_LONG)
            orphans = client.list_orphan()
            for orphan in orphans:
                if orphan.name == orphan_name:
                    found = True
                    break
            if not found:
                break
        except Exception as e:
            print(e)
            break
    assert not found</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.delete_orphaned_directory_on_host"><code class="name flex">
<span>def <span class="ident">delete_orphaned_directory_on_host</span></span>(<span>directories)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_orphaned_directory_on_host(directories):  # NOQA
    for path in directories:
        exec_local(&#34;rm -rf {}&#34;.format(path))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.delete_orphans"><code class="name flex">
<span>def <span class="ident">delete_orphans</span></span>(<span>client)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_orphans(client):  # NOQA
    orphans = client.list_orphan()
    for orphan in orphans:
        delete_orphan(client, orphan.name)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.generate_random_id"><code class="name flex">
<span>def <span class="ident">generate_random_id</span></span>(<span>num_bytes)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_random_id(num_bytes):
    return &#39;&#39;.join(random.choice(string.ascii_lowercase + string.digits)
                   for _ in range(num_bytes))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.test_delete_orphan_after_orphaned_dir_is_deleted"><code class="name flex">
<span>def <span class="ident">test_delete_orphan_after_orphaned_dir_is_deleted</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_delete_orphan_after_orphaned_dir_is_deleted(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test the immediate deletion of orphan CRs after the orphaned replica
    directory is deleted
    1. Create a new disk for holding valid and invalid orphaned replica
       directories
    2. Create a volume and attach to the current node
    3. Create a valid orphaned replica directories by copying the active
       replica directory
    4. Clean up volume
    5. Verify orphan list contains the orphan CR for the orphaned replica
       directories
    6. Delete the on-disk orphaned replica directories
    7. Delete the orphan CRs immediately
    8. Verify orphan list is empty
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    orphaned_directories = create_orphaned_directories_on_host(volume,
                                                               disk_paths,
                                                               1)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    assert wait_for_orphan_count(client, 1, 180) == 1

    # Step 6
    delete_orphaned_directory_on_host(orphaned_directories)

    # Step 7
    delete_orphans(client)

    # Step 8
    assert wait_for_orphan_count(client, 0, 180) == 0</code></pre>
</details>
<div class="desc"><p>Test the immediate deletion of orphan CRs after the orphaned replica
directory is deleted
1. Create a new disk for holding valid and invalid orphaned replica
directories
2. Create a volume and attach to the current node
3. Create a valid orphaned replica directories by copying the active
replica directory
4. Clean up volume
5. Verify orphan list contains the orphan CR for the orphaned replica
directories
6. Delete the on-disk orphaned replica directories
7. Delete the orphan CRs immediately
8. Verify orphan list is empty</p></div>
</dd>
<dt id="tests.test_orphan.test_delete_orphans"><code class="name flex">
<span>def <span class="ident">test_delete_orphans</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_delete_orphans(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test the deletion of orphaned replica directories
    1. Create a new disk holding valid and invalid orphaned replica
       directories
    2. Create a volume and attach to the current node
    3. Create multiple orphaned replica directories by copying the
       active replica directory
    4. Clean up volume
    5. Verify orphan list contains CRs for the valid orphaned replica
       directories
    6. Delete all orphan CRs
    7. Verify orphan list is empty and the orphan replica directories are
       deleted
    8. Verify all orphaned replica directories are deleted
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    num_orphans = 5
    create_orphaned_directories_on_host(volume, disk_paths, num_orphans)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    assert wait_for_orphan_count(client, num_orphans, 180) == num_orphans

    # Step 6
    delete_orphans(client)

    # Step 7
    assert wait_for_orphan_count(client, 0, 180) == 0

    # Step 8
    assert wait_for_file_count(os.path.join(disk_paths[0], &#34;replicas&#34;),
                               0,
                               180) == 0</code></pre>
</details>
<div class="desc"><p>Test the deletion of orphaned replica directories
1. Create a new disk holding valid and invalid orphaned replica
directories
2. Create a volume and attach to the current node
3. Create multiple orphaned replica directories by copying the
active replica directory
4. Clean up volume
5. Verify orphan list contains CRs for the valid orphaned replica
directories
6. Delete all orphan CRs
7. Verify orphan list is empty and the orphan replica directories are
deleted
8. Verify all orphaned replica directories are deleted</p></div>
</dd>
<dt id="tests.test_orphan.test_disk_evicted"><code class="name flex">
<span>def <span class="ident">test_disk_evicted</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_disk_evicted(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test the orphan CR is deleted in background but on-disk data still exists
    if the disk is evicted
    1. Create a new disk for holding valid and invalid orphaned
       replica directories
    2. Create a volume and attach to the current node
    3. Create a valid orphaned replica directories by copying the active
       replica directory
    4. Clean up volume
    5. Verify orphan list contains the valid orphaned replica directory
    6. Evict the disk containing the orphaned replica directory
    7. Verify the orphan CR is deleted in background, but the on-disk orphaned
       replica directory still exists
    8. Set the disk scheduleable again
    9. Verify the orphan CR is created again and the on-disk orphaned replica
       directory still exists
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4)]

    lht_hostId = get_self_host_id()

    # Step 1
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    create_orphaned_directories_on_host(volume, disk_paths, 1)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    assert wait_for_orphan_count(client, 1, 180) == 1

    # Step 6: Request disk eviction evictionRequested
    node = client.by_id_node(lht_hostId)
    update_disks = get_update_disks(node.disks)

    update_disks[disk_names[0]].allowScheduling = False
    update_disks[disk_names[0]].evictionRequested = True
    node = node.diskUpdate(disks=update_disks)
    node = wait_for_disk_update(client, node.name, len(update_disks))

    # Step 7
    assert wait_for_orphan_count(client, 0, 180) == 0

    assert wait_for_file_count(os.path.join(disk_paths[0], &#34;replicas&#34;),
                               1,
                               180) == 1

    # Step 8: Set disk allowScheduling to true and evictionRequested to false
    node = client.by_id_node(lht_hostId)
    update_disks = get_update_disks(node.disks)

    update_disks[disk_names[0]].allowScheduling = True
    update_disks[disk_names[0]].evictionRequested = False
    node = node.diskUpdate(disks=update_disks)
    node = wait_for_disk_update(client, node.name, len(update_disks))

    # Step 9
    assert wait_for_orphan_count(client, 1, 180) == 1

    assert wait_for_file_count(os.path.join(disk_paths[0], &#34;replicas&#34;),
                               1,
                               180) == 1</code></pre>
</details>
<div class="desc"><p>Test the orphan CR is deleted in background but on-disk data still exists
if the disk is evicted
1. Create a new disk for holding valid and invalid orphaned
replica directories
2. Create a volume and attach to the current node
3. Create a valid orphaned replica directories by copying the active
replica directory
4. Clean up volume
5. Verify orphan list contains the valid orphaned replica directory
6. Evict the disk containing the orphaned replica directory
7. Verify the orphan CR is deleted in background, but the on-disk orphaned
replica directory still exists
8. Set the disk scheduleable again
9. Verify the orphan CR is created again and the on-disk orphaned replica
directory still exists</p></div>
</dd>
<dt id="tests.test_orphan.test_node_evicted"><code class="name flex">
<span>def <span class="ident">test_node_evicted</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_node_evicted(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test the orphan CR is deleted in background but on-disk data still exists
    if the node is evicted
    1. Create a new-disk for holding valid and invalid orphaned replica
       directories
    2. Create a volume and attach to the current node
    3. Create a valid orphaned replica directories by copying the active
       replica directory
    4. Clean up volume
    5. Verify orphan list contains the valid orphaned replica directory
    6. Evict the node containing the orphaned replica directory
    7. Verify the orphan CR is deleted in background, but the on-disk
       orphaned replica directory still exists
    8. Disable node eviction
    9. Verify the orphan CR is created again and the on-disk orphaned replica
       directory still exists
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    create_orphaned_directories_on_host(volume, disk_paths, 1)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    assert wait_for_orphan_count(client, 1, 180) == 1

    # Step 6: request node eviction
    node = client.by_id_node(lht_hostId)
    client.update(node, allowScheduling=False, evictionRequested=True)
    node = wait_for_node_update(client, lht_hostId,
                                &#34;allowScheduling&#34;, False)

    # Step 7
    assert wait_for_orphan_count(client, 0, 180) == 0
    assert wait_for_file_count(os.path.join(disk_paths[0], &#34;replicas&#34;),
                               1,
                               180) == 1

    # Step 8: Disable node eviction
    node = client.by_id_node(lht_hostId)
    client.update(node, allowScheduling=True, evictionRequested=False)
    node = wait_for_node_update(client, lht_hostId,
                                &#34;allowScheduling&#34;, True)

    # Step 9
    assert wait_for_orphan_count(client, 1, 180) == 1
    assert wait_for_file_count(os.path.join(disk_paths[0], &#34;replicas&#34;),
                               1,
                               180) == 1</code></pre>
</details>
<div class="desc"><p>Test the orphan CR is deleted in background but on-disk data still exists
if the node is evicted
1. Create a new-disk for holding valid and invalid orphaned replica
directories
2. Create a volume and attach to the current node
3. Create a valid orphaned replica directories by copying the active
replica directory
4. Clean up volume
5. Verify orphan list contains the valid orphaned replica directory
6. Evict the node containing the orphaned replica directory
7. Verify the orphan CR is deleted in background, but the on-disk
orphaned replica directory still exists
8. Disable node eviction
9. Verify the orphan CR is created again and the on-disk orphaned replica
directory still exists</p></div>
</dd>
<dt id="tests.test_orphan.test_orphan_auto_deletion"><code class="name flex">
<span>def <span class="ident">test_orphan_auto_deletion</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_orphan_auto_deletion(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test orphaned dirs creation and background deletion in multiple disks
    1. Create a new disks for holding orphaned replica directories
    2. Create a volume and attach to the current node
    3. Create orphaned replica directories by copying the active
       replica directory
    4. Clean up volume
    5. Verify orphan list contains the orphan CRs for replica
       directories
    6. Enable the replica data cleanup in orphan-resource-auto-deletion setting
    7. Verify orphan list is empty and the orphaned directory is
       deleted in background
    8. Clean up disk
    &#34;&#34;&#34;
    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    create_orphaned_directories_on_host(volume, disk_paths, 1)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    assert wait_for_orphan_count(client, 1, 180) == 1

    # Step 6: enable orphan auto deletion
    setting = client.by_id_setting(SETTING_ORPHAN_RESOURCE_AUTO_DELETION)
    client.update(setting, value=&#34;replica-data&#34;)

    # Step 7
    assert wait_for_orphan_count(client, 0, 180) == 0
    assert wait_for_file_count(os.path.join(disk_paths[0], &#34;replicas&#34;),
                               0,
                               180) == 0</code></pre>
</details>
<div class="desc"><p>Test orphaned dirs creation and background deletion in multiple disks
1. Create a new disks for holding orphaned replica directories
2. Create a volume and attach to the current node
3. Create orphaned replica directories by copying the active
replica directory
4. Clean up volume
5. Verify orphan list contains the orphan CRs for replica
directories
6. Enable the replica data cleanup in orphan-resource-auto-deletion setting
7. Verify orphan list is empty and the orphaned directory is
deleted in background
8. Clean up disk</p></div>
</dd>
<dt id="tests.test_orphan.test_orphan_creation_and_background_deletion_in_multiple_disks"><code class="name flex">
<span>def <span class="ident">test_orphan_creation_and_background_deletion_in_multiple_disks</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_orphan_creation_and_background_deletion_in_multiple_disks(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test orphaned dirs creation and background deletion in multiple disks
    1. Create multiple new-disks for holding orphaned replica directories
    2. Create a volume and attach to the current node
    3. Create multiple orphaned replica directories by copying the active
       replica directory
    4. Clean up volume
    5. Verify orphan list contains the orphan CRs for replica
       directories
    6. Delete the orphaned replica directories in background
    7. Verify orphan list is empty
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4),
                  &#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    num_orphans = 5
    orphaned_directories = create_orphaned_directories_on_host(volume,
                                                               disk_paths,
                                                               num_orphans)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    count = wait_for_orphan_count(client,
                                  num_orphans * len(disk_paths),
                                  180)
    assert count == num_orphans * len(disk_paths)

    # Step 6
    delete_orphaned_directory_on_host(orphaned_directories)

    # Step 7
    assert wait_for_orphan_count(client, 0, 180) == 0</code></pre>
</details>
<div class="desc"><p>Test orphaned dirs creation and background deletion in multiple disks
1. Create multiple new-disks for holding orphaned replica directories
2. Create a volume and attach to the current node
3. Create multiple orphaned replica directories by copying the active
replica directory
4. Clean up volume
5. Verify orphan list contains the orphan CRs for replica
directories
6. Delete the orphaned replica directories in background
7. Verify orphan list is empty</p></div>
</dd>
<dt id="tests.test_orphan.test_orphan_creation_and_deletion_in_multiple_disks"><code class="name flex">
<span>def <span class="ident">test_orphan_creation_and_deletion_in_multiple_disks</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_orphan_creation_and_deletion_in_multiple_disks(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test orphan creation and deletion in multiple disks
    1. Create multiple new-disks for holding orphaned replica directories
    2. Create a volume and attach to the current node
    3. Create multiple orphaned replica directories by copying the active
       replica directory
    4. Clean up volume
    5. Verify orphan list contains the orphan CRs for replica
       directories
    6. Delete all orphaned CRs
    7. Verify orphan list is empty
    8. Verify orphaned replica directories are deleted
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4),
                  &#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    num_orphans = 5
    create_orphaned_directories_on_host(volume, disk_paths, num_orphans)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    count = wait_for_orphan_count(client,
                                  num_orphans * len(disk_paths),
                                  180)
    assert count == num_orphans * len(disk_paths)

    # Step 6
    delete_orphans(client)

    # Step 7
    assert wait_for_orphan_count(client, 0, 180) == 0

    # Step 8
    assert wait_for_file_count(os.path.join(disk_paths[0], &#34;replicas&#34;),
                               0,
                               180) == 0

    assert wait_for_file_count(os.path.join(disk_paths[1], &#34;replicas&#34;),
                               0,
                               180) == 0</code></pre>
</details>
<div class="desc"><p>Test orphan creation and deletion in multiple disks
1. Create multiple new-disks for holding orphaned replica directories
2. Create a volume and attach to the current node
3. Create multiple orphaned replica directories by copying the active
replica directory
4. Clean up volume
5. Verify orphan list contains the orphan CRs for replica
directories
6. Delete all orphaned CRs
7. Verify orphan list is empty
8. Verify orphaned replica directories are deleted</p></div>
</dd>
<dt id="tests.test_orphan.test_orphan_with_same_orphaned_dir_name_in_another_disk"><code class="name flex">
<span>def <span class="ident">test_orphan_with_same_orphaned_dir_name_in_another_disk</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_orphan_with_same_orphaned_dir_name_in_another_disk(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test orphan creation and deletion with same orphaned dir name in
    another disk
    1. Create a volume and attach to the current node&#39;s default disk
    2. Create a new disks for holding orphaned replica directories
    3. Create orphaned replica directories by copying the active
       replica directory
    4. Clean up volume
    5. Verify orphan list contains the orphan CRs for replica directories
    6. Delete the orphaned replica directories
    7. Verify orphan list is empty
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 2
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 3
    create_orphaned_directories_on_host(volume, disk_paths, 1)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    assert wait_for_orphan_count(client, 1, 180) == 1

    # Step 6
    delete_orphans(client)

    # Step 7
    assert wait_for_orphan_count(client, 0, 180) == 0</code></pre>
</details>
<div class="desc"><p>Test orphan creation and deletion with same orphaned dir name in
another disk
1. Create a volume and attach to the current node's default disk
2. Create a new disks for holding orphaned replica directories
3. Create orphaned replica directories by copying the active
replica directory
4. Clean up volume
5. Verify orphan list contains the orphan CRs for replica directories
6. Delete the orphaned replica directories
7. Verify orphan list is empty</p></div>
</dd>
<dt id="tests.test_orphan.test_orphaned_dirs_in_duplicated_disks"><code class="name flex">
<span>def <span class="ident">test_orphaned_dirs_in_duplicated_disks</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_orphaned_dirs_in_duplicated_disks(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test orphaned dirs in duplicated disks. LH should not create a orphan CR
    for the orphaned dir in the deduplicate and unscheduled disk.
    1. Create a new disk for holding orphaned replica directories
    2. Create a folder under the new disk. This folder will be the duplicated
       disk. Add it to the node.
    3. Create a volume and attach to the current node
    4. Create multiple orphaned replica directories in the two disks by
       copying the active replica directory
    5. Clean up volume
    6. Verify orphan list only contains the orphan CRs for replica directories
       in the ready disk
    7. Delete all orphan CRs
    8. Verify orphan list is empty
    9. Verify orphaned directories in the new disk are deleted
    10. Verify orphaned directories in the duplicated disk are no deleted
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4),
                  &#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    disk_paths = []

    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, [disk_names[0]], request)

    # Step 2: create duplicated disks for node
    node = client.by_id_node(lht_hostId)
    disks = node.disks
    disk_path = os.path.join(disk_paths[0], disk_names[1])
    disk_paths.append(disk_path)
    os.makedirs(disk_path)
    disk2 = {&#34;path&#34;: disk_path, &#34;allowScheduling&#34;: True}

    update_disk = get_update_disks(disks)
    update_disk[disk_names[1]] = disk2
    node = node.diskUpdate(disks=update_disk)
    node = wait_for_disk_update(client, lht_hostId, len(update_disk))

    # Step 3
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 4
    num_orphans = 5
    create_orphaned_directories_on_host(volume, disk_paths, num_orphans)

    # Step 5
    cleanup_volume_by_name(client, volume_name)

    # Step 6
    assert wait_for_orphan_count(client, num_orphans, 180) == num_orphans

    # Step 7
    delete_orphans(client)

    # Step 8
    count = wait_for_orphan_count(client, 0, 180)
    assert count == 0

    # Step 9: The orphaned directories in the ready disk should be deleted
    assert wait_for_file_count(os.path.join(disk_paths[0], &#34;replicas&#34;),
                               0,
                               180) == 0

    # Step 10: The orphaned directories in the duplicated disk should not be
    # deleted
    assert wait_for_file_count(os.path.join(disk_paths[1], &#34;replicas&#34;),
                               num_orphans,
                               180) == num_orphans</code></pre>
</details>
<div class="desc"><p>Test orphaned dirs in duplicated disks. LH should not create a orphan CR
for the orphaned dir in the deduplicate and unscheduled disk.
1. Create a new disk for holding orphaned replica directories
2. Create a folder under the new disk. This folder will be the duplicated
disk. Add it to the node.
3. Create a volume and attach to the current node
4. Create multiple orphaned replica directories in the two disks by
copying the active replica directory
5. Clean up volume
6. Verify orphan list only contains the orphan CRs for replica directories
in the ready disk
7. Delete all orphan CRs
8. Verify orphan list is empty
9. Verify orphaned directories in the new disk are deleted
10. Verify orphaned directories in the duplicated disk are no deleted</p></div>
</dd>
<dt id="tests.test_orphan.test_orphaned_dirs_with_wrong_naming_format"><code class="name flex">
<span>def <span class="ident">test_orphaned_dirs_with_wrong_naming_format</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_orphaned_dirs_with_wrong_naming_format(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test orphan CRs are not created for the orphaned directories with wrong
    naming formats
    1. Create a new disk holding valid and invalid orphaned replica
       directories
    2. Create a volume and then attach to the current node
    3. Create one valid orphaned replica directories by copying the active
       replica directory
    4. Create multiple invalid orphan replica directories with wrong naming
       format
    5. Clean up volume
    6. Verify orphan list only contains the orphan CR for valid orphaned
       replica directory
    7. Clean up disk
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    create_orphaned_directories_on_host(volume, disk_paths, 1)

    # Step 4
    for replica in volume.replicas:
        if replica.hostId != lht_hostId:
            continue

        # Create invalid orphaned directories.
        # 8-byte random id missing
        os.makedirs(os.path.join(replica.diskPath, &#34;replicas&#34;, volume_name))

        # wrong random id length
        os.makedirs(os.path.join(replica.diskPath, &#34;replicas&#34;,
                                 volume_name + &#34;-&#34; + generate_random_id(4)))

        # volume.meta missing
        path = os.path.join(replica.diskPath, &#34;replicas&#34;,
                            volume_name + &#34;-&#34; + generate_random_id(8))
        shutil.copytree(replica.dataPath, path)
        os.remove(os.path.join(path, &#34;volume.meta&#34;))

        # corrupted volume.meta
        path = os.path.join(replica.diskPath, &#34;replicas&#34;,
                            volume_name + &#34;-&#34; + generate_random_id(8))
        shutil.copytree(replica.dataPath, path)
        with open(os.path.join(path, &#34;volume.meta&#34;), &#39;w&#39;) as file:
            file.write(&#34;xxx&#34;)

    # Step 5
    cleanup_volume_by_name(client, volume_name)

    # Step 6
    assert wait_for_orphan_count(client, 1, 180) == 1</code></pre>
</details>
<div class="desc"><p>Test orphan CRs are not created for the orphaned directories with wrong
naming formats
1. Create a new disk holding valid and invalid orphaned replica
directories
2. Create a volume and then attach to the current node
3. Create one valid orphaned replica directories by copying the active
replica directory
4. Create multiple invalid orphan replica directories with wrong naming
format
5. Clean up volume
6. Verify orphan list only contains the orphan CR for valid orphaned
replica directory
7. Clean up disk</p></div>
</dd>
<dt id="tests.test_orphan.test_orphaned_replica_dir_missing"><code class="name flex">
<span>def <span class="ident">test_orphaned_replica_dir_missing</span></span>(<span>client, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.orphan
def test_orphaned_replica_dir_missing(client, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Test orphan CRs are deleted in background if the orphaned replica
    directories are missing
    1. Create a new disk for holding valid and invalid orphaned replica
       directories
    2. Create a volume and attach to the current node
    3. Create a orphaned replica directories by copying the active
       replica directory
    4. Clean up volume
    5. Verify orphan list contains the orphan CRs for the orphaned replica
       directories
    6. Delete the on-disk orphaned replica directories
    7. Verify the orphan CR is deleted in background
    8. Clean up disk
    &#34;&#34;&#34;

    disk_names = [&#34;vol-disk-&#34; + generate_random_id(4)]

    # Step 1
    lht_hostId = get_self_host_id()
    cleanup_node_disks(client, lht_hostId)
    disk_paths = create_disks_on_host(client, disk_names, request)

    # Step 2
    volume = create_volume_with_replica_on_host(client, volume_name)

    # Step 3
    orphaned_directories = create_orphaned_directories_on_host(volume,
                                                               disk_paths,
                                                               1)

    # Step 4
    cleanup_volume_by_name(client, volume_name)

    # Step 5
    assert wait_for_orphan_count(client, 1, 180) == 1

    # Step 6
    delete_orphaned_directory_on_host(orphaned_directories)

    # Step 7
    assert wait_for_orphan_count(client, 0, 180) == 0</code></pre>
</details>
<div class="desc"><p>Test orphan CRs are deleted in background if the orphaned replica
directories are missing
1. Create a new disk for holding valid and invalid orphaned replica
directories
2. Create a volume and attach to the current node
3. Create a orphaned replica directories by copying the active
replica directory
4. Clean up volume
5. Verify orphan list contains the orphan CRs for the orphaned replica
directories
6. Delete the on-disk orphaned replica directories
7. Verify the orphan CR is deleted in background
8. Clean up disk</p></div>
</dd>
<dt id="tests.test_orphan.wait_for_file_count"><code class="name flex">
<span>def <span class="ident">wait_for_file_count</span></span>(<span>path, number, retry_counts=120)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_file_count(path, number, retry_counts=120):
    for _ in range(retry_counts):
        if len(os.listdir(path)) == number:
            break
        time.sleep(RETRY_INTERVAL_LONG)

    return len(os.listdir(path))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_orphan.wait_for_orphan_count"><code class="name flex">
<span>def <span class="ident">wait_for_orphan_count</span></span>(<span>client, number, retry_counts=120)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_orphan_count(client, number, retry_counts=120):  # NOQA
    for _ in range(retry_counts):
        orphans = client.list_orphan()
        if len(orphans) == number:
            break
        time.sleep(RETRY_INTERVAL_LONG)
    return len(orphans)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_orphan.create_disks_on_host" href="#tests.test_orphan.create_disks_on_host">create_disks_on_host</a></code></li>
<li><code><a title="tests.test_orphan.create_orphaned_directories_on_host" href="#tests.test_orphan.create_orphaned_directories_on_host">create_orphaned_directories_on_host</a></code></li>
<li><code><a title="tests.test_orphan.create_volume_with_replica_on_host" href="#tests.test_orphan.create_volume_with_replica_on_host">create_volume_with_replica_on_host</a></code></li>
<li><code><a title="tests.test_orphan.delete_extra_disks_on_host" href="#tests.test_orphan.delete_extra_disks_on_host">delete_extra_disks_on_host</a></code></li>
<li><code><a title="tests.test_orphan.delete_orphan" href="#tests.test_orphan.delete_orphan">delete_orphan</a></code></li>
<li><code><a title="tests.test_orphan.delete_orphaned_directory_on_host" href="#tests.test_orphan.delete_orphaned_directory_on_host">delete_orphaned_directory_on_host</a></code></li>
<li><code><a title="tests.test_orphan.delete_orphans" href="#tests.test_orphan.delete_orphans">delete_orphans</a></code></li>
<li><code><a title="tests.test_orphan.generate_random_id" href="#tests.test_orphan.generate_random_id">generate_random_id</a></code></li>
<li><code><a title="tests.test_orphan.test_delete_orphan_after_orphaned_dir_is_deleted" href="#tests.test_orphan.test_delete_orphan_after_orphaned_dir_is_deleted">test_delete_orphan_after_orphaned_dir_is_deleted</a></code></li>
<li><code><a title="tests.test_orphan.test_delete_orphans" href="#tests.test_orphan.test_delete_orphans">test_delete_orphans</a></code></li>
<li><code><a title="tests.test_orphan.test_disk_evicted" href="#tests.test_orphan.test_disk_evicted">test_disk_evicted</a></code></li>
<li><code><a title="tests.test_orphan.test_node_evicted" href="#tests.test_orphan.test_node_evicted">test_node_evicted</a></code></li>
<li><code><a title="tests.test_orphan.test_orphan_auto_deletion" href="#tests.test_orphan.test_orphan_auto_deletion">test_orphan_auto_deletion</a></code></li>
<li><code><a title="tests.test_orphan.test_orphan_creation_and_background_deletion_in_multiple_disks" href="#tests.test_orphan.test_orphan_creation_and_background_deletion_in_multiple_disks">test_orphan_creation_and_background_deletion_in_multiple_disks</a></code></li>
<li><code><a title="tests.test_orphan.test_orphan_creation_and_deletion_in_multiple_disks" href="#tests.test_orphan.test_orphan_creation_and_deletion_in_multiple_disks">test_orphan_creation_and_deletion_in_multiple_disks</a></code></li>
<li><code><a title="tests.test_orphan.test_orphan_with_same_orphaned_dir_name_in_another_disk" href="#tests.test_orphan.test_orphan_with_same_orphaned_dir_name_in_another_disk">test_orphan_with_same_orphaned_dir_name_in_another_disk</a></code></li>
<li><code><a title="tests.test_orphan.test_orphaned_dirs_in_duplicated_disks" href="#tests.test_orphan.test_orphaned_dirs_in_duplicated_disks">test_orphaned_dirs_in_duplicated_disks</a></code></li>
<li><code><a title="tests.test_orphan.test_orphaned_dirs_with_wrong_naming_format" href="#tests.test_orphan.test_orphaned_dirs_with_wrong_naming_format">test_orphaned_dirs_with_wrong_naming_format</a></code></li>
<li><code><a title="tests.test_orphan.test_orphaned_replica_dir_missing" href="#tests.test_orphan.test_orphaned_replica_dir_missing">test_orphaned_replica_dir_missing</a></code></li>
<li><code><a title="tests.test_orphan.wait_for_file_count" href="#tests.test_orphan.wait_for_file_count">wait_for_file_count</a></code></li>
<li><code><a title="tests.test_orphan.wait_for_orphan_count" href="#tests.test_orphan.wait_for_orphan_count">wait_for_orphan_count</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
