<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>tests.test_recurring_job API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_recurring_job</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pytest
import time
import json

from datetime import datetime

from kubernetes.client.rest import ApiException

import backupstore
from backupstore import set_random_backupstore  # NOQA

import common
from common import client, core_api, apps_api, batch_v1_beta_api  # NOQA
from common import random_labels, volume_name  # NOQA
from common import storage_class, statefulset, pvc  # NOQA
from common import make_deployment_with_pvc  # NOQA

from common import get_self_host_id

from common import create_storage_class

from common import create_pv_for_volume

from common import create_pvc_for_volume

from common import create_and_check_volume
from common import read_volume_data
from common import wait_for_volume_detached
from common import wait_for_volume_healthy
from common import wait_for_volume_healthy_no_frontend
from common import wait_for_volume_recurring_job_update
from common import wait_volume_kubernetes_status
from common import write_pod_volume_random_data
from common import write_volume_random_data

from common import create_and_wait_deployment
from common import wait_deployment_replica_ready

from common import create_and_wait_statefulset
from common import get_statefulset_pod_info
from common import update_statefulset_manifests

from common import check_pod_existence

from common import crash_engine_process_with_sigkill

from common import wait_for_backup_completion
from common import wait_for_backup_count
from common import wait_for_backup_to_start

from common import wait_for_snapshot_count

from common import check_recurring_jobs
from common import cleanup_all_recurring_jobs
from common import create_recurring_jobs
from common import update_recurring_job
from common import wait_for_recurring_jobs_cleanup

from common import wait_for_cron_job_count
from common import wait_for_cron_job_create
from common import wait_for_cron_job_delete

from common import JOB_LABEL
from common import KUBERNETES_STATUS_LABEL
from common import LONGHORN_NAMESPACE
from common import RETRY_BACKUP_COUNTS
from common import RETRY_BACKUP_INTERVAL
from common import SETTING_RECURRING_JOB_WHILE_VOLUME_DETACHED
from common import SIZE, Mi, Gi


RECURRING_JOB_LABEL = &#34;RecurringJob&#34;
RECURRING_JOB_NAME = &#34;recurring-test&#34;

NAME = &#34;name&#34;
ISGROUP = &#34;isGroup&#34;
TASK = &#34;task&#34;
GROUPS = &#34;groups&#34;
CRON = &#34;cron&#34;
RETAIN = &#34;retain&#34;
SNAPSHOT = &#34;snapshot&#34;
BACKUP = &#34;backup&#34;
CONCURRENCY = &#34;concurrency&#34;
LABELS = &#34;labels&#34;
DEFAULT = &#34;default&#34;
SCHEDULE_1MIN = &#34;* * * * *&#34;


def wait_until_begin_of_a_minute():
    while True:
        current_time = datetime.utcnow()
        if current_time.second == 0:
            break
        time.sleep(1)


def wait_until_begin_of_an_even_minute():
    while True:
        current_time = datetime.utcnow()
        if current_time.second == 0 and current_time.minute % 2 == 0:
            break
        time.sleep(1)


# wait for backup progress created by recurring job to
# exceed the minimum_progress percentage.
def wait_for_recurring_backup_to_start(client, core_api, volume_name, expected_snapshot_count, minimum_progress=0):  # NOQA
    job_pod_name = volume_name + &#39;-backup-c&#39;
    snapshot_name = &#39;&#39;
    snapshots = []
    check_pod_existence(core_api, job_pod_name, namespace=LONGHORN_NAMESPACE)

    # Find the snapshot which is being backed up
    for _ in range(RETRY_BACKUP_COUNTS):
        volume = client.by_id_volume(volume_name)
        try:
            snapshots = volume.snapshotList()

            assert len(snapshots) == expected_snapshot_count + 1
            for snapshot in snapshots:
                if snapshot.children[&#39;volume-head&#39;]:
                    snapshot_name = snapshot.name
                    break
            if len(snapshot_name) != 0:
                break
        except (AttributeError, ApiException, AssertionError):
            time.sleep(RETRY_BACKUP_INTERVAL)
    assert len(snapshot_name) != 0

    # To ensure the progress of backup
    wait_for_backup_to_start(client, volume_name,
                             snapshot_name=snapshot_name,
                             chk_progress=minimum_progress)

    return snapshot_name


@pytest.mark.recurring_job  # NOQA
def test_recurring_job(set_random_backupstore, client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario : test recurring job (S3/NFS)

    Given `snapshot1` recurring job created and cron at 1 min and retain 2.
          `backup1`   recurring job created and cron at 2 min and retain 1.
          `backup2`   recurring job created and cron at 1 min and retain 2.
    And a volume created and attached.

    When label volume with recurring job `snapshot1`.
         label volume with recurring job `backup1`.
    And wait until the 20th second since the beginning of an even minute.
    And write data to volume.
        wait for 2 minutes.
    And write data to volume.
        wait for 2 minutes.
    Then volume have 4 snapshots.
         (2 from `snapshot1`, 1 from `backup1`, 1 from `volume-head`)

    When label volume with recurring job `backup2`
    And write data to volume.
        wait for 2 minutes.
    And write data to volume.
        wait for 2 minutes.
    Then volume have 5 snapshots.
         (2 from `snapshot1`, 1 from `backup1`, 1 from `backup2`,
          1 from `volume-head`)

    When wait until backups complete.
    Then `backup1` completed 2 backups.
         `backup2` completed 3 backups.
    &#34;&#34;&#34;

    &#39;&#39;&#39;
    The timeline looks like this:
    0   1   2   3   4   5   6   7   8   9   10     (minute)
    |W  |   | W |   |   |W  |   | W |   |   |      (write data)
    |   S   |   S   |   |   S   |   S   |   |      (snapshot1)
    |   |   B   |   B   |   |   |   |   |   |      (backup1)
    |   |   |   |   |   |   B   B   B   |   |      (backup2)
    &#39;&#39;&#39;

    snap1 = SNAPSHOT + &#34;1&#34;
    back1 = BACKUP + &#34;1&#34;
    back2 = BACKUP + &#34;2&#34;
    recurring_jobs = {
        snap1: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
        back1: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
        back2: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=2)
    volume = wait_for_volume_detached(client, volume_name)
    volume = volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)
    volume.recurringJobAdd(name=snap1, isGroup=False)
    volume.recurringJobAdd(name=back1, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[snap1, back1],
                                         groups=[DEFAULT])

    wait_until_begin_of_an_even_minute()
    # wait until the 20th second of an even minute
    # make sure that snapshot job happens before the backup job
    time.sleep(20)

    write_volume_random_data(volume)
    time.sleep(60 * 2)
    write_volume_random_data(volume)
    time.sleep(60 * 2)

    wait_for_snapshot_count(volume, 4)

    volume.recurringJobAdd(name=back2, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[snap1, back1, back2],
                                         groups=[DEFAULT])

    write_volume_random_data(volume)
    time.sleep(60 * 2)
    write_volume_random_data(volume)
    time.sleep(60 * 2)

    # 2 from job_snap, 1 from job_backup, 1 from job_backup2, 1 volume-head
    wait_for_snapshot_count(volume, 5)

    complete_backup_1_count = 0
    complete_backup_2_count = 0
    volume = client.by_id_volume(volume_name)
    wait_for_backup_completion(client, volume_name)
    for b in volume.backupStatus:
        if &#34;backup1-&#34; in b.snapshot:
            complete_backup_1_count += 1
        elif &#34;backup2-&#34; in b.snapshot:
            complete_backup_2_count += 1

    # 2 completed backups from backup1
    # 2 or more completed backups from backup2
    # NOTE: NFS backup can be slow sometimes and error prone
    assert complete_backup_1_count == 2
    assert complete_backup_2_count &gt;= 2
    assert complete_backup_2_count &lt; 4


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_in_volume_creation(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test create volume with recurring jobs

    Given 2 recurring jobs created.
    And volume create and a attached.

    When label recurring job to volume.
    And write data to volume.
        wait 2.5 minutes.
    And write data to volume.
        wait 2.5 minutes.

    Then volume have 4 snapshots.
    &#34;&#34;&#34;
    recurring_jobs = {
        SNAPSHOT: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
        BACKUP: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    client.create_volume(name=volume_name, size=SIZE,
                         numberOfReplicas=2)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    volume.recurringJobAdd(name=SNAPSHOT, isGroup=False)
    volume.recurringJobAdd(name=BACKUP, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[SNAPSHOT, BACKUP],
                                         groups=[DEFAULT])

    wait_until_begin_of_an_even_minute()
    # wait until the 10th second of an even minute
    # to avoid writing data at the same time backup is taking
    time.sleep(10)

    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes
    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes

    wait_for_snapshot_count(volume, 4)


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_duplicated(client):  # NOQA
    &#34;&#34;&#34;
    Scenario: test create duplicated recurring jobs

    Given recurring job created.
    When create same recurring job again.
    Then should fail.
    &#34;&#34;&#34;
    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    with pytest.raises(Exception) as e:
        create_recurring_jobs(client, recurring_jobs)
    assert &#34;already exists&#34; in str(e.value)


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_in_storageclass(set_random_backupstore, client, core_api, storage_class, statefulset):  # NOQA
    &#34;&#34;&#34;
    Test create volume with StorageClass contains recurring jobs

    1. Create a StorageClass with recurring jobs
    2. Create a StatefulSet with PVC template and StorageClass
    3. Verify the recurring jobs run correctly.
    &#34;&#34;&#34;
    recurring_jobs = {
        SNAPSHOT: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
        BACKUP: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    recurring_job_selector = [
        {
            NAME: SNAPSHOT,
            ISGROUP: False,
        },
        {
            NAME: BACKUP,
            ISGROUP: False,
        },
    ]
    storage_class[&#34;parameters&#34;][&#34;recurringJobSelector&#34;] = \
        json.dumps(recurring_job_selector)
    create_storage_class(storage_class)

    # wait until the beginning of an even minute
    wait_until_begin_of_an_even_minute()

    statefulset_name = &#39;recurring-job-in-storageclass-test&#39;
    update_statefulset_manifests(statefulset, storage_class, statefulset_name)
    start_time = datetime.utcnow()
    create_and_wait_statefulset(statefulset)
    statefulset_creating_duration = datetime.utcnow() - start_time

    assert 150 &gt; statefulset_creating_duration.seconds

    # We want to write data exactly at the 150th second since the start_time
    time.sleep(150 - statefulset_creating_duration.seconds)

    pod_info = get_statefulset_pod_info(core_api, statefulset)
    volume_info = [p[&#39;pv_name&#39;] for p in pod_info]
    pod_names = [p[&#39;pod_name&#39;] for p in pod_info]

    # write random data to volume to trigger recurring snapshot and backup job
    volume_data_path = &#34;/data/test&#34;
    for pod_name in pod_names:
        write_pod_volume_random_data(core_api, pod_name, volume_data_path, 2)

    time.sleep(150)  # 2.5 minutes

    for volume_name in volume_info:  # NOQA
        volume = client.by_id_volume(volume_name)
        wait_for_snapshot_count(volume, 4)


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_labels(set_random_backupstore, client, random_labels, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test a recurring job with labels (S3/NFS)

    Given a recurring job created,
            with `default` in groups,
            with random labels.
    And volume created and attached.
    And write data to volume.

    When add another label to the recurring job.
    And write data to volume.
    And wait after scheduled time.

    Then should have 2 snapshots.
    And backup should have correct labels.
    &#34;&#34;&#34;
    recurring_job_labels_test(client, random_labels, volume_name)  # NOQA


def recurring_job_labels_test(client, labels, volume_name, size=SIZE, backing_image=&#34;&#34;):  # NOQA
    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: labels,
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    client.create_volume(name=volume_name, size=size,
                         numberOfReplicas=2, backingImage=backing_image)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    write_volume_random_data(volume)

    time.sleep(75)  # 1 minute 15 second
    labels[&#34;we-added-this-label&#34;] = &#34;definitely&#34;
    update_recurring_job(client, RECURRING_JOB_NAME,
                         recurring_jobs[RECURRING_JOB_NAME][GROUPS],
                         labels)
    write_volume_random_data(volume)

    time.sleep(135)  # 2 minute 15 second
    # 1 from Backup, 1 from Volume Head.
    wait_for_snapshot_count(volume, 2)

    # Verify the Labels on the actual Backup.
    bv = client.by_id_backupVolume(volume_name)
    wait_for_backup_count(bv, 1)

    backups = bv.backupList().data
    b = bv.backupGet(name=backups[0].name)
    for key, val in iter(labels.items()):
        assert b.labels.get(key) == val
    assert b.labels.get(RECURRING_JOB_LABEL) == RECURRING_JOB_NAME
    # One extra Label from RecurringJob.
    assert len(b.labels) == len(labels) + 1
    if backing_image:
        assert bv.backingImageName == backing_image
        assert bv.backingImageChecksum != &#34;&#34;
        assert b.volumeBackingImageName == backing_image


@pytest.mark.csi  # NOQA
@pytest.mark.recurring_job
def test_recurring_job_kubernetes_status(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurringJob properly backs up the KubernetesStatus (S3/NFS)

    Given volume created and detached.
    And PV from volume created and verified.

    When create backup recurring job to run every 2 minutes.
    And attach volume.
    And write some data to volume.
    And wait 5 minutes.

    Then volume have 2 snapshots.
         volume have 1 backup.
    And backup have the Kubernetes Status labels.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=SIZE, numberOfReplicas=2)
    volume = wait_for_volume_detached(client, volume_name)

    pv_name = &#34;pv-&#34; + volume_name
    create_pv_for_volume(client, core_api, volume, pv_name)
    ks = {
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;pvcName&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;lastPodRefAt&#39;: &#39;&#39;,
    }
    wait_volume_kubernetes_status(client, volume_name, ks)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    write_volume_random_data(volume)

    time.sleep(60 * 5)
    # 1 from Backup, 1 from Volume Head.
    wait_for_snapshot_count(volume, 2)

    # Verify the Labels on the actual Backup.
    bv = client.by_id_backupVolume(volume_name)
    backups = bv.backupList().data
    assert len(backups) == 1

    b = bv.backupGet(name=backups[0].name)
    status = json.loads(b.labels.get(KUBERNETES_STATUS_LABEL))
    assert b.labels.get(RECURRING_JOB_LABEL) == RECURRING_JOB_NAME
    assert status == {
        &#39;lastPodRefAt&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;pvcName&#39;: &#39;&#39;,
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;workloadsStatus&#39;: None
    }
    # Two Labels: KubernetesStatus and RecurringJob.
    assert len(b.labels) == 2


def test_recurring_jobs_maximum_retain(client, core_api, volume_name): # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring jobs&#39; maximum retain

    Given set a recurring job retain to 51.

    When create recurring job.
    Then should fail.

    When set recurring job retain to 50.
    And create recurring job.
    Then recurring job created with retain equals to 50.

    When update recurring job retain to 51.
    Then should fail.
    &#34;&#34;&#34;
    # set max total number of retain to exceed 50
    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 51,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    with pytest.raises(Exception) as e:
        create_recurring_jobs(client, recurring_jobs)
    assert &#34;Job Can\\&#39;t retain more than 50 snapshots&#34; in str(e.value)

    recurring_jobs[RECURRING_JOB_NAME][RETAIN] = 50
    create_recurring_jobs(client, recurring_jobs)
    recurring_job = client.by_id_recurring_job(RECURRING_JOB_NAME)
    assert recurring_job.retain == 50

    with pytest.raises(Exception) as e:
        update_recurring_job(client, RECURRING_JOB_NAME,
                             groups=[], labels={}, retain=51)
    assert &#34;Job Can\\&#39;t retain more than 50 snapshots&#34; in str(e.value)


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_detached_volume(client, batch_v1_beta_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job while volume is detached

    Given a volume created, and attached.
    And write some data to the volume.
    And detach the volume.

    When create a recurring job running at 1 minute interval,
            and with `default` in groups,
            and with `retain` set to `2`.
    And 1 cron job should be created.
    And wait for 2 minutes.
    And attach volume and wait until healthy

    Then the volume should have 1 snapshot

    When wait for 1 minute.
    Then then volume should have only 2 snapshots.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=SIZE)
    volume = wait_for_volume_detached(client, volume_name)

    self_host = get_self_host_id()
    volume.attach(hostId=self_host)
    volume = wait_for_volume_healthy(client, volume_name)
    write_volume_random_data(volume)
    volume.detach()

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 1)

    time.sleep(60 * 2)
    wait_until_begin_of_a_minute()
    time.sleep(5)
    volume.attach(hostId=self_host)
    volume = wait_for_volume_healthy(client, volume_name)
    wait_for_snapshot_count(volume, 1)

    time.sleep(60)
    wait_for_snapshot_count(volume, 2)


def test_recurring_jobs_allow_detached_volume(set_random_backupstore, client, core_api, apps_api, volume_name, make_deployment_with_pvc):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring jobs for detached volume with
    `allow-recurring-job-while-volume-detached` set to true

    Context: In the current Longhorn implementation, users cannot do recurring
             backup when volumes are detached.
             This feature gives the users an option to do recurring backup
             even when volumes are detached.
             longhorn/longhorn#1509

    Given `allow-recurring-job-while-volume-detached` set to `true`.
    And volume created and attached.
    And 50MB data written to volume.
    And volume detached.

    When a recurring job created runs every minute.
    And wait for backup to complete.

    Then volume have 1 backup in 2 minutes retry loop.

    When delete the recurring job.
    And create a PV from volume.
    And create a PVC from volume.
    And create a deployment from PVC.
    And write 400MB data to the volume from the pod.
    And scale deployment replicas to 0.
        wait until the volume is detached.
    And create a recurring job runs every 2 minutes.
    And wait for backup to start.
    And scale deployment replicas to 1.
    Then volume&#39;s frontend is disabled.
    And pod cannot start.

    When wait until backup complete.
    And delete the recurring job.
    Then pod can start in 10 minutes retry loop.
    &#34;&#34;&#34;
    common.update_setting(client,
                          SETTING_RECURRING_JOB_WHILE_VOLUME_DETACHED, &#34;true&#34;)

    volume = create_and_check_volume(client, volume_name, size=str(1 * Gi))
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume.name)

    data = {
        &#39;pos&#39;: 0,
        &#39;content&#39;: common.generate_random_data(50 * Mi),
    }
    common.write_volume_data(volume, data)

    # Give sometimes for data to flush to disk
    time.sleep(15)

    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume.name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    wait_for_backup_completion(client, volume.name)
    for _ in range(4):
        bv = client.by_id_backupVolume(volume.name)
        wait_for_backup_count(bv, 1)
        time.sleep(30)

    cleanup_all_recurring_jobs(client)

    pv_name = volume_name + &#34;-pv&#34;
    create_pv_for_volume(client, core_api, volume, pv_name)

    pvc_name = volume_name + &#34;-pvc&#34;
    create_pvc_for_volume(client, core_api, volume, pvc_name)

    deployment_name = volume_name + &#34;-dep&#34;
    deployment = make_deployment_with_pvc(deployment_name, pvc_name)
    create_and_wait_deployment(apps_api, deployment)

    size_mb = 400
    pod_names = common.get_deployment_pod_names(core_api, deployment)
    write_pod_volume_random_data(core_api, pod_names[0], &#34;/data/test&#34;,
                                 size_mb)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 0
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])

    volume = wait_for_volume_detached(client, volume.name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    wait_for_backup_to_start(client, volume.name)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 1
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])

    deployment_label_name = deployment[&#34;metadata&#34;][&#34;labels&#34;][&#34;name&#34;]
    common.wait_pod_auto_attach_after_first_backup_completion(
        client, core_api, volume.name, deployment_label_name)

    cleanup_all_recurring_jobs(client)

    pod_names = common.get_deployment_pod_names(core_api, deployment)
    common.wait_for_pod_phase(core_api, pod_names[0], pod_phase=&#34;Running&#34;)


def test_recurring_jobs_when_volume_detached_unexpectedly(
    set_random_backupstore, client, core_api, apps_api, volume_name, make_deployment_with_pvc):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring jobs when volume detached unexpectedly

    Context: If the volume is automatically attached by the recurring backup
             job, make sure that workload pod eventually is able to use the
             volume when volume is detached unexpectedly during the backup
             process.

    Given `allow-recurring-job-while-volume-detached` set to `true`.
    And volume created and detached.
    And PV created from volume.
    And PVC created from volume.
    And deployment created from PVC.
    And 500MB data written to the volume.
    And deployment replica scaled to 0.
    And volume detached.

    When create a backup recurring job runs every 2 minutes.
    And wait for backup to start.
        wait for backup progress &gt; 50%.
    And kill the engine process of the volume.
    Then volume is attached and healthy.

    When backup completed.
    Then volume is detached with `frontendDisabled=false`.

    When deployment replica scaled to 1.
    Then the data exist in the deployment pod.
    &#34;&#34;&#34;
    common.update_setting(client,
                          SETTING_RECURRING_JOB_WHILE_VOLUME_DETACHED, &#34;true&#34;)

    volume = create_and_check_volume(client, volume_name, size=str(1 * Gi))
    volume = wait_for_volume_detached(client, volume.name)

    pv_name = volume_name + &#34;-pv&#34;
    create_pv_for_volume(client, core_api, volume, pv_name)

    pvc_name = volume_name + &#34;-pvc&#34;
    create_pvc_for_volume(client, core_api, volume, pvc_name)

    deployment_name = volume_name + &#34;-dep&#34;
    deployment = make_deployment_with_pvc(deployment_name, pvc_name)
    create_and_wait_deployment(apps_api, deployment)

    size_mb = 500
    pod_names = common.get_deployment_pod_names(core_api, deployment)
    write_pod_volume_random_data(core_api, pod_names[0], &#34;/data/test&#34;,
                                 size_mb)
    data = read_volume_data(core_api, pod_names[0], &#39;default&#39;)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 0
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])
    volume = wait_for_volume_detached(client, volume_name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    volume.recurringJobAdd(name=RECURRING_JOB_NAME, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[RECURRING_JOB_NAME],
                                         groups=[DEFAULT])

    time.sleep(60)
    wait_for_recurring_backup_to_start(client, core_api, volume_name,
                                       expected_snapshot_count=1,
                                       minimum_progress=50)

    crash_engine_process_with_sigkill(client, core_api, volume_name)
    time.sleep(10)
    wait_for_volume_healthy_no_frontend(client, volume_name)

    # Since the backup state is removed after the backup complete and it
    # could happen quickly. Checking for the both in-progress and complete
    # state could be hard to catch, thus we only check the complete state
    wait_for_backup_completion(client, volume_name)

    wait_for_volume_detached(client, volume_name)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 1
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])
    wait_deployment_replica_ready(apps_api, deployment[&#34;metadata&#34;][&#34;name&#34;], 1)
    pod_names = common.get_deployment_pod_names(core_api, deployment)
    assert read_volume_data(core_api, pod_names[0], &#39;default&#39;) == data

    # Use fixture to cleanup the backupstore and since we
    # crashed the engine replica initiated the backup, it&#39;s
    # backupstore lock will still be present, so we need
    # to wait till the lock is expired, before we can delete
    # the backups
    volume.recurringJobDelete(name=RECURRING_JOB_NAME, isGroup=False)
    backupstore.backupstore_wait_for_lock_expiration()


@pytest.mark.skip(reason=&#34;TODO&#34;)
def test_recurring_jobs_on_nodes_with_taints():  # NOQA
    &#34;&#34;&#34;
    Test recurring jobs on nodes with taints

    Context:

    Test the prevention of creation of multiple pods due to
    recurring job&#39;s pod being rejected by Taint controller
    on nodes with taints

    Steps:

    1. Set taint toleration for Longhorn components
       `persistence=true:NoExecute`
    2. Taint `node-1` with `persistence=true:NoExecute`
    3. Create a volume, vol-1.
       Attach vol-1 to node-1
       Write some data to vol-1
    4. Create a recurring backup job which:
       Has retain count 10
       Runs every minute
    5. Wait for 3 minutes.
       Verify that the there is 1 backup created
       Verify that the total number of pod in longhorn-system namespace &lt; 50
       Verify that the number of pods of the cronjob is &lt;= 2

    6. Taint all nodes with `persistence=true:NoExecute`
    7. Write some data to vol-1
    8. Wait for 3 minutes.
       Verify that the there are 2 backups created in total
       Verify that the total number of pod in longhorn-system namespace &lt; 50
       Verify that the number of pods of the cronjob is &lt;= 2

    9. Remove `persistence=true:NoExecute` from all nodes and Longhorn setting
       Clean up backups, volumes
    &#34;&#34;&#34;
    pass


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_groups(set_random_backupstore, client, batch_v1_beta_api):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job groups (S3/NFS)

    Given volume `test-job-1` created, attached, and healthy.
          volume `test-job-2` created, attached, and healthy.
    And create `snapshot` recurring job with `group-1, group-2` in groups.
            set cron job to run every 2 minutes.
            set retain to 1.
        create `backup`   recurring job with `group-1`          in groups.
            set cron job to run every 3 minutes.
            set retain to 1

    When set `group1` recurring job in volume `test-job-1` label.
         set `group2` recurring job in volume `test-job-2` label.
    And write some data to volume `test-job-1`.
        write some data to volume `test-job-2`.
    And wait for 2 minutes.
    And write some data to volume `test-job-1`.
        write some data to volume `test-job-2`.
    And wait for 1 minute.

    Then volume `test-job-1` should have 3 snapshots after scheduled time.
         volume `test-job-2` should have 2 snapshots after scheduled time.
     And volume `test-job-1` should have 1 backup after scheduled time.
         volume `test-job-2` should have 0 backup after scheduled time.
    &#34;&#34;&#34;
    volume1_name = &#34;test-job-1&#34;
    volume2_name = &#34;test-job-2&#34;
    client.create_volume(name=volume1_name, size=SIZE)
    client.create_volume(name=volume2_name, size=SIZE)
    volume1 = wait_for_volume_detached(client, volume1_name)
    volume2 = wait_for_volume_detached(client, volume2_name)

    self_id = get_self_host_id()
    volume1.attach(hostId=self_id)
    volume2.attach(hostId=self_id)
    volume1 = wait_for_volume_healthy(client, volume1_name)
    volume2 = wait_for_volume_healthy(client, volume2_name)

    group1 = &#34;group-1&#34;
    group2 = &#34;group-2&#34;
    recurring_jobs = {
        SNAPSHOT: {
            TASK: SNAPSHOT,
            GROUPS: [group1, group2],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        BACKUP: {
            TASK: BACKUP,
            GROUPS: [group1],
            CRON: &#34;*/3 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    volume1.recurringJobAdd(name=group1, isGroup=True)
    volume2.recurringJobAdd(name=group2, isGroup=True)

    wait_for_cron_job_count(batch_v1_beta_api, 2)

    wait_until_begin_of_a_minute()
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60 * 2)
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_snapshot_count(volume1, 3)  # volume-head,snapshot,backup-snapshot
    wait_for_snapshot_count(volume2, 2)  # volume-head,snapshot

    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 1)
    backup_created = True
    try:
        wait_for_backup_count(client.by_id_backupVolume(volume2_name), 1,
                              retry_counts=60)
    except AssertionError:
        backup_created = False
    assert not backup_created


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_default(client, batch_v1_beta_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job set with default in groups

    Given 1 volume created, attached, and healthy.

    # Setting recurring job in volume label should not remove the defaults.
    When set `snapshot` recurring job in volume label.
    Then should contain `default`  job-group in volume labels.
         should contain `snapshot` job       in volume labels.

    # Should be able to remove the default label.
    When delete recurring job-group `default` in volume label.
    Then volume should have     `snapshot`  job   in job label.
         volume should not have `default`   group in job label.

    # Remove all volume recurring job labels should bring in default
    When delete all recurring jobs in volume label.
    Then volume should not have `snapshot`  job   in job label.
         volume should     have `deault`    group in job label.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=SIZE)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    # Setting recurring job in volume label should not remove the defaults.
    volume.recurringJobAdd(name=SNAPSHOT, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[SNAPSHOT], groups=[DEFAULT])

    # Should be able to remove the default label.
    volume.recurringJobDelete(name=DEFAULT, isGroup=True)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[SNAPSHOT], groups=[])

    # Remove all volume recurring job labels should bring in default
    volume.recurringJobDelete(name=SNAPSHOT, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[], groups=[DEFAULT])


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_delete(client, batch_v1_beta_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test delete recurring job

    Given 1 volume created, attached, and healthy.

    When create `snapshot1` recurring job with `default, group-1` in groups.
         create `snapshot2` recurring job with `default`          in groups..
         create `snapshot3` recurring job with ``                 in groups.
         create `backup1`   recurring job with `default, group-1` in groups.
         create `backup2`   recurring job with `default`          in groups.
         create `backup3`   recurring job with ``                 in groups.
    Then default `snapshot1` cron job should exist.
         default `snapshot2` cron job should exist.
                 `snapshot3` cron job should exist.
         default `backup1`   cron job should exist.
         default `backup2`   cron job should exist.
                 `backup3`   cron job should exist.

    # Delete `snapshot2` recurring job should delete the cron job
    When delete `snapshot-2` recurring job.
    Then default `snapshot1` cron job should     exist.
         default `snapshot2` cron job should not exist.
                 `snapshot3` cron job should     exist.
         default `backup1`   cron job should     exist.
         default `backup2`   cron job should     exist.
                 `backup3`   cron job should     exist.

    # Delete multiple recurring jobs should reflect on the cron jobs.
    When delete `backup-1` recurring job.
         delete `backup-2` recurring job.
         delete `backup-3` recurring job.
    Then default `snapshot1` cron job should     exist.
         default `snapshot2` cron job should not exist.
                 `snapshot3` cron job should     exist.
         default `backup1`   cron job should not exist.
         default `backup2`   cron job should not exist.
                 `backup3`   cron job should not exist.

     # Should be able to delete recurring job while existing in volume label
     When add `snapshot1` recurring job to volume label.
          add `snapshot3` recurring job to volume label.
     And default `snapshot1` cron job should     exist.
         default `snapshot2` cron job should not exist.
                 `snapshot3` cron job should     exist.
     And delete `snapshot1` recurring job.
         delete `snapshot3` recurring job.
     Then default `snapshot1` cron job should not exist.
          default `snapshot2` cron job should not exist.
                  `snapshot3` cron job should not exist.
     And `snapshot1` job should exist in volume recurring job label.
         `snapshot2` job should exist in volume recurring job label.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=SIZE)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    snap1 = SNAPSHOT + &#34;1&#34;
    snap2 = SNAPSHOT + &#34;2&#34;
    snap3 = SNAPSHOT + &#34;3&#34;
    back1 = BACKUP + &#34;1&#34;
    back2 = BACKUP + &#34;2&#34;
    back3 = BACKUP + &#34;3&#34;
    group1 = &#34;group-1&#34;
    recurring_jobs = {
        snap1: {
            TASK: SNAPSHOT,
            GROUPS: [DEFAULT, group1],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        snap2: {
            TASK: SNAPSHOT,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        snap3: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        back1: {
            TASK: BACKUP,
            GROUPS: [DEFAULT, group1],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        back2: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        back3: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 6)

    # snapshot
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap1)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap3)
    # backup
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back1)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back3)

    # Delete `snapshot2` recurring job should delete the cron job
    snap2_recurring_job = client.by_id_recurring_job(snap2)
    client.delete(snap2_recurring_job)
    wait_for_cron_job_count(batch_v1_beta_api, 5)
    # snapshot
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap1)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap3)
    # backup
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back1)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back3)

    # Delete multiple recurring jobs should reflect on the cron jobs.
    back1_recurring_job = client.by_id_recurring_job(back1)
    back2_recurring_job = client.by_id_recurring_job(back2)
    back3_recurring_job = client.by_id_recurring_job(back3)
    client.delete(back1_recurring_job)
    client.delete(back2_recurring_job)
    client.delete(back3_recurring_job)
    wait_for_cron_job_count(batch_v1_beta_api, 2)
    # snapshot
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap1)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap3)
    # backup
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back1)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back2)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back3)

    # Should be able to delete recurring job while existing in volume label
    volume.recurringJobAdd(name=snap1, isGroup=False)
    volume.recurringJobAdd(name=snap3, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[snap1, snap3], groups=[DEFAULT])
    wait_for_cron_job_count(batch_v1_beta_api, 2)
    # snapshot
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap1)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap3)

    snap1_recurring_job = client.by_id_recurring_job(snap1)
    snap3_recurring_job = client.by_id_recurring_job(snap3)
    client.delete(snap1_recurring_job)
    client.delete(snap3_recurring_job)
    wait_for_cron_job_count(batch_v1_beta_api, 0)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[snap1, snap3], groups=[DEFAULT])


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_volume_labeled_none_existing_recurring_job(client, batch_v1_beta_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test volume with a none-existing recurring job label
              and later on added back.

    Given create `snapshot` recurring job.
          create `backup`   recurring job.
    And 1 volume created, attached, and healthy.
    And add `snapshot` recurring job to volume label.
        add `backup`   recurring job to volume label.
    And `snapshot1` cron job exist.
        `backup1`   cron job exist.

    When delete `snapshot` recurring job.
         delete `backup`   recurring job.
    Then `snapshot` cron job should not exist.
         `backup`   cron job should not exist.
    And `snapshot` job  should exist in volume recurring job label.
        `backup`   job  should exist in volume recurring job label.
        `default` group should exist in volume recurring job label.

    # Add back the recurring jobs.
    When create `snapshot` recurring job.
         create `backup`   recurring job.
    Then `snapshot` cron job should exist.
         `backup`   cron job should exist.
    &#34;&#34;&#34;
    recurring_jobs = {
        SNAPSHOT: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        BACKUP: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    client.create_volume(name=volume_name, size=SIZE)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)
    volume.recurringJobAdd(name=SNAPSHOT, isGroup=False)
    volume.recurringJobAdd(name=BACKUP, isGroup=False)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+SNAPSHOT)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+BACKUP)

    snap1_recurring_job = client.by_id_recurring_job(SNAPSHOT)
    back1_recurring_job = client.by_id_recurring_job(BACKUP)
    client.delete(snap1_recurring_job)
    client.delete(back1_recurring_job)
    wait_for_cron_job_count(batch_v1_beta_api, 0)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[SNAPSHOT, BACKUP],
                                         groups=[DEFAULT])
    wait_for_recurring_jobs_cleanup(client)

    # Add back the recurring jobs.
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+SNAPSHOT)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+BACKUP)


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_multiple_volumes(set_random_backupstore, client, batch_v1_beta_api):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job with multiple volumes

    Given volume `test-job-1` created, attached and healthy.
    And create `backup1`   recurring job with `default` in groups.
        create `backup2`   recurring job with ``        in groups.
    And `default` group exist in `test-job-1` volume recurring job label.
    And `backup1` cron job exist.
        `backup2` cron job exist.
    And write data to  `test-job-1` volume.
    And 2 snapshot exist in `test-job-1` volume.
    And 1 backup   exist in `test-job-1` volume.

    When create and attach volume `test-job-2`.
         wait for volume `test-job-2` to be healthy.
    And `default` group exist in `test-job-2` volume recurring job label.
    And write data to  `test-job-1` volume.
    Then 2 snapshot exist in `test-job-2` volume.
         1 backup   exist in `test-job-2` volume.

    When add `backup2` in `test-job-2` volume label.
    And `default` group exist in `test-job-1` volume recurring job label.
        `default` group exist in `test-job-2` volume recurring job label.
        `backup2` group exist in `test-job-2` volume recurring job label.
    And write data to `test-job-1`.
        write data to `test-job-2`.
    Then wait for schedule time.
    And 2 backup exist in `test-job-2` volume.
        1 backup exist in `test-job-1` volume.
    &#34;&#34;&#34;
    volume1_name = &#34;test-job-1&#34;
    client.create_volume(name=volume1_name, size=SIZE)
    volume1 = wait_for_volume_detached(client, volume1_name)
    volume1.attach(hostId=get_self_host_id())
    volume1 = wait_for_volume_healthy(client, volume1_name)

    back1 = BACKUP + &#34;1&#34;
    back2 = BACKUP + &#34;2&#34;
    recurring_jobs = {
        back1: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        back2: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_volume_recurring_job_update(volume1, jobs=[], groups=[DEFAULT])
    wait_for_cron_job_count(batch_v1_beta_api, 2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back1)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back2)

    write_volume_random_data(volume1)
    wait_for_snapshot_count(volume1, 2)
    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 1)

    volume2_name = &#34;test-job-2&#34;
    client.create_volume(name=volume2_name, size=SIZE)
    volume2 = wait_for_volume_detached(client, volume2_name)
    volume2.attach(hostId=get_self_host_id())
    volume2 = wait_for_volume_healthy(client, volume2_name)
    wait_for_volume_recurring_job_update(volume2, jobs=[], groups=[DEFAULT])

    write_volume_random_data(volume2)
    wait_for_snapshot_count(volume2, 2)
    wait_for_backup_count(client.by_id_backupVolume(volume2_name), 1)

    volume2.recurringJobAdd(name=back2, isGroup=False)
    wait_for_volume_recurring_job_update(volume1,
                                         jobs=[], groups=[DEFAULT])
    wait_for_volume_recurring_job_update(volume2,
                                         jobs=[back2], groups=[DEFAULT])

    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(70)
    wait_for_backup_count(client.by_id_backupVolume(volume2_name), 2)
    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 1)


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_snapshot(client, batch_v1_beta_api):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job snapshot

    Given volume `test-job-1` created, attached, and healthy.
          volume `test-job-2` created, attached, and healthy.

    When create a recurring job with `default` in groups.
    Then should have 1 cron job.
    And volume `test-job-1` should have volume-head 1 snapshot.
        volume `test-job-2` should have volume-head 1 snapshot.

    When write some data to volume `test-job-1`.
         write some data to volume `test-job-2`.
    And wait for cron job scheduled time.
    Then volume `test-job-1` should have 2 snapshots after scheduled time.
         volume `test-job-2` should have 2 snapshots after scheduled time.

    When write some data to volume `test-job-1`.
         write some data to volume `test-job-2`.
    And wait for cron job scheduled time.
    Then volume `test-job-1` should have 3 snapshots after scheduled time.
         volume `test-job-2` should have 3 snapshots after scheduled time.
    &#34;&#34;&#34;
    volume1_name = &#34;test-job-1&#34;
    volume2_name = &#34;test-job-2&#34;
    client.create_volume(name=volume1_name, size=SIZE)
    client.create_volume(name=volume2_name, size=SIZE)
    volume1 = wait_for_volume_detached(client, volume1_name)
    volume2 = wait_for_volume_detached(client, volume2_name)

    self_host = get_self_host_id()

    volume1.attach(hostId=self_host)
    volume2.attach(hostId=self_host)
    volume1 = wait_for_volume_healthy(client, volume1_name)
    volume2 = wait_for_volume_healthy(client, volume2_name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: SNAPSHOT,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 1)

    # volume-head
    wait_for_snapshot_count(volume1, 1)
    wait_for_snapshot_count(volume2, 1)

    # 1st job
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_snapshot_count(volume1, 2)
    wait_for_snapshot_count(volume2, 2)

    # 2nd job
    # wait_until_begin_of_a_minute()
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_snapshot_count(volume1, 3)
    wait_for_snapshot_count(volume2, 3)


@pytest.mark.recurring_job  # NOQA
def test_recurring_job_backup(set_random_backupstore, client, batch_v1_beta_api):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job backup (S3/NFS)

    Given volume `test-job-1` created, attached, and healthy.
          volume `test-job-2` created, attached, and healthy.

    When create a recurring job with `default` in groups.
    Then should have 1 cron job.

    When write some data to volume `test-job-1`.
         write some data to volume `test-job-2`.
    And wait for `backup1` cron job scheduled time.
    Then volume `test-job-1` should have 1 backups.
         volume `test-job-2` should have 1 backups.

    When write some data to volume `test-job-1`.
         write some data to volume `test-job-2`.
    And wait for `backup1` cron job scheduled time.
    Then volume `test-job-1` should have 2 backups.
         volume `test-job-2` should have 2 backups.
    &#34;&#34;&#34;
    volume1_name = &#34;test-job-1&#34;
    volume2_name = &#34;test-job-2&#34;
    client.create_volume(name=volume1_name, size=SIZE)
    client.create_volume(name=volume2_name, size=SIZE)
    volume1 = wait_for_volume_detached(client, volume1_name)
    volume2 = wait_for_volume_detached(client, volume2_name)

    self_host = get_self_host_id()
    volume1.attach(hostId=self_host)
    volume2.attach(hostId=self_host)
    volume1 = wait_for_volume_healthy(client, volume1_name)
    volume2 = wait_for_volume_healthy(client, volume2_name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 1)

    # 1st job
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 1)
    wait_for_backup_count(client.by_id_backupVolume(volume2_name), 1)

    # 2nd job
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 2)
    wait_for_backup_count(client.by_id_backupVolume(volume2_name), 2)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_recurring_job.recurring_job_labels_test"><code class="name flex">
<span>def <span class="ident">recurring_job_labels_test</span></span>(<span>client, labels, volume_name, size='16777216', backing_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recurring_job_labels_test(client, labels, volume_name, size=SIZE, backing_image=&#34;&#34;):  # NOQA
    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: labels,
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    client.create_volume(name=volume_name, size=size,
                         numberOfReplicas=2, backingImage=backing_image)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    write_volume_random_data(volume)

    time.sleep(75)  # 1 minute 15 second
    labels[&#34;we-added-this-label&#34;] = &#34;definitely&#34;
    update_recurring_job(client, RECURRING_JOB_NAME,
                         recurring_jobs[RECURRING_JOB_NAME][GROUPS],
                         labels)
    write_volume_random_data(volume)

    time.sleep(135)  # 2 minute 15 second
    # 1 from Backup, 1 from Volume Head.
    wait_for_snapshot_count(volume, 2)

    # Verify the Labels on the actual Backup.
    bv = client.by_id_backupVolume(volume_name)
    wait_for_backup_count(bv, 1)

    backups = bv.backupList().data
    b = bv.backupGet(name=backups[0].name)
    for key, val in iter(labels.items()):
        assert b.labels.get(key) == val
    assert b.labels.get(RECURRING_JOB_LABEL) == RECURRING_JOB_NAME
    # One extra Label from RecurringJob.
    assert len(b.labels) == len(labels) + 1
    if backing_image:
        assert bv.backingImageName == backing_image
        assert bv.backingImageChecksum != &#34;&#34;
        assert b.volumeBackingImageName == backing_image</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job"><code class="name flex">
<span>def <span class="ident">test_recurring_job</span></span>(<span>set_random_backupstore, client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario : test recurring job (S3/NFS)</p>
<p>Given <code>snapshot1</code> recurring job created and cron at 1 min and retain 2.
<code>backup1</code>
recurring job created and cron at 2 min and retain 1.
<code>backup2</code>
recurring job created and cron at 1 min and retain 2.
And a volume created and attached.</p>
<p>When label volume with recurring job <code>snapshot1</code>.
label volume with recurring job <code>backup1</code>.
And wait until the 20th second since the beginning of an even minute.
And write data to volume.
wait for 2 minutes.
And write data to volume.
wait for 2 minutes.
Then volume have 4 snapshots.
(2 from <code>snapshot1</code>, 1 from <code>backup1</code>, 1 from <code>volume-head</code>)</p>
<p>When label volume with recurring job <code>backup2</code>
And write data to volume.
wait for 2 minutes.
And write data to volume.
wait for 2 minutes.
Then volume have 5 snapshots.
(2 from <code>snapshot1</code>, 1 from <code>backup1</code>, 1 from <code>backup2</code>,
1 from <code>volume-head</code>)</p>
<p>When wait until backups complete.
Then <code>backup1</code> completed 2 backups.
<code>backup2</code> completed 3 backups.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job(set_random_backupstore, client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario : test recurring job (S3/NFS)

    Given `snapshot1` recurring job created and cron at 1 min and retain 2.
          `backup1`   recurring job created and cron at 2 min and retain 1.
          `backup2`   recurring job created and cron at 1 min and retain 2.
    And a volume created and attached.

    When label volume with recurring job `snapshot1`.
         label volume with recurring job `backup1`.
    And wait until the 20th second since the beginning of an even minute.
    And write data to volume.
        wait for 2 minutes.
    And write data to volume.
        wait for 2 minutes.
    Then volume have 4 snapshots.
         (2 from `snapshot1`, 1 from `backup1`, 1 from `volume-head`)

    When label volume with recurring job `backup2`
    And write data to volume.
        wait for 2 minutes.
    And write data to volume.
        wait for 2 minutes.
    Then volume have 5 snapshots.
         (2 from `snapshot1`, 1 from `backup1`, 1 from `backup2`,
          1 from `volume-head`)

    When wait until backups complete.
    Then `backup1` completed 2 backups.
         `backup2` completed 3 backups.
    &#34;&#34;&#34;

    &#39;&#39;&#39;
    The timeline looks like this:
    0   1   2   3   4   5   6   7   8   9   10     (minute)
    |W  |   | W |   |   |W  |   | W |   |   |      (write data)
    |   S   |   S   |   |   S   |   S   |   |      (snapshot1)
    |   |   B   |   B   |   |   |   |   |   |      (backup1)
    |   |   |   |   |   |   B   B   B   |   |      (backup2)
    &#39;&#39;&#39;

    snap1 = SNAPSHOT + &#34;1&#34;
    back1 = BACKUP + &#34;1&#34;
    back2 = BACKUP + &#34;2&#34;
    recurring_jobs = {
        snap1: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
        back1: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
        back2: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=2)
    volume = wait_for_volume_detached(client, volume_name)
    volume = volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)
    volume.recurringJobAdd(name=snap1, isGroup=False)
    volume.recurringJobAdd(name=back1, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[snap1, back1],
                                         groups=[DEFAULT])

    wait_until_begin_of_an_even_minute()
    # wait until the 20th second of an even minute
    # make sure that snapshot job happens before the backup job
    time.sleep(20)

    write_volume_random_data(volume)
    time.sleep(60 * 2)
    write_volume_random_data(volume)
    time.sleep(60 * 2)

    wait_for_snapshot_count(volume, 4)

    volume.recurringJobAdd(name=back2, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[snap1, back1, back2],
                                         groups=[DEFAULT])

    write_volume_random_data(volume)
    time.sleep(60 * 2)
    write_volume_random_data(volume)
    time.sleep(60 * 2)

    # 2 from job_snap, 1 from job_backup, 1 from job_backup2, 1 volume-head
    wait_for_snapshot_count(volume, 5)

    complete_backup_1_count = 0
    complete_backup_2_count = 0
    volume = client.by_id_volume(volume_name)
    wait_for_backup_completion(client, volume_name)
    for b in volume.backupStatus:
        if &#34;backup1-&#34; in b.snapshot:
            complete_backup_1_count += 1
        elif &#34;backup2-&#34; in b.snapshot:
            complete_backup_2_count += 1

    # 2 completed backups from backup1
    # 2 or more completed backups from backup2
    # NOTE: NFS backup can be slow sometimes and error prone
    assert complete_backup_1_count == 2
    assert complete_backup_2_count &gt;= 2
    assert complete_backup_2_count &lt; 4</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_backup"><code class="name flex">
<span>def <span class="ident">test_recurring_job_backup</span></span>(<span>set_random_backupstore, client, batch_v1_beta_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring job backup (S3/NFS)</p>
<p>Given volume <code>test-job-1</code> created, attached, and healthy.
volume <code>test-job-2</code> created, attached, and healthy.</p>
<p>When create a recurring job with <code>default</code> in groups.
Then should have 1 cron job.</p>
<p>When write some data to volume <code>test-job-1</code>.
write some data to volume <code>test-job-2</code>.
And wait for <code>backup1</code> cron job scheduled time.
Then volume <code>test-job-1</code> should have 1 backups.
volume <code>test-job-2</code> should have 1 backups.</p>
<p>When write some data to volume <code>test-job-1</code>.
write some data to volume <code>test-job-2</code>.
And wait for <code>backup1</code> cron job scheduled time.
Then volume <code>test-job-1</code> should have 2 backups.
volume <code>test-job-2</code> should have 2 backups.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_backup(set_random_backupstore, client, batch_v1_beta_api):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job backup (S3/NFS)

    Given volume `test-job-1` created, attached, and healthy.
          volume `test-job-2` created, attached, and healthy.

    When create a recurring job with `default` in groups.
    Then should have 1 cron job.

    When write some data to volume `test-job-1`.
         write some data to volume `test-job-2`.
    And wait for `backup1` cron job scheduled time.
    Then volume `test-job-1` should have 1 backups.
         volume `test-job-2` should have 1 backups.

    When write some data to volume `test-job-1`.
         write some data to volume `test-job-2`.
    And wait for `backup1` cron job scheduled time.
    Then volume `test-job-1` should have 2 backups.
         volume `test-job-2` should have 2 backups.
    &#34;&#34;&#34;
    volume1_name = &#34;test-job-1&#34;
    volume2_name = &#34;test-job-2&#34;
    client.create_volume(name=volume1_name, size=SIZE)
    client.create_volume(name=volume2_name, size=SIZE)
    volume1 = wait_for_volume_detached(client, volume1_name)
    volume2 = wait_for_volume_detached(client, volume2_name)

    self_host = get_self_host_id()
    volume1.attach(hostId=self_host)
    volume2.attach(hostId=self_host)
    volume1 = wait_for_volume_healthy(client, volume1_name)
    volume2 = wait_for_volume_healthy(client, volume2_name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 1)

    # 1st job
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 1)
    wait_for_backup_count(client.by_id_backupVolume(volume2_name), 1)

    # 2nd job
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 2)
    wait_for_backup_count(client.by_id_backupVolume(volume2_name), 2)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_default"><code class="name flex">
<span>def <span class="ident">test_recurring_job_default</span></span>(<span>client, batch_v1_beta_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring job set with default in groups</p>
<p>Given 1 volume created, attached, and healthy.</p>
<h1 id="setting-recurring-job-in-volume-label-should-not-remove-the-defaults">Setting recurring job in volume label should not remove the defaults.</h1>
<p>When set <code>snapshot</code> recurring job in volume label.
Then should contain <code>default</code>
job-group in volume labels.
should contain <code>snapshot</code> job
in volume labels.</p>
<h1 id="should-be-able-to-remove-the-default-label">Should be able to remove the default label.</h1>
<p>When delete recurring job-group <code>default</code> in volume label.
Then volume should have
<code>snapshot</code>
job
in job label.
volume should not have <code>default</code>
group in job label.</p>
<h1 id="remove-all-volume-recurring-job-labels-should-bring-in-default">Remove all volume recurring job labels should bring in default</h1>
<p>When delete all recurring jobs in volume label.
Then volume should not have <code>snapshot</code>
job
in job label.
volume should
have <code>deault</code>
group in job label.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_default(client, batch_v1_beta_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job set with default in groups

    Given 1 volume created, attached, and healthy.

    # Setting recurring job in volume label should not remove the defaults.
    When set `snapshot` recurring job in volume label.
    Then should contain `default`  job-group in volume labels.
         should contain `snapshot` job       in volume labels.

    # Should be able to remove the default label.
    When delete recurring job-group `default` in volume label.
    Then volume should have     `snapshot`  job   in job label.
         volume should not have `default`   group in job label.

    # Remove all volume recurring job labels should bring in default
    When delete all recurring jobs in volume label.
    Then volume should not have `snapshot`  job   in job label.
         volume should     have `deault`    group in job label.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=SIZE)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    # Setting recurring job in volume label should not remove the defaults.
    volume.recurringJobAdd(name=SNAPSHOT, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[SNAPSHOT], groups=[DEFAULT])

    # Should be able to remove the default label.
    volume.recurringJobDelete(name=DEFAULT, isGroup=True)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[SNAPSHOT], groups=[])

    # Remove all volume recurring job labels should bring in default
    volume.recurringJobDelete(name=SNAPSHOT, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[], groups=[DEFAULT])</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_delete"><code class="name flex">
<span>def <span class="ident">test_recurring_job_delete</span></span>(<span>client, batch_v1_beta_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test delete recurring job</p>
<p>Given 1 volume created, attached, and healthy.</p>
<p>When create <code>snapshot1</code> recurring job with <code>default, group-1</code> in groups.
create <code>snapshot2</code> recurring job with <code>default</code>
in groups..
create <code>snapshot3</code> recurring job with <code>in groups.
create &lt;code&gt;backup1&lt;/code&gt;
recurring job with `default, group-1` in groups.
create &lt;code&gt;backup2&lt;/code&gt;
recurring job with &lt;code&gt;default&lt;/code&gt;
in groups.
create &lt;code&gt;backup3&lt;/code&gt;
recurring job with</code>
in groups.
Then default <code>snapshot1</code> cron job should exist.
default <code>snapshot2</code> cron job should exist.
<code>snapshot3</code> cron job should exist.
default <code>backup1</code>
cron job should exist.
default <code>backup2</code>
cron job should exist.
<code>backup3</code>
cron job should exist.</p>
<h1 id="delete-snapshot2-recurring-job-should-delete-the-cron-job">Delete <code>snapshot2</code> recurring job should delete the cron job</h1>
<p>When delete <code>snapshot-2</code> recurring job.
Then default <code>snapshot1</code> cron job should
exist.
default <code>snapshot2</code> cron job should not exist.
<code>snapshot3</code> cron job should
exist.
default <code>backup1</code>
cron job should
exist.
default <code>backup2</code>
cron job should
exist.
<code>backup3</code>
cron job should
exist.</p>
<h1 id="delete-multiple-recurring-jobs-should-reflect-on-the-cron-jobs">Delete multiple recurring jobs should reflect on the cron jobs.</h1>
<p>When delete <code>backup-1</code> recurring job.
delete <code>backup-2</code> recurring job.
delete <code>backup-3</code> recurring job.
Then default <code>snapshot1</code> cron job should
exist.
default <code>snapshot2</code> cron job should not exist.
<code>snapshot3</code> cron job should
exist.
default <code>backup1</code>
cron job should not exist.
default <code>backup2</code>
cron job should not exist.
<code>backup3</code>
cron job should not exist.</p>
<p># Should be able to delete recurring job while existing in volume label
When add <code>snapshot1</code> recurring job to volume label.
add <code>snapshot3</code> recurring job to volume label.
And default <code>snapshot1</code> cron job should
exist.
default <code>snapshot2</code> cron job should not exist.
<code>snapshot3</code> cron job should
exist.
And delete <code>snapshot1</code> recurring job.
delete <code>snapshot3</code> recurring job.
Then default <code>snapshot1</code> cron job should not exist.
default <code>snapshot2</code> cron job should not exist.
<code>snapshot3</code> cron job should not exist.
And <code>snapshot1</code> job should exist in volume recurring job label.
<code>snapshot2</code> job should exist in volume recurring job label.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_delete(client, batch_v1_beta_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test delete recurring job

    Given 1 volume created, attached, and healthy.

    When create `snapshot1` recurring job with `default, group-1` in groups.
         create `snapshot2` recurring job with `default`          in groups..
         create `snapshot3` recurring job with ``                 in groups.
         create `backup1`   recurring job with `default, group-1` in groups.
         create `backup2`   recurring job with `default`          in groups.
         create `backup3`   recurring job with ``                 in groups.
    Then default `snapshot1` cron job should exist.
         default `snapshot2` cron job should exist.
                 `snapshot3` cron job should exist.
         default `backup1`   cron job should exist.
         default `backup2`   cron job should exist.
                 `backup3`   cron job should exist.

    # Delete `snapshot2` recurring job should delete the cron job
    When delete `snapshot-2` recurring job.
    Then default `snapshot1` cron job should     exist.
         default `snapshot2` cron job should not exist.
                 `snapshot3` cron job should     exist.
         default `backup1`   cron job should     exist.
         default `backup2`   cron job should     exist.
                 `backup3`   cron job should     exist.

    # Delete multiple recurring jobs should reflect on the cron jobs.
    When delete `backup-1` recurring job.
         delete `backup-2` recurring job.
         delete `backup-3` recurring job.
    Then default `snapshot1` cron job should     exist.
         default `snapshot2` cron job should not exist.
                 `snapshot3` cron job should     exist.
         default `backup1`   cron job should not exist.
         default `backup2`   cron job should not exist.
                 `backup3`   cron job should not exist.

     # Should be able to delete recurring job while existing in volume label
     When add `snapshot1` recurring job to volume label.
          add `snapshot3` recurring job to volume label.
     And default `snapshot1` cron job should     exist.
         default `snapshot2` cron job should not exist.
                 `snapshot3` cron job should     exist.
     And delete `snapshot1` recurring job.
         delete `snapshot3` recurring job.
     Then default `snapshot1` cron job should not exist.
          default `snapshot2` cron job should not exist.
                  `snapshot3` cron job should not exist.
     And `snapshot1` job should exist in volume recurring job label.
         `snapshot2` job should exist in volume recurring job label.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=SIZE)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    snap1 = SNAPSHOT + &#34;1&#34;
    snap2 = SNAPSHOT + &#34;2&#34;
    snap3 = SNAPSHOT + &#34;3&#34;
    back1 = BACKUP + &#34;1&#34;
    back2 = BACKUP + &#34;2&#34;
    back3 = BACKUP + &#34;3&#34;
    group1 = &#34;group-1&#34;
    recurring_jobs = {
        snap1: {
            TASK: SNAPSHOT,
            GROUPS: [DEFAULT, group1],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        snap2: {
            TASK: SNAPSHOT,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        snap3: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        back1: {
            TASK: BACKUP,
            GROUPS: [DEFAULT, group1],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        back2: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        back3: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 6)

    # snapshot
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap1)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap3)
    # backup
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back1)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back3)

    # Delete `snapshot2` recurring job should delete the cron job
    snap2_recurring_job = client.by_id_recurring_job(snap2)
    client.delete(snap2_recurring_job)
    wait_for_cron_job_count(batch_v1_beta_api, 5)
    # snapshot
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap1)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap3)
    # backup
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back1)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back3)

    # Delete multiple recurring jobs should reflect on the cron jobs.
    back1_recurring_job = client.by_id_recurring_job(back1)
    back2_recurring_job = client.by_id_recurring_job(back2)
    back3_recurring_job = client.by_id_recurring_job(back3)
    client.delete(back1_recurring_job)
    client.delete(back2_recurring_job)
    client.delete(back3_recurring_job)
    wait_for_cron_job_count(batch_v1_beta_api, 2)
    # snapshot
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap1)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap3)
    # backup
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back1)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back2)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back3)

    # Should be able to delete recurring job while existing in volume label
    volume.recurringJobAdd(name=snap1, isGroup=False)
    volume.recurringJobAdd(name=snap3, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[snap1, snap3], groups=[DEFAULT])
    wait_for_cron_job_count(batch_v1_beta_api, 2)
    # snapshot
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap1)
    wait_for_cron_job_delete(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+snap3)

    snap1_recurring_job = client.by_id_recurring_job(snap1)
    snap3_recurring_job = client.by_id_recurring_job(snap3)
    client.delete(snap1_recurring_job)
    client.delete(snap3_recurring_job)
    wait_for_cron_job_count(batch_v1_beta_api, 0)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[snap1, snap3], groups=[DEFAULT])</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_detached_volume"><code class="name flex">
<span>def <span class="ident">test_recurring_job_detached_volume</span></span>(<span>client, batch_v1_beta_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring job while volume is detached</p>
<p>Given a volume created, and attached.
And write some data to the volume.
And detach the volume.</p>
<p>When create a recurring job running at 1 minute interval,
and with <code>default</code> in groups,
and with <code>retain</code> set to <code>2</code>.
And 1 cron job should be created.
And wait for 2 minutes.
And attach volume and wait until healthy</p>
<p>Then the volume should have 1 snapshot</p>
<p>When wait for 1 minute.
Then then volume should have only 2 snapshots.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_detached_volume(client, batch_v1_beta_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job while volume is detached

    Given a volume created, and attached.
    And write some data to the volume.
    And detach the volume.

    When create a recurring job running at 1 minute interval,
            and with `default` in groups,
            and with `retain` set to `2`.
    And 1 cron job should be created.
    And wait for 2 minutes.
    And attach volume and wait until healthy

    Then the volume should have 1 snapshot

    When wait for 1 minute.
    Then then volume should have only 2 snapshots.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=SIZE)
    volume = wait_for_volume_detached(client, volume_name)

    self_host = get_self_host_id()
    volume.attach(hostId=self_host)
    volume = wait_for_volume_healthy(client, volume_name)
    write_volume_random_data(volume)
    volume.detach()

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 1)

    time.sleep(60 * 2)
    wait_until_begin_of_a_minute()
    time.sleep(5)
    volume.attach(hostId=self_host)
    volume = wait_for_volume_healthy(client, volume_name)
    wait_for_snapshot_count(volume, 1)

    time.sleep(60)
    wait_for_snapshot_count(volume, 2)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_duplicated"><code class="name flex">
<span>def <span class="ident">test_recurring_job_duplicated</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test create duplicated recurring jobs</p>
<p>Given recurring job created.
When create same recurring job again.
Then should fail.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_duplicated(client):  # NOQA
    &#34;&#34;&#34;
    Scenario: test create duplicated recurring jobs

    Given recurring job created.
    When create same recurring job again.
    Then should fail.
    &#34;&#34;&#34;
    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    with pytest.raises(Exception) as e:
        create_recurring_jobs(client, recurring_jobs)
    assert &#34;already exists&#34; in str(e.value)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_groups"><code class="name flex">
<span>def <span class="ident">test_recurring_job_groups</span></span>(<span>set_random_backupstore, client, batch_v1_beta_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring job groups (S3/NFS)</p>
<p>Given volume <code>test-job-1</code> created, attached, and healthy.
volume <code>test-job-2</code> created, attached, and healthy.
And create <code>snapshot</code> recurring job with <code>group-1, group-2</code> in groups.
set cron job to run every 2 minutes.
set retain to 1.
create <code>backup</code>
recurring job with <code>group-1</code>
in groups.
set cron job to run every 3 minutes.
set retain to 1</p>
<p>When set <code>group1</code> recurring job in volume <code>test-job-1</code> label.
set <code>group2</code> recurring job in volume <code>test-job-2</code> label.
And write some data to volume <code>test-job-1</code>.
write some data to volume <code>test-job-2</code>.
And wait for 2 minutes.
And write some data to volume <code>test-job-1</code>.
write some data to volume <code>test-job-2</code>.
And wait for 1 minute.</p>
<p>Then volume <code>test-job-1</code> should have 3 snapshots after scheduled time.
volume <code>test-job-2</code> should have 2 snapshots after scheduled time.
And volume <code>test-job-1</code> should have 1 backup after scheduled time.
volume <code>test-job-2</code> should have 0 backup after scheduled time.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_groups(set_random_backupstore, client, batch_v1_beta_api):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job groups (S3/NFS)

    Given volume `test-job-1` created, attached, and healthy.
          volume `test-job-2` created, attached, and healthy.
    And create `snapshot` recurring job with `group-1, group-2` in groups.
            set cron job to run every 2 minutes.
            set retain to 1.
        create `backup`   recurring job with `group-1`          in groups.
            set cron job to run every 3 minutes.
            set retain to 1

    When set `group1` recurring job in volume `test-job-1` label.
         set `group2` recurring job in volume `test-job-2` label.
    And write some data to volume `test-job-1`.
        write some data to volume `test-job-2`.
    And wait for 2 minutes.
    And write some data to volume `test-job-1`.
        write some data to volume `test-job-2`.
    And wait for 1 minute.

    Then volume `test-job-1` should have 3 snapshots after scheduled time.
         volume `test-job-2` should have 2 snapshots after scheduled time.
     And volume `test-job-1` should have 1 backup after scheduled time.
         volume `test-job-2` should have 0 backup after scheduled time.
    &#34;&#34;&#34;
    volume1_name = &#34;test-job-1&#34;
    volume2_name = &#34;test-job-2&#34;
    client.create_volume(name=volume1_name, size=SIZE)
    client.create_volume(name=volume2_name, size=SIZE)
    volume1 = wait_for_volume_detached(client, volume1_name)
    volume2 = wait_for_volume_detached(client, volume2_name)

    self_id = get_self_host_id()
    volume1.attach(hostId=self_id)
    volume2.attach(hostId=self_id)
    volume1 = wait_for_volume_healthy(client, volume1_name)
    volume2 = wait_for_volume_healthy(client, volume2_name)

    group1 = &#34;group-1&#34;
    group2 = &#34;group-2&#34;
    recurring_jobs = {
        SNAPSHOT: {
            TASK: SNAPSHOT,
            GROUPS: [group1, group2],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        BACKUP: {
            TASK: BACKUP,
            GROUPS: [group1],
            CRON: &#34;*/3 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    volume1.recurringJobAdd(name=group1, isGroup=True)
    volume2.recurringJobAdd(name=group2, isGroup=True)

    wait_for_cron_job_count(batch_v1_beta_api, 2)

    wait_until_begin_of_a_minute()
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60 * 2)
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_snapshot_count(volume1, 3)  # volume-head,snapshot,backup-snapshot
    wait_for_snapshot_count(volume2, 2)  # volume-head,snapshot

    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 1)
    backup_created = True
    try:
        wait_for_backup_count(client.by_id_backupVolume(volume2_name), 1,
                              retry_counts=60)
    except AssertionError:
        backup_created = False
    assert not backup_created</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_in_storageclass"><code class="name flex">
<span>def <span class="ident">test_recurring_job_in_storageclass</span></span>(<span>set_random_backupstore, client, core_api, storage_class, statefulset)</span>
</code></dt>
<dd>
<div class="desc"><p>Test create volume with StorageClass contains recurring jobs</p>
<ol>
<li>Create a StorageClass with recurring jobs</li>
<li>Create a StatefulSet with PVC template and StorageClass</li>
<li>Verify the recurring jobs run correctly.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_in_storageclass(set_random_backupstore, client, core_api, storage_class, statefulset):  # NOQA
    &#34;&#34;&#34;
    Test create volume with StorageClass contains recurring jobs

    1. Create a StorageClass with recurring jobs
    2. Create a StatefulSet with PVC template and StorageClass
    3. Verify the recurring jobs run correctly.
    &#34;&#34;&#34;
    recurring_jobs = {
        SNAPSHOT: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
        BACKUP: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    recurring_job_selector = [
        {
            NAME: SNAPSHOT,
            ISGROUP: False,
        },
        {
            NAME: BACKUP,
            ISGROUP: False,
        },
    ]
    storage_class[&#34;parameters&#34;][&#34;recurringJobSelector&#34;] = \
        json.dumps(recurring_job_selector)
    create_storage_class(storage_class)

    # wait until the beginning of an even minute
    wait_until_begin_of_an_even_minute()

    statefulset_name = &#39;recurring-job-in-storageclass-test&#39;
    update_statefulset_manifests(statefulset, storage_class, statefulset_name)
    start_time = datetime.utcnow()
    create_and_wait_statefulset(statefulset)
    statefulset_creating_duration = datetime.utcnow() - start_time

    assert 150 &gt; statefulset_creating_duration.seconds

    # We want to write data exactly at the 150th second since the start_time
    time.sleep(150 - statefulset_creating_duration.seconds)

    pod_info = get_statefulset_pod_info(core_api, statefulset)
    volume_info = [p[&#39;pv_name&#39;] for p in pod_info]
    pod_names = [p[&#39;pod_name&#39;] for p in pod_info]

    # write random data to volume to trigger recurring snapshot and backup job
    volume_data_path = &#34;/data/test&#34;
    for pod_name in pod_names:
        write_pod_volume_random_data(core_api, pod_name, volume_data_path, 2)

    time.sleep(150)  # 2.5 minutes

    for volume_name in volume_info:  # NOQA
        volume = client.by_id_volume(volume_name)
        wait_for_snapshot_count(volume, 4)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_in_volume_creation"><code class="name flex">
<span>def <span class="ident">test_recurring_job_in_volume_creation</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test create volume with recurring jobs</p>
<p>Given 2 recurring jobs created.
And volume create and a attached.</p>
<p>When label recurring job to volume.
And write data to volume.
wait 2.5 minutes.
And write data to volume.
wait 2.5 minutes.</p>
<p>Then volume have 4 snapshots.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_in_volume_creation(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test create volume with recurring jobs

    Given 2 recurring jobs created.
    And volume create and a attached.

    When label recurring job to volume.
    And write data to volume.
        wait 2.5 minutes.
    And write data to volume.
        wait 2.5 minutes.

    Then volume have 4 snapshots.
    &#34;&#34;&#34;
    recurring_jobs = {
        SNAPSHOT: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 1,
            LABELS: {},
        },
        BACKUP: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    client.create_volume(name=volume_name, size=SIZE,
                         numberOfReplicas=2)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    volume.recurringJobAdd(name=SNAPSHOT, isGroup=False)
    volume.recurringJobAdd(name=BACKUP, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[SNAPSHOT, BACKUP],
                                         groups=[DEFAULT])

    wait_until_begin_of_an_even_minute()
    # wait until the 10th second of an even minute
    # to avoid writing data at the same time backup is taking
    time.sleep(10)

    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes
    write_volume_random_data(volume)
    time.sleep(150)  # 2.5 minutes

    wait_for_snapshot_count(volume, 4)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_kubernetes_status"><code class="name flex">
<span>def <span class="ident">test_recurring_job_kubernetes_status</span></span>(<span>set_random_backupstore, client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurringJob properly backs up the KubernetesStatus (S3/NFS)</p>
<p>Given volume created and detached.
And PV from volume created and verified.</p>
<p>When create backup recurring job to run every 2 minutes.
And attach volume.
And write some data to volume.
And wait 5 minutes.</p>
<p>Then volume have 2 snapshots.
volume have 1 backup.
And backup have the Kubernetes Status labels.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.csi  # NOQA
@pytest.mark.recurring_job
def test_recurring_job_kubernetes_status(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurringJob properly backs up the KubernetesStatus (S3/NFS)

    Given volume created and detached.
    And PV from volume created and verified.

    When create backup recurring job to run every 2 minutes.
    And attach volume.
    And write some data to volume.
    And wait 5 minutes.

    Then volume have 2 snapshots.
         volume have 1 backup.
    And backup have the Kubernetes Status labels.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=SIZE, numberOfReplicas=2)
    volume = wait_for_volume_detached(client, volume_name)

    pv_name = &#34;pv-&#34; + volume_name
    create_pv_for_volume(client, core_api, volume, pv_name)
    ks = {
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;pvcName&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;lastPodRefAt&#39;: &#39;&#39;,
    }
    wait_volume_kubernetes_status(client, volume_name, ks)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    write_volume_random_data(volume)

    time.sleep(60 * 5)
    # 1 from Backup, 1 from Volume Head.
    wait_for_snapshot_count(volume, 2)

    # Verify the Labels on the actual Backup.
    bv = client.by_id_backupVolume(volume_name)
    backups = bv.backupList().data
    assert len(backups) == 1

    b = bv.backupGet(name=backups[0].name)
    status = json.loads(b.labels.get(KUBERNETES_STATUS_LABEL))
    assert b.labels.get(RECURRING_JOB_LABEL) == RECURRING_JOB_NAME
    assert status == {
        &#39;lastPodRefAt&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;pvcName&#39;: &#39;&#39;,
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;workloadsStatus&#39;: None
    }
    # Two Labels: KubernetesStatus and RecurringJob.
    assert len(b.labels) == 2</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_labels"><code class="name flex">
<span>def <span class="ident">test_recurring_job_labels</span></span>(<span>set_random_backupstore, client, random_labels, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test a recurring job with labels (S3/NFS)</p>
<p>Given a recurring job created,
with <code>default</code> in groups,
with random labels.
And volume created and attached.
And write data to volume.</p>
<p>When add another label to the recurring job.
And write data to volume.
And wait after scheduled time.</p>
<p>Then should have 2 snapshots.
And backup should have correct labels.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_labels(set_random_backupstore, client, random_labels, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test a recurring job with labels (S3/NFS)

    Given a recurring job created,
            with `default` in groups,
            with random labels.
    And volume created and attached.
    And write data to volume.

    When add another label to the recurring job.
    And write data to volume.
    And wait after scheduled time.

    Then should have 2 snapshots.
    And backup should have correct labels.
    &#34;&#34;&#34;
    recurring_job_labels_test(client, random_labels, volume_name)  # NOQA</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_multiple_volumes"><code class="name flex">
<span>def <span class="ident">test_recurring_job_multiple_volumes</span></span>(<span>set_random_backupstore, client, batch_v1_beta_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring job with multiple volumes</p>
<p>Given volume <code>test-job-1</code> created, attached and healthy.
And create <code>backup1</code>
recurring job with <code>default</code> in groups.
create <code>backup2</code>
recurring job with <code>`
in groups.
And &lt;code&gt;default&lt;/code&gt; group exist in</code>test-job-1<code>volume recurring job label.
And &lt;code&gt;backup1&lt;/code&gt; cron job exist.
&lt;code&gt;backup2&lt;/code&gt; cron job exist.
And write data to</code>test-job-1<code>volume.
And 2 snapshot exist in</code>test-job-1<code>volume.
And 1 backup
exist in</code>test-job-1` volume.</p>
<p>When create and attach volume <code>test-job-2</code>.
wait for volume <code>test-job-2</code> to be healthy.
And <code>default</code> group exist in <code>test-job-2</code> volume recurring job label.
And write data to
<code>test-job-1</code> volume.
Then 2 snapshot exist in <code>test-job-2</code> volume.
1 backup
exist in <code>test-job-2</code> volume.</p>
<p>When add <code>backup2</code> in <code>test-job-2</code> volume label.
And <code>default</code> group exist in <code>test-job-1</code> volume recurring job label.
<code>default</code> group exist in <code>test-job-2</code> volume recurring job label.
<code>backup2</code> group exist in <code>test-job-2</code> volume recurring job label.
And write data to <code>test-job-1</code>.
write data to <code>test-job-2</code>.
Then wait for schedule time.
And 2 backup exist in <code>test-job-2</code> volume.
1 backup exist in <code>test-job-1</code> volume.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_multiple_volumes(set_random_backupstore, client, batch_v1_beta_api):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job with multiple volumes

    Given volume `test-job-1` created, attached and healthy.
    And create `backup1`   recurring job with `default` in groups.
        create `backup2`   recurring job with ``        in groups.
    And `default` group exist in `test-job-1` volume recurring job label.
    And `backup1` cron job exist.
        `backup2` cron job exist.
    And write data to  `test-job-1` volume.
    And 2 snapshot exist in `test-job-1` volume.
    And 1 backup   exist in `test-job-1` volume.

    When create and attach volume `test-job-2`.
         wait for volume `test-job-2` to be healthy.
    And `default` group exist in `test-job-2` volume recurring job label.
    And write data to  `test-job-1` volume.
    Then 2 snapshot exist in `test-job-2` volume.
         1 backup   exist in `test-job-2` volume.

    When add `backup2` in `test-job-2` volume label.
    And `default` group exist in `test-job-1` volume recurring job label.
        `default` group exist in `test-job-2` volume recurring job label.
        `backup2` group exist in `test-job-2` volume recurring job label.
    And write data to `test-job-1`.
        write data to `test-job-2`.
    Then wait for schedule time.
    And 2 backup exist in `test-job-2` volume.
        1 backup exist in `test-job-1` volume.
    &#34;&#34;&#34;
    volume1_name = &#34;test-job-1&#34;
    client.create_volume(name=volume1_name, size=SIZE)
    volume1 = wait_for_volume_detached(client, volume1_name)
    volume1.attach(hostId=get_self_host_id())
    volume1 = wait_for_volume_healthy(client, volume1_name)

    back1 = BACKUP + &#34;1&#34;
    back2 = BACKUP + &#34;2&#34;
    recurring_jobs = {
        back1: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        back2: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_volume_recurring_job_update(volume1, jobs=[], groups=[DEFAULT])
    wait_for_cron_job_count(batch_v1_beta_api, 2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back1)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+back2)

    write_volume_random_data(volume1)
    wait_for_snapshot_count(volume1, 2)
    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 1)

    volume2_name = &#34;test-job-2&#34;
    client.create_volume(name=volume2_name, size=SIZE)
    volume2 = wait_for_volume_detached(client, volume2_name)
    volume2.attach(hostId=get_self_host_id())
    volume2 = wait_for_volume_healthy(client, volume2_name)
    wait_for_volume_recurring_job_update(volume2, jobs=[], groups=[DEFAULT])

    write_volume_random_data(volume2)
    wait_for_snapshot_count(volume2, 2)
    wait_for_backup_count(client.by_id_backupVolume(volume2_name), 1)

    volume2.recurringJobAdd(name=back2, isGroup=False)
    wait_for_volume_recurring_job_update(volume1,
                                         jobs=[], groups=[DEFAULT])
    wait_for_volume_recurring_job_update(volume2,
                                         jobs=[back2], groups=[DEFAULT])

    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(70)
    wait_for_backup_count(client.by_id_backupVolume(volume2_name), 2)
    wait_for_backup_count(client.by_id_backupVolume(volume1_name), 1)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_snapshot"><code class="name flex">
<span>def <span class="ident">test_recurring_job_snapshot</span></span>(<span>client, batch_v1_beta_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring job snapshot</p>
<p>Given volume <code>test-job-1</code> created, attached, and healthy.
volume <code>test-job-2</code> created, attached, and healthy.</p>
<p>When create a recurring job with <code>default</code> in groups.
Then should have 1 cron job.
And volume <code>test-job-1</code> should have volume-head 1 snapshot.
volume <code>test-job-2</code> should have volume-head 1 snapshot.</p>
<p>When write some data to volume <code>test-job-1</code>.
write some data to volume <code>test-job-2</code>.
And wait for cron job scheduled time.
Then volume <code>test-job-1</code> should have 2 snapshots after scheduled time.
volume <code>test-job-2</code> should have 2 snapshots after scheduled time.</p>
<p>When write some data to volume <code>test-job-1</code>.
write some data to volume <code>test-job-2</code>.
And wait for cron job scheduled time.
Then volume <code>test-job-1</code> should have 3 snapshots after scheduled time.
volume <code>test-job-2</code> should have 3 snapshots after scheduled time.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_snapshot(client, batch_v1_beta_api):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring job snapshot

    Given volume `test-job-1` created, attached, and healthy.
          volume `test-job-2` created, attached, and healthy.

    When create a recurring job with `default` in groups.
    Then should have 1 cron job.
    And volume `test-job-1` should have volume-head 1 snapshot.
        volume `test-job-2` should have volume-head 1 snapshot.

    When write some data to volume `test-job-1`.
         write some data to volume `test-job-2`.
    And wait for cron job scheduled time.
    Then volume `test-job-1` should have 2 snapshots after scheduled time.
         volume `test-job-2` should have 2 snapshots after scheduled time.

    When write some data to volume `test-job-1`.
         write some data to volume `test-job-2`.
    And wait for cron job scheduled time.
    Then volume `test-job-1` should have 3 snapshots after scheduled time.
         volume `test-job-2` should have 3 snapshots after scheduled time.
    &#34;&#34;&#34;
    volume1_name = &#34;test-job-1&#34;
    volume2_name = &#34;test-job-2&#34;
    client.create_volume(name=volume1_name, size=SIZE)
    client.create_volume(name=volume2_name, size=SIZE)
    volume1 = wait_for_volume_detached(client, volume1_name)
    volume2 = wait_for_volume_detached(client, volume2_name)

    self_host = get_self_host_id()

    volume1.attach(hostId=self_host)
    volume2.attach(hostId=self_host)
    volume1 = wait_for_volume_healthy(client, volume1_name)
    volume2 = wait_for_volume_healthy(client, volume2_name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: SNAPSHOT,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 2,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 1)

    # volume-head
    wait_for_snapshot_count(volume1, 1)
    wait_for_snapshot_count(volume2, 1)

    # 1st job
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_snapshot_count(volume1, 2)
    wait_for_snapshot_count(volume2, 2)

    # 2nd job
    # wait_until_begin_of_a_minute()
    write_volume_random_data(volume1)
    write_volume_random_data(volume2)
    time.sleep(60)
    wait_for_snapshot_count(volume1, 3)
    wait_for_snapshot_count(volume2, 3)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_job_volume_labeled_none_existing_recurring_job"><code class="name flex">
<span>def <span class="ident">test_recurring_job_volume_labeled_none_existing_recurring_job</span></span>(<span>client, batch_v1_beta_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test volume with a none-existing recurring job label
and later on added back.</p>
<p>Given create <code>snapshot</code> recurring job.
create <code>backup</code>
recurring job.
And 1 volume created, attached, and healthy.
And add <code>snapshot</code> recurring job to volume label.
add <code>backup</code>
recurring job to volume label.
And <code>snapshot1</code> cron job exist.
<code>backup1</code>
cron job exist.</p>
<p>When delete <code>snapshot</code> recurring job.
delete <code>backup</code>
recurring job.
Then <code>snapshot</code> cron job should not exist.
<code>backup</code>
cron job should not exist.
And <code>snapshot</code> job
should exist in volume recurring job label.
<code>backup</code>
job
should exist in volume recurring job label.
<code>default</code> group should exist in volume recurring job label.</p>
<h1 id="add-back-the-recurring-jobs">Add back the recurring jobs.</h1>
<p>When create <code>snapshot</code> recurring job.
create <code>backup</code>
recurring job.
Then <code>snapshot</code> cron job should exist.
<code>backup</code>
cron job should exist.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.recurring_job  # NOQA
def test_recurring_job_volume_labeled_none_existing_recurring_job(client, batch_v1_beta_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: test volume with a none-existing recurring job label
              and later on added back.

    Given create `snapshot` recurring job.
          create `backup`   recurring job.
    And 1 volume created, attached, and healthy.
    And add `snapshot` recurring job to volume label.
        add `backup`   recurring job to volume label.
    And `snapshot1` cron job exist.
        `backup1`   cron job exist.

    When delete `snapshot` recurring job.
         delete `backup`   recurring job.
    Then `snapshot` cron job should not exist.
         `backup`   cron job should not exist.
    And `snapshot` job  should exist in volume recurring job label.
        `backup`   job  should exist in volume recurring job label.
        `default` group should exist in volume recurring job label.

    # Add back the recurring jobs.
    When create `snapshot` recurring job.
         create `backup`   recurring job.
    Then `snapshot` cron job should exist.
         `backup`   cron job should exist.
    &#34;&#34;&#34;
    recurring_jobs = {
        SNAPSHOT: {
            TASK: SNAPSHOT,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
        BACKUP: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 2,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    client.create_volume(name=volume_name, size=SIZE)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)
    volume.recurringJobAdd(name=SNAPSHOT, isGroup=False)
    volume.recurringJobAdd(name=BACKUP, isGroup=False)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+SNAPSHOT)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+BACKUP)

    snap1_recurring_job = client.by_id_recurring_job(SNAPSHOT)
    back1_recurring_job = client.by_id_recurring_job(BACKUP)
    client.delete(snap1_recurring_job)
    client.delete(back1_recurring_job)
    wait_for_cron_job_count(batch_v1_beta_api, 0)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[SNAPSHOT, BACKUP],
                                         groups=[DEFAULT])
    wait_for_recurring_jobs_cleanup(client)

    # Add back the recurring jobs.
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)
    wait_for_cron_job_count(batch_v1_beta_api, 2)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+SNAPSHOT)
    wait_for_cron_job_create(batch_v1_beta_api, JOB_LABEL+&#34;=&#34;+BACKUP)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_jobs_allow_detached_volume"><code class="name flex">
<span>def <span class="ident">test_recurring_jobs_allow_detached_volume</span></span>(<span>set_random_backupstore, client, core_api, apps_api, volume_name, make_deployment_with_pvc)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring jobs for detached volume with
<code>allow-recurring-job-while-volume-detached</code> set to true</p>
<p>Context: In the current Longhorn implementation, users cannot do recurring
backup when volumes are detached.
This feature gives the users an option to do recurring backup
even when volumes are detached.
longhorn/longhorn#1509</p>
<p>Given <code>allow-recurring-job-while-volume-detached</code> set to <code>true</code>.
And volume created and attached.
And 50MB data written to volume.
And volume detached.</p>
<p>When a recurring job created runs every minute.
And wait for backup to complete.</p>
<p>Then volume have 1 backup in 2 minutes retry loop.</p>
<p>When delete the recurring job.
And create a PV from volume.
And create a PVC from volume.
And create a deployment from PVC.
And write 400MB data to the volume from the pod.
And scale deployment replicas to 0.
wait until the volume is detached.
And create a recurring job runs every 2 minutes.
And wait for backup to start.
And scale deployment replicas to 1.
Then volume's frontend is disabled.
And pod cannot start.</p>
<p>When wait until backup complete.
And delete the recurring job.
Then pod can start in 10 minutes retry loop.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_recurring_jobs_allow_detached_volume(set_random_backupstore, client, core_api, apps_api, volume_name, make_deployment_with_pvc):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring jobs for detached volume with
    `allow-recurring-job-while-volume-detached` set to true

    Context: In the current Longhorn implementation, users cannot do recurring
             backup when volumes are detached.
             This feature gives the users an option to do recurring backup
             even when volumes are detached.
             longhorn/longhorn#1509

    Given `allow-recurring-job-while-volume-detached` set to `true`.
    And volume created and attached.
    And 50MB data written to volume.
    And volume detached.

    When a recurring job created runs every minute.
    And wait for backup to complete.

    Then volume have 1 backup in 2 minutes retry loop.

    When delete the recurring job.
    And create a PV from volume.
    And create a PVC from volume.
    And create a deployment from PVC.
    And write 400MB data to the volume from the pod.
    And scale deployment replicas to 0.
        wait until the volume is detached.
    And create a recurring job runs every 2 minutes.
    And wait for backup to start.
    And scale deployment replicas to 1.
    Then volume&#39;s frontend is disabled.
    And pod cannot start.

    When wait until backup complete.
    And delete the recurring job.
    Then pod can start in 10 minutes retry loop.
    &#34;&#34;&#34;
    common.update_setting(client,
                          SETTING_RECURRING_JOB_WHILE_VOLUME_DETACHED, &#34;true&#34;)

    volume = create_and_check_volume(client, volume_name, size=str(1 * Gi))
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume.name)

    data = {
        &#39;pos&#39;: 0,
        &#39;content&#39;: common.generate_random_data(50 * Mi),
    }
    common.write_volume_data(volume, data)

    # Give sometimes for data to flush to disk
    time.sleep(15)

    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume.name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: SCHEDULE_1MIN,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    wait_for_backup_completion(client, volume.name)
    for _ in range(4):
        bv = client.by_id_backupVolume(volume.name)
        wait_for_backup_count(bv, 1)
        time.sleep(30)

    cleanup_all_recurring_jobs(client)

    pv_name = volume_name + &#34;-pv&#34;
    create_pv_for_volume(client, core_api, volume, pv_name)

    pvc_name = volume_name + &#34;-pvc&#34;
    create_pvc_for_volume(client, core_api, volume, pvc_name)

    deployment_name = volume_name + &#34;-dep&#34;
    deployment = make_deployment_with_pvc(deployment_name, pvc_name)
    create_and_wait_deployment(apps_api, deployment)

    size_mb = 400
    pod_names = common.get_deployment_pod_names(core_api, deployment)
    write_pod_volume_random_data(core_api, pod_names[0], &#34;/data/test&#34;,
                                 size_mb)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 0
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])

    volume = wait_for_volume_detached(client, volume.name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [DEFAULT],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    wait_for_backup_to_start(client, volume.name)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 1
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])

    deployment_label_name = deployment[&#34;metadata&#34;][&#34;labels&#34;][&#34;name&#34;]
    common.wait_pod_auto_attach_after_first_backup_completion(
        client, core_api, volume.name, deployment_label_name)

    cleanup_all_recurring_jobs(client)

    pod_names = common.get_deployment_pod_names(core_api, deployment)
    common.wait_for_pod_phase(core_api, pod_names[0], pod_phase=&#34;Running&#34;)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_jobs_maximum_retain"><code class="name flex">
<span>def <span class="ident">test_recurring_jobs_maximum_retain</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring jobs' maximum retain</p>
<p>Given set a recurring job retain to 51.</p>
<p>When create recurring job.
Then should fail.</p>
<p>When set recurring job retain to 50.
And create recurring job.
Then recurring job created with retain equals to 50.</p>
<p>When update recurring job retain to 51.
Then should fail.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_recurring_jobs_maximum_retain(client, core_api, volume_name): # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring jobs&#39; maximum retain

    Given set a recurring job retain to 51.

    When create recurring job.
    Then should fail.

    When set recurring job retain to 50.
    And create recurring job.
    Then recurring job created with retain equals to 50.

    When update recurring job retain to 51.
    Then should fail.
    &#34;&#34;&#34;
    # set max total number of retain to exceed 50
    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: SCHEDULE_1MIN,
            RETAIN: 51,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    with pytest.raises(Exception) as e:
        create_recurring_jobs(client, recurring_jobs)
    assert &#34;Job Can\\&#39;t retain more than 50 snapshots&#34; in str(e.value)

    recurring_jobs[RECURRING_JOB_NAME][RETAIN] = 50
    create_recurring_jobs(client, recurring_jobs)
    recurring_job = client.by_id_recurring_job(RECURRING_JOB_NAME)
    assert recurring_job.retain == 50

    with pytest.raises(Exception) as e:
        update_recurring_job(client, RECURRING_JOB_NAME,
                             groups=[], labels={}, retain=51)
    assert &#34;Job Can\\&#39;t retain more than 50 snapshots&#34; in str(e.value)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_jobs_on_nodes_with_taints"><code class="name flex">
<span>def <span class="ident">test_recurring_jobs_on_nodes_with_taints</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Test recurring jobs on nodes with taints</p>
<p>Context:</p>
<p>Test the prevention of creation of multiple pods due to
recurring job's pod being rejected by Taint controller
on nodes with taints</p>
<p>Steps:</p>
<ol>
<li>Set taint toleration for Longhorn components
<code>persistence=true:NoExecute</code></li>
<li>Taint <code>node-1</code> with <code>persistence=true:NoExecute</code></li>
<li>Create a volume, vol-1.
Attach vol-1 to node-1
Write some data to vol-1</li>
<li>Create a recurring backup job which:
Has retain count 10
Runs every minute</li>
<li>
<p>Wait for 3 minutes.
Verify that the there is 1 backup created
Verify that the total number of pod in longhorn-system namespace &lt; 50
Verify that the number of pods of the cronjob is &lt;= 2</p>
</li>
<li>
<p>Taint all nodes with <code>persistence=true:NoExecute</code></p>
</li>
<li>Write some data to vol-1</li>
<li>
<p>Wait for 3 minutes.
Verify that the there are 2 backups created in total
Verify that the total number of pod in longhorn-system namespace &lt; 50
Verify that the number of pods of the cronjob is &lt;= 2</p>
</li>
<li>
<p>Remove <code>persistence=true:NoExecute</code> from all nodes and Longhorn setting
Clean up backups, volumes</p>
</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;TODO&#34;)
def test_recurring_jobs_on_nodes_with_taints():  # NOQA
    &#34;&#34;&#34;
    Test recurring jobs on nodes with taints

    Context:

    Test the prevention of creation of multiple pods due to
    recurring job&#39;s pod being rejected by Taint controller
    on nodes with taints

    Steps:

    1. Set taint toleration for Longhorn components
       `persistence=true:NoExecute`
    2. Taint `node-1` with `persistence=true:NoExecute`
    3. Create a volume, vol-1.
       Attach vol-1 to node-1
       Write some data to vol-1
    4. Create a recurring backup job which:
       Has retain count 10
       Runs every minute
    5. Wait for 3 minutes.
       Verify that the there is 1 backup created
       Verify that the total number of pod in longhorn-system namespace &lt; 50
       Verify that the number of pods of the cronjob is &lt;= 2

    6. Taint all nodes with `persistence=true:NoExecute`
    7. Write some data to vol-1
    8. Wait for 3 minutes.
       Verify that the there are 2 backups created in total
       Verify that the total number of pod in longhorn-system namespace &lt; 50
       Verify that the number of pods of the cronjob is &lt;= 2

    9. Remove `persistence=true:NoExecute` from all nodes and Longhorn setting
       Clean up backups, volumes
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.test_recurring_jobs_when_volume_detached_unexpectedly"><code class="name flex">
<span>def <span class="ident">test_recurring_jobs_when_volume_detached_unexpectedly</span></span>(<span>set_random_backupstore, client, core_api, apps_api, volume_name, make_deployment_with_pvc)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: test recurring jobs when volume detached unexpectedly</p>
<p>Context: If the volume is automatically attached by the recurring backup
job, make sure that workload pod eventually is able to use the
volume when volume is detached unexpectedly during the backup
process.</p>
<p>Given <code>allow-recurring-job-while-volume-detached</code> set to <code>true</code>.
And volume created and detached.
And PV created from volume.
And PVC created from volume.
And deployment created from PVC.
And 500MB data written to the volume.
And deployment replica scaled to 0.
And volume detached.</p>
<p>When create a backup recurring job runs every 2 minutes.
And wait for backup to start.
wait for backup progress &gt; 50%.
And kill the engine process of the volume.
Then volume is attached and healthy.</p>
<p>When backup completed.
Then volume is detached with <code>frontendDisabled=false</code>.</p>
<p>When deployment replica scaled to 1.
Then the data exist in the deployment pod.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_recurring_jobs_when_volume_detached_unexpectedly(
    set_random_backupstore, client, core_api, apps_api, volume_name, make_deployment_with_pvc):  # NOQA
    &#34;&#34;&#34;
    Scenario: test recurring jobs when volume detached unexpectedly

    Context: If the volume is automatically attached by the recurring backup
             job, make sure that workload pod eventually is able to use the
             volume when volume is detached unexpectedly during the backup
             process.

    Given `allow-recurring-job-while-volume-detached` set to `true`.
    And volume created and detached.
    And PV created from volume.
    And PVC created from volume.
    And deployment created from PVC.
    And 500MB data written to the volume.
    And deployment replica scaled to 0.
    And volume detached.

    When create a backup recurring job runs every 2 minutes.
    And wait for backup to start.
        wait for backup progress &gt; 50%.
    And kill the engine process of the volume.
    Then volume is attached and healthy.

    When backup completed.
    Then volume is detached with `frontendDisabled=false`.

    When deployment replica scaled to 1.
    Then the data exist in the deployment pod.
    &#34;&#34;&#34;
    common.update_setting(client,
                          SETTING_RECURRING_JOB_WHILE_VOLUME_DETACHED, &#34;true&#34;)

    volume = create_and_check_volume(client, volume_name, size=str(1 * Gi))
    volume = wait_for_volume_detached(client, volume.name)

    pv_name = volume_name + &#34;-pv&#34;
    create_pv_for_volume(client, core_api, volume, pv_name)

    pvc_name = volume_name + &#34;-pvc&#34;
    create_pvc_for_volume(client, core_api, volume, pvc_name)

    deployment_name = volume_name + &#34;-dep&#34;
    deployment = make_deployment_with_pvc(deployment_name, pvc_name)
    create_and_wait_deployment(apps_api, deployment)

    size_mb = 500
    pod_names = common.get_deployment_pod_names(core_api, deployment)
    write_pod_volume_random_data(core_api, pod_names[0], &#34;/data/test&#34;,
                                 size_mb)
    data = read_volume_data(core_api, pod_names[0], &#39;default&#39;)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 0
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])
    volume = wait_for_volume_detached(client, volume_name)

    recurring_jobs = {
        RECURRING_JOB_NAME: {
            TASK: BACKUP,
            GROUPS: [],
            CRON: &#34;*/2 * * * *&#34;,
            RETAIN: 1,
            CONCURRENCY: 1,
            LABELS: {},
        },
    }
    create_recurring_jobs(client, recurring_jobs)
    check_recurring_jobs(client, recurring_jobs)

    volume.recurringJobAdd(name=RECURRING_JOB_NAME, isGroup=False)
    wait_for_volume_recurring_job_update(volume,
                                         jobs=[RECURRING_JOB_NAME],
                                         groups=[DEFAULT])

    time.sleep(60)
    wait_for_recurring_backup_to_start(client, core_api, volume_name,
                                       expected_snapshot_count=1,
                                       minimum_progress=50)

    crash_engine_process_with_sigkill(client, core_api, volume_name)
    time.sleep(10)
    wait_for_volume_healthy_no_frontend(client, volume_name)

    # Since the backup state is removed after the backup complete and it
    # could happen quickly. Checking for the both in-progress and complete
    # state could be hard to catch, thus we only check the complete state
    wait_for_backup_completion(client, volume_name)

    wait_for_volume_detached(client, volume_name)

    deployment[&#39;spec&#39;][&#39;replicas&#39;] = 1
    apps_api.patch_namespaced_deployment(body=deployment,
                                         namespace=&#39;default&#39;,
                                         name=deployment[&#34;metadata&#34;][&#34;name&#34;])
    wait_deployment_replica_ready(apps_api, deployment[&#34;metadata&#34;][&#34;name&#34;], 1)
    pod_names = common.get_deployment_pod_names(core_api, deployment)
    assert read_volume_data(core_api, pod_names[0], &#39;default&#39;) == data

    # Use fixture to cleanup the backupstore and since we
    # crashed the engine replica initiated the backup, it&#39;s
    # backupstore lock will still be present, so we need
    # to wait till the lock is expired, before we can delete
    # the backups
    volume.recurringJobDelete(name=RECURRING_JOB_NAME, isGroup=False)
    backupstore.backupstore_wait_for_lock_expiration()</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.wait_for_recurring_backup_to_start"><code class="name flex">
<span>def <span class="ident">wait_for_recurring_backup_to_start</span></span>(<span>client, core_api, volume_name, expected_snapshot_count, minimum_progress=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_recurring_backup_to_start(client, core_api, volume_name, expected_snapshot_count, minimum_progress=0):  # NOQA
    job_pod_name = volume_name + &#39;-backup-c&#39;
    snapshot_name = &#39;&#39;
    snapshots = []
    check_pod_existence(core_api, job_pod_name, namespace=LONGHORN_NAMESPACE)

    # Find the snapshot which is being backed up
    for _ in range(RETRY_BACKUP_COUNTS):
        volume = client.by_id_volume(volume_name)
        try:
            snapshots = volume.snapshotList()

            assert len(snapshots) == expected_snapshot_count + 1
            for snapshot in snapshots:
                if snapshot.children[&#39;volume-head&#39;]:
                    snapshot_name = snapshot.name
                    break
            if len(snapshot_name) != 0:
                break
        except (AttributeError, ApiException, AssertionError):
            time.sleep(RETRY_BACKUP_INTERVAL)
    assert len(snapshot_name) != 0

    # To ensure the progress of backup
    wait_for_backup_to_start(client, volume_name,
                             snapshot_name=snapshot_name,
                             chk_progress=minimum_progress)

    return snapshot_name</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.wait_until_begin_of_a_minute"><code class="name flex">
<span>def <span class="ident">wait_until_begin_of_a_minute</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_until_begin_of_a_minute():
    while True:
        current_time = datetime.utcnow()
        if current_time.second == 0:
            break
        time.sleep(1)</code></pre>
</details>
</dd>
<dt id="tests.test_recurring_job.wait_until_begin_of_an_even_minute"><code class="name flex">
<span>def <span class="ident">wait_until_begin_of_an_even_minute</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_until_begin_of_an_even_minute():
    while True:
        current_time = datetime.utcnow()
        if current_time.second == 0 and current_time.minute % 2 == 0:
            break
        time.sleep(1)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_recurring_job.recurring_job_labels_test" href="#tests.test_recurring_job.recurring_job_labels_test">recurring_job_labels_test</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job" href="#tests.test_recurring_job.test_recurring_job">test_recurring_job</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_backup" href="#tests.test_recurring_job.test_recurring_job_backup">test_recurring_job_backup</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_default" href="#tests.test_recurring_job.test_recurring_job_default">test_recurring_job_default</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_delete" href="#tests.test_recurring_job.test_recurring_job_delete">test_recurring_job_delete</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_detached_volume" href="#tests.test_recurring_job.test_recurring_job_detached_volume">test_recurring_job_detached_volume</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_duplicated" href="#tests.test_recurring_job.test_recurring_job_duplicated">test_recurring_job_duplicated</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_groups" href="#tests.test_recurring_job.test_recurring_job_groups">test_recurring_job_groups</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_in_storageclass" href="#tests.test_recurring_job.test_recurring_job_in_storageclass">test_recurring_job_in_storageclass</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_in_volume_creation" href="#tests.test_recurring_job.test_recurring_job_in_volume_creation">test_recurring_job_in_volume_creation</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_kubernetes_status" href="#tests.test_recurring_job.test_recurring_job_kubernetes_status">test_recurring_job_kubernetes_status</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_labels" href="#tests.test_recurring_job.test_recurring_job_labels">test_recurring_job_labels</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_multiple_volumes" href="#tests.test_recurring_job.test_recurring_job_multiple_volumes">test_recurring_job_multiple_volumes</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_snapshot" href="#tests.test_recurring_job.test_recurring_job_snapshot">test_recurring_job_snapshot</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_job_volume_labeled_none_existing_recurring_job" href="#tests.test_recurring_job.test_recurring_job_volume_labeled_none_existing_recurring_job">test_recurring_job_volume_labeled_none_existing_recurring_job</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_jobs_allow_detached_volume" href="#tests.test_recurring_job.test_recurring_jobs_allow_detached_volume">test_recurring_jobs_allow_detached_volume</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_jobs_maximum_retain" href="#tests.test_recurring_job.test_recurring_jobs_maximum_retain">test_recurring_jobs_maximum_retain</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_jobs_on_nodes_with_taints" href="#tests.test_recurring_job.test_recurring_jobs_on_nodes_with_taints">test_recurring_jobs_on_nodes_with_taints</a></code></li>
<li><code><a title="tests.test_recurring_job.test_recurring_jobs_when_volume_detached_unexpectedly" href="#tests.test_recurring_job.test_recurring_jobs_when_volume_detached_unexpectedly">test_recurring_jobs_when_volume_detached_unexpectedly</a></code></li>
<li><code><a title="tests.test_recurring_job.wait_for_recurring_backup_to_start" href="#tests.test_recurring_job.wait_for_recurring_backup_to_start">wait_for_recurring_backup_to_start</a></code></li>
<li><code><a title="tests.test_recurring_job.wait_until_begin_of_a_minute" href="#tests.test_recurring_job.wait_until_begin_of_a_minute">wait_until_begin_of_a_minute</a></code></li>
<li><code><a title="tests.test_recurring_job.wait_until_begin_of_an_even_minute" href="#tests.test_recurring_job.wait_until_begin_of_an_even_minute">wait_until_begin_of_an_even_minute</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>