<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tests.test_settings API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_settings</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import time
import pytest
import os
import subprocess
import yaml

from common import (  # NOQA
    get_longhorn_api_client, get_self_host_id,
    get_core_api_client, get_apps_api_client,
    create_and_check_volume, cleanup_volume,
    wait_for_volume_healthy, wait_for_volume_detached,
    write_volume_random_data, check_volume_data,
    get_default_engine_image,
    wait_for_engine_image_state,
    wait_for_instance_manager_desire_state,
    generate_volume_name,
    wait_for_volume_condition_scheduled,
    client, core_api, settings_reset,
    apps_api, scheduling_api, priority_class, volume_name,
    get_engine_image_status_value,
    create_volume, cleanup_volume_by_name,
    Gi,

    LONGHORN_NAMESPACE,
    SETTING_TAINT_TOLERATION,
    SETTING_GUARANTEED_ENGINE_CPU,
    SETTING_GUARANTEED_ENGINE_MANAGER_CPU,
    SETTING_GUARANTEED_REPLICA_MANAGER_CPU,
    SETTING_PRIORITY_CLASS,
    SETTING_DEFAULT_REPLICA_COUNT,
    SETTING_BACKUP_TARGET,
    SIZE, RETRY_COUNTS, RETRY_INTERVAL, RETRY_INTERVAL_LONG,
    update_setting, BACKING_IMAGE_QCOW2_URL, BACKING_IMAGE_NAME,
    create_backing_image_with_matching_url, BACKING_IMAGE_EXT4_SIZE,
    check_backing_image_disk_map_status, wait_for_volume_delete,
    SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, wait_for_node_update,
    crash_replica_processes, wait_for_engine_image_ref_count,
    get_volume_engine, wait_for_volume_current_image,
    wait_for_rebuild_start, wait_for_rebuild_complete,
    wait_for_volume_degraded, Gi, write_volume_dev_random_mb_data,
    get_volume_endpoint, RETRY_EXEC_COUNTS, RETRY_SNAPSHOT_INTERVAL,
    delete_replica_on_test_node
)

from test_infra import wait_for_node_up_longhorn

KUBERNETES_DEFAULT_TOLERATION = &#34;kubernetes.io&#34;
BACKING_IMAGE_CLEANUP_WAIT_INTERVAL = &#34;50&#34;


def check_workload_update(core_api, apps_api, count):  # NOQA
    da_list = apps_api.list_namespaced_daemon_set(LONGHORN_NAMESPACE).items
    for da in da_list:
        if da.status.updated_number_scheduled != count:
            return False

    dp_list = apps_api.list_namespaced_deployment(LONGHORN_NAMESPACE).items
    for dp in dp_list:
        if dp.status.updated_replicas != dp.spec.replicas:
            return False

    im_pod_list = core_api.list_namespaced_pod(
        LONGHORN_NAMESPACE,
        label_selector=&#34;longhorn.io/component=instance-manager&#34;).items
    if len(im_pod_list) != 2 * count:
        return False

    for p in im_pod_list:
        if p.status.phase != &#34;Running&#34;:
            return False

    client = get_longhorn_api_client()  # NOQA
    images = client.list_engine_image()
    assert len(images) == 1
    ei_state = get_engine_image_status_value(client, images[0].name)
    if images[0].state != ei_state:
        return False

    return True


def wait_for_longhorn_node_ready():
    client = get_longhorn_api_client()  # NOQA

    ei = get_default_engine_image(client)
    ei_name = ei[&#34;name&#34;]
    ei_state = get_engine_image_status_value(client, ei_name)
    wait_for_engine_image_state(client, ei_name, ei_state)

    node = get_self_host_id()
    wait_for_node_up_longhorn(node, client)

    return client, node


def test_setting_toleration():
    &#34;&#34;&#34;
    Test toleration setting

    1.  Set `taint-toleration` to &#34;key1=value1:NoSchedule; key2:InvalidEffect&#34;.
    2.  Verify the request fails.
    3.  Create a volume and attach it.
    4.  Set `taint-toleration` to &#34;key1=value1:NoSchedule; key2:NoExecute&#34;.
    5.  Verify that cannot update toleration setting when any volume is
        attached.
    6.  Generate and write `data1` into the volume.
    7.  Detach the volume.
    8.  Set `taint-toleration` to &#34;key1=value1:NoSchedule; key2:NoExecute&#34;.
    9.  Wait for all the Longhorn system components to restart with new
        toleration.
    10. Verify that UI, manager, and drive deployer don&#39;t restart and
        don&#39;t have new toleration.
    11. Attach the volume again and verify the volume `data1`.
    12. Generate and write `data2` to the volume.
    13. Detach the volume.
    14. Clean the `toleration` setting.
    15. Wait for all the Longhorn system components to restart with no
        toleration.
    16. Attach the volume and validate `data2`.
    17. Generate and write `data3` to the volume.
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    apps_api = get_apps_api_client()  # NOQA
    core_api = get_core_api_client()  # NOQA
    count = len(client.list_node())

    setting = client.by_id_setting(SETTING_TAINT_TOLERATION)

    with pytest.raises(Exception) as e:
        client.update(setting,
                      value=&#34;key1=value1:NoSchedule; key2:InvalidEffect&#34;)
    assert &#39;invalid effect&#39; in str(e.value)

    volume_name = &#34;test-toleration-vol&#34;  # NOQA
    volume = create_and_check_volume(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    setting_value_str = &#34;key1=value1:NoSchedule; key2:NoExecute&#34;
    setting_value_dicts = [
        {
            &#34;key&#34;: &#34;key1&#34;,
            &#34;value&#34;: &#34;value1&#34;,
            &#34;operator&#34;: &#34;Equal&#34;,
            &#34;effect&#34;: &#34;NoSchedule&#34;
        },
        {
            &#34;key&#34;: &#34;key2&#34;,
            &#34;value&#34;: None,
            &#34;operator&#34;: &#34;Exists&#34;,
            &#34;effect&#34;: &#34;NoExecute&#34;
        },
    ]
    with pytest.raises(Exception) as e:
        client.update(setting, value=setting_value_str)
    assert &#39;cannot modify toleration setting before all volumes are detached&#39; \
           in str(e.value)

    data1 = write_volume_random_data(volume)
    check_volume_data(volume, data1)

    volume.detach(hostId=&#34;&#34;)
    wait_for_volume_detached(client, volume_name)

    setting = client.update(setting, value=setting_value_str)
    assert setting.value == setting_value_str
    wait_for_toleration_update(core_api, apps_api, count, setting_value_dicts)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data1)
    data2 = write_volume_random_data(volume)
    check_volume_data(volume, data2)
    volume.detach(hostId=&#34;&#34;)
    wait_for_volume_detached(client, volume_name)

    # cleanup
    setting_value_str = &#34;&#34;
    setting_value_dicts = []
    setting = client.by_id_setting(SETTING_TAINT_TOLERATION)
    setting = client.update(setting, value=setting_value_str)
    assert setting.value == setting_value_str
    wait_for_toleration_update(core_api, apps_api, count, setting_value_dicts)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data2)
    data3 = write_volume_random_data(volume)
    check_volume_data(volume, data3)

    cleanup_volume(client, volume)


def test_setting_toleration_extra(core_api, apps_api):  # NOQA
    &#34;&#34;&#34;
    Steps:
    1. Set Kubernetes Taint Toleration to:
       `ex.com/foobar:NoExecute;ex.com/foobar:NoSchedule`.
    2. Verify that all system components have the 2 tolerations
       `ex.com/foobar:NoExecute; ex.com/foobar:NoSchedule`.
       Verify that UI, manager, and drive deployer don&#39;t restart and
       don&#39;t have toleration.
    3. Set Kubernetes Taint Toleration to:
       `node-role.kubernetes.io/controlplane=true:NoSchedule`.
    4. Verify that all system components have the the toleration
       `node-role.kubernetes.io/controlplane=true:NoSchedule`,
       and don&#39;t have the 2 tolerations
       `ex.com/foobar:NoExecute;ex.com/foobar:NoSchedule`.
       Verify that UI, manager, and drive deployer don&#39;t restart and
       don&#39;t have toleration.
    5. Set Kubernetes Taint Toleration to special value:
       `:`.
    6. Verify that all system components have the toleration with
       `operator: Exists` and other field of the toleration are empty.
       Verify that all system components don&#39;t have the toleration
       `node-role.kubernetes.io/controlplane=true:NoSchedule`.
       Verify that UI, manager, and drive deployer don&#39;t restart and
       don&#39;t have toleration.
    7. Clear Kubernetes Taint Toleration

    Note: system components are workloads other than UI, manager, driver
    deployer
    &#34;&#34;&#34;
    settings = [
        {
            &#34;value&#34;: &#34;ex.com/foobar:NoExecute;ex.com/foobar:NoSchedule&#34;,
            &#34;expect&#34;: [
                {
                    &#34;key&#34;: &#34;ex.com/foobar&#34;,
                    &#34;value&#34;: None,
                    &#34;operator&#34;: &#34;Exists&#34;,
                    &#34;effect&#34;: &#34;NoExecute&#34;
                },
                {
                    &#34;key&#34;: &#34;ex.com/foobar&#34;,
                    &#34;value&#34;: None,
                    &#34;operator&#34;: &#34;Exists&#34;,
                    &#34;effect&#34;: &#34;NoSchedule&#34;
                },
            ],
        },
        {
            &#34;value&#34;: &#34;node-role.kubernetes.io/controlplane=true:NoSchedule&#34;,
            &#34;expect&#34;: [
                {
                    &#34;key&#34;: &#34;node-role.kubernetes.io/controlplane&#34;,
                    &#34;value&#34;: &#34;true&#34;,
                    &#34;operator&#34;: &#34;Equal&#34;,
                    &#34;effect&#34;: &#34;NoSchedule&#34;
                },
            ],
        },
        # Skip the this special toleration for now because it makes
        # Longhorn deploy manager pods on control/etcd nodes
        # and the control/etcd nodes become &#34;down&#34; after the test
        # clear this toleration.
        # We will enable this test once we implement logic for
        # deleting failed nodes.
        # {
        #     &#34;value&#34;: &#34;:&#34;,
        #     &#34;expect&#34;: [
        #         {
        #             &#34;key&#34;: None,
        #             &#34;value&#34;: None,
        #             &#34;operator&#34;: &#34;Exists&#34;,
        #             &#34;effect&#34;: None,
        #         },
        #     ]
        # },
        {
            &#34;value&#34;: &#34;&#34;,
            &#34;expect&#34;: [],
        },
    ]

    chk_removed_tolerations = []
    for setting in settings:
        client = get_longhorn_api_client()  # NOQA
        taint_toleration = client.by_id_setting(SETTING_TAINT_TOLERATION)
        updated = client.update(taint_toleration,
                                value=setting[&#34;value&#34;])
        assert updated.value == setting[&#34;value&#34;]

        node_count = len(client.list_node())
        wait_for_toleration_update(core_api, apps_api, node_count,
                                   setting[&#34;expect&#34;], chk_removed_tolerations)
        chk_removed_tolerations = setting[&#34;expect&#34;]


def wait_for_toleration_update(core_api, apps_api, count,  # NOQA
                               expected_tolerations,
                               chk_removed_tolerations=[]):
    not_managed_apps = [
        &#34;csi-attacher&#34;,
        &#34;csi-provisioner&#34;,
        &#34;csi-resizer&#34;,
        &#34;csi-snapshotter&#34;,
        &#34;longhorn-csi-plugin&#34;,
        &#34;longhorn-driver-deployer&#34;,
        &#34;longhorn-manager&#34;,
        &#34;longhorn-ui&#34;,
    ]
    updated = False
    for _ in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL_LONG)

        updated = True
        if not check_workload_update(core_api, apps_api, count):
            updated = False
            continue

        pod_list = core_api.list_namespaced_pod(LONGHORN_NAMESPACE).items
        for p in pod_list:
            managed_by = p.metadata.labels.get(&#39;longhorn.io/managed-by&#39;, &#39;&#39;)
            if str(managed_by) != &#34;longhorn-manager&#34;:
                continue
            else:
                app_name = str(p.metadata.labels.get(&#39;app&#39;, &#39;&#39;))
                assert app_name not in not_managed_apps

            if p.status.phase != &#34;Running&#34; \
                or not check_tolerations_set(p.spec.tolerations,
                                             expected_tolerations,
                                             chk_removed_tolerations):
                updated = False
                break
        if updated:
            break
    assert updated


def check_tolerations_set(current_toleration_list, expected_tolerations,
                          chk_removed_tolerations=[]):
    found = 0
    unexpected = 0
    for t in current_toleration_list:
        current_toleration = {
            &#34;key&#34;: t.key,
            &#34;value&#34;: t.value,
            &#34;operator&#34;: t.operator,
            &#34;effect&#34;: t.effect
        }
        for expected in expected_tolerations:
            if current_toleration == expected:
                found += 1

        for removed in chk_removed_tolerations:
            if current_toleration == removed:
                unexpected += 1
    return len(expected_tolerations) == found and unexpected == 0


def test_instance_manager_cpu_reservation(client, core_api):  # NOQA
    &#34;&#34;&#34;
    Test if the CPU requests of instance manager pods are controlled by
    the settings and the node specs correctly.

    1. Try to change the deprecated setting `Guaranteed Engine CPU`.
       --&gt; The setting update should fail.
    2. Pick up node 1, set `node.engineManagerCPURequest` and
       `node.replicaManagerCPURequest` to 150 and 250, respectively.
       --&gt; The IM pods on this node will be restarted. And the CPU requests
       of these IM pods matches the above milli value.
    3. Change the new settings `Guaranteed Engine Manager CPU` and
       `Guaranteed Replica Manager CPU` to 10 and 20, respectively.
       Then wait for all IM pods except for the pods on node 1 restarting.
       --&gt; The CPU requests of the restarted IM pods equals to
           the new setting value multiply the kube node allocatable CPU.
    4. Set the both new settings to 0.
       --&gt; All IM pods except for the pod on node 1 will be restarted without
        CPU requests.
    5. Set the fields on node 1 to 0.
       --&gt; The IM pods on node 1 will be restarted without CPU requests.
    6. Set the both new settings to 2 random values,
       and the sum of the 2 values is small than 40.
       Then wait for all IM pods restarting.
       --&gt; The CPU requests of all IM pods equals to
           the new setting value multiply the kube node allocatable CPU.
    7. Set the both new settings to 2 random values,
       and the single value or the sum of the 2 values is greater than 40.
       --&gt; The setting update should fail.
    8. Create a volume, verify everything works as normal

    Note: use fixture to restore the setting into the original state
    &#34;&#34;&#34;

    instance_managers = client.list_instance_manager()
    deprecated_setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_CPU)
    with pytest.raises(Exception) as e:
        client.update(deprecated_setting, value=&#34;0.1&#34;)

    host_node_name = get_self_host_id()
    host_node = client.by_id_node(host_node_name)
    other_ems, other_rms = [], []
    for im in instance_managers:
        if im.managerType == &#34;engine&#34;:
            if im.nodeID == host_node_name:
                em_on_host = im
            else:
                other_ems.append(im)
        else:
            if im.nodeID == host_node_name:
                rm_on_host = im
            else:
                other_rms.append(im)
    assert em_on_host and rm_on_host
    host_kb_node = core_api.read_node(host_node_name)
    if host_kb_node.status.allocatable[&#34;cpu&#34;].endswith(&#39;m&#39;):
        allocatable_millicpu = int(host_kb_node.status.allocatable[&#34;cpu&#34;][:-1])
    else:
        allocatable_millicpu = int(host_kb_node.status.allocatable[&#34;cpu&#34;])*1000

    client.update(host_node, allowScheduling=True,
                  engineManagerCPURequest=150, replicaManagerCPURequest=250)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, [em_on_host], &#34;Running&#34;, True, &#34;150m&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, [rm_on_host], &#34;Running&#34;, True, &#34;250m&#34;)

    em_setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_MANAGER_CPU)
    client.update(em_setting, value=&#34;10&#34;)
    rm_setting = client.by_id_setting(SETTING_GUARANTEED_REPLICA_MANAGER_CPU)
    client.update(rm_setting, value=&#34;20&#34;)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, other_ems, &#34;Running&#34;, True,
        str(int(allocatable_millicpu*10/100)) + &#34;m&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, other_rms, &#34;Running&#34;, True,
        str(int(allocatable_millicpu*20/100)) + &#34;m&#34;)

    em_setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_MANAGER_CPU)
    client.update(em_setting, value=&#34;0&#34;)
    rm_setting = client.by_id_setting(SETTING_GUARANTEED_REPLICA_MANAGER_CPU)
    client.update(rm_setting, value=&#34;0&#34;)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, other_ems, &#34;Running&#34;, True, &#34;&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, other_rms, &#34;Running&#34;, True, &#34;&#34;)

    ems, rms = other_ems, other_rms
    ems.append(em_on_host)
    rms.append(rm_on_host)

    host_node = client.by_id_node(host_node_name)
    client.update(host_node, allowScheduling=True,
                  engineManagerCPURequest=0, replicaManagerCPURequest=0)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, ems, &#34;Running&#34;, True, &#34;&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, rms, &#34;Running&#34;, True, &#34;&#34;)

    client.update(em_setting, value=&#34;20&#34;)
    rm_setting = client.by_id_setting(SETTING_GUARANTEED_REPLICA_MANAGER_CPU)
    client.update(rm_setting, value=&#34;15&#34;)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, ems, &#34;Running&#34;, True,
        str(int(allocatable_millicpu*20/100)) + &#34;m&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, rms, &#34;Running&#34;, True,
        str(int(allocatable_millicpu*15/100)) + &#34;m&#34;)

    with pytest.raises(Exception) as e:
        client.update(em_setting, value=&#34;41&#34;)
    assert &#34;should be between 0 to 40&#34; in \
           str(e.value)

    em_setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_MANAGER_CPU)
    with pytest.raises(Exception) as e:
        client.update(em_setting, value=&#34;35&#34;)
    assert &#34;The sum should not be smaller than 0% or greater than 40%&#34; in \
           str(e.value)

    # Create a volume to test
    vol_name = generate_volume_name()
    volume = create_and_check_volume(client, vol_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, vol_name)
    assert len(volume.replicas) == 3
    data = write_volume_random_data(volume)
    check_volume_data(volume, data)
    cleanup_volume(client, volume)


def guaranteed_engine_cpu_setting_check(  # NOQA
        client, core_api, instance_managers, state, desire, cpu_val):  # NOQA
    &#34;&#34;&#34;
    We check if instance managers are in the desired state with
    correct setting
    desire is for reflect the state we are looking for.
    If desire is True, meanning we need the state to be the same.
    Otherwise, we are looking for the state to be different.
    e.g. &#39;Pending&#39;, &#39;OutofCPU&#39;, &#39;Terminating&#39; they are all &#39;Not Running&#39;.
    &#34;&#34;&#34;

    # Give sometime to k8s to update the instance manager status
    for im in instance_managers:
        wait_for_instance_manager_desire_state(client, core_api,
                                               im.name, state, desire)

    if desire:
        # Verify guaranteed CPU set correctly
        for im in instance_managers:
            pod = core_api.read_namespaced_pod(name=im.name,
                                               namespace=LONGHORN_NAMESPACE)
            if cpu_val:
                assert (pod.spec.containers[0].resources.requests[&#39;cpu&#39;] ==
                        cpu_val)
            else:
                assert (not pod.spec.containers[0].resources.requests)


def test_setting_priority_class(core_api, apps_api, scheduling_api, priority_class, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the Priority Class setting is validated and utilized correctly.

    1. Verify that the name of a non-existent Priority Class cannot be used
    for the Setting.
    2. Create a new Priority Class in Kubernetes.
    3. Create and attach a Volume.
    4. Verify that the Priority Class Setting cannot be updated with an
    attached Volume.
    5. Generate and write `data1`.
    6. Detach the Volume.
    7. Update the Priority Class Setting to the new Priority Class.
    8. Wait for all the Longhorn system components to restart with the new
       Priority Class.
    9. Verify that UI, manager, and drive deployer don&#39;t have Priority Class
    10. Attach the Volume and verify `data1`.
    11. Generate and write `data2`.
    12. Unset the Priority Class Setting.
    13. Wait for all the Longhorn system components to restart with the new
        Priority Class.
    14. Verify that UI, manager, and drive deployer don&#39;t have Priority Class
    15. Attach the Volume and verify `data2`.
    16. Generate and write `data3`.

    Note: system components are workloads other than UI, manager, driver
     deployer
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    count = len(client.list_node())
    name = priority_class[&#39;metadata&#39;][&#39;name&#39;]
    setting = client.by_id_setting(SETTING_PRIORITY_CLASS)

    with pytest.raises(Exception) as e:
        client.update(setting, value=name)
    assert &#39;failed to get priority class &#39; in str(e.value)

    scheduling_api.create_priority_class(priority_class)

    volume = create_and_check_volume(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        client.update(setting, value=name)
    assert &#39;cannot modify priority class setting before all volumes are &#39; \
           &#39;detached&#39; in str(e.value)

    data1 = write_volume_random_data(volume)
    check_volume_data(volume, data1)

    volume.detach(hostId=&#34;&#34;)
    wait_for_volume_detached(client, volume_name)

    setting = client.update(setting, value=name)
    assert setting.value == name

    wait_for_priority_class_update(core_api, apps_api, count, priority_class)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data1)
    data2 = write_volume_random_data(volume)
    check_volume_data(volume, data2)
    volume.detach(hostId=&#34;&#34;)
    wait_for_volume_detached(client, volume_name)

    setting = client.by_id_setting(SETTING_PRIORITY_CLASS)
    setting = client.update(setting, value=&#39;&#39;)
    assert setting.value == &#39;&#39;
    wait_for_priority_class_update(core_api, apps_api, count)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data2)
    data3 = write_volume_random_data(volume)
    check_volume_data(volume, data3)

    cleanup_volume(client, volume)


def check_priority_class(pod, priority_class=None):  # NOQA
    if priority_class:
        return pod.spec.priority == priority_class[&#39;value&#39;] and \
               pod.spec.priority_class_name == \
               priority_class[&#39;metadata&#39;][&#39;name&#39;]
    else:
        return pod.spec.priority == 0 and pod.spec.priority_class_name == &#39;&#39;


def wait_for_priority_class_update(core_api, apps_api, count, priority_class=None):  # NOQA
    updated = False

    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL_LONG)
        updated = True

        if not check_workload_update(core_api, apps_api, count):
            updated = False
            continue

        pod_list = core_api.list_namespaced_pod(LONGHORN_NAMESPACE).items
        for p in pod_list:
            if p.status.phase != &#34;Running&#34; and \
                    not check_priority_class(p, priority_class):
                updated = False
                break
        if not updated:
            continue

        if updated:
            break

    assert updated


@pytest.mark.backing_image  # NOQA
def test_setting_backing_image_auto_cleanup(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the Backing Image Cleanup Wait Interval setting works correctly.

    The default value of setting `BackingImageCleanupWaitInterval` is 60.

    1. Clean up the backing image work directory so that the current case
       won&#39;t be intervened by previous tests.
    2. Create a backing image.
    3. Create multiple volumes using the backing image.
    4. Attach all volumes, Then:
        1. Wait for all volumes can become running.
        2. Verify the correct in all volumes.
        3. Verify the backing image disk status map.
        4. Verify the only backing image file in each disk is reused by
           multiple replicas. The backing image file path is
           `&lt;Data path&gt;/&lt;The backing image name&gt;/backing`
    5. Unschedule test node to guarantee when replica removed from test node,
       no new replica can be rebuilt on the test node.
    6. Remove all replicas in one disk.
       Wait for 50 seconds.
       Then verify nothing changes in the backing image disk state map
       (before the cleanup wait interval is passed).
    7. Modify `BackingImageCleanupWaitInterval` to a small value. Then verify:
        1. The download state of the disk containing no replica becomes
           terminating first, and the entry will be removed from the map later.
        2. The related backing image file is removed.
        3. The download state of other disks keep unchanged.
           All volumes still work fine.
    8. Delete all volumes. Verify that there will only remain 1 entry in the
       backing image disk map
    9. Delete the backing image.
    &#34;&#34;&#34;

    # Step 1
    subprocess.check_call([&#34;rm&#34;, &#34;-rf&#34;, &#34;/var/lib/longhorn/backing-images&#34;])

    # Step 2
    create_backing_image_with_matching_url(
            client, BACKING_IMAGE_NAME, BACKING_IMAGE_QCOW2_URL)

    # Step 3
    volume_names = [
        volume_name + &#34;-1&#34;,
        volume_name + &#34;-2&#34;,
        volume_name + &#34;-3&#34;
    ]

    for volume_name in volume_names:
        create_and_check_volume(
            client, volume_name, 3, str(BACKING_IMAGE_EXT4_SIZE),
            BACKING_IMAGE_NAME)

    # Step 4
    lht_host_id = get_self_host_id()
    for volume_name in volume_names:
        volume = client.by_id_volume(volume_name)
        volume.attach(hostId=lht_host_id)
    for volume_name in volume_names:
        volume = wait_for_volume_healthy(client, volume_name)
        assert volume.backingImage == BACKING_IMAGE_NAME

    backing_image = client.by_id_backing_image(BACKING_IMAGE_NAME)
    assert len(backing_image.diskFileStatusMap) == 3
    for disk_id, status in iter(backing_image.diskFileStatusMap.items()):
        assert status.state == &#34;ready&#34;

    backing_images_in_disk = os.listdir(&#34;/var/lib/longhorn/backing-images&#34;)
    assert len(backing_images_in_disk) == 1
    assert os.path.exists(&#34;/var/lib/longhorn/backing-images/{}/backing&#34;
                          .format(backing_images_in_disk[0]))
    assert os.path.exists(&#34;/var/lib/longhorn/backing-images/{}/backing.cfg&#34;
                          .format(backing_images_in_disk[0]))

    # Step 5
    current_host = client.by_id_node(id=lht_host_id)
    client.update(current_host, allowScheduling=False)
    wait_for_node_update(client, lht_host_id, &#34;allowScheduling&#34;, False)

    # Step 6
    for volume_name in volume_names:
        volume = client.by_id_volume(volume_name)
        for replica in volume.replicas:
            if replica.hostId == lht_host_id:
                replica_name = replica.name
                volume.replicaRemove(name=replica_name)
    # This wait interval should be smaller than the setting value.
    # Otherwise, the backing image files may be cleaned up.
    time.sleep(int(BACKING_IMAGE_CLEANUP_WAIT_INTERVAL))
    check_backing_image_disk_map_status(client, BACKING_IMAGE_NAME, 3, &#34;ready&#34;)

    # Step 7
    update_setting(client, &#34;backing-image-cleanup-wait-interval&#34;, &#34;1&#34;)
    check_backing_image_disk_map_status(client, BACKING_IMAGE_NAME, 2, &#34;ready&#34;)

    backing_images_in_disk = os.listdir(&#34;/var/lib/longhorn/backing-images&#34;)
    assert len(backing_images_in_disk) == 0

    # Step 8
    for volume_name in volume_names:
        volume = client.by_id_volume(volume_name)
        client.delete(volume)
        wait_for_volume_delete(client, volume_name)

    check_backing_image_disk_map_status(client, BACKING_IMAGE_NAME, 1, &#34;ready&#34;)


def test_setting_concurrent_rebuild_limit(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test if setting Concurrent Replica Rebuild Per Node Limit works correctly.

    The default setting value is 0, which means no limit.

    Case 1 - the setting will limit the rebuilding correctly:
    1. Set `ConcurrentReplicaRebuildPerNodeLimit` to 1.
    2. Create 2 volumes then attach both volumes.
    3. Write a large amount of data into both volumes,
       so that the rebuilding will take a while.
    4. Delete one replica for volume 1 then the replica on the same node for
       volume 2 to trigger (concurrent) rebuilding.
    5. Verify the new replica of volume 2 won&#39;t be started until volume 1
       rebuilding complete.
       And the new replica of volume 2 will be started immediately once
       the 1st rebuilding is done.
    6. Wait for rebuilding complete then repeat step 4.
    7. Set `ConcurrentReplicaRebuildPerNodeLimit` to 0 or 2 while the volume 1
       rebuilding is still in progress.
       Then the new replica of volume 2 will be started immediately before
       the 1st rebuilding is done.
    8. Wait for rebuilding complete then repeat step 4.
    9. Set `ConcurrentReplicaRebuildPerNodeLimit` to 1
    10. Crash the replica process of volume 1 while the rebuilding is
        in progress.
        Then the rebuilding of volume 2 will be started, and the rebuilding of
        volume 1 will wait for the volume 2 becoming healthy.

   (There is no need to clean up the above 2 volumes.)

    Case 2 - the setting won&#39;t intervene normal attachment:
    1. Set `ConcurrentReplicaRebuildPerNodeLimit` to 1.
    2. Make volume 1 state attached and healthy while volume 2 is detached.
    3. Delete one replica for volume 1 to trigger the rebuilding.
    4. Attach then detach volume 2. The attachment/detachment should succeed
       even if the rebuilding in volume 1 is still in progress.
   &#34;&#34;&#34;
    # Step 1-1
    update_setting(client,
                   &#34;concurrent-replica-rebuild-per-node-limit&#34;,
                   &#34;1&#34;)

    # Step 1-2
    volume1_name = &#34;test-vol-1&#34;  # NOQA
    volume1 = create_and_check_volume(client, volume1_name, size=str(4 * Gi))
    volume1.attach(hostId=get_self_host_id())
    volume1 = wait_for_volume_healthy(client, volume1_name)

    volume2_name = &#34;test-vol-2&#34;  # NOQA
    volume2 = create_and_check_volume(client, volume2_name, size=str(4 * Gi))
    volume2.attach(hostId=get_self_host_id())
    volume2 = wait_for_volume_healthy(client, volume2_name)

    # Step 1-3
    volume1_endpoint = get_volume_endpoint(volume1)
    volume2_endpoint = get_volume_endpoint(volume2)
    write_volume_dev_random_mb_data(volume1_endpoint,
                                    1, 3500)
    write_volume_dev_random_mb_data(volume2_endpoint,
                                    1, 3500)

    # Step 1-4, 1-5
    delete_replica_on_test_node(client, volume1_name)
    wait_for_rebuild_start(client, volume1_name)
    delete_replica_on_test_node(client, volume2_name)

    for i in range(RETRY_COUNTS):
        volume1 = client.by_id_volume(volume1_name)
        volume2 = client.by_id_volume(volume2_name)

        if volume1.rebuildStatus == []:
            break

        assert volume1.rebuildStatus[0].state == &#34;in_progress&#34;
        assert volume2.rebuildStatus == []

        time.sleep(RETRY_INTERVAL)

    wait_for_rebuild_complete(client, volume1_name)
    wait_for_rebuild_start(client, volume2_name)
    wait_for_rebuild_complete(client, volume2_name)

    # Step 1-6
    wait_for_volume_healthy(client, volume1_name)
    wait_for_volume_healthy(client, volume2_name)

    # Step 1-7
    delete_replica_on_test_node(client, volume1_name)
    delete_replica_on_test_node(client, volume2_name)
    update_setting(client,
                   &#34;concurrent-replica-rebuild-per-node-limit&#34;,
                   &#34;2&#34;)

    # In a 2 minutes retry loop:
    # verify that volume 2 start rebuilding while volume 1 is still rebuilding
    concourent_build = False
    for i in range(RETRY_EXEC_COUNTS):
        volume1 = client.by_id_volume(volume1_name)
        volume2 = client.by_id_volume(volume2_name)
        try:
            if volume1.rebuildStatus[0].state == &#34;in_progress&#34; and \
                    volume2.rebuildStatus[0].state == &#34;in_progress&#34;:
                concourent_build = True
                break
        except: # NOQA
            pass
        time.sleep(RETRY_SNAPSHOT_INTERVAL)
    assert concourent_build is True

    # Step 1-8
    wait_for_rebuild_complete(client, volume1_name)
    wait_for_rebuild_complete(client, volume2_name)

    # Step 1-9
    update_setting(client,
                   &#34;concurrent-replica-rebuild-per-node-limit&#34;,
                   &#34;1&#34;)

    # Step 1-10
    delete_replica_on_test_node(client, volume1_name)
    wait_for_rebuild_start(client, volume1_name)
    volume1 = client.by_id_volume(volume1_name)
    current_node = get_self_host_id()
    replicas = []
    for r in volume1.replicas:
        if r[&#34;hostId&#34;] == current_node:
            replicas.append(r)

    assert len(replicas) &gt; 0
    crash_replica_processes(client, core_api, volume1_name, replicas)
    delete_replica_on_test_node(client, volume2_name)

    # While volume 2 is rebuilding, verify that volume 1 is not
    # rebuilding and stuck in degrading state
    wait_for_rebuild_start(client, volume2_name)
    for i in range(RETRY_COUNTS):
        volume1 = client.by_id_volume(volume1_name)
        volume2 = client.by_id_volume(volume2_name)

        if volume2.rebuildStatus == []:
            break

        assert volume2.rebuildStatus[0].state == &#34;in_progress&#34;
        assert volume1.rebuildStatus == []

        time.sleep(RETRY_INTERVAL)

    wait_for_rebuild_complete(client, volume2_name)
    wait_for_rebuild_start(client, volume1_name)
    wait_for_rebuild_complete(client, volume1_name)

    # Step 2-1
    # Step 2-2
    wait_for_volume_healthy(client, volume1_name)
    wait_for_volume_healthy(client, volume2_name)

    volume2 = client.by_id_volume(volume2_name)
    lht_host_id = get_self_host_id()
    volume2.detach(hostId=lht_host_id)

    # Step 2-2
    delete_replica_on_test_node(client, volume1_name)
    wait_for_rebuild_start(client, volume1_name)

    # Step 2-3
    volume2 = client.by_id_volume(volume2_name)
    volume2.attach(hostId=lht_host_id)

    # In a 2 minutes retry loop:
    # verify that we can see the case: volume2 becomes healthy while
    # volume1 is rebuilding
    expect_case = False
    for i in range(RETRY_COUNTS):
        volume1 = client.by_id_volume(volume1_name)
        volume2 = client.by_id_volume(volume2_name)

        try:
            if volume1.rebuildStatus[0].state == &#34;in_progress&#34; and \
                    volume2[&#34;robustness&#34;] == &#34;healthy&#34;:
                expect_case = True
                break
        except: # NOQA
            pass
        time.sleep(RETRY_INTERVAL)
    assert expect_case is True

    wait_for_volume_healthy(client, volume1_name)
    volume2.detach(hostId=lht_host_id)
    wait_for_volume_detached(client, volume2_name)

    volume2.attach(hostId=lht_host_id)
    wait_for_volume_healthy(client, volume2_name)


def config_map_with_value(configmap_name, setting_names, setting_values):
    setting = {}
    num_settings = len(setting_names)
    if num_settings &gt; 0:
        for i in range(num_settings):
            setting.update({setting_names[i]: setting_values[i]})
    return {
        &#34;apiVersion&#34;: &#34;v1&#34;,
        &#34;kind&#34;: &#34;ConfigMap&#34;,
        &#34;metadata&#34;: {
            &#34;name&#34;: configmap_name,
        },
        &#34;data&#34;: {
            &#34;default-setting.yaml&#34;: yaml.dump(setting),
        }
    }


def wait_for_setting_updated(client, name, expected_value):  # NOQA
    for _ in range(RETRY_COUNTS):
        setting = client.by_id_setting(name)
        if setting.value == expected_value:
            return True
        time.sleep(RETRY_INTERVAL)
    return False


def retry_setting_update(client, setting_name, setting_value):  # NOQA
    for i in range(RETRY_COUNTS):
        try:
            update_setting(client, setting_name, setting_value)
        except Exception as e:
            if i &lt; RETRY_COUNTS:
                time.sleep(RETRY_INTERVAL)
                continue
            print(e)
            raise
        else:
            break


def init_longhorn_default_setting_configmap(core_api, client): # NOQA
    core_api.delete_namespaced_config_map(name=&#34;longhorn-default-setting&#34;,
                                          namespace=&#39;longhorn-system&#39;)

    configmap_body = config_map_with_value(&#34;longhorn-default-setting&#34;, [], [])
    core_api.create_namespaced_config_map(body=configmap_body,
                                          namespace=&#39;longhorn-system&#39;)


def update_settings_via_configmap(core_api, client, setting_names, setting_values, request):  # NOQA
    configmap_body = config_map_with_value(&#34;longhorn-default-setting&#34;,
                                           setting_names,
                                           setting_values)
    core_api.patch_namespaced_config_map(name=&#34;longhorn-default-setting&#34;,
                                         namespace=&#39;longhorn-system&#39;,
                                         body=configmap_body)

    def reset_default_settings():
        configmap_body = config_map_with_value(&#34;longhorn-default-setting&#34;,
                                               [SETTING_DEFAULT_REPLICA_COUNT,
                                                SETTING_BACKUP_TARGET,
                                                SETTING_TAINT_TOLERATION],
                                               [&#34;3&#34;,
                                                &#34;&#34;,
                                                &#34;&#34;])
        core_api.patch_namespaced_config_map(name=&#34;longhorn-default-setting&#34;,
                                             namespace=&#39;longhorn-system&#39;,
                                             body=configmap_body)
    request.addfinalizer(reset_default_settings)


def validate_settings(core_api, client, setting_names, setting_values):  # NOQA
    num_settings = len(setting_names)
    for i in range(num_settings):
        name = setting_names[i]
        value = setting_values[i]
        assert wait_for_setting_updated(client, name, value)


def test_setting_replica_count_update_via_configmap(core_api, request):  # NOQA
    &#34;&#34;&#34;
    Test the default-replica-count setting via configmap
    1. Get default-replica-count value
    2. Initialize longhorn-default-setting configmap
    3. Verify default-replica-count is not changed
    4. Update longhorn-default-setting configmap with a new
       default-replica-count value
    5. Verify the updated settings
    6. Update default-replica-count setting CR with the old value
    &#34;&#34;&#34;

    # Step 1
    client = get_longhorn_api_client()  # NOQA
    old_setting = client.by_id_setting(SETTING_DEFAULT_REPLICA_COUNT)

    # Step 2
    init_longhorn_default_setting_configmap(core_api, client)

    # Step 3
    assert wait_for_setting_updated(client,
                                    SETTING_DEFAULT_REPLICA_COUNT,
                                    old_setting.value)

    # Step 4
    replica_count = &#34;1&#34;
    update_settings_via_configmap(core_api,
                                  client,
                                  [SETTING_DEFAULT_REPLICA_COUNT],
                                  [replica_count],
                                  request)
    # Step 5
    validate_settings(core_api,
                      client,
                      [SETTING_DEFAULT_REPLICA_COUNT],
                      [replica_count])
    # Step 6
    retry_setting_update(client,
                         SETTING_DEFAULT_REPLICA_COUNT,
                         old_setting.definition.default)


def test_setting_backup_target_update_via_configmap(core_api, request):  # NOQA
    &#34;&#34;&#34;
    Test the backup target setting via configmap
    1. Initialize longhorn-default-setting configmap
    2. Update longhorn-default-setting configmap with a new backup-target
       value
    3. Verify the updated settings
    &#34;&#34;&#34;

    # Step 1
    client = get_longhorn_api_client()  # NOQA
    init_longhorn_default_setting_configmap(core_api, client)

    # Step 2
    target = &#34;s3://backupbucket-invalid@us-east-1/backupstore&#34;
    update_settings_via_configmap(core_api,
                                  client,
                                  [SETTING_BACKUP_TARGET],
                                  [target],
                                  request)
    # Step 3
    validate_settings(core_api,
                      client,
                      [SETTING_BACKUP_TARGET],
                      [target])


def test_setting_update_with_invalid_value_via_configmap(core_api, request):  # NOQA
    &#34;&#34;&#34;
    Test the default settings update with invalid value via configmap
    1. Create an attached volume
    2. Initialize longhorn-default-setting configmap containing
       valid and invalid settings
    3. Update longhorn-default-setting configmap with invalid settings.
       The invalid settings SETTING_TAINT_TOLERATION will be ingored
       when there is an attached volume.
    4. Validate the default settings values.
    &#34;&#34;&#34;

    # Step 1
    client = get_longhorn_api_client() # NOQA
    lht_hostId = get_self_host_id()

    vol_name = generate_volume_name()
    volume = create_volume(client, vol_name, str(Gi), lht_hostId, 3)

    volume.attach(hostId=lht_hostId)
    volume = wait_for_volume_healthy(client, vol_name)

    # Step 2
    init_longhorn_default_setting_configmap(core_api, client)

    # Step 3
    target = &#34;s3://backupbucket-invalid@us-east-1/backupstore&#34;
    update_settings_via_configmap(core_api,
                                  client,
                                  [SETTING_BACKUP_TARGET,
                                   SETTING_TAINT_TOLERATION],
                                  [target,
                                   &#34;key1=value1:NoSchedule&#34;],
                                  request)
    # Step 4
    validate_settings(core_api,
                      client,
                      [SETTING_BACKUP_TARGET,
                       SETTING_TAINT_TOLERATION],
                      [target,
                       &#34;&#34;])

    cleanup_volume_by_name(client, vol_name)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_settings.check_priority_class"><code class="name flex">
<span>def <span class="ident">check_priority_class</span></span>(<span>pod, priority_class=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_priority_class(pod, priority_class=None):  # NOQA
    if priority_class:
        return pod.spec.priority == priority_class[&#39;value&#39;] and \
               pod.spec.priority_class_name == \
               priority_class[&#39;metadata&#39;][&#39;name&#39;]
    else:
        return pod.spec.priority == 0 and pod.spec.priority_class_name == &#39;&#39;</code></pre>
</details>
</dd>
<dt id="tests.test_settings.check_tolerations_set"><code class="name flex">
<span>def <span class="ident">check_tolerations_set</span></span>(<span>current_toleration_list, expected_tolerations, chk_removed_tolerations=[])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_tolerations_set(current_toleration_list, expected_tolerations,
                          chk_removed_tolerations=[]):
    found = 0
    unexpected = 0
    for t in current_toleration_list:
        current_toleration = {
            &#34;key&#34;: t.key,
            &#34;value&#34;: t.value,
            &#34;operator&#34;: t.operator,
            &#34;effect&#34;: t.effect
        }
        for expected in expected_tolerations:
            if current_toleration == expected:
                found += 1

        for removed in chk_removed_tolerations:
            if current_toleration == removed:
                unexpected += 1
    return len(expected_tolerations) == found and unexpected == 0</code></pre>
</details>
</dd>
<dt id="tests.test_settings.check_workload_update"><code class="name flex">
<span>def <span class="ident">check_workload_update</span></span>(<span>core_api, apps_api, count)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_workload_update(core_api, apps_api, count):  # NOQA
    da_list = apps_api.list_namespaced_daemon_set(LONGHORN_NAMESPACE).items
    for da in da_list:
        if da.status.updated_number_scheduled != count:
            return False

    dp_list = apps_api.list_namespaced_deployment(LONGHORN_NAMESPACE).items
    for dp in dp_list:
        if dp.status.updated_replicas != dp.spec.replicas:
            return False

    im_pod_list = core_api.list_namespaced_pod(
        LONGHORN_NAMESPACE,
        label_selector=&#34;longhorn.io/component=instance-manager&#34;).items
    if len(im_pod_list) != 2 * count:
        return False

    for p in im_pod_list:
        if p.status.phase != &#34;Running&#34;:
            return False

    client = get_longhorn_api_client()  # NOQA
    images = client.list_engine_image()
    assert len(images) == 1
    ei_state = get_engine_image_status_value(client, images[0].name)
    if images[0].state != ei_state:
        return False

    return True</code></pre>
</details>
</dd>
<dt id="tests.test_settings.config_map_with_value"><code class="name flex">
<span>def <span class="ident">config_map_with_value</span></span>(<span>configmap_name, setting_names, setting_values)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def config_map_with_value(configmap_name, setting_names, setting_values):
    setting = {}
    num_settings = len(setting_names)
    if num_settings &gt; 0:
        for i in range(num_settings):
            setting.update({setting_names[i]: setting_values[i]})
    return {
        &#34;apiVersion&#34;: &#34;v1&#34;,
        &#34;kind&#34;: &#34;ConfigMap&#34;,
        &#34;metadata&#34;: {
            &#34;name&#34;: configmap_name,
        },
        &#34;data&#34;: {
            &#34;default-setting.yaml&#34;: yaml.dump(setting),
        }
    }</code></pre>
</details>
</dd>
<dt id="tests.test_settings.guaranteed_engine_cpu_setting_check"><code class="name flex">
<span>def <span class="ident">guaranteed_engine_cpu_setting_check</span></span>(<span>client, core_api, instance_managers, state, desire, cpu_val)</span>
</code></dt>
<dd>
<div class="desc"><p>We check if instance managers are in the desired state with
correct setting
desire is for reflect the state we are looking for.
If desire is True, meanning we need the state to be the same.
Otherwise, we are looking for the state to be different.
e.g. 'Pending', 'OutofCPU', 'Terminating' they are all 'Not Running'.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def guaranteed_engine_cpu_setting_check(  # NOQA
        client, core_api, instance_managers, state, desire, cpu_val):  # NOQA
    &#34;&#34;&#34;
    We check if instance managers are in the desired state with
    correct setting
    desire is for reflect the state we are looking for.
    If desire is True, meanning we need the state to be the same.
    Otherwise, we are looking for the state to be different.
    e.g. &#39;Pending&#39;, &#39;OutofCPU&#39;, &#39;Terminating&#39; they are all &#39;Not Running&#39;.
    &#34;&#34;&#34;

    # Give sometime to k8s to update the instance manager status
    for im in instance_managers:
        wait_for_instance_manager_desire_state(client, core_api,
                                               im.name, state, desire)

    if desire:
        # Verify guaranteed CPU set correctly
        for im in instance_managers:
            pod = core_api.read_namespaced_pod(name=im.name,
                                               namespace=LONGHORN_NAMESPACE)
            if cpu_val:
                assert (pod.spec.containers[0].resources.requests[&#39;cpu&#39;] ==
                        cpu_val)
            else:
                assert (not pod.spec.containers[0].resources.requests)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.init_longhorn_default_setting_configmap"><code class="name flex">
<span>def <span class="ident">init_longhorn_default_setting_configmap</span></span>(<span>core_api, client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_longhorn_default_setting_configmap(core_api, client): # NOQA
    core_api.delete_namespaced_config_map(name=&#34;longhorn-default-setting&#34;,
                                          namespace=&#39;longhorn-system&#39;)

    configmap_body = config_map_with_value(&#34;longhorn-default-setting&#34;, [], [])
    core_api.create_namespaced_config_map(body=configmap_body,
                                          namespace=&#39;longhorn-system&#39;)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.retry_setting_update"><code class="name flex">
<span>def <span class="ident">retry_setting_update</span></span>(<span>client, setting_name, setting_value)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def retry_setting_update(client, setting_name, setting_value):  # NOQA
    for i in range(RETRY_COUNTS):
        try:
            update_setting(client, setting_name, setting_value)
        except Exception as e:
            if i &lt; RETRY_COUNTS:
                time.sleep(RETRY_INTERVAL)
                continue
            print(e)
            raise
        else:
            break</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_instance_manager_cpu_reservation"><code class="name flex">
<span>def <span class="ident">test_instance_manager_cpu_reservation</span></span>(<span>client, core_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Test if the CPU requests of instance manager pods are controlled by
the settings and the node specs correctly.</p>
<ol>
<li>Try to change the deprecated setting <code>Guaranteed Engine CPU</code>.
&ndash;&gt; The setting update should fail.</li>
<li>Pick up node 1, set <code>node.engineManagerCPURequest</code> and
<code>node.replicaManagerCPURequest</code> to 150 and 250, respectively.
&ndash;&gt; The IM pods on this node will be restarted. And the CPU requests
of these IM pods matches the above milli value.</li>
<li>Change the new settings <code>Guaranteed Engine Manager CPU</code> and
<code>Guaranteed Replica Manager CPU</code> to 10 and 20, respectively.
Then wait for all IM pods except for the pods on node 1 restarting.
&ndash;&gt; The CPU requests of the restarted IM pods equals to
the new setting value multiply the kube node allocatable CPU.</li>
<li>Set the both new settings to 0.
&ndash;&gt; All IM pods except for the pod on node 1 will be restarted without
CPU requests.</li>
<li>Set the fields on node 1 to 0.
&ndash;&gt; The IM pods on node 1 will be restarted without CPU requests.</li>
<li>Set the both new settings to 2 random values,
and the sum of the 2 values is small than 40.
Then wait for all IM pods restarting.
&ndash;&gt; The CPU requests of all IM pods equals to
the new setting value multiply the kube node allocatable CPU.</li>
<li>Set the both new settings to 2 random values,
and the single value or the sum of the 2 values is greater than 40.
&ndash;&gt; The setting update should fail.</li>
<li>Create a volume, verify everything works as normal</li>
</ol>
<p>Note: use fixture to restore the setting into the original state</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_instance_manager_cpu_reservation(client, core_api):  # NOQA
    &#34;&#34;&#34;
    Test if the CPU requests of instance manager pods are controlled by
    the settings and the node specs correctly.

    1. Try to change the deprecated setting `Guaranteed Engine CPU`.
       --&gt; The setting update should fail.
    2. Pick up node 1, set `node.engineManagerCPURequest` and
       `node.replicaManagerCPURequest` to 150 and 250, respectively.
       --&gt; The IM pods on this node will be restarted. And the CPU requests
       of these IM pods matches the above milli value.
    3. Change the new settings `Guaranteed Engine Manager CPU` and
       `Guaranteed Replica Manager CPU` to 10 and 20, respectively.
       Then wait for all IM pods except for the pods on node 1 restarting.
       --&gt; The CPU requests of the restarted IM pods equals to
           the new setting value multiply the kube node allocatable CPU.
    4. Set the both new settings to 0.
       --&gt; All IM pods except for the pod on node 1 will be restarted without
        CPU requests.
    5. Set the fields on node 1 to 0.
       --&gt; The IM pods on node 1 will be restarted without CPU requests.
    6. Set the both new settings to 2 random values,
       and the sum of the 2 values is small than 40.
       Then wait for all IM pods restarting.
       --&gt; The CPU requests of all IM pods equals to
           the new setting value multiply the kube node allocatable CPU.
    7. Set the both new settings to 2 random values,
       and the single value or the sum of the 2 values is greater than 40.
       --&gt; The setting update should fail.
    8. Create a volume, verify everything works as normal

    Note: use fixture to restore the setting into the original state
    &#34;&#34;&#34;

    instance_managers = client.list_instance_manager()
    deprecated_setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_CPU)
    with pytest.raises(Exception) as e:
        client.update(deprecated_setting, value=&#34;0.1&#34;)

    host_node_name = get_self_host_id()
    host_node = client.by_id_node(host_node_name)
    other_ems, other_rms = [], []
    for im in instance_managers:
        if im.managerType == &#34;engine&#34;:
            if im.nodeID == host_node_name:
                em_on_host = im
            else:
                other_ems.append(im)
        else:
            if im.nodeID == host_node_name:
                rm_on_host = im
            else:
                other_rms.append(im)
    assert em_on_host and rm_on_host
    host_kb_node = core_api.read_node(host_node_name)
    if host_kb_node.status.allocatable[&#34;cpu&#34;].endswith(&#39;m&#39;):
        allocatable_millicpu = int(host_kb_node.status.allocatable[&#34;cpu&#34;][:-1])
    else:
        allocatable_millicpu = int(host_kb_node.status.allocatable[&#34;cpu&#34;])*1000

    client.update(host_node, allowScheduling=True,
                  engineManagerCPURequest=150, replicaManagerCPURequest=250)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, [em_on_host], &#34;Running&#34;, True, &#34;150m&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, [rm_on_host], &#34;Running&#34;, True, &#34;250m&#34;)

    em_setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_MANAGER_CPU)
    client.update(em_setting, value=&#34;10&#34;)
    rm_setting = client.by_id_setting(SETTING_GUARANTEED_REPLICA_MANAGER_CPU)
    client.update(rm_setting, value=&#34;20&#34;)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, other_ems, &#34;Running&#34;, True,
        str(int(allocatable_millicpu*10/100)) + &#34;m&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, other_rms, &#34;Running&#34;, True,
        str(int(allocatable_millicpu*20/100)) + &#34;m&#34;)

    em_setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_MANAGER_CPU)
    client.update(em_setting, value=&#34;0&#34;)
    rm_setting = client.by_id_setting(SETTING_GUARANTEED_REPLICA_MANAGER_CPU)
    client.update(rm_setting, value=&#34;0&#34;)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, other_ems, &#34;Running&#34;, True, &#34;&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, other_rms, &#34;Running&#34;, True, &#34;&#34;)

    ems, rms = other_ems, other_rms
    ems.append(em_on_host)
    rms.append(rm_on_host)

    host_node = client.by_id_node(host_node_name)
    client.update(host_node, allowScheduling=True,
                  engineManagerCPURequest=0, replicaManagerCPURequest=0)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, ems, &#34;Running&#34;, True, &#34;&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, rms, &#34;Running&#34;, True, &#34;&#34;)

    client.update(em_setting, value=&#34;20&#34;)
    rm_setting = client.by_id_setting(SETTING_GUARANTEED_REPLICA_MANAGER_CPU)
    client.update(rm_setting, value=&#34;15&#34;)
    time.sleep(5)
    guaranteed_engine_cpu_setting_check(
        client, core_api, ems, &#34;Running&#34;, True,
        str(int(allocatable_millicpu*20/100)) + &#34;m&#34;)
    guaranteed_engine_cpu_setting_check(
        client, core_api, rms, &#34;Running&#34;, True,
        str(int(allocatable_millicpu*15/100)) + &#34;m&#34;)

    with pytest.raises(Exception) as e:
        client.update(em_setting, value=&#34;41&#34;)
    assert &#34;should be between 0 to 40&#34; in \
           str(e.value)

    em_setting = client.by_id_setting(SETTING_GUARANTEED_ENGINE_MANAGER_CPU)
    with pytest.raises(Exception) as e:
        client.update(em_setting, value=&#34;35&#34;)
    assert &#34;The sum should not be smaller than 0% or greater than 40%&#34; in \
           str(e.value)

    # Create a volume to test
    vol_name = generate_volume_name()
    volume = create_and_check_volume(client, vol_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, vol_name)
    assert len(volume.replicas) == 3
    data = write_volume_random_data(volume)
    check_volume_data(volume, data)
    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_backing_image_auto_cleanup"><code class="name flex">
<span>def <span class="ident">test_setting_backing_image_auto_cleanup</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that the Backing Image Cleanup Wait Interval setting works correctly.</p>
<p>The default value of setting <code>BackingImageCleanupWaitInterval</code> is 60.</p>
<ol>
<li>Clean up the backing image work directory so that the current case
won't be intervened by previous tests.</li>
<li>Create a backing image.</li>
<li>Create multiple volumes using the backing image.</li>
<li>Attach all volumes, Then:<ol>
<li>Wait for all volumes can become running.</li>
<li>Verify the correct in all volumes.</li>
<li>Verify the backing image disk status map.</li>
<li>Verify the only backing image file in each disk is reused by
multiple replicas. The backing image file path is
<code>&lt;Data path&gt;/&lt;The backing image name&gt;/backing</code></li>
</ol>
</li>
<li>Unschedule test node to guarantee when replica removed from test node,
no new replica can be rebuilt on the test node.</li>
<li>Remove all replicas in one disk.
Wait for 50 seconds.
Then verify nothing changes in the backing image disk state map
(before the cleanup wait interval is passed).</li>
<li>Modify <code>BackingImageCleanupWaitInterval</code> to a small value. Then verify:<ol>
<li>The download state of the disk containing no replica becomes
terminating first, and the entry will be removed from the map later.</li>
<li>The related backing image file is removed.</li>
<li>The download state of other disks keep unchanged.
All volumes still work fine.</li>
</ol>
</li>
<li>Delete all volumes. Verify that there will only remain 1 entry in the
backing image disk map</li>
<li>Delete the backing image.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.backing_image  # NOQA
def test_setting_backing_image_auto_cleanup(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the Backing Image Cleanup Wait Interval setting works correctly.

    The default value of setting `BackingImageCleanupWaitInterval` is 60.

    1. Clean up the backing image work directory so that the current case
       won&#39;t be intervened by previous tests.
    2. Create a backing image.
    3. Create multiple volumes using the backing image.
    4. Attach all volumes, Then:
        1. Wait for all volumes can become running.
        2. Verify the correct in all volumes.
        3. Verify the backing image disk status map.
        4. Verify the only backing image file in each disk is reused by
           multiple replicas. The backing image file path is
           `&lt;Data path&gt;/&lt;The backing image name&gt;/backing`
    5. Unschedule test node to guarantee when replica removed from test node,
       no new replica can be rebuilt on the test node.
    6. Remove all replicas in one disk.
       Wait for 50 seconds.
       Then verify nothing changes in the backing image disk state map
       (before the cleanup wait interval is passed).
    7. Modify `BackingImageCleanupWaitInterval` to a small value. Then verify:
        1. The download state of the disk containing no replica becomes
           terminating first, and the entry will be removed from the map later.
        2. The related backing image file is removed.
        3. The download state of other disks keep unchanged.
           All volumes still work fine.
    8. Delete all volumes. Verify that there will only remain 1 entry in the
       backing image disk map
    9. Delete the backing image.
    &#34;&#34;&#34;

    # Step 1
    subprocess.check_call([&#34;rm&#34;, &#34;-rf&#34;, &#34;/var/lib/longhorn/backing-images&#34;])

    # Step 2
    create_backing_image_with_matching_url(
            client, BACKING_IMAGE_NAME, BACKING_IMAGE_QCOW2_URL)

    # Step 3
    volume_names = [
        volume_name + &#34;-1&#34;,
        volume_name + &#34;-2&#34;,
        volume_name + &#34;-3&#34;
    ]

    for volume_name in volume_names:
        create_and_check_volume(
            client, volume_name, 3, str(BACKING_IMAGE_EXT4_SIZE),
            BACKING_IMAGE_NAME)

    # Step 4
    lht_host_id = get_self_host_id()
    for volume_name in volume_names:
        volume = client.by_id_volume(volume_name)
        volume.attach(hostId=lht_host_id)
    for volume_name in volume_names:
        volume = wait_for_volume_healthy(client, volume_name)
        assert volume.backingImage == BACKING_IMAGE_NAME

    backing_image = client.by_id_backing_image(BACKING_IMAGE_NAME)
    assert len(backing_image.diskFileStatusMap) == 3
    for disk_id, status in iter(backing_image.diskFileStatusMap.items()):
        assert status.state == &#34;ready&#34;

    backing_images_in_disk = os.listdir(&#34;/var/lib/longhorn/backing-images&#34;)
    assert len(backing_images_in_disk) == 1
    assert os.path.exists(&#34;/var/lib/longhorn/backing-images/{}/backing&#34;
                          .format(backing_images_in_disk[0]))
    assert os.path.exists(&#34;/var/lib/longhorn/backing-images/{}/backing.cfg&#34;
                          .format(backing_images_in_disk[0]))

    # Step 5
    current_host = client.by_id_node(id=lht_host_id)
    client.update(current_host, allowScheduling=False)
    wait_for_node_update(client, lht_host_id, &#34;allowScheduling&#34;, False)

    # Step 6
    for volume_name in volume_names:
        volume = client.by_id_volume(volume_name)
        for replica in volume.replicas:
            if replica.hostId == lht_host_id:
                replica_name = replica.name
                volume.replicaRemove(name=replica_name)
    # This wait interval should be smaller than the setting value.
    # Otherwise, the backing image files may be cleaned up.
    time.sleep(int(BACKING_IMAGE_CLEANUP_WAIT_INTERVAL))
    check_backing_image_disk_map_status(client, BACKING_IMAGE_NAME, 3, &#34;ready&#34;)

    # Step 7
    update_setting(client, &#34;backing-image-cleanup-wait-interval&#34;, &#34;1&#34;)
    check_backing_image_disk_map_status(client, BACKING_IMAGE_NAME, 2, &#34;ready&#34;)

    backing_images_in_disk = os.listdir(&#34;/var/lib/longhorn/backing-images&#34;)
    assert len(backing_images_in_disk) == 0

    # Step 8
    for volume_name in volume_names:
        volume = client.by_id_volume(volume_name)
        client.delete(volume)
        wait_for_volume_delete(client, volume_name)

    check_backing_image_disk_map_status(client, BACKING_IMAGE_NAME, 1, &#34;ready&#34;)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_backup_target_update_via_configmap"><code class="name flex">
<span>def <span class="ident">test_setting_backup_target_update_via_configmap</span></span>(<span>core_api, request)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the backup target setting via configmap
1. Initialize longhorn-default-setting configmap
2. Update longhorn-default-setting configmap with a new backup-target
value
3. Verify the updated settings</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_backup_target_update_via_configmap(core_api, request):  # NOQA
    &#34;&#34;&#34;
    Test the backup target setting via configmap
    1. Initialize longhorn-default-setting configmap
    2. Update longhorn-default-setting configmap with a new backup-target
       value
    3. Verify the updated settings
    &#34;&#34;&#34;

    # Step 1
    client = get_longhorn_api_client()  # NOQA
    init_longhorn_default_setting_configmap(core_api, client)

    # Step 2
    target = &#34;s3://backupbucket-invalid@us-east-1/backupstore&#34;
    update_settings_via_configmap(core_api,
                                  client,
                                  [SETTING_BACKUP_TARGET],
                                  [target],
                                  request)
    # Step 3
    validate_settings(core_api,
                      client,
                      [SETTING_BACKUP_TARGET],
                      [target])</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_concurrent_rebuild_limit"><code class="name flex">
<span>def <span class="ident">test_setting_concurrent_rebuild_limit</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test if setting Concurrent Replica Rebuild Per Node Limit works correctly.</p>
<p>The default setting value is 0, which means no limit.</p>
<p>Case 1 - the setting will limit the rebuilding correctly:
1. Set <code>ConcurrentReplicaRebuildPerNodeLimit</code> to 1.
2. Create 2 volumes then attach both volumes.
3. Write a large amount of data into both volumes,
so that the rebuilding will take a while.
4. Delete one replica for volume 1 then the replica on the same node for
volume 2 to trigger (concurrent) rebuilding.
5. Verify the new replica of volume 2 won't be started until volume 1
rebuilding complete.
And the new replica of volume 2 will be started immediately once
the 1st rebuilding is done.
6. Wait for rebuilding complete then repeat step 4.
7. Set <code>ConcurrentReplicaRebuildPerNodeLimit</code> to 0 or 2 while the volume 1
rebuilding is still in progress.
Then the new replica of volume 2 will be started immediately before
the 1st rebuilding is done.
8. Wait for rebuilding complete then repeat step 4.
9. Set <code>ConcurrentReplicaRebuildPerNodeLimit</code> to 1
10. Crash the replica process of volume 1 while the rebuilding is
in progress.
Then the rebuilding of volume 2 will be started, and the rebuilding of
volume 1 will wait for the volume 2 becoming healthy.</p>
<p>(There is no need to clean up the above 2 volumes.)</p>
<p>Case 2 - the setting won't intervene normal attachment:
1. Set <code>ConcurrentReplicaRebuildPerNodeLimit</code> to 1.
2. Make volume 1 state attached and healthy while volume 2 is detached.
3. Delete one replica for volume 1 to trigger the rebuilding.
4. Attach then detach volume 2. The attachment/detachment should succeed
even if the rebuilding in volume 1 is still in progress.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_concurrent_rebuild_limit(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test if setting Concurrent Replica Rebuild Per Node Limit works correctly.

    The default setting value is 0, which means no limit.

    Case 1 - the setting will limit the rebuilding correctly:
    1. Set `ConcurrentReplicaRebuildPerNodeLimit` to 1.
    2. Create 2 volumes then attach both volumes.
    3. Write a large amount of data into both volumes,
       so that the rebuilding will take a while.
    4. Delete one replica for volume 1 then the replica on the same node for
       volume 2 to trigger (concurrent) rebuilding.
    5. Verify the new replica of volume 2 won&#39;t be started until volume 1
       rebuilding complete.
       And the new replica of volume 2 will be started immediately once
       the 1st rebuilding is done.
    6. Wait for rebuilding complete then repeat step 4.
    7. Set `ConcurrentReplicaRebuildPerNodeLimit` to 0 or 2 while the volume 1
       rebuilding is still in progress.
       Then the new replica of volume 2 will be started immediately before
       the 1st rebuilding is done.
    8. Wait for rebuilding complete then repeat step 4.
    9. Set `ConcurrentReplicaRebuildPerNodeLimit` to 1
    10. Crash the replica process of volume 1 while the rebuilding is
        in progress.
        Then the rebuilding of volume 2 will be started, and the rebuilding of
        volume 1 will wait for the volume 2 becoming healthy.

   (There is no need to clean up the above 2 volumes.)

    Case 2 - the setting won&#39;t intervene normal attachment:
    1. Set `ConcurrentReplicaRebuildPerNodeLimit` to 1.
    2. Make volume 1 state attached and healthy while volume 2 is detached.
    3. Delete one replica for volume 1 to trigger the rebuilding.
    4. Attach then detach volume 2. The attachment/detachment should succeed
       even if the rebuilding in volume 1 is still in progress.
   &#34;&#34;&#34;
    # Step 1-1
    update_setting(client,
                   &#34;concurrent-replica-rebuild-per-node-limit&#34;,
                   &#34;1&#34;)

    # Step 1-2
    volume1_name = &#34;test-vol-1&#34;  # NOQA
    volume1 = create_and_check_volume(client, volume1_name, size=str(4 * Gi))
    volume1.attach(hostId=get_self_host_id())
    volume1 = wait_for_volume_healthy(client, volume1_name)

    volume2_name = &#34;test-vol-2&#34;  # NOQA
    volume2 = create_and_check_volume(client, volume2_name, size=str(4 * Gi))
    volume2.attach(hostId=get_self_host_id())
    volume2 = wait_for_volume_healthy(client, volume2_name)

    # Step 1-3
    volume1_endpoint = get_volume_endpoint(volume1)
    volume2_endpoint = get_volume_endpoint(volume2)
    write_volume_dev_random_mb_data(volume1_endpoint,
                                    1, 3500)
    write_volume_dev_random_mb_data(volume2_endpoint,
                                    1, 3500)

    # Step 1-4, 1-5
    delete_replica_on_test_node(client, volume1_name)
    wait_for_rebuild_start(client, volume1_name)
    delete_replica_on_test_node(client, volume2_name)

    for i in range(RETRY_COUNTS):
        volume1 = client.by_id_volume(volume1_name)
        volume2 = client.by_id_volume(volume2_name)

        if volume1.rebuildStatus == []:
            break

        assert volume1.rebuildStatus[0].state == &#34;in_progress&#34;
        assert volume2.rebuildStatus == []

        time.sleep(RETRY_INTERVAL)

    wait_for_rebuild_complete(client, volume1_name)
    wait_for_rebuild_start(client, volume2_name)
    wait_for_rebuild_complete(client, volume2_name)

    # Step 1-6
    wait_for_volume_healthy(client, volume1_name)
    wait_for_volume_healthy(client, volume2_name)

    # Step 1-7
    delete_replica_on_test_node(client, volume1_name)
    delete_replica_on_test_node(client, volume2_name)
    update_setting(client,
                   &#34;concurrent-replica-rebuild-per-node-limit&#34;,
                   &#34;2&#34;)

    # In a 2 minutes retry loop:
    # verify that volume 2 start rebuilding while volume 1 is still rebuilding
    concourent_build = False
    for i in range(RETRY_EXEC_COUNTS):
        volume1 = client.by_id_volume(volume1_name)
        volume2 = client.by_id_volume(volume2_name)
        try:
            if volume1.rebuildStatus[0].state == &#34;in_progress&#34; and \
                    volume2.rebuildStatus[0].state == &#34;in_progress&#34;:
                concourent_build = True
                break
        except: # NOQA
            pass
        time.sleep(RETRY_SNAPSHOT_INTERVAL)
    assert concourent_build is True

    # Step 1-8
    wait_for_rebuild_complete(client, volume1_name)
    wait_for_rebuild_complete(client, volume2_name)

    # Step 1-9
    update_setting(client,
                   &#34;concurrent-replica-rebuild-per-node-limit&#34;,
                   &#34;1&#34;)

    # Step 1-10
    delete_replica_on_test_node(client, volume1_name)
    wait_for_rebuild_start(client, volume1_name)
    volume1 = client.by_id_volume(volume1_name)
    current_node = get_self_host_id()
    replicas = []
    for r in volume1.replicas:
        if r[&#34;hostId&#34;] == current_node:
            replicas.append(r)

    assert len(replicas) &gt; 0
    crash_replica_processes(client, core_api, volume1_name, replicas)
    delete_replica_on_test_node(client, volume2_name)

    # While volume 2 is rebuilding, verify that volume 1 is not
    # rebuilding and stuck in degrading state
    wait_for_rebuild_start(client, volume2_name)
    for i in range(RETRY_COUNTS):
        volume1 = client.by_id_volume(volume1_name)
        volume2 = client.by_id_volume(volume2_name)

        if volume2.rebuildStatus == []:
            break

        assert volume2.rebuildStatus[0].state == &#34;in_progress&#34;
        assert volume1.rebuildStatus == []

        time.sleep(RETRY_INTERVAL)

    wait_for_rebuild_complete(client, volume2_name)
    wait_for_rebuild_start(client, volume1_name)
    wait_for_rebuild_complete(client, volume1_name)

    # Step 2-1
    # Step 2-2
    wait_for_volume_healthy(client, volume1_name)
    wait_for_volume_healthy(client, volume2_name)

    volume2 = client.by_id_volume(volume2_name)
    lht_host_id = get_self_host_id()
    volume2.detach(hostId=lht_host_id)

    # Step 2-2
    delete_replica_on_test_node(client, volume1_name)
    wait_for_rebuild_start(client, volume1_name)

    # Step 2-3
    volume2 = client.by_id_volume(volume2_name)
    volume2.attach(hostId=lht_host_id)

    # In a 2 minutes retry loop:
    # verify that we can see the case: volume2 becomes healthy while
    # volume1 is rebuilding
    expect_case = False
    for i in range(RETRY_COUNTS):
        volume1 = client.by_id_volume(volume1_name)
        volume2 = client.by_id_volume(volume2_name)

        try:
            if volume1.rebuildStatus[0].state == &#34;in_progress&#34; and \
                    volume2[&#34;robustness&#34;] == &#34;healthy&#34;:
                expect_case = True
                break
        except: # NOQA
            pass
        time.sleep(RETRY_INTERVAL)
    assert expect_case is True

    wait_for_volume_healthy(client, volume1_name)
    volume2.detach(hostId=lht_host_id)
    wait_for_volume_detached(client, volume2_name)

    volume2.attach(hostId=lht_host_id)
    wait_for_volume_healthy(client, volume2_name)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_priority_class"><code class="name flex">
<span>def <span class="ident">test_setting_priority_class</span></span>(<span>core_api, apps_api, scheduling_api, priority_class, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that the Priority Class setting is validated and utilized correctly.</p>
<ol>
<li>Verify that the name of a non-existent Priority Class cannot be used
for the Setting.</li>
<li>Create a new Priority Class in Kubernetes.</li>
<li>Create and attach a Volume.</li>
<li>Verify that the Priority Class Setting cannot be updated with an
attached Volume.</li>
<li>Generate and write <code>data1</code>.</li>
<li>Detach the Volume.</li>
<li>Update the Priority Class Setting to the new Priority Class.</li>
<li>Wait for all the Longhorn system components to restart with the new
Priority Class.</li>
<li>Verify that UI, manager, and drive deployer don't have Priority Class</li>
<li>Attach the Volume and verify <code>data1</code>.</li>
<li>Generate and write <code>data2</code>.</li>
<li>Unset the Priority Class Setting.</li>
<li>Wait for all the Longhorn system components to restart with the new
Priority Class.</li>
<li>Verify that UI, manager, and drive deployer don't have Priority Class</li>
<li>Attach the Volume and verify <code>data2</code>.</li>
<li>Generate and write <code>data3</code>.</li>
</ol>
<p>Note: system components are workloads other than UI, manager, driver
deployer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_priority_class(core_api, apps_api, scheduling_api, priority_class, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the Priority Class setting is validated and utilized correctly.

    1. Verify that the name of a non-existent Priority Class cannot be used
    for the Setting.
    2. Create a new Priority Class in Kubernetes.
    3. Create and attach a Volume.
    4. Verify that the Priority Class Setting cannot be updated with an
    attached Volume.
    5. Generate and write `data1`.
    6. Detach the Volume.
    7. Update the Priority Class Setting to the new Priority Class.
    8. Wait for all the Longhorn system components to restart with the new
       Priority Class.
    9. Verify that UI, manager, and drive deployer don&#39;t have Priority Class
    10. Attach the Volume and verify `data1`.
    11. Generate and write `data2`.
    12. Unset the Priority Class Setting.
    13. Wait for all the Longhorn system components to restart with the new
        Priority Class.
    14. Verify that UI, manager, and drive deployer don&#39;t have Priority Class
    15. Attach the Volume and verify `data2`.
    16. Generate and write `data3`.

    Note: system components are workloads other than UI, manager, driver
     deployer
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    count = len(client.list_node())
    name = priority_class[&#39;metadata&#39;][&#39;name&#39;]
    setting = client.by_id_setting(SETTING_PRIORITY_CLASS)

    with pytest.raises(Exception) as e:
        client.update(setting, value=name)
    assert &#39;failed to get priority class &#39; in str(e.value)

    scheduling_api.create_priority_class(priority_class)

    volume = create_and_check_volume(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        client.update(setting, value=name)
    assert &#39;cannot modify priority class setting before all volumes are &#39; \
           &#39;detached&#39; in str(e.value)

    data1 = write_volume_random_data(volume)
    check_volume_data(volume, data1)

    volume.detach(hostId=&#34;&#34;)
    wait_for_volume_detached(client, volume_name)

    setting = client.update(setting, value=name)
    assert setting.value == name

    wait_for_priority_class_update(core_api, apps_api, count, priority_class)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data1)
    data2 = write_volume_random_data(volume)
    check_volume_data(volume, data2)
    volume.detach(hostId=&#34;&#34;)
    wait_for_volume_detached(client, volume_name)

    setting = client.by_id_setting(SETTING_PRIORITY_CLASS)
    setting = client.update(setting, value=&#39;&#39;)
    assert setting.value == &#39;&#39;
    wait_for_priority_class_update(core_api, apps_api, count)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data2)
    data3 = write_volume_random_data(volume)
    check_volume_data(volume, data3)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_replica_count_update_via_configmap"><code class="name flex">
<span>def <span class="ident">test_setting_replica_count_update_via_configmap</span></span>(<span>core_api, request)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the default-replica-count setting via configmap
1. Get default-replica-count value
2. Initialize longhorn-default-setting configmap
3. Verify default-replica-count is not changed
4. Update longhorn-default-setting configmap with a new
default-replica-count value
5. Verify the updated settings
6. Update default-replica-count setting CR with the old value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_replica_count_update_via_configmap(core_api, request):  # NOQA
    &#34;&#34;&#34;
    Test the default-replica-count setting via configmap
    1. Get default-replica-count value
    2. Initialize longhorn-default-setting configmap
    3. Verify default-replica-count is not changed
    4. Update longhorn-default-setting configmap with a new
       default-replica-count value
    5. Verify the updated settings
    6. Update default-replica-count setting CR with the old value
    &#34;&#34;&#34;

    # Step 1
    client = get_longhorn_api_client()  # NOQA
    old_setting = client.by_id_setting(SETTING_DEFAULT_REPLICA_COUNT)

    # Step 2
    init_longhorn_default_setting_configmap(core_api, client)

    # Step 3
    assert wait_for_setting_updated(client,
                                    SETTING_DEFAULT_REPLICA_COUNT,
                                    old_setting.value)

    # Step 4
    replica_count = &#34;1&#34;
    update_settings_via_configmap(core_api,
                                  client,
                                  [SETTING_DEFAULT_REPLICA_COUNT],
                                  [replica_count],
                                  request)
    # Step 5
    validate_settings(core_api,
                      client,
                      [SETTING_DEFAULT_REPLICA_COUNT],
                      [replica_count])
    # Step 6
    retry_setting_update(client,
                         SETTING_DEFAULT_REPLICA_COUNT,
                         old_setting.definition.default)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_toleration"><code class="name flex">
<span>def <span class="ident">test_setting_toleration</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Test toleration setting</p>
<ol>
<li>Set <code>taint-toleration</code> to "key1=value1:NoSchedule; key2:InvalidEffect".</li>
<li>Verify the request fails.</li>
<li>Create a volume and attach it.</li>
<li>Set <code>taint-toleration</code> to "key1=value1:NoSchedule; key2:NoExecute".</li>
<li>Verify that cannot update toleration setting when any volume is
attached.</li>
<li>Generate and write <code>data1</code> into the volume.</li>
<li>Detach the volume.</li>
<li>Set <code>taint-toleration</code> to "key1=value1:NoSchedule; key2:NoExecute".</li>
<li>Wait for all the Longhorn system components to restart with new
toleration.</li>
<li>Verify that UI, manager, and drive deployer don't restart and
don't have new toleration.</li>
<li>Attach the volume again and verify the volume <code>data1</code>.</li>
<li>Generate and write <code>data2</code> to the volume.</li>
<li>Detach the volume.</li>
<li>Clean the <code>toleration</code> setting.</li>
<li>Wait for all the Longhorn system components to restart with no
toleration.</li>
<li>Attach the volume and validate <code>data2</code>.</li>
<li>Generate and write <code>data3</code> to the volume.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_toleration():
    &#34;&#34;&#34;
    Test toleration setting

    1.  Set `taint-toleration` to &#34;key1=value1:NoSchedule; key2:InvalidEffect&#34;.
    2.  Verify the request fails.
    3.  Create a volume and attach it.
    4.  Set `taint-toleration` to &#34;key1=value1:NoSchedule; key2:NoExecute&#34;.
    5.  Verify that cannot update toleration setting when any volume is
        attached.
    6.  Generate and write `data1` into the volume.
    7.  Detach the volume.
    8.  Set `taint-toleration` to &#34;key1=value1:NoSchedule; key2:NoExecute&#34;.
    9.  Wait for all the Longhorn system components to restart with new
        toleration.
    10. Verify that UI, manager, and drive deployer don&#39;t restart and
        don&#39;t have new toleration.
    11. Attach the volume again and verify the volume `data1`.
    12. Generate and write `data2` to the volume.
    13. Detach the volume.
    14. Clean the `toleration` setting.
    15. Wait for all the Longhorn system components to restart with no
        toleration.
    16. Attach the volume and validate `data2`.
    17. Generate and write `data3` to the volume.
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    apps_api = get_apps_api_client()  # NOQA
    core_api = get_core_api_client()  # NOQA
    count = len(client.list_node())

    setting = client.by_id_setting(SETTING_TAINT_TOLERATION)

    with pytest.raises(Exception) as e:
        client.update(setting,
                      value=&#34;key1=value1:NoSchedule; key2:InvalidEffect&#34;)
    assert &#39;invalid effect&#39; in str(e.value)

    volume_name = &#34;test-toleration-vol&#34;  # NOQA
    volume = create_and_check_volume(client, volume_name)
    volume.attach(hostId=get_self_host_id())
    volume = wait_for_volume_healthy(client, volume_name)

    setting_value_str = &#34;key1=value1:NoSchedule; key2:NoExecute&#34;
    setting_value_dicts = [
        {
            &#34;key&#34;: &#34;key1&#34;,
            &#34;value&#34;: &#34;value1&#34;,
            &#34;operator&#34;: &#34;Equal&#34;,
            &#34;effect&#34;: &#34;NoSchedule&#34;
        },
        {
            &#34;key&#34;: &#34;key2&#34;,
            &#34;value&#34;: None,
            &#34;operator&#34;: &#34;Exists&#34;,
            &#34;effect&#34;: &#34;NoExecute&#34;
        },
    ]
    with pytest.raises(Exception) as e:
        client.update(setting, value=setting_value_str)
    assert &#39;cannot modify toleration setting before all volumes are detached&#39; \
           in str(e.value)

    data1 = write_volume_random_data(volume)
    check_volume_data(volume, data1)

    volume.detach(hostId=&#34;&#34;)
    wait_for_volume_detached(client, volume_name)

    setting = client.update(setting, value=setting_value_str)
    assert setting.value == setting_value_str
    wait_for_toleration_update(core_api, apps_api, count, setting_value_dicts)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data1)
    data2 = write_volume_random_data(volume)
    check_volume_data(volume, data2)
    volume.detach(hostId=&#34;&#34;)
    wait_for_volume_detached(client, volume_name)

    # cleanup
    setting_value_str = &#34;&#34;
    setting_value_dicts = []
    setting = client.by_id_setting(SETTING_TAINT_TOLERATION)
    setting = client.update(setting, value=setting_value_str)
    assert setting.value == setting_value_str
    wait_for_toleration_update(core_api, apps_api, count, setting_value_dicts)

    client, node = wait_for_longhorn_node_ready()

    volume = client.by_id_volume(volume_name)
    volume.attach(hostId=node)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data2)
    data3 = write_volume_random_data(volume)
    check_volume_data(volume, data3)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_toleration_extra"><code class="name flex">
<span>def <span class="ident">test_setting_toleration_extra</span></span>(<span>core_api, apps_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Steps:
1. Set Kubernetes Taint Toleration to:
<code>ex.com/foobar:NoExecute;ex.com/foobar:NoSchedule</code>.
2. Verify that all system components have the 2 tolerations
<code>ex.com/foobar:NoExecute; ex.com/foobar:NoSchedule</code>.
Verify that UI, manager, and drive deployer don't restart and
don't have toleration.
3. Set Kubernetes Taint Toleration to:
<code>node-role.kubernetes.io/controlplane=true:NoSchedule</code>.
4. Verify that all system components have the the toleration
<code>node-role.kubernetes.io/controlplane=true:NoSchedule</code>,
and don't have the 2 tolerations
<code>ex.com/foobar:NoExecute;ex.com/foobar:NoSchedule</code>.
Verify that UI, manager, and drive deployer don't restart and
don't have toleration.
5. Set Kubernetes Taint Toleration to special value:
<code>:</code>.
6. Verify that all system components have the toleration with
<code>operator: Exists</code> and other field of the toleration are empty.
Verify that all system components don't have the toleration
<code>node-role.kubernetes.io/controlplane=true:NoSchedule</code>.
Verify that UI, manager, and drive deployer don't restart and
don't have toleration.
7. Clear Kubernetes Taint Toleration</p>
<p>Note: system components are workloads other than UI, manager, driver
deployer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_toleration_extra(core_api, apps_api):  # NOQA
    &#34;&#34;&#34;
    Steps:
    1. Set Kubernetes Taint Toleration to:
       `ex.com/foobar:NoExecute;ex.com/foobar:NoSchedule`.
    2. Verify that all system components have the 2 tolerations
       `ex.com/foobar:NoExecute; ex.com/foobar:NoSchedule`.
       Verify that UI, manager, and drive deployer don&#39;t restart and
       don&#39;t have toleration.
    3. Set Kubernetes Taint Toleration to:
       `node-role.kubernetes.io/controlplane=true:NoSchedule`.
    4. Verify that all system components have the the toleration
       `node-role.kubernetes.io/controlplane=true:NoSchedule`,
       and don&#39;t have the 2 tolerations
       `ex.com/foobar:NoExecute;ex.com/foobar:NoSchedule`.
       Verify that UI, manager, and drive deployer don&#39;t restart and
       don&#39;t have toleration.
    5. Set Kubernetes Taint Toleration to special value:
       `:`.
    6. Verify that all system components have the toleration with
       `operator: Exists` and other field of the toleration are empty.
       Verify that all system components don&#39;t have the toleration
       `node-role.kubernetes.io/controlplane=true:NoSchedule`.
       Verify that UI, manager, and drive deployer don&#39;t restart and
       don&#39;t have toleration.
    7. Clear Kubernetes Taint Toleration

    Note: system components are workloads other than UI, manager, driver
    deployer
    &#34;&#34;&#34;
    settings = [
        {
            &#34;value&#34;: &#34;ex.com/foobar:NoExecute;ex.com/foobar:NoSchedule&#34;,
            &#34;expect&#34;: [
                {
                    &#34;key&#34;: &#34;ex.com/foobar&#34;,
                    &#34;value&#34;: None,
                    &#34;operator&#34;: &#34;Exists&#34;,
                    &#34;effect&#34;: &#34;NoExecute&#34;
                },
                {
                    &#34;key&#34;: &#34;ex.com/foobar&#34;,
                    &#34;value&#34;: None,
                    &#34;operator&#34;: &#34;Exists&#34;,
                    &#34;effect&#34;: &#34;NoSchedule&#34;
                },
            ],
        },
        {
            &#34;value&#34;: &#34;node-role.kubernetes.io/controlplane=true:NoSchedule&#34;,
            &#34;expect&#34;: [
                {
                    &#34;key&#34;: &#34;node-role.kubernetes.io/controlplane&#34;,
                    &#34;value&#34;: &#34;true&#34;,
                    &#34;operator&#34;: &#34;Equal&#34;,
                    &#34;effect&#34;: &#34;NoSchedule&#34;
                },
            ],
        },
        # Skip the this special toleration for now because it makes
        # Longhorn deploy manager pods on control/etcd nodes
        # and the control/etcd nodes become &#34;down&#34; after the test
        # clear this toleration.
        # We will enable this test once we implement logic for
        # deleting failed nodes.
        # {
        #     &#34;value&#34;: &#34;:&#34;,
        #     &#34;expect&#34;: [
        #         {
        #             &#34;key&#34;: None,
        #             &#34;value&#34;: None,
        #             &#34;operator&#34;: &#34;Exists&#34;,
        #             &#34;effect&#34;: None,
        #         },
        #     ]
        # },
        {
            &#34;value&#34;: &#34;&#34;,
            &#34;expect&#34;: [],
        },
    ]

    chk_removed_tolerations = []
    for setting in settings:
        client = get_longhorn_api_client()  # NOQA
        taint_toleration = client.by_id_setting(SETTING_TAINT_TOLERATION)
        updated = client.update(taint_toleration,
                                value=setting[&#34;value&#34;])
        assert updated.value == setting[&#34;value&#34;]

        node_count = len(client.list_node())
        wait_for_toleration_update(core_api, apps_api, node_count,
                                   setting[&#34;expect&#34;], chk_removed_tolerations)
        chk_removed_tolerations = setting[&#34;expect&#34;]</code></pre>
</details>
</dd>
<dt id="tests.test_settings.test_setting_update_with_invalid_value_via_configmap"><code class="name flex">
<span>def <span class="ident">test_setting_update_with_invalid_value_via_configmap</span></span>(<span>core_api, request)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the default settings update with invalid value via configmap
1. Create an attached volume
2. Initialize longhorn-default-setting configmap containing
valid and invalid settings
3. Update longhorn-default-setting configmap with invalid settings.
The invalid settings SETTING_TAINT_TOLERATION will be ingored
when there is an attached volume.
4. Validate the default settings values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_setting_update_with_invalid_value_via_configmap(core_api, request):  # NOQA
    &#34;&#34;&#34;
    Test the default settings update with invalid value via configmap
    1. Create an attached volume
    2. Initialize longhorn-default-setting configmap containing
       valid and invalid settings
    3. Update longhorn-default-setting configmap with invalid settings.
       The invalid settings SETTING_TAINT_TOLERATION will be ingored
       when there is an attached volume.
    4. Validate the default settings values.
    &#34;&#34;&#34;

    # Step 1
    client = get_longhorn_api_client() # NOQA
    lht_hostId = get_self_host_id()

    vol_name = generate_volume_name()
    volume = create_volume(client, vol_name, str(Gi), lht_hostId, 3)

    volume.attach(hostId=lht_hostId)
    volume = wait_for_volume_healthy(client, vol_name)

    # Step 2
    init_longhorn_default_setting_configmap(core_api, client)

    # Step 3
    target = &#34;s3://backupbucket-invalid@us-east-1/backupstore&#34;
    update_settings_via_configmap(core_api,
                                  client,
                                  [SETTING_BACKUP_TARGET,
                                   SETTING_TAINT_TOLERATION],
                                  [target,
                                   &#34;key1=value1:NoSchedule&#34;],
                                  request)
    # Step 4
    validate_settings(core_api,
                      client,
                      [SETTING_BACKUP_TARGET,
                       SETTING_TAINT_TOLERATION],
                      [target,
                       &#34;&#34;])

    cleanup_volume_by_name(client, vol_name)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.update_settings_via_configmap"><code class="name flex">
<span>def <span class="ident">update_settings_via_configmap</span></span>(<span>core_api, client, setting_names, setting_values, request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_settings_via_configmap(core_api, client, setting_names, setting_values, request):  # NOQA
    configmap_body = config_map_with_value(&#34;longhorn-default-setting&#34;,
                                           setting_names,
                                           setting_values)
    core_api.patch_namespaced_config_map(name=&#34;longhorn-default-setting&#34;,
                                         namespace=&#39;longhorn-system&#39;,
                                         body=configmap_body)

    def reset_default_settings():
        configmap_body = config_map_with_value(&#34;longhorn-default-setting&#34;,
                                               [SETTING_DEFAULT_REPLICA_COUNT,
                                                SETTING_BACKUP_TARGET,
                                                SETTING_TAINT_TOLERATION],
                                               [&#34;3&#34;,
                                                &#34;&#34;,
                                                &#34;&#34;])
        core_api.patch_namespaced_config_map(name=&#34;longhorn-default-setting&#34;,
                                             namespace=&#39;longhorn-system&#39;,
                                             body=configmap_body)
    request.addfinalizer(reset_default_settings)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.validate_settings"><code class="name flex">
<span>def <span class="ident">validate_settings</span></span>(<span>core_api, client, setting_names, setting_values)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_settings(core_api, client, setting_names, setting_values):  # NOQA
    num_settings = len(setting_names)
    for i in range(num_settings):
        name = setting_names[i]
        value = setting_values[i]
        assert wait_for_setting_updated(client, name, value)</code></pre>
</details>
</dd>
<dt id="tests.test_settings.wait_for_longhorn_node_ready"><code class="name flex">
<span>def <span class="ident">wait_for_longhorn_node_ready</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_longhorn_node_ready():
    client = get_longhorn_api_client()  # NOQA

    ei = get_default_engine_image(client)
    ei_name = ei[&#34;name&#34;]
    ei_state = get_engine_image_status_value(client, ei_name)
    wait_for_engine_image_state(client, ei_name, ei_state)

    node = get_self_host_id()
    wait_for_node_up_longhorn(node, client)

    return client, node</code></pre>
</details>
</dd>
<dt id="tests.test_settings.wait_for_priority_class_update"><code class="name flex">
<span>def <span class="ident">wait_for_priority_class_update</span></span>(<span>core_api, apps_api, count, priority_class=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_priority_class_update(core_api, apps_api, count, priority_class=None):  # NOQA
    updated = False

    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL_LONG)
        updated = True

        if not check_workload_update(core_api, apps_api, count):
            updated = False
            continue

        pod_list = core_api.list_namespaced_pod(LONGHORN_NAMESPACE).items
        for p in pod_list:
            if p.status.phase != &#34;Running&#34; and \
                    not check_priority_class(p, priority_class):
                updated = False
                break
        if not updated:
            continue

        if updated:
            break

    assert updated</code></pre>
</details>
</dd>
<dt id="tests.test_settings.wait_for_setting_updated"><code class="name flex">
<span>def <span class="ident">wait_for_setting_updated</span></span>(<span>client, name, expected_value)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_setting_updated(client, name, expected_value):  # NOQA
    for _ in range(RETRY_COUNTS):
        setting = client.by_id_setting(name)
        if setting.value == expected_value:
            return True
        time.sleep(RETRY_INTERVAL)
    return False</code></pre>
</details>
</dd>
<dt id="tests.test_settings.wait_for_toleration_update"><code class="name flex">
<span>def <span class="ident">wait_for_toleration_update</span></span>(<span>core_api, apps_api, count, expected_tolerations, chk_removed_tolerations=[])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_toleration_update(core_api, apps_api, count,  # NOQA
                               expected_tolerations,
                               chk_removed_tolerations=[]):
    not_managed_apps = [
        &#34;csi-attacher&#34;,
        &#34;csi-provisioner&#34;,
        &#34;csi-resizer&#34;,
        &#34;csi-snapshotter&#34;,
        &#34;longhorn-csi-plugin&#34;,
        &#34;longhorn-driver-deployer&#34;,
        &#34;longhorn-manager&#34;,
        &#34;longhorn-ui&#34;,
    ]
    updated = False
    for _ in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL_LONG)

        updated = True
        if not check_workload_update(core_api, apps_api, count):
            updated = False
            continue

        pod_list = core_api.list_namespaced_pod(LONGHORN_NAMESPACE).items
        for p in pod_list:
            managed_by = p.metadata.labels.get(&#39;longhorn.io/managed-by&#39;, &#39;&#39;)
            if str(managed_by) != &#34;longhorn-manager&#34;:
                continue
            else:
                app_name = str(p.metadata.labels.get(&#39;app&#39;, &#39;&#39;))
                assert app_name not in not_managed_apps

            if p.status.phase != &#34;Running&#34; \
                or not check_tolerations_set(p.spec.tolerations,
                                             expected_tolerations,
                                             chk_removed_tolerations):
                updated = False
                break
        if updated:
            break
    assert updated</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_settings.check_priority_class" href="#tests.test_settings.check_priority_class">check_priority_class</a></code></li>
<li><code><a title="tests.test_settings.check_tolerations_set" href="#tests.test_settings.check_tolerations_set">check_tolerations_set</a></code></li>
<li><code><a title="tests.test_settings.check_workload_update" href="#tests.test_settings.check_workload_update">check_workload_update</a></code></li>
<li><code><a title="tests.test_settings.config_map_with_value" href="#tests.test_settings.config_map_with_value">config_map_with_value</a></code></li>
<li><code><a title="tests.test_settings.guaranteed_engine_cpu_setting_check" href="#tests.test_settings.guaranteed_engine_cpu_setting_check">guaranteed_engine_cpu_setting_check</a></code></li>
<li><code><a title="tests.test_settings.init_longhorn_default_setting_configmap" href="#tests.test_settings.init_longhorn_default_setting_configmap">init_longhorn_default_setting_configmap</a></code></li>
<li><code><a title="tests.test_settings.retry_setting_update" href="#tests.test_settings.retry_setting_update">retry_setting_update</a></code></li>
<li><code><a title="tests.test_settings.test_instance_manager_cpu_reservation" href="#tests.test_settings.test_instance_manager_cpu_reservation">test_instance_manager_cpu_reservation</a></code></li>
<li><code><a title="tests.test_settings.test_setting_backing_image_auto_cleanup" href="#tests.test_settings.test_setting_backing_image_auto_cleanup">test_setting_backing_image_auto_cleanup</a></code></li>
<li><code><a title="tests.test_settings.test_setting_backup_target_update_via_configmap" href="#tests.test_settings.test_setting_backup_target_update_via_configmap">test_setting_backup_target_update_via_configmap</a></code></li>
<li><code><a title="tests.test_settings.test_setting_concurrent_rebuild_limit" href="#tests.test_settings.test_setting_concurrent_rebuild_limit">test_setting_concurrent_rebuild_limit</a></code></li>
<li><code><a title="tests.test_settings.test_setting_priority_class" href="#tests.test_settings.test_setting_priority_class">test_setting_priority_class</a></code></li>
<li><code><a title="tests.test_settings.test_setting_replica_count_update_via_configmap" href="#tests.test_settings.test_setting_replica_count_update_via_configmap">test_setting_replica_count_update_via_configmap</a></code></li>
<li><code><a title="tests.test_settings.test_setting_toleration" href="#tests.test_settings.test_setting_toleration">test_setting_toleration</a></code></li>
<li><code><a title="tests.test_settings.test_setting_toleration_extra" href="#tests.test_settings.test_setting_toleration_extra">test_setting_toleration_extra</a></code></li>
<li><code><a title="tests.test_settings.test_setting_update_with_invalid_value_via_configmap" href="#tests.test_settings.test_setting_update_with_invalid_value_via_configmap">test_setting_update_with_invalid_value_via_configmap</a></code></li>
<li><code><a title="tests.test_settings.update_settings_via_configmap" href="#tests.test_settings.update_settings_via_configmap">update_settings_via_configmap</a></code></li>
<li><code><a title="tests.test_settings.validate_settings" href="#tests.test_settings.validate_settings">validate_settings</a></code></li>
<li><code><a title="tests.test_settings.wait_for_longhorn_node_ready" href="#tests.test_settings.wait_for_longhorn_node_ready">wait_for_longhorn_node_ready</a></code></li>
<li><code><a title="tests.test_settings.wait_for_priority_class_update" href="#tests.test_settings.wait_for_priority_class_update">wait_for_priority_class_update</a></code></li>
<li><code><a title="tests.test_settings.wait_for_setting_updated" href="#tests.test_settings.wait_for_setting_updated">wait_for_setting_updated</a></code></li>
<li><code><a title="tests.test_settings.wait_for_toleration_update" href="#tests.test_settings.wait_for_toleration_update">wait_for_toleration_update</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>