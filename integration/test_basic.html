<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tests.test_basic API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_basic</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pytest

import os
import subprocess
import time
import random
import yaml

import common
from common import client, random_labels, volume_name, clients  # NOQA
from common import core_api, apps_api, pod, statefulset   # NOQA
from common import SIZE, EXPAND_SIZE
from common import check_device_data, write_device_random_data
from common import check_volume_data, write_volume_random_data
from common import get_self_host_id, volume_valid
from common import iscsi_login, iscsi_logout
from common import wait_for_volume_status
from common import wait_for_volume_delete
from common import wait_for_snapshot_purge
from common import generate_volume_name
from common import get_volume_endpoint, get_volume_engine
from common import check_volume_endpoint
from common import activate_standby_volume, check_volume_last_backup
from common import create_pv_for_volume, create_pvc_for_volume
from common import create_and_wait_pod, delete_and_wait_pod
from common import delete_and_wait_pvc, delete_and_wait_pv
from common import CONDITION_STATUS_FALSE, CONDITION_STATUS_TRUE
from common import RETRY_COUNTS, RETRY_INTERVAL, RETRY_COMMAND_COUNT
from common import DEFAULT_POD_TIMEOUT, DEFAULT_POD_INTERVAL
from common import cleanup_volume, create_and_check_volume, create_backup
from common import DEFAULT_VOLUME_SIZE
from common import Gi, Mi, Ki
from common import wait_for_volume_detached
from common import create_pvc_spec
from common import generate_random_data, write_volume_data
from common import VOLUME_RWTEST_SIZE
from common import write_pod_volume_data
from common import find_backup, find_replica_for_backup
from common import wait_for_backup_completion
from common import create_storage_class
from common import wait_for_backup_restore_completed
from common import wait_for_volume_restoration_completed
from common import read_volume_data
from common import pvc_name  # NOQA
from common import storage_class  # NOQA
from common import pod_make, csi_pv, pvc  # NOQA
from common import create_snapshot
from common import expand_attached_volume
from common import wait_for_dr_volume_expansion
from common import check_block_device_size, get_device_checksum
from common import wait_for_volume_expansion
from common import fail_replica_expansion, wait_for_expansion_failure
from common import wait_for_volume_restoration_start
from common import write_pod_volume_random_data, get_pod_data_md5sum
from common import prepare_pod_with_data_in_mb, exec_command_in_pod
from common import crash_replica_processes
from common import wait_for_volume_condition_scheduled
from common import wait_for_volume_condition_toomanysnapshots
from common import wait_for_volume_degraded, wait_for_volume_healthy
from common import VOLUME_FRONTEND_BLOCKDEV, VOLUME_FRONTEND_ISCSI
from common import VOLUME_CONDITION_SCHEDULED
from common import DATA_SIZE_IN_MB_1
from common import SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY
from common import SETTING_REPLICA_REPLENISHMENT_WAIT_INTERVAL
from common import CONDITION_REASON_SCHEDULING_FAILURE
from common import delete_backup, get_backupstores
from common import delete_backup_volume
from common import BACKUP_BLOCK_SIZE
from common import assert_backup_state
from common import wait_for_backup_delete
from common import VOLUME_FIELD_ROBUSTNESS, VOLUME_FIELD_READY
from common import VOLUME_ROBUSTNESS_HEALTHY, VOLUME_ROBUSTNESS_FAULTED
from common import DATA_SIZE_IN_MB_2, DATA_SIZE_IN_MB_3
from common import wait_for_backup_to_start
from common import SETTING_DEFAULT_LONGHORN_STATIC_SC
from common import wait_for_backup_volume
from common import create_and_wait_statefulset
from common import create_backup_from_volume_attached_to_pod
from common import restore_backup_and_get_data_checksum
from common import write_volume_dev_random_mb_data
from common import VOLUME_HEAD_NAME
from common import set_node_scheduling
from common import SETTING_FAILED_BACKUP_TTL
from common import wait_for_volume_creation

from backupstore import backupstore_delete_volume_cfg_file
from backupstore import backupstore_cleanup
from backupstore import backupstore_count_backup_block_files
from backupstore import backupstore_create_dummy_in_progress_backup
from backupstore import backupstore_delete_dummy_in_progress_backup
from backupstore import backupstore_create_file
from backupstore import backupstore_delete_file
from backupstore import set_random_backupstore  # NOQA
from backupstore import backupstore_get_backup_volume_prefix
from backupstore import set_backupstore_url, set_backupstore_credential_secret, set_backupstore_poll_interval  # NOQA
from backupstore import reset_backupstore_setting  # NOQA
from backupstore import set_backupstore_s3, backupstore_get_secret  # NOQA

from kubernetes import client as k8sclient

BACKUPSTORE = get_backupstores()


@pytest.mark.coretest   # NOQA
def test_hosts(client):  # NOQA
    &#34;&#34;&#34;
    Check node name and IP
    &#34;&#34;&#34;
    hosts = client.list_node()
    for host in hosts:
        assert host.name is not None
        assert host.address is not None

    host_id = []
    for i in range(0, len(hosts)):
        host_id.append(hosts.data[i].name)

    host0_from_i = {}
    for i in range(0, len(hosts)):
        if len(host0_from_i) == 0:
            host0_from_i = client.by_id_node(host_id[0])
        else:
            assert host0_from_i.name == \
                client.by_id_node(host_id[0]).name
            assert host0_from_i.address == \
                client.by_id_node(host_id[0]).address


@pytest.mark.coretest   # NOQA
def test_settings(client):  # NOQA
    &#34;&#34;&#34;
    Check input for settings
    &#34;&#34;&#34;

    setting_names = [common.SETTING_BACKUP_TARGET,
                     common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET,
                     common.SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE,
                     common.SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE,
                     common.SETTING_DEFAULT_REPLICA_COUNT]
    settings = client.list_setting()

    settingMap = {}
    for setting in settings:
        settingMap[setting.name] = setting

    for name in setting_names:
        assert settingMap[name] is not None
        assert settingMap[name].definition.description is not None

    for name in setting_names:
        setting = client.by_id_setting(name)
        assert settingMap[name].value == setting.value

        old_value = setting.value

        if name == common.SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-100&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            setting = client.update(setting, value=&#34;200&#34;)
            assert setting.value == &#34;200&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;200&#34;
        elif name == common.SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;300&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-30&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            setting = client.update(setting, value=&#34;30&#34;)
            assert setting.value == &#34;30&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;30&#34;
        elif name == common.SETTING_BACKUP_TARGET:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue$test&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            setting = client.update(setting, value=&#34;nfs://test&#34;)
            assert setting.value == &#34;nfs://test&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;nfs://test&#34;
        elif name == common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET:
            setting = client.update(setting, value=&#34;testvalue&#34;)
            assert setting.value == &#34;testvalue&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;testvalue&#34;
        elif name == common.SETTING_DEFAULT_REPLICA_COUNT:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-1&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;21&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            setting = client.update(setting, value=&#34;2&#34;)
            assert setting.value == &#34;2&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;2&#34;

        setting = client.update(setting, value=old_value)
        assert setting.value == old_value


def volume_rw_test(dev):
    assert volume_valid(dev)
    data = write_device_random_data(dev)
    check_device_data(dev, data)


@pytest.mark.coretest   # NOQA
def test_volume_basic(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic volume operations:

    1. Check volume name and parameter
    2. Create a volume and attach to the current node, then check volume states
    3. Check soft anti-affinity rule
    4. Write then read back to check volume data
    &#34;&#34;&#34;
    volume_basic_test(client, volume_name)


def volume_basic_test(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    num_hosts = len(client.list_node())
    num_replicas = 3

    with pytest.raises(Exception):
        volume = client.create_volume(name=&#34;wrong_volume-name-1.0&#34;, size=SIZE,
                                      numberOfReplicas=2)
        volume = client.create_volume(name=&#34;wrong_volume-name&#34;, size=SIZE,
                                      numberOfReplicas=2)
        volume = client.create_volume(name=&#34;wrong_volume-name&#34;, size=SIZE,
                                      numberOfReplicas=2,
                                      frontend=&#34;invalid_frontend&#34;)

    volume = create_and_check_volume(client, volume_name, num_replicas, SIZE,
                                     backing_image)
    assert volume.restoreRequired is False

    def validate_volume_basic(expected, actual):
        assert actual.name == expected.name
        assert actual.size == expected.size
        assert actual.numberOfReplicas == expected.numberOfReplicas
        assert actual.frontend == VOLUME_FRONTEND_BLOCKDEV
        assert actual.backingImage == backing_image
        assert actual.state == expected.state
        assert actual.created == expected.created

    volumes = client.list_volume().data
    assert len(volumes) == 1
    validate_volume_basic(volume, volumes[0])

    volumeByName = client.by_id_volume(volume_name)
    validate_volume_basic(volume, volumeByName)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert volume.restoreRequired is False

    volumeByName = client.by_id_volume(volume_name)
    validate_volume_basic(volume, volumeByName)
    check_volume_endpoint(volumeByName)

    # validate soft anti-affinity
    hosts = {}
    for replica in volume.replicas:
        id = replica.hostId
        assert id != &#34;&#34;
        hosts[id] = True
    if num_hosts &gt;= num_replicas:
        assert len(hosts) == num_replicas
    else:
        assert len(hosts) == num_hosts

    volumes = client.list_volume().data
    assert len(volumes) == 1
    assert volumes[0].name == volume.name
    assert volumes[0].size == volume.size
    assert volumes[0].numberOfReplicas == volume.numberOfReplicas
    assert volumes[0].state == volume.state
    assert volumes[0].created == volume.created
    check_volume_endpoint(volumes[0])

    volume = client.by_id_volume(volume_name)
    volume_rw_test(get_volume_endpoint(volume))

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert volume.restoreRequired is False

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    volumes = client.list_volume().data
    assert len(volumes) == 0


def test_volume_iscsi_basic(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic volume operations with iscsi frontend

    1. Create and attach a volume with iscsi frontend
    2. Check the volume endpoint and connect it using the iscsi
    initator on the node.
    3. Write then read back volume data for validation

    &#34;&#34;&#34;
    volume_iscsi_basic_test(client, volume_name)


def volume_iscsi_basic_test(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    host_id = get_self_host_id()
    volume = create_and_check_volume(client, volume_name, 3, SIZE,
                                     backing_image, VOLUME_FRONTEND_ISCSI)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volumes = client.list_volume().data
    assert len(volumes) == 1
    assert volumes[0].name == volume.name
    assert volumes[0].size == volume.size
    assert volumes[0].numberOfReplicas == volume.numberOfReplicas
    assert volumes[0].state == volume.state
    assert volumes[0].created == volume.created
    assert volumes[0].frontend == VOLUME_FRONTEND_ISCSI
    assert volumes[0].backingImage == volume.backingImage
    endpoint = get_volume_endpoint(volumes[0])

    try:
        dev = iscsi_login(endpoint)
        volume_rw_test(dev)
    finally:
        iscsi_logout(endpoint)

    cleanup_volume(client, volume)


@pytest.mark.coretest   # NOQA
def test_snapshot(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    &#34;&#34;&#34;
    Test snapshot operations

    1. Create a volume and attach to the node
    2. Create the empty snapshot `snap1`
    3. Generate and write data `snap2_data`, then create `snap2`
    4. Generate and write data `snap3_data`, then create `snap3`
    5. List snapshot. Validate the snapshot chain relationship
    6. Mark `snap3` as removed. Make sure volume&#39;s data didn&#39;t change
    7. List snapshot. Make sure `snap3` is marked as removed
    8. Detach and reattach the volume in maintenance mode.
    9. Make sure the volume frontend is still `blockdev` but disabled
    10. Revert to `snap2`
    11. Detach and reattach the volume with frontend enabled
    12. Make sure volume&#39;s data is `snap2_data`
    13. List snapshot. Make sure `volume-head` is now `snap2`&#39;s child
    14. Delete `snap1` and `snap2`
    15. Purge the snapshot.
    16. List the snapshot, make sure `snap1` and `snap3`
    are gone. `snap2` is marked as removed.
    17. Check volume data, make sure it&#39;s still `snap2_data`.
    &#34;&#34;&#34;
    snapshot_test(client, volume_name, backing_image)


def snapshot_test(client, volume_name, backing_image):  # NOQA
    volume = create_and_check_volume(client, volume_name,
                                     backing_image=backing_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    positions = {}

    snap1 = create_snapshot(client, volume_name)

    snap2_data = write_volume_random_data(volume, positions)
    snap2 = create_snapshot(client, volume_name)

    snap3_data = write_volume_random_data(volume, positions)
    snap3 = create_snapshot(client, volume_name)

    snapshots = volume.snapshotList()
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert snapMap[snap3.name].removed is False

    volume.snapshotDelete(name=snap3.name)
    check_volume_data(volume, snap3_data)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert len(snapMap[snap3.name].children) == 1
    assert &#34;volume-head&#34; in snapMap[snap3.name].children.keys()
    assert snapMap[snap3.name].removed is True

    snap = volume.snapshotGet(name=snap3.name)
    assert snap.name == snap3.name
    assert snap.parent == snap3.parent
    assert len(snap3.children) == 1
    assert len(snap.children) == 1
    assert &#34;volume-head&#34; in snap3.children.keys()
    assert &#34;volume-head&#34; in snap.children.keys()
    assert snap.removed is True

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    common.wait_for_volume_healthy_no_frontend(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is True
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV
    check_volume_endpoint(volume)

    volume.snapshotRevert(name=snap2.name)

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    check_volume_data(volume, snap2_data)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert &#34;volume-head&#34; in snapMap[snap2.name].children.keys()
    assert snap3.name in snapMap[snap2.name].children.keys()
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert len(snapMap[snap3.name].children) == 0
    assert snapMap[snap3.name].removed is True

    volume.snapshotDelete(name=snap1.name)
    volume.snapshotDelete(name=snap2.name)

    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap1.name,
                                     snap3.name)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap
    assert snap1.name not in snapMap
    assert snap3.name not in snapMap

    # it&#39;s the parent of volume-head, so it cannot be purged at this time
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == &#34;&#34;
    assert &#34;volume-head&#34; in snapMap[snap2.name].children.keys()
    assert snapMap[snap2.name].removed is True
    check_volume_data(volume, snap2_data)

    cleanup_volume(client, volume)


def test_backup_status_for_unavailable_replicas(set_random_backupstore, client, volume_name):    # NOQA
    &#34;&#34;&#34;
    Test backup status for unavailable replicas

    Context:

    We want to make sure that during the backup creation, once the responsible
    replica gone, the backup should in Error state and with the error message.

    Setup:

    1. Create a volume and attach to the current node
    2. Run the test for all the available backupstores

    Steps:

    1. Create a backup of volume
    2. Find the replica for that backup
    3. Disable scheduling on the node of that replica
    4. Delete the replica
    5. Verify backup status with Error state and with an error message
    6. Create a new backup
    7. Verify new backup was successful
    8. Cleanup (delete backups, delete volume)
    &#34;&#34;&#34;
    backup_status_for_unavailable_replicas_test(
        client, volume_name, size=str(512 * Mi))


def backup_status_for_unavailable_replicas_test(client, volume_name,  # NOQA
                                                size, backing_image=&#34;&#34;):  # NOQA
    volume = create_and_check_volume(client, volume_name, 2, size,
                                     backing_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    # write data to the volume
    data = {
        &#39;pos&#39;: 0,
        &#39;content&#39;: common.generate_random_data(int(size)),
    }
    write_volume_data(volume, data)

    # create a snapshot and backup
    snap = create_snapshot(client, volume_name)
    volume.snapshotBackup(name=snap.name)
    bv, b = find_backup(client, volume_name, snap.name)
    backup_id = b.id

    # find the replica for this backup
    replica_name = find_replica_for_backup(client, volume_name, backup_id)

    # disable scheduling on that node
    volume = client.by_id_volume(volume_name)
    for r in volume.replicas:
        if r.name == replica_name:
            node = client.by_id_node(r.hostId)
            node = set_node_scheduling(client, node, allowScheduling=False)
            common.wait_for_node_update(client, node.id,
                                        &#34;allowScheduling&#34;, False)
    assert node

    # remove the replica with the backup
    volume.replicaRemove(name=replica_name)
    volume = common.wait_for_volume_degraded(client, volume_name)

    # now the backup status should in an Error state and with an error message
    def backup_failure_predicate(b):
        return b.id == backup_id and &#34;Error&#34; in b.state and b.error != &#34;&#34;
    volume = common.wait_for_backup_state(client, volume_name,
                                          backup_failure_predicate)

    # re enable scheduling on the previously disabled node
    node = client.by_id_node(node.id)
    node = set_node_scheduling(client, node, allowScheduling=True)
    common.wait_for_node_update(client, node.id,
                                &#34;allowScheduling&#34;, True)

    # delete the old backup
    delete_backup(client, bv.name, b.name)
    volume = wait_for_volume_status(client, volume_name,
                                    &#34;lastBackup&#34;, &#34;&#34;)
    assert volume.lastBackupAt == &#34;&#34;

    # check that we can create another successful backup
    bv, b, _, _ = create_backup(client, volume_name)

    # delete the new backup
    delete_backup(client, bv.name, b.name)
    volume = wait_for_volume_status(client, volume_name, &#34;lastBackup&#34;, &#34;&#34;)
    assert volume.lastBackupAt == &#34;&#34;


def test_backup_block_deletion(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test backup block deletion

    Context:

    We want to make sure that we only delete non referenced backup blocks,
    we also don&#39;t want to delete blocks while there other backups in progress.
    The reason for this is that we don&#39;t yet know which blocks are required by
    the in progress backup, so blocks deletion could lead to a faulty backup.

    Setup:

    1. Setup minio as S3 backupstore

    Steps:

    1.  Create a volume and attach to the current node
    2.  Write 4 MB to the beginning of the volume (2 x 2MB backup blocks)
    3.  Create backup(1) of the volume
    4.  Overwrite the first of the backup blocks of data on the volume
    5.  Create backup(2) of the volume
    6.  Overwrite the first of the backup blocks of data on the volume
    7.  Create backup(3) of the volume
    8.  Verify backup block count == 4
        assert volume[&#34;DataStored&#34;] == str(BLOCK_SIZE * expected_count)
        assert count of *.blk files for that volume == expected_count
    9.  Create an artificial in progress backup.cfg file
        json.dumps({&#34;Name&#34;: name, &#34;VolumeName&#34;: volume, &#34;CreatedTime&#34;: &#34;&#34;})
    10. Delete backup(2)
    11. Verify backup block count == 4 (because of the in progress backup)
    12. Delete the artificial in progress backup.cfg file
    13. Delete backup(1)
    14. Verify backup block count == 2
    15. Delete backup(3)
    16. Verify backup block count == 0
    17. Delete the backup volume
    18. Cleanup the volume
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    volume = create_and_check_volume(client, volume_name)
    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    data0 = {&#39;pos&#39;: 0,
             &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}

    bv0, backup0, _, _ = create_backup(client, volume_name, data0)

    data1 = {&#39;pos&#39;: 0,
             &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}

    bv1, backup1, _, _ = create_backup(client, volume_name, data1)

    data2 = {&#39;pos&#39;: 0,
             &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}

    bv2, backup2, _, _ = create_backup(client, volume_name, data2)

    backup_blocks_count = backupstore_count_backup_block_files(client,
                                                               core_api,
                                                               volume_name)
    assert backup_blocks_count == 4

    bvs = client.list_backupVolume()

    for bv in bvs:
        if bv[&#39;name&#39;] == volume_name:
            assert bv[&#39;dataStored&#39;] == \
                str(backup_blocks_count * BACKUP_BLOCK_SIZE)

    backupstore_create_dummy_in_progress_backup(client, core_api, volume_name)
    delete_backup(client, volume_name, backup1.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 4

    backupstore_delete_dummy_in_progress_backup(client, core_api, volume_name)

    delete_backup(client, volume_name, backup0.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 2

    delete_backup(client, volume_name, backup2.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 0

    delete_backup_volume(client, volume_name)


def test_dr_volume_with_backup_block_deletion(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test DR volume last backup after block deletion.

    Context:

    We want to make sure that when the block is delete, the DR volume picks up
    the correct last backup.

    Steps:

    1.  Create a volume and attach to the current node.
    2.  Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).
    3.  Create backup(0) of the volume.
    4.  Overwrite backup(0) 1st blocks of data on the volume.
        (Since backup(0) contains 2 blocks of data, the updated data is
        data1[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:])
    5.  Create backup(1) of the volume.
    6.  Verify backup block count == 3.
    7.  Create DR volume from backup(1).
    8.  Verify DR volume last backup is backup(1).
    9.  Delete backup(1).
    10. Verify backup block count == 2.
    11. Verify DR volume last backup is backup(0).
    12. Overwrite backup(0) 1st blocks of data on the volume.
        (Since backup(0) contains 2 blocks of data, the updated data is
        data2[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:])
    13. Create backup(2) of the volume.
    14. Verify DR volume last backup is backup(2).
    15. Activate and verify DR volume data is
        data2[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:].
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    host_id = get_self_host_id()

    vol = create_and_check_volume(client, volume_name, 2, SIZE)
    vol.attach(hostId=host_id)
    vol = common.wait_for_volume_healthy(client, volume_name)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}
    _, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    data1 = {&#39;pos&#39;: 0, &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}
    _, backup1, _, data1 = create_backup(
        client, volume_name, data1)

    backup_blocks_count = backupstore_count_backup_block_files(client,
                                                               core_api,
                                                               volume_name)
    assert backup_blocks_count == 3

    dr_vol_name = &#34;dr-&#34; + volume_name
    client.create_volume(name=dr_vol_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup1.url,
                         frontend=&#34;&#34;, standby=True)
    check_volume_last_backup(client, dr_vol_name, backup1.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup1.name)

    delete_backup(client, volume_name, backup1.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 2
    check_volume_last_backup(client, dr_vol_name, backup0.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup0.name)

    data2 = {&#39;pos&#39;: 0,
             &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}
    _, backup2, _, _ = create_backup(client, volume_name, data2)

    check_volume_last_backup(client, dr_vol_name, backup2.name)
    wait_for_volume_restoration_start(client, dr_vol_name, backup2.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup2.name)

    activate_standby_volume(client, dr_vol_name)
    dr_vol = client.by_id_volume(dr_vol_name)
    dr_vol.attach(hostId=host_id)
    dr_vol = common.wait_for_volume_healthy(client, dr_vol_name)
    final_data = {
        &#39;pos&#39;: 0,
        &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
        &#39;content&#39;: data2[&#39;content&#39;] + data0[&#39;content&#39;][BACKUP_BLOCK_SIZE:],
    }
    check_volume_data(dr_vol, final_data, False)


def test_dr_volume_with_backup_block_deletion_abort_during_backup_in_progress(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test DR volume last backup after block deletion aborted. This will set the
    last backup to be empty.

    Context:

    We want to make sure that when the block deletion for the last backup is
    aborted by operations such as backups in progress, the DR volume will still
    pick up the correct last backup.

    Steps:

    1.  Create a volume and attach to the current node.
    2.  Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).
    3.  Create backup(0) of the volume.
    4.  Overwrite backup(0) 1st blocks of data on the volume.
        (Since backup(0) contains 2 blocks of data, the updated data is
        data1[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:])
    5.  Create backup(1) of the volume.
    6.  Verify backup block count == 3.
    7.  Create DR volume from backup(1).
    8.  Verify DR volume last backup is backup(1).
    9.  Create an artificial in progress backup.cfg file.
        This cfg file will convince the longhorn manager that there is a
        backup being created. Then all subsequent backup block cleanup will be
        skipped.
    10. Delete backup(1).
    11. Verify backup block count == 3 (because of the in progress backup).
    12. Verify DR volume last backup is empty.
    13. Delete the artificial in progress backup.cfg file.
    14. Overwrite backup(0) 1st blocks of data on the volume.
        (Since backup(0) contains 2 blocks of data, the updated data is
        data2[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:])
    15. Create backup(2) of the volume.
    16. Verify DR volume last backup is backup(2).
    17. Activate and verify DR volume data is
        data2[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:].
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    host_id = get_self_host_id()

    vol = create_and_check_volume(client, volume_name, 2, SIZE)
    vol.attach(hostId=host_id)
    vol = common.wait_for_volume_healthy(client, volume_name)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}
    create_backup(client, volume_name, data0)

    data1 = {&#39;pos&#39;: 0, &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}
    _, backup1, _, data1 = create_backup(
        client, volume_name, data1)

    backup_blocks_count = backupstore_count_backup_block_files(client,
                                                               core_api,
                                                               volume_name)
    assert backup_blocks_count == 3

    dr_vol_name = &#34;dr-&#34; + volume_name
    client.create_volume(name=dr_vol_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup1.url,
                         frontend=&#34;&#34;, standby=True)
    check_volume_last_backup(client, dr_vol_name, backup1.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup1.name)

    backupstore_create_dummy_in_progress_backup(client, core_api, volume_name)
    delete_backup(client, volume_name, backup1.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 3
    check_volume_last_backup(client, dr_vol_name, &#34;&#34;)
    backupstore_delete_dummy_in_progress_backup(client, core_api, volume_name)

    data2 = {&#39;pos&#39;: 0,
             &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}
    _, backup2, _, _ = create_backup(client, volume_name, data2)

    check_volume_last_backup(client, dr_vol_name, backup2.name)
    wait_for_volume_restoration_start(client, dr_vol_name, backup2.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup2.name)

    activate_standby_volume(client, dr_vol_name)
    dr_vol = client.by_id_volume(dr_vol_name)
    dr_vol.attach(hostId=host_id)
    dr_vol = common.wait_for_volume_healthy(client, dr_vol_name)
    final_data = {
        &#39;pos&#39;: 0,
        &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
        &#39;content&#39;: data2[&#39;content&#39;] + data0[&#39;content&#39;][BACKUP_BLOCK_SIZE:],
    }
    check_volume_data(dr_vol, final_data, False)


def test_dr_volume_with_all_backup_blocks_deleted(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test DR volume can be activate after delete all backups.

    Context:

    We want to make sure that DR volume can activate after delete all backups.

    Steps:

    1.  Create a volume and attach to the current node.
    2.  Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).
    3.  Create backup(0) of the volume.
    6.  Verify backup block count == 2.
    7.  Create DR volume from backup(0).
    8.  Verify DR volume last backup is backup(0).
    9.  Delete backup(0).
    10. Verify backup block count == 0.
    11. Verify DR volume last backup is empty.
    15. Activate and verify DR volume data is data(0).
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    host_id = get_self_host_id()

    vol = create_and_check_volume(client, volume_name, 2, SIZE)
    vol.attach(hostId=host_id)
    vol = common.wait_for_volume_healthy(client, volume_name)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}
    _, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    backup_blocks_count = backupstore_count_backup_block_files(client,
                                                               core_api,
                                                               volume_name)
    assert backup_blocks_count == 2

    dr_vol_name = &#34;dr-&#34; + volume_name
    client.create_volume(name=dr_vol_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    check_volume_last_backup(client, dr_vol_name, backup0.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup0.name)

    delete_backup(client, volume_name, backup0.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 0
    check_volume_last_backup(client, dr_vol_name, &#34;&#34;)

    activate_standby_volume(client, dr_vol_name)
    dr_vol = client.by_id_volume(dr_vol_name)
    dr_vol.attach(hostId=host_id)
    dr_vol = common.wait_for_volume_healthy(client, dr_vol_name)
    check_volume_data(dr_vol, data0, False)


def test_backup_volume_list(set_random_backupstore, client, core_api):  # NOQA
    &#34;&#34;&#34;
    Test backup volume list
    Context:
    We want to make sure that an error when listing a single backup volume
    does not stop us from listing all the other backup volumes. Otherwise a
    single faulty backup can block the retrieval of all known backup volumes.
    Setup:
    1. Setup minio as S3 backupstore
    Steps:
    1.  Create a volume(1,2) and attach to the current node
    2.  write some data to volume(1,2)
    3.  Create a backup(1) of volume(1,2)
    4.  request a backup list
    5.  verify backup list contains no error messages for volume(1,2)
    6.  verify backup list contains backup(1) for volume(1,2)
    7.  place a file named &#34;backup_1234@failure.cfg&#34;
        into the backups folder of volume(1)
    8.  request a backup list
    9.  verify backup list contains no error messages for volume(1,2)
    10. verify backup list contains backup(1) for volume(1,2)
    11. delete backup volumes(1 &amp; 2)
    12. cleanup
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    # create 2 volumes.
    volume1_name, volume2_name = generate_volume_name(), generate_volume_name()

    volume1 = create_and_check_volume(client, volume1_name)
    volume2 = create_and_check_volume(client, volume2_name)

    host_id = get_self_host_id()
    volume1 = volume1.attach(hostId=host_id)
    volume1 = common.wait_for_volume_healthy(client, volume1_name)
    volume2 = volume2.attach(hostId=host_id)
    volume2 = common.wait_for_volume_healthy(client, volume2_name)

    bv1, backup1, snap1, _ = create_backup(client, volume1_name)
    bv2, backup2, snap2, _ = create_backup(client, volume2_name)

    def verify_no_err():
        &#39;&#39;&#39;
        request a backup list
        verify backup list contains no error messages for volume(1,2)
        verify backup list contains backup(1) for volume(1,2)
        &#39;&#39;&#39;
        for _ in range(RETRY_COUNTS):
            verified_bvs = set()
            backup_volume_list = client.list_backupVolume()
            for bv in backup_volume_list:
                if bv.name in (volume1_name, volume2_name):
                    assert not bv[&#39;messages&#39;]
                    for b in bv.backupList().data:
                        if bv.name == volume1_name \
                                and b.name == backup1.name \
                                or bv.name == volume2_name \
                                and b.name == backup2.name:
                            verified_bvs.add(bv.name)
            if len(verified_bvs) == 2:
                break
            time.sleep(RETRY_INTERVAL)
        assert len(verified_bvs) == 2

    verify_no_err()

    # place a bad named file into the backups folder of volume(1)
    prefix = \
        backupstore_get_backup_volume_prefix(client, volume1_name) + &#34;/backups&#34;
    backupstore_create_file(client,
                            core_api,
                            prefix + &#34;/backup_1234@failure.cfg&#34;)

    verify_no_err()

    backupstore_delete_file(client,
                            core_api,
                            prefix + &#34;/backup_1234@failure.cfg&#34;)

    backupstore_cleanup(client)


def test_backup_metadata_deletion(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test backup metadata deletion

    Context:

    We want to be able to delete the metadata (.cfg) files,
    even if they are corrupt or in a bad state (missing volume.cfg).

    Setup:

    1. Setup minio as S3 backupstore
    2. Cleanup backupstore

    Steps:

    1.  Create volume(1,2) and attach to the current node
    2.  write some data to volume(1,2)
    3.  Create backup(1,2) of volume(1,2)
    4.  request a backup list
    5.  verify backup list contains no error messages for volume(1,2)
    6.  verify backup list contains backup(1,2) information for volume(1,2)
    7.  delete backup(1) of volume(1,2)
    8.  request a backup list
    9.  verify backup list contains no error messages for volume(1,2)
    10. verify backup list only contains backup(2) information for volume(1,2)
    11. delete volume.cfg of volume(2)
    12. request backup volume deletion for volume(2)
    13. verify that volume(2) has been deleted in the backupstore.
    14. request a backup list
    15. verify backup list only contains volume(1) and no errors
    16. verify backup list only contains backup(2) information for volume(1)
    17. delete backup volume(1)
    18. verify that volume(1) has been deleted in the backupstore.
    19. cleanup
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    volume1_name = volume_name + &#34;-1&#34;
    volume2_name = volume_name + &#34;-2&#34;

    host_id = get_self_host_id()

    volume1 = create_and_check_volume(client, volume1_name)
    volume2 = create_and_check_volume(client, volume2_name)

    volume1.attach(hostId=host_id)
    volume2.attach(hostId=host_id)

    volume1 = wait_for_volume_healthy(client, volume1_name)
    volume2 = wait_for_volume_healthy(client, volume2_name)

    v1bv, v1b1, _, _ = create_backup(client, volume1_name)
    v2bv, v2b1, _, _ = create_backup(client, volume2_name)
    _, v1b2, _, _ = create_backup(client, volume1_name)
    _, v2b2, _, _ = create_backup(client, volume2_name)

    for i in range(RETRY_COUNTS):
        found1 = found2 = found3 = found4 = False

        bvs = client.list_backupVolume()
        for bv in bvs:
            backups = bv.backupList()
            for b in backups:
                if b.name == v1b1.name:
                    found1 = True
                elif b.name == v1b2.name:
                    found2 = True
                elif b.name == v2b1.name:
                    found3 = True
                elif b.name == v2b2.name:
                    found4 = True
        if found1 &amp; found2 &amp; found3 &amp; found4:
            break
        time.sleep(RETRY_INTERVAL)
    assert found1 &amp; found2 &amp; found3 &amp; found4

    v1b2_new = v1bv.backupGet(name=v1b2.name)
    assert_backup_state(v1b2, v1b2_new)

    v2b1_new = v2bv.backupGet(name=v2b1.name)
    assert_backup_state(v2b1, v2b1_new)

    v2b2_new = v2bv.backupGet(name=v2b2.name)
    assert_backup_state(v2b2, v2b2_new)

    delete_backup(client, volume1_name, v1b1.name)
    delete_backup(client, volume2_name, v2b1.name)

    for i in range(RETRY_COUNTS):
        found1 = found2 = found3 = found4 = False

        bvs = client.list_backupVolume()
        for bv in bvs:
            backups = bv.backupList()
            for b in backups:
                if b.name == v1b1.name:
                    found1 = True
                elif b.name == v1b2.name:
                    found2 = True
                elif b.name == v2b1.name:
                    found3 = True
                elif b.name == v2b2.name:
                    found4 = True
        if (not found1) &amp; found2 &amp; (not found3) &amp; found4:
            break
        time.sleep(RETRY_INTERVAL)
    assert (not found1) &amp; found2 &amp; (not found3) &amp; found4

    assert len(v1bv.backupList()) == 1
    assert len(v2bv.backupList()) == 1
    assert v1bv.backupList()[0].name == v1b2.name
    assert v2bv.backupList()[0].name == v2b2.name

    backupstore_delete_volume_cfg_file(client, core_api, volume2_name)

    delete_backup(client, volume2_name, v2b2.name)
    assert len(v2bv.backupList()) == 0

    delete_backup_volume(client, v2bv.name)
    for i in range(RETRY_COUNTS):
        if backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume2_name) == 0:
            break
        time.sleep(RETRY_INTERVAL)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume2_name) == 0

    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()

        found1 = found2 = found3 = found4 = False
        for bv in bvs:
            backups = bv.backupList()
            for b in backups:
                if b.name == v1b1.name:
                    found1 = True
                elif b.name == v1b2.name:
                    found2 = True
                elif b.name == v2b1.name:
                    found3 = True
                elif b.name == v2b2.name:
                    found4 = True
        if (not found1) &amp; found2 &amp; (not found3) &amp; (not found4):
            break
        time.sleep(RETRY_INTERVAL)
    assert (not found1) &amp; found2 &amp; (not found3) &amp; (not found4)

    v1b2_new = v1bv.backupGet(name=v1b2.name)
    assert_backup_state(v1b2, v1b2_new)
    assert v1b2_new.messages == v1b2.messages is None

    delete_backup(client, volume1_name, v1b2.name)
    for i in range(RETRY_COUNTS):
        if backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume1_name) == 0:
            break
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume1_name) == 0

    for i in range(RETRY_COUNTS):
        found1 = found2 = found3 = found4 = False

        bvs = client.list_backupVolume()
        for bv in bvs:
            backups = bv.backupList()
            for b in backups:
                if b.name == v1b1.name:
                    found1 = True
                elif b.name == v1b2.name:
                    found2 = True
                elif b.name == v2b1.name:
                    found3 = True
                elif b.name == v2b2.name:
                    found4 = True
        if (not found1) &amp; (not found2) &amp; (not found3) &amp; (not found4):
            break
        time.sleep(RETRY_INTERVAL)
    assert (not found1) &amp; (not found2) &amp; (not found3) &amp; (not found4)


@pytest.mark.coretest   # NOQA
def test_backup(set_random_backupstore, client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic backup

    Setup:

    1. Create a volume and attach to the current node
    2. Run the test for all the available backupstores.

    Steps:

    1. Create a backup of volume
    2. Restore the backup to a new volume
    3. Attach the new volume and make sure the data is the same as the old one
    4. Detach the volume and delete the backup.
    5. Wait for the restored volume&#39;s `lastBackup` to be cleaned (due to remove
    the backup)
    6. Delete the volume
    &#34;&#34;&#34;
    backup_test(client, volume_name, SIZE)


def backup_test(client, volume_name, size, backing_image=&#34;&#34;):  # NOQA
    volume = create_and_check_volume(client, volume_name, 2, size,
                                     backing_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    backupstore_test(client, lht_hostId, volume_name, size)


def backupstore_test(client, host_id, volname, size):  # NOQA
    bv, b, snap2, data = create_backup(client, volname)

    # test restore
    restore_name = generate_volume_name()
    volume = client.create_volume(name=restore_name, size=size,
                                  numberOfReplicas=2,
                                  fromBackup=b.url)

    volume = common.wait_for_volume_restoration_completed(client, restore_name)
    volume = common.wait_for_volume_detached(client, restore_name)
    assert volume.name == restore_name
    assert volume.size == size
    assert volume.numberOfReplicas == 2
    assert volume.state == &#34;detached&#34;
    assert volume.restoreRequired is False

    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, restore_name)
    check_volume_data(volume, data)
    volume = volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, restore_name)

    delete_backup(client, bv.name, b.name)
    volume = wait_for_volume_status(client, volume.name,
                                    &#34;lastBackup&#34;, &#34;&#34;)
    assert volume.lastBackupAt == &#34;&#34;

    client.delete(volume)
    volume = wait_for_volume_delete(client, restore_name)


@pytest.mark.coretest  # NOQA
def test_backup_labels(set_random_backupstore, client, random_labels, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the proper Labels are applied when creating a Backup manually.

    1. Create a volume
    2. Run the following steps on all backupstores
    3. Create a backup with some random labels
    4. Get backup from backupstore, verify the labels are set on the backups
    &#34;&#34;&#34;
    backup_labels_test(client, random_labels, volume_name)


def backup_labels_test(client, random_labels, volume_name, size=SIZE, backing_image=&#34;&#34;):  # NOQA
    host_id = get_self_host_id()

    volume = create_and_check_volume(client, volume_name, 2, size,
                                     backing_image)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    bv, b, _, _ = create_backup(client, volume_name, labels=random_labels)
    # If we&#39;re running the test with a BackingImage,
    # check field `volumeBackingImageName` is set properly.
    backup = bv.backupGet(name=b.name)
    # Longhorn will automatically add a label `longhorn.io/volume-access-mode`
    # to a newly created backup
    assert len(backup.labels) == len(random_labels) + 1
    assert random_labels[&#34;key&#34;] == backup.labels[&#34;key&#34;]
    assert &#34;longhorn.io/volume-access-mode&#34; in backup.labels.keys()
    wait_for_backup_volume(client, volume_name, backing_image)


@pytest.mark.coretest   # NOQA
def test_restore_inc(set_random_backupstore, client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test restore from disaster recovery volume (incremental restore)

    Run test against all the backupstores

    1. Create a volume and attach to the current node
    2. Generate `data0`, write to the volume, make a backup `backup0`
    3. Create three DR(standby) volumes from the backup: `sb_volume0/1/2`
    4. Wait for all three DR volumes to start the initial restoration
    5. Verify DR volumes&#39;s `lastBackup` is `backup0`
    6. Verify snapshot/pv/pvc/change backup target are not allowed as long
    as the DR volume exists
    7. Activate standby `sb_volume0` and attach it to check the volume data
    8. Generate `data1` and write to the original volume and create `backup1`
    9. Make sure `sb_volume1`&#39;s `lastBackup` field has been updated to
    `backup1`
    10. Wait for `sb_volume1` to finish incremental restoration then activate
    11. Attach and check `sb_volume1`&#39;s data
    12. Generate `data2` and write to the original volume and create `backup2`
    13. Make sure `sb_volume2`&#39;s `lastBackup` field has been updated to
    `backup1`
    14. Wait for `sb_volume2` to finish incremental restoration then activate
    15. Attach and check `sb_volume2`&#39;s data
    16. Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good

    FIXME: Step 16 works because the disk will be treated as a unformatted disk
    &#34;&#34;&#34;
    restore_inc_test(client, core_api, volume_name, pod)


def restore_inc_test(client, core_api, volume_name, pod):  # NOQA
    std_volume = create_and_check_volume(client, volume_name, 2, SIZE)
    lht_host_id = get_self_host_id()
    std_volume.attach(hostId=lht_host_id)
    std_volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        std_volume.activate(frontend=VOLUME_FRONTEND_BLOCKDEV)
        assert &#34;already in active mode&#34; in str(e.value)

    data0 = {&#39;len&#39;: 2 * BACKUP_BLOCK_SIZE, &#39;pos&#39;: 0,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}
    _, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    sb_volume0_name = &#34;sb-0-&#34; + volume_name
    sb_volume1_name = &#34;sb-1-&#34; + volume_name
    sb_volume2_name = &#34;sb-2-&#34; + volume_name
    client.create_volume(name=sb_volume0_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=sb_volume1_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=sb_volume2_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    wait_for_backup_restore_completed(client, sb_volume0_name, backup0.name)
    wait_for_backup_restore_completed(client, sb_volume1_name, backup0.name)
    wait_for_backup_restore_completed(client, sb_volume2_name, backup0.name)

    sb_volume0 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume0_name)
    sb_volume1 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume1_name)
    sb_volume2 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume2_name)

    for _ in range(RETRY_COUNTS):
        client.list_backupVolume()
        sb_volume0 = client.by_id_volume(sb_volume0_name)
        sb_volume1 = client.by_id_volume(sb_volume1_name)
        sb_volume2 = client.by_id_volume(sb_volume2_name)
        sb_engine0 = get_volume_engine(sb_volume0)
        sb_engine1 = get_volume_engine(sb_volume1)
        sb_engine2 = get_volume_engine(sb_volume2)
        if sb_volume0.restoreRequired is False or \
           sb_volume1.restoreRequired is False or \
           sb_volume2.restoreRequired is False or \
                not sb_engine0.lastRestoredBackup or \
                not sb_engine1.lastRestoredBackup or \
                not sb_engine2.lastRestoredBackup:
            time.sleep(RETRY_INTERVAL)
        else:
            break
    assert sb_volume0.standby is True
    assert sb_volume0.lastBackup == backup0.name
    assert sb_volume0.frontend == &#34;&#34;
    assert sb_volume0.restoreRequired is True
    sb_engine0 = get_volume_engine(sb_volume0)
    assert sb_engine0.lastRestoredBackup == backup0.name
    assert sb_engine0.requestedBackupRestore == backup0.name
    assert sb_volume1.standby is True
    assert sb_volume1.lastBackup == backup0.name
    assert sb_volume1.frontend == &#34;&#34;
    assert sb_volume1.restoreRequired is True
    sb_engine1 = get_volume_engine(sb_volume1)
    assert sb_engine1.lastRestoredBackup == backup0.name
    assert sb_engine1.requestedBackupRestore == backup0.name
    assert sb_volume2.standby is True
    assert sb_volume2.lastBackup == backup0.name
    assert sb_volume2.frontend == &#34;&#34;
    assert sb_volume2.restoreRequired is True
    sb_engine2 = get_volume_engine(sb_volume2)
    assert sb_engine2.lastRestoredBackup == backup0.name
    assert sb_engine2.requestedBackupRestore == backup0.name

    sb0_snaps = sb_volume0.snapshotList()
    assert len(sb0_snaps) == 2
    for s in sb0_snaps:
        if s.name != &#34;volume-head&#34;:
            sb0_snap = s
    assert sb0_snaps
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotCreate()
        assert &#34;cannot create snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotRevert(name=sb0_snap.name)
        assert &#34;cannot revert snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotDelete(name=sb0_snap.name)
        assert &#34;cannot delete snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotBackup(name=sb0_snap.name)
        assert &#34;cannot create backup for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.pvCreate(pvName=sb_volume0_name)
        assert &#34;cannot create PV for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.pvcCreate(pvcName=sb_volume0_name)
        assert &#34;cannot create PVC for standby volume&#34; in str(e.value)
    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    with pytest.raises(Exception) as e:
        client.update(setting, value=&#34;random.backup.target&#34;)
        assert &#34;cannot modify BackupTarget &#34; \
               &#34;since there are existing standby volumes&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.activate(frontend=&#34;wrong_frontend&#34;)
        assert &#34;invalid frontend&#34; in str(e.value)

    activate_standby_volume(client, sb_volume0_name)
    sb_volume0 = client.by_id_volume(sb_volume0_name)
    sb_volume0.attach(hostId=lht_host_id)
    sb_volume0 = common.wait_for_volume_healthy(client, sb_volume0_name)
    check_volume_data(sb_volume0, data0, False)

    zero_string = b&#39;\x00&#39;.decode(&#39;utf-8&#39;)
    _, backup1, _, data1 = create_backup(
        client, volume_name,
        {&#39;len&#39;: BACKUP_BLOCK_SIZE, &#39;pos&#39;: 0,
         &#39;content&#39;: zero_string * BACKUP_BLOCK_SIZE})
    check_volume_last_backup(client, sb_volume1_name, backup1.name)
    activate_standby_volume(client, sb_volume1_name)
    sb_volume1 = client.by_id_volume(sb_volume1_name)
    sb_volume1.attach(hostId=lht_host_id)
    sb_volume1 = common.wait_for_volume_healthy(client, sb_volume1_name)
    data0_modified1 = {
        &#39;len&#39;: data0[&#39;len&#39;],
        &#39;pos&#39;: 0,
        &#39;content&#39;: data1[&#39;content&#39;] + data0[&#39;content&#39;][data1[&#39;len&#39;]:],
    }
    check_volume_data(sb_volume1, data0_modified1, False)

    data2_len = int(BACKUP_BLOCK_SIZE/2)
    data2 = {&#39;len&#39;: data2_len, &#39;pos&#39;: 0,
             &#39;content&#39;: common.generate_random_data(data2_len)}
    _, backup2, _, data2 = create_backup(client, volume_name, data2)

    check_volume_last_backup(client, sb_volume2_name, backup2.name)
    activate_standby_volume(client, sb_volume2_name)
    sb_volume2 = client.by_id_volume(sb_volume2_name)
    sb_volume2.attach(hostId=lht_host_id)
    sb_volume2 = common.wait_for_volume_healthy(client, sb_volume2_name)
    data0_modified2 = {
        &#39;len&#39;: data0[&#39;len&#39;],
        &#39;pos&#39;: 0,
        &#39;content&#39;:
            data2[&#39;content&#39;] +
            data1[&#39;content&#39;][data2[&#39;len&#39;]:] +
            data0[&#39;content&#39;][data1[&#39;len&#39;]:],
    }
    check_volume_data(sb_volume2, data0_modified2, False)

    # allocated this active volume to a pod
    sb_volume2.detach(hostId=&#34;&#34;)
    sb_volume2 = common.wait_for_volume_detached(client, sb_volume2_name)

    create_pv_for_volume(client, core_api, sb_volume2, sb_volume2_name)
    create_pvc_for_volume(client, core_api, sb_volume2, sb_volume2_name)

    sb_volume2_pod_name = &#34;pod-&#34; + sb_volume2_name
    pod[&#39;metadata&#39;][&#39;name&#39;] = sb_volume2_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: sb_volume2_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    sb_volume2 = client.by_id_volume(sb_volume2_name)
    k_status = sb_volume2.kubernetesStatus
    workloads = k_status.workloadsStatus
    assert k_status.pvName == sb_volume2_name
    assert k_status.pvStatus == &#39;Bound&#39;
    assert len(workloads) == 1
    for i in range(RETRY_COUNTS):
        if workloads[0].podStatus == &#39;Running&#39;:
            break
        time.sleep(RETRY_INTERVAL)
        sb_volume2 = client.by_id_volume(sb_volume2_name)
        k_status = sb_volume2.kubernetesStatus
        workloads = k_status.workloadsStatus
        assert len(workloads) == 1
    assert workloads[0].podName == sb_volume2_pod_name
    assert workloads[0].podStatus == &#39;Running&#39;
    assert not workloads[0].workloadName
    assert not workloads[0].workloadType
    assert k_status.namespace == &#39;default&#39;
    assert k_status.pvcName == sb_volume2_name
    assert not k_status.lastPVCRefAt
    assert not k_status.lastPodRefAt

    delete_and_wait_pod(core_api, sb_volume2_pod_name)
    delete_and_wait_pvc(core_api, sb_volume2_name)
    delete_and_wait_pv(core_api, sb_volume2_name)


def test_deleting_backup_volume(set_random_backupstore, client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test deleting backup volumes

    1. Create volume and create backup
    2. Delete the backup and make sure it&#39;s gone in the backupstore
    &#34;&#34;&#34;
    lht_host_id = get_self_host_id()
    volume = create_and_check_volume(client, volume_name)

    volume.attach(hostId=lht_host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    create_backup(client, volume_name)
    create_backup(client, volume_name)

    delete_backup_volume(client, volume_name)
    cleanup_volume(client, volume)


@pytest.mark.coretest   # NOQA
@pytest.mark.skipif(&#39;nfs&#39; not in BACKUPSTORE, reason=&#39;This test is only applicable for nfs&#39;)  # NOQA
def test_listing_backup_volume(client, backing_image=&#34;&#34;):   # NOQA
    &#34;&#34;&#34;
    Test listing backup volumes

    1. Create three volumes: `volume1/2/3`
    2. Setup NFS backupstore since we can manipulate the content easily
    3. Create multiple snapshots for all three volumes
    4. Rename `volume1`&#39;s `volume.cfg` to `volume.cfg.tmp` in backupstore
    5. List backup volumes. Make sure `volume1` errors out but found other two
    6. Restore `volume1`&#39;s `volume.cfg`.
    7. Make sure now backup volume `volume1` can be found
    8. Delete backups for `volume1/2`, make sure they cannot be found later
    9. Corrupt a backup.cfg on volume3
    11. Check that the backup is listed with the other backups of volume3
    12. Verify that the corrupted backup has Messages of type error
    13. Check that backup inspection for the previously corrupted backup fails
    14. Delete backups for `volume3`, make sure they cannot be found later
    &#34;&#34;&#34;
    lht_hostId = get_self_host_id()

    # create 3 volumes.
    volume1_name = generate_volume_name()
    volume2_name = generate_volume_name()
    volume3_name = generate_volume_name()

    volume1 = create_and_check_volume(client, volume1_name)
    volume2 = create_and_check_volume(client, volume2_name)
    volume3 = create_and_check_volume(client, volume3_name)

    volume1.attach(hostId=lht_hostId)
    volume1 = common.wait_for_volume_healthy(client, volume1_name)
    volume2.attach(hostId=lht_hostId)
    volume2 = common.wait_for_volume_healthy(client, volume2_name)
    volume3.attach(hostId=lht_hostId)
    volume3 = common.wait_for_volume_healthy(client, volume3_name)

    # we only test NFS here.
    # Since it is difficult to directly remove volume.cfg from s3 buckets
    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_nfs(backupstore):
            updated = False
            for i in range(RETRY_COMMAND_COUNT):
                nfs_url = backupstore.strip(&#34;nfs://&#34;)
                setting = client.update(setting, value=backupstore)
                assert setting.value == backupstore
                setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
                if &#34;nfs&#34; in setting.value:
                    updated = True
                    break
            assert updated

    _, _, snap1, _ = create_backup(client, volume1_name)
    _, _, snap2, _ = create_backup(client, volume2_name)
    _, _, snap3, _ = create_backup(client, volume3_name)
    subprocess.check_output([&#34;sync&#34;])
    _, _, snap4, _ = create_backup(client, volume3_name)
    subprocess.check_output([&#34;sync&#34;])
    _, _, snap5, _ = create_backup(client, volume3_name)
    subprocess.check_output([&#34;sync&#34;])

    # invalidate backup volume 1 by renaming volume.cfg to volume.cfg.tmp
    cmd = [&#34;mkdir&#34;, &#34;-p&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;mount&#34;, &#34;-t&#34;, &#34;nfs4&#34;, nfs_url, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;find&#34;, &#34;/mnt/nfs&#34;, &#34;-type&#34;, &#34;d&#34;, &#34;-name&#34;, volume1_name]
    volume1_backup_volume_path = \
        subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)

    cmd = [&#34;find&#34;, volume1_backup_volume_path, &#34;-name&#34;, &#34;volume.cfg&#34;]
    volume1_backup_volume_cfg_path = \
        subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)
    cmd = [&#34;mv&#34;, volume1_backup_volume_cfg_path,
           volume1_backup_volume_cfg_path + &#34;.tmp&#34;]
    subprocess.check_output(cmd)
    subprocess.check_output([&#34;sync&#34;])

    found1 = True
    found2 = found3 = False
    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()
        if volume1_name not in bvs:
            found1 = False
        if volume2_name in bvs:
            found2 = True
        if volume3_name in bvs:
            found3 = True
        if not found1 &amp; found2 &amp; found3:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found1 &amp; found2 &amp; found3

    cmd = [&#34;mv&#34;, volume1_backup_volume_cfg_path + &#34;.tmp&#34;,
           volume1_backup_volume_cfg_path]
    subprocess.check_output(cmd)
    subprocess.check_output([&#34;sync&#34;])

    bv1, b1 = common.find_backup(client, volume1_name, snap1.name)
    common.delete_backup(client, volume1_name, b1.name)

    bv2, b2 = common.find_backup(client, volume2_name, snap2.name)
    common.delete_backup(client, volume2_name, b2.name)

    # corrupt backup for snap4
    bv4, b4 = common.find_backup(client, volume3_name, snap4.name)
    b4_cfg_name = &#34;backup_&#34; + b4[&#34;name&#34;] + &#34;.cfg&#34;
    cmd = [&#34;find&#34;, &#34;/mnt/nfs&#34;, &#34;-type&#34;, &#34;d&#34;, &#34;-name&#34;, volume3_name]
    v3_backup_path = subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)
    b4_cfg_path = os.path.join(v3_backup_path, &#34;backups&#34;, b4_cfg_name)
    assert os.path.exists(b4_cfg_path)
    b4_tmp_cfg_path = os.path.join(v3_backup_path, b4_cfg_name)
    os.rename(b4_cfg_path, b4_tmp_cfg_path)
    assert os.path.exists(b4_tmp_cfg_path)

    corrupt_backup = open(b4_cfg_path, &#34;w&#34;)
    assert corrupt_backup
    assert corrupt_backup.write(&#34;{corrupt: definitely&#34;) &gt; 0
    corrupt_backup.close()
    subprocess.check_output([&#34;sync&#34;])

    # a corrupt backup cannot provide information about the snapshot
    found = True
    for i in range(RETRY_COMMAND_COUNT):
        if b4[&#34;name&#34;] not in bv4.backupList():
            found = False
            break
    assert not found

    # cleanup b4
    os.remove(b4_cfg_path)
    os.rename(b4_tmp_cfg_path, b4_cfg_path)
    subprocess.check_output([&#34;sync&#34;])

    bv3, b3 = common.find_backup(client, volume3_name, snap3.name)
    common.delete_backup(client, volume3_name, b3.name)
    bv4, b4 = common.find_backup(client, volume3_name, snap4.name)
    common.delete_backup(client, volume3_name, b4.name)
    bv5, b5 = common.find_backup(client, volume3_name, snap5.name)
    common.delete_backup(client, volume3_name, b5.name)

    common.delete_backup_volume(client, volume3_name)
    common.wait_for_backup_volume_delete(client, volume3_name)

    volume1.detach(hostId=&#34;&#34;)
    volume1 = common.wait_for_volume_detached(client, volume1_name)
    client.delete(volume1)
    wait_for_volume_delete(client, volume1_name)

    volume2.detach(hostId=&#34;&#34;)
    volume2 = common.wait_for_volume_detached(client, volume2_name)
    client.delete(volume2)
    wait_for_volume_delete(client, volume2_name)

    volume3.detach(hostId=&#34;&#34;)
    volume3 = common.wait_for_volume_detached(client, volume3_name)
    client.delete(volume3)
    wait_for_volume_delete(client, volume3_name)

    volumes = client.list_volume()
    assert len(volumes) == 0


@pytest.mark.coretest   # NOQA
def test_volume_multinode(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test the volume can be attached on multiple nodes

    1. Create one volume
    2. Attach it on every node once, verify the state, then detach it
    &#34;&#34;&#34;
    hosts = [node[&#39;name&#39;] for node in client.list_node()]

    volume = client.create_volume(name=volume_name,
                                  size=SIZE,
                                  numberOfReplicas=2)
    volume = common.wait_for_volume_detached(client,
                                             volume_name)

    for host_id in hosts:
        volume = volume.attach(hostId=host_id)
        volume = common.wait_for_volume_healthy(client,
                                                volume_name)
        engine = get_volume_engine(volume)
        assert engine.hostId == host_id
        volume = volume.detach(hostId=&#34;&#34;)
        volume = common.wait_for_volume_detached(client,
                                                 volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    volumes = client.list_volume()
    assert len(volumes) == 0


@pytest.mark.coretest  # NOQA
def test_volume_scheduling_failure(client, volume_name):  # NOQA
    &#39;&#39;&#39;
    Test fail to schedule by disable scheduling for all the nodes

    Also test cannot attach a scheduling failed volume

    1. Disable `allowScheduling` for all nodes
    2. Create a volume.
    3. Verify the volume condition `Scheduled` is false
    4. Verify the volume is not ready for workloads
    5. Verify attaching the volume will result in error
    6. Enable `allowScheduling` for all nodes
    7. Volume should be automatically scheduled (condition become true)
    8. Volume can be attached now
    &#39;&#39;&#39;
    nodes = client.list_node()
    assert len(nodes) &gt; 0

    for node in nodes:
        node = set_node_scheduling(client, node, allowScheduling=False)
        node = common.wait_for_node_update(client, node.id,
                                           &#34;allowScheduling&#34;, False)

    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=3)

    volume = common.wait_for_volume_condition_scheduled(client, volume_name,
                                                        &#34;status&#34;,
                                                        CONDITION_STATUS_FALSE)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert not volume.ready
    self_node = get_self_host_id()
    with pytest.raises(Exception) as e:
        volume.attach(hostId=self_node)
    assert &#34;unable to attach volume&#34; in str(e.value)

    for node in nodes:
        node = set_node_scheduling(client, node, allowScheduling=True)
        node = common.wait_for_node_update(client, node.id,
                                           &#34;allowScheduling&#34;, True)

    volume = common.wait_for_volume_condition_scheduled(client, volume_name,
                                                        &#34;status&#34;,
                                                        CONDITION_STATUS_TRUE)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume = volume.attach(hostId=self_node)
    volume = common.wait_for_volume_healthy(client, volume_name)
    endpoint = get_volume_endpoint(volume)
    volume_rw_test(endpoint)

    volume = volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest   # NOQA
def test_setting_default_replica_count(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test `Default Replica Count` setting

    1. Set default replica count in the global settings to 5
    2. Create a volume without specify the replica count
    3. The volume should have 5 replicas (instead of the previous default 3)
    &#34;&#34;&#34;
    setting = client.by_id_setting(common.SETTING_DEFAULT_REPLICA_COUNT)
    old_value = setting.value
    setting = client.update(setting, value=&#34;5&#34;)

    volume = client.create_volume(name=volume_name, size=SIZE)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert len(volume.replicas) == int(setting.value)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    setting = client.update(setting, value=old_value)


@pytest.mark.coretest   # NOQA
def test_volume_update_replica_count(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test updating volume&#39;s replica count

    1. Create a volume with 2 replicas
    2. Attach the volume
    3. Increase the replica to 3.
    4. Volume will become degraded and start rebuilding
    5. Wait for rebuilding to complete
    6. Update the replica count to 2. Volume should remain healthy
    7. Remove 1 replicas, so there will be 2 replicas in the volume
    8. Verify the volume is still healthy

    Volume should always be healthy even only with 2 replicas.
    &#34;&#34;&#34;
    host_id = get_self_host_id()

    replica_count = 2
    volume = create_and_check_volume(client, volume_name, replica_count)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    replica_count = 3
    volume = volume.updateReplicaCount(replicaCount=replica_count)
    volume = common.wait_for_volume_degraded(client, volume_name)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == replica_count

    old_replica_count = replica_count
    replica_count = 2
    volume = volume.updateReplicaCount(replicaCount=replica_count)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == old_replica_count

    volume.replicaRemove(name=volume.replicas[0].name)

    volume = common.wait_for_volume_replica_count(client, volume_name,
                                                  replica_count)
    assert volume.robustness == &#34;healthy&#34;
    assert len(volume.replicas) == replica_count

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest   # NOQA
def test_attach_without_frontend(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test attach in maintenance mode (without frontend)

    1. Create a volume and attach to the current node with enabled frontend
    2. Check volume has `blockdev`
    3. Write `snap1_data` into volume and create snapshot `snap1`
    4. Write more random data into volume and create another anspshot
    5. Detach the volume and reattach with disabled frontend
    6. Check volume still has `blockdev` as frontend but no endpoint
    7. Revert back to `snap1`
    8. Detach and reattach the volume with enabled frontend
    9. Check volume contains data `snap1_data`
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    snap1_data = write_volume_random_data(volume)
    snap1 = create_snapshot(client, volume_name)

    write_volume_random_data(volume)
    create_snapshot(client, volume_name)

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    common.wait_for_volume_healthy_no_frontend(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is True
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV
    check_volume_endpoint(volume)

    volume.snapshotRevert(name=snap1.name)

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    check_volume_data(volume, snap1_data)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest  # NOQA
def test_storage_class_from_backup(set_random_backupstore, volume_name, pvc_name, storage_class, client, core_api, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test restore backup using StorageClass

    1. Create volume and PV/PVC/POD
    2. Write `test_data` into pod
    3. Create a snapshot and back it up. Get the backup URL
    4. Create a new StorageClass `longhorn-from-backup` and set backup URL.
    5. Use `longhorn-from-backup` to create a new PVC
    6. Wait for the volume to be created and complete the restoration.
    7. Create the pod using the PVC. Verify the data
    &#34;&#34;&#34;
    VOLUME_SIZE = str(DEFAULT_VOLUME_SIZE * Gi)

    pv_name = pvc_name

    volume = create_and_check_volume(
        client,
        volume_name,
        size=VOLUME_SIZE
    )

    wait_for_volume_detached(client, volume_name)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)

    pod_manifest = pod_make()
    pod_manifest[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    pod_name = pod_manifest[&#39;metadata&#39;][&#39;name&#39;]
    create_and_wait_pod(core_api, pod_manifest)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)
    write_pod_volume_data(core_api, pod_name, test_data)

    volume_id = client.by_id_volume(volume_name)
    snapshot = volume_id.snapshotCreate()

    volume_id.snapshotBackup(name=snapshot.name)
    wait_for_backup_completion(client, volume_name, snapshot.name)
    bv, b = find_backup(client, volume_name, snapshot.name)

    backup_url = b.url

    storage_class[&#39;metadata&#39;][&#39;name&#39;] = &#34;longhorn-from-backup&#34;
    storage_class[&#39;parameters&#39;][&#39;fromBackup&#39;] = backup_url

    create_storage_class(storage_class)

    backup_pvc_name = generate_volume_name()

    backup_pvc_spec = {
        &#34;apiVersion&#34;: &#34;v1&#34;,
        &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;,
        &#34;metadata&#34;: {
                &#34;name&#34;: backup_pvc_name,
        },
        &#34;spec&#34;: {
            &#34;accessModes&#34;: [
                &#34;ReadWriteOnce&#34;
            ],
            &#34;storageClassName&#34;: storage_class[&#39;metadata&#39;][&#39;name&#39;],
            &#34;resources&#34;: {
                &#34;requests&#34;: {
                    &#34;storage&#34;: VOLUME_SIZE
                }
            }
        }
    }

    volume_count = len(client.list_volume())

    core_api.create_namespaced_persistent_volume_claim(
        &#39;default&#39;,
        backup_pvc_spec
    )

    backup_volume_created = False

    for i in range(RETRY_COUNTS):
        if len(client.list_volume()) == volume_count + 1:
            backup_volume_created = True
            break
        time.sleep(RETRY_INTERVAL)

    assert backup_volume_created

    for i in range(RETRY_COUNTS):
        pvc_status = core_api.read_namespaced_persistent_volume_claim_status(
            name=backup_pvc_name,
            namespace=&#39;default&#39;
        )

        if pvc_status.status.phase == &#39;Bound&#39;:
            break
        time.sleep(RETRY_INTERVAL)

    found = False
    for i in range(RETRY_COUNTS):
        volumes = client.list_volume()
        for volume in volumes:
            if volume.kubernetesStatus.pvcName == backup_pvc_name:
                backup_volume_name = volume.name
                found = True
                break
        if found:
            break
        time.sleep(RETRY_INTERVAL)
    assert found

    wait_for_volume_restoration_completed(client, backup_volume_name)
    wait_for_volume_detached(client, backup_volume_name)

    backup_pod_manifest = pod_make(name=&#34;backup-pod&#34;)
    backup_pod_manifest[&#39;spec&#39;][&#39;volumes&#39;] = \
        [create_pvc_spec(backup_pvc_name)]
    backup_pod_name = backup_pod_manifest[&#39;metadata&#39;][&#39;name&#39;]
    create_and_wait_pod(core_api, backup_pod_manifest)

    restored_data = read_volume_data(core_api, backup_pod_name)
    assert test_data == restored_data


@pytest.mark.coretest   # NOQA
def test_expansion_basic(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test volume expansion using Longhorn API

    1. Create volume and attach to the current node
    2. Generate data `snap1_data` and write it to the volume
    3. Create snapshot `snap1`
    4. Expand the volume (volume will be detached, expanded, then attached)
    5. Verify the volume has been expanded
    6. Generate data `snap2_data` and write it to the volume
    7. Create snapshot `snap2`
    8. Gerneate data `snap3_data` and write it after the original size
    9. Create snapshot `snap3` and verify the `snap3_data` with location
    10. Detach and reattach the volume.
    11. Verify the volume is still expanded, and `snap3_data` remain valid
    12. Detach the volume.
    13. Reattach the volume in maintence mode
    14. Revert to `snap2` and detach.
    15. Attach the volume and check data `snap2_data`
    16. Generate `snap4_data` and write it after the original size
    17. Create snapshot `snap4` and verify `snap4_data`.
    18. Detach the volume and revert to `snap1`
    19. Validate `snap1_data`
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    snap1_data = write_volume_random_data(volume)
    snap1 = create_snapshot(client, volume_name)

    expand_attached_volume(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))

    snap2_data = write_volume_random_data(volume)
    snap2 = create_snapshot(client, volume_name)

    snap3_data = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    snap3_data = write_volume_data(volume, snap3_data)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap3_data)

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))
    check_volume_data(volume, snap3_data)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    assert volume.disableFrontend is True
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV
    check_volume_endpoint(volume)
    volume.snapshotRevert(name=snap2.name)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_volume_data(volume, snap2_data, False)
    snap4_data = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    snap4_data = write_volume_data(volume, snap4_data)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap4_data)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    volume.snapshotRevert(name=snap1.name)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_volume_data(volume, snap1_data, False)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


@pytest.mark.coretest   # NOQA
def test_restore_inc_with_expansion(set_random_backupstore, client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test restore from disaster recovery volume with volume expansion

    Run test against a random backupstores

    1. Create a volume and attach to the current node
    2. Generate `data0`, write to the volume, make a backup `backup0`
    3. Create three DR(standby) volumes from the backup: `dr_volume0/1/2`
    4. Wait for all three DR volumes to start the initial restoration
    5. Verify DR volumes&#39;s `lastBackup` is `backup0`
    6. Verify snapshot/pv/pvc/change backup target are not allowed as long
    as the DR volume exists
    7. Activate standby `dr_volume0` and attach it to check the volume data
    8. Expand the original volume. Make sure the expansion is successful.
    8. Generate `data1` and write to the original volume and create `backup1`
    9. Make sure `dr_volume1`&#39;s `lastBackup` field has been updated to
    `backup1`
    10. Activate `dr_volume1` and check data `data0` and `data1`
    11. Generate `data2` and write to the original volume after original SIZE
    12. Create `backup2`
    13. Wait for `dr_volume2` to finish expansion, show `backup2` as latest
    14. Activate `dr_volume2` and verify `data2`
    15. Detach `dr_volume2`
    16. Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good

    FIXME: Step 16 works because the disk will be treated as a unformatted disk
    &#34;&#34;&#34;
    lht_host_id = get_self_host_id()

    std_volume = create_and_check_volume(client, volume_name, 2, SIZE)
    std_volume.attach(hostId=lht_host_id)
    std_volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        std_volume.activate(frontend=VOLUME_FRONTEND_BLOCKDEV)
        assert &#34;already in active mode&#34; in str(e.value)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    dr_volume0_name = &#34;dr-expand-0-&#34; + volume_name
    dr_volume1_name = &#34;dr-expand-1-&#34; + volume_name
    dr_volume2_name = &#34;dr-expand-2-&#34; + volume_name
    client.create_volume(name=dr_volume0_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=dr_volume1_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=dr_volume2_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    wait_for_backup_restore_completed(client, dr_volume0_name, backup0.name)
    wait_for_backup_restore_completed(client, dr_volume1_name, backup0.name)
    wait_for_backup_restore_completed(client, dr_volume2_name, backup0.name)

    dr_volume0 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume0_name)
    dr_volume1 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume1_name)
    dr_volume2 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume2_name)

    for i in range(RETRY_COUNTS):
        client.list_backupVolume()
        dr_volume0 = client.by_id_volume(dr_volume0_name)
        dr_volume1 = client.by_id_volume(dr_volume1_name)
        dr_volume2 = client.by_id_volume(dr_volume2_name)
        dr_engine0 = get_volume_engine(dr_volume0)
        dr_engine1 = get_volume_engine(dr_volume1)
        dr_engine2 = get_volume_engine(dr_volume2)
        if dr_volume0.restoreRequired is False or \
                dr_volume1.restoreRequired is False or \
                dr_volume2.restoreRequired is False or \
                not dr_engine0.lastRestoredBackup or \
                not dr_engine1.lastRestoredBackup or \
                not dr_engine2.lastRestoredBackup:
            time.sleep(RETRY_INTERVAL)
        else:
            break
    assert dr_volume0.standby is True
    assert dr_volume0.lastBackup == backup0.name
    assert dr_volume0.frontend == &#34;&#34;
    assert dr_volume0.restoreRequired is True
    dr_engine0 = get_volume_engine(dr_volume0)
    assert dr_engine0.lastRestoredBackup == backup0.name
    assert dr_engine0.requestedBackupRestore == backup0.name
    assert dr_volume1.standby is True
    assert dr_volume1.lastBackup == backup0.name
    assert dr_volume1.frontend == &#34;&#34;
    assert dr_volume1.restoreRequired is True
    dr_engine1 = get_volume_engine(dr_volume1)
    assert dr_engine1.lastRestoredBackup == backup0.name
    assert dr_engine1.requestedBackupRestore == backup0.name
    assert dr_volume2.standby is True
    assert dr_volume2.lastBackup == backup0.name
    assert dr_volume2.frontend == &#34;&#34;
    assert dr_volume2.restoreRequired is True
    dr_engine2 = get_volume_engine(dr_volume2)
    assert dr_engine2.lastRestoredBackup == backup0.name
    assert dr_engine2.requestedBackupRestore == backup0.name

    dr0_snaps = dr_volume0.snapshotList()
    assert len(dr0_snaps) == 2

    activate_standby_volume(client, dr_volume0_name)
    dr_volume0 = client.by_id_volume(dr_volume0_name)
    dr_volume0.attach(hostId=lht_host_id)
    dr_volume0 = common.wait_for_volume_healthy(client, dr_volume0_name)
    check_volume_data(dr_volume0, data0, False)

    expand_attached_volume(client, volume_name)
    std_volume = client.by_id_volume(volume_name)
    check_block_device_size(std_volume, int(EXPAND_SIZE))

    data1 = {&#39;pos&#39;: VOLUME_RWTEST_SIZE, &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup1, _, data1 = create_backup(
        client, volume_name, data1)

    check_volume_last_backup(client, dr_volume1_name, backup1.name)
    activate_standby_volume(client, dr_volume1_name)
    dr_volume1 = client.by_id_volume(dr_volume1_name)
    dr_volume1.attach(hostId=lht_host_id)
    dr_volume1 = common.wait_for_volume_healthy(client, dr_volume1_name)
    check_volume_data(dr_volume1, data0, False)
    check_volume_data(dr_volume1, data1, False)

    data2 = {&#39;pos&#39;: int(SIZE), &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup2, _, data2 = create_backup(
        client, volume_name, data2)
    assert backup2.volumeSize == EXPAND_SIZE

    wait_for_dr_volume_expansion(client, dr_volume2_name, EXPAND_SIZE)
    check_volume_last_backup(client, dr_volume2_name, backup2.name)
    activate_standby_volume(client, dr_volume2_name)
    dr_volume2 = client.by_id_volume(dr_volume2_name)
    dr_volume2.attach(hostId=lht_host_id)
    dr_volume2 = common.wait_for_volume_healthy(client, dr_volume2_name)
    check_volume_data(dr_volume2, data2)

    # allocated this active volume to a pod
    dr_volume2.detach(hostId=&#34;&#34;)
    dr_volume2 = common.wait_for_volume_detached(client, dr_volume2_name)

    create_pv_for_volume(client, core_api, dr_volume2, dr_volume2_name)
    create_pvc_for_volume(client, core_api, dr_volume2, dr_volume2_name)

    dr_volume2_pod_name = &#34;pod-&#34; + dr_volume2_name
    pod[&#39;metadata&#39;][&#39;name&#39;] = dr_volume2_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: dr_volume2_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    dr_volume2 = client.by_id_volume(dr_volume2_name)
    k_status = dr_volume2.kubernetesStatus
    workloads = k_status.workloadsStatus
    assert k_status.pvName == dr_volume2_name
    assert k_status.pvStatus == &#39;Bound&#39;
    assert len(workloads) == 1
    for i in range(RETRY_COUNTS):
        if workloads[0].podStatus == &#39;Running&#39;:
            break
        time.sleep(RETRY_INTERVAL)
        dr_volume2 = client.by_id_volume(dr_volume2_name)
        k_status = dr_volume2.kubernetesStatus
        workloads = k_status.workloadsStatus
        assert len(workloads) == 1
    assert workloads[0].podName == dr_volume2_pod_name
    assert workloads[0].podStatus == &#39;Running&#39;
    assert not workloads[0].workloadName
    assert not workloads[0].workloadType
    assert k_status.namespace == &#39;default&#39;
    assert k_status.pvcName == dr_volume2_name
    assert not k_status.lastPVCRefAt
    assert not k_status.lastPodRefAt

    delete_and_wait_pod(core_api, dr_volume2_pod_name)
    delete_and_wait_pvc(core_api, dr_volume2_name)
    delete_and_wait_pv(core_api, dr_volume2_name)

    # cleanup
    std_volume.detach(hostId=&#34;&#34;)
    dr_volume0.detach(hostId=&#34;&#34;)
    dr_volume1.detach(hostId=&#34;&#34;)
    std_volume = common.wait_for_volume_detached(client, volume_name)
    dr_volume0 = common.wait_for_volume_detached(client, dr_volume0_name)
    dr_volume1 = common.wait_for_volume_detached(client, dr_volume1_name)
    dr_volume2 = common.wait_for_volume_detached(client, dr_volume2_name)

    backupstore_cleanup(client)

    client.delete(std_volume)
    client.delete(dr_volume0)
    client.delete(dr_volume1)
    client.delete(dr_volume2)

    wait_for_volume_delete(client, volume_name)
    wait_for_volume_delete(client, dr_volume0_name)
    wait_for_volume_delete(client, dr_volume1_name)
    wait_for_volume_delete(client, dr_volume2_name)

    volumes = client.list_volume().data
    assert len(volumes) == 0


def test_engine_image_daemonset_restart(client, apps_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test restarting engine image daemonset

    1. Get the default engine image
    2. Create a volume and attach to the current node
    3. Write random data to the volume and create a snapshot
    4. Delete the engine image daemonset
    5. Engine image daemonset should be recreated
    6. In the meantime, validate the volume data to prove it&#39;s still functional
    7. Wait for the engine image to become `ready` again
    8. Check the volume data again.
    9. Write some data and create a new snapshot.
        1. Since create snapshot will use engine image binary.
    10. Check the volume data again
    &#34;&#34;&#34;
    default_img = common.get_default_engine_image(client)
    ds_name = &#34;engine-image-&#34; + default_img.name

    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    volume = common.wait_for_volume_healthy(client, volume_name)
    snap1_data = write_volume_random_data(volume)
    create_snapshot(client, volume_name)

    # The engine image DaemonSet will be recreated/restarted automatically
    apps_api.delete_namespaced_daemon_set(ds_name, common.LONGHORN_NAMESPACE)

    # Let DaemonSet really restarted
    common.wait_for_engine_image_condition(client, default_img.name, &#34;False&#34;)

    # The Longhorn volume is still available
    # during the engine image DaemonSet restarting
    check_volume_data(volume, snap1_data)

    # Wait for the restart complete
    common.wait_for_engine_image_condition(client, default_img.name, &#34;True&#34;)

    # Longhorn is still able to use the corresponding engine binary to
    # operate snapshot
    check_volume_data(volume, snap1_data)
    snap2_data = write_volume_random_data(volume)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap2_data)


@pytest.mark.coretest  # NOQA
def test_expansion_canceling(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test expansion canceling

    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Generate `test_data` and write to the pod
    3. Create an empty directory with expansion snapshot tmp meta file path
       so that the following expansion will fail
    4. Delete the pod and wait for volume detachment
    5. Try to expand the volume using Longhorn API
    6. Wait for expansion failure then use Longhorn API to cancel it
    7. Create a new pod and validate the volume content,
       then re-write random data to the pod
    8. Delete the pod and wait for volume detachment
    9. Retry expansion then verify the expansion done using Longhorn API
    10. Create a new pod
    11. Validate the volume content, then check if data writing looks fine
    12. Clean up pod, PVC, and PV
    &#34;&#34;&#34;
    expansion_pvc_name = &#34;pvc-&#34; + volume_name
    expansion_pv_name = &#34;pv-&#34; + volume_name
    pod_name = &#34;pod-&#34; + volume_name
    volume = create_and_check_volume(client, volume_name, 2, SIZE)
    create_pv_for_volume(client, core_api, volume, expansion_pv_name)
    create_pvc_for_volume(client, core_api, volume, expansion_pvc_name)
    pod[&#39;metadata&#39;][&#39;name&#39;] = pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: expansion_pvc_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    volume = client.by_id_volume(volume_name)
    replicas = volume.replicas
    fail_replica_expansion(client, core_api,
                           volume_name, EXPAND_SIZE, replicas)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)
    write_pod_volume_data(core_api, pod_name, test_data)

    delete_and_wait_pod(core_api, pod_name)
    volume = wait_for_volume_detached(client, volume_name)

    volume.expand(size=EXPAND_SIZE)
    wait_for_expansion_failure(client, volume_name)
    volume = client.by_id_volume(volume_name)
    volume.cancelExpansion()
    wait_for_volume_expansion(client, volume_name)
    volume = client.by_id_volume(volume_name)
    assert volume.state == &#34;detached&#34;
    assert volume.size == SIZE

    # check if the volume still works fine
    create_and_wait_pod(core_api, pod)
    resp = read_volume_data(core_api, pod_name)
    assert resp == test_data
    test_data = generate_random_data(VOLUME_RWTEST_SIZE)
    write_pod_volume_data(core_api, pod_name, test_data)

    # retry expansion
    delete_and_wait_pod(core_api, pod_name)
    volume = wait_for_volume_detached(client, volume_name)
    volume.expand(size=EXPAND_SIZE)
    wait_for_volume_expansion(client, volume_name)
    volume = client.by_id_volume(volume_name)
    assert volume.state == &#34;detached&#34;
    assert volume.size == str(EXPAND_SIZE)

    create_and_wait_pod(core_api, pod)
    volume = client.by_id_volume(volume_name)
    engine = get_volume_engine(volume)
    assert volume.size == EXPAND_SIZE
    assert volume.size == engine.size
    resp = read_volume_data(core_api, pod_name)
    assert resp == test_data
    write_pod_volume_data(core_api, pod_name, test_data)
    resp = read_volume_data(core_api, pod_name)
    assert resp == test_data

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, expansion_pvc_name)
    delete_and_wait_pv(core_api, expansion_pv_name)


@pytest.mark.coretest  # NOQA
def test_running_volume_with_scheduling_failure(
        client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test if the running volume still work fine
    when there is a scheduling failed replica

    Prerequisite:
    Setting &#34;soft anti-affinity&#34; is false.
    Setting &#34;replica-replenishment-wait-interval&#34; is 0

    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Disable the scheduling for a node contains a running replica.
    5. Crash the replica on the scheduling disabled node for the volume.
    6. Wait for the scheduling failure which is caused
       by the new replica creation.
    7. Verify:
      7.1. `volume.ready == True`.
      7.2. `volume.conditions[scheduled].status == False`.
      7.3. the volume is Degraded.
      7.4. the new replica is created but it is not running.
    8. Write more data to the volume and get the md5sum
    9. Delete the pod and wait for the volume detached.
    10. Verify the scheduling failed replica is removed.
    11. Verify:
      11.1. `volume.ready == True`.
      11.2. `volume.conditions[scheduled].status == True`
    12. Recreate a new pod for the volume and wait for the pod running.
    13. Validate the volume content, then check if data writing looks fine.
    14. Clean up pod, PVC, and PV.
    &#34;&#34;&#34;

    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_node_soft_anti_affinity_setting, value=&#34;false&#34;)

    replenish_wait_setting = \
        client.by_id_setting(SETTING_REPLICA_REPLENISHMENT_WAIT_INTERVAL)
    client.update(replenish_wait_setting, value=&#34;0&#34;)

    data_path1 = &#34;/data/test1&#34;
    test_pv_name = &#34;pv-&#34; + volume_name
    test_pvc_name = &#34;pvc-&#34; + volume_name
    test_pod_name = &#34;pod-&#34; + volume_name

    volume = create_and_check_volume(client, volume_name, size=str(1 * Gi))
    create_pv_for_volume(client, core_api, volume, test_pv_name)
    create_pvc_for_volume(client, core_api, volume, test_pvc_name)

    pod[&#39;metadata&#39;][&#39;name&#39;] = test_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: test_pvc_name,
        },
    }]
    create_and_wait_pod(core_api, pod)
    wait_for_volume_healthy(client, volume_name)
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path1, DATA_SIZE_IN_MB_1)
    original_md5sum1 = get_pod_data_md5sum(core_api, test_pod_name,
                                           data_path1)

    volume = client.by_id_volume(volume_name)
    existing_replicas = {}
    for r in volume.replicas:
        existing_replicas[r.name] = r
    node = client.by_id_node(volume.replicas[0].hostId)
    node = set_node_scheduling(client, node, allowScheduling=False)
    common.wait_for_node_update(client, node.id,
                                &#34;allowScheduling&#34;, False)

    crash_replica_processes(client, core_api, volume_name,
                            replicas=[volume.replicas[0]],
                            wait_to_fail=False)

    # Wait for scheduling failure.
    # It means the new replica is created but fails to be scheduled.
    wait_for_volume_condition_scheduled(client, volume_name, &#34;status&#34;,
                                        CONDITION_STATUS_FALSE)
    wait_for_volume_condition_scheduled(client, volume_name, &#34;reason&#34;,
                                        CONDITION_REASON_SCHEDULING_FAILURE)
    volume = wait_for_volume_degraded(client, volume_name)
    assert len(volume.replicas) == 4
    assert volume.ready
    for r in volume.replicas:
        if r.name not in existing_replicas:
            new_replica = r
            break
    assert new_replica
    assert not new_replica.running
    assert not new_replica.hostId

    data_path2 = &#34;/data/test2&#34;
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path2, DATA_SIZE_IN_MB_1)
    original_md5sum2 = get_pod_data_md5sum(core_api, test_pod_name, data_path2)

    delete_and_wait_pod(core_api, test_pod_name)
    wait_for_volume_detached(client, volume_name)
    volume = wait_for_volume_condition_scheduled(client, volume_name, &#34;status&#34;,
                                                 CONDITION_STATUS_TRUE)
    assert volume.ready
    # The scheduling failed replica will be removed
    # so that the volume can be reattached later.
    assert len(volume.replicas) == 3
    for r in volume.replicas:
        assert r.hostId != &#34;&#34;
        assert r.name != new_replica.name

    create_and_wait_pod(core_api, pod)
    wait_for_volume_degraded(client, volume_name)

    md5sum1 = get_pod_data_md5sum(core_api, test_pod_name, data_path1)
    assert md5sum1 == original_md5sum1
    md5sum2 = get_pod_data_md5sum(core_api, test_pod_name, data_path2)
    assert md5sum2 == original_md5sum2

    # The data writing is fine
    data_path3 = &#34;/data/test3&#34;
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path3, DATA_SIZE_IN_MB_1)
    get_pod_data_md5sum(core_api, test_pod_name, data_path3)

    delete_and_wait_pod(core_api, test_pod_name)
    delete_and_wait_pvc(core_api, test_pvc_name)
    delete_and_wait_pv(core_api, test_pv_name)


@pytest.mark.coretest  # NOQA
def test_expansion_with_scheduling_failure(
        client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test if the running volume with scheduling failure
    can be expanded after the detachment.

    Prerequisite:
    Setting &#34;soft anti-affinity&#34; is false.

    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Disable the scheduling for a node contains a running replica.
    5. Crash the replica on the scheduling disabled node for the volume.
       Then delete the failed replica so that it won&#39;t be reused.
    6. Wait for the scheduling failure which is caused
       by the new replica creation.
    7. Verify:
      7.1. `volume.ready == True`.
      7.2. `volume.conditions[scheduled].status == False`.
      7.3. the volume is Degraded.
      7.4. the new replica is created but it is not running.
    8. Write more data to the volume and get the md5sum
    9. Delete the pod and wait for the volume detached.
    10. Verify the scheduling failed replica is removed.
    11. Verify:
      11.1. `volume.ready == True`.
      11.2. `volume.conditions[scheduled].status == True`
    12. Expand the volume and wait for the expansion succeeds.
    13. Verify there is no rebuild replica after the expansion.
    14. Recreate a new pod for the volume and wait for the pod running.
    15. Validate the volume content.
    16. Verify the expanded part can be read/written correctly.
    17. Enable the node scheduling.
    18. Wait for the volume rebuild succeeds.
    19. Verify the data written in the expanded part.
    20. Clean up pod, PVC, and PV.

    Notice that the step 1 to step 11 is identical with
    those of the case test_running_volume_with_scheduling_failure().
    &#34;&#34;&#34;
    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_node_soft_anti_affinity_setting, value=&#34;false&#34;)

    data_path1 = &#34;/data/test1&#34;
    test_pv_name = &#34;pv-&#34; + volume_name
    test_pvc_name = &#34;pvc-&#34; + volume_name
    test_pod_name = &#34;pod-&#34; + volume_name

    volume = create_and_check_volume(client, volume_name, size=str(300 * Mi))
    create_pv_for_volume(client, core_api, volume, test_pv_name)
    create_pvc_for_volume(client, core_api, volume, test_pvc_name)

    pod[&#39;metadata&#39;][&#39;name&#39;] = test_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: test_pvc_name,
        },
    }]
    create_and_wait_pod(core_api, pod)
    wait_for_volume_healthy(client, volume_name)
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path1, DATA_SIZE_IN_MB_1)
    original_md5sum1 = get_pod_data_md5sum(core_api, test_pod_name,
                                           data_path1)

    volume = client.by_id_volume(volume_name)
    old_replicas = {}
    for r in volume.replicas:
        old_replicas[r.name] = r
    failed_replica = volume.replicas[0]
    node = client.by_id_node(failed_replica.hostId)
    node = set_node_scheduling(client, node, allowScheduling=False)
    common.wait_for_node_update(client, node.id,
                                &#34;allowScheduling&#34;, False)

    crash_replica_processes(client, core_api, volume_name,
                            replicas=[failed_replica],
                            wait_to_fail=False)

    # Remove the failed replica so that it won&#39;t be reused later
    volume = wait_for_volume_degraded(client, volume_name)
    volume.replicaRemove(name=failed_replica.name)

    # Wait for scheduling failure.
    # It means the new replica is created but fails to be scheduled.
    wait_for_volume_condition_scheduled(client, volume_name, &#34;status&#34;,
                                        CONDITION_STATUS_FALSE)
    wait_for_volume_condition_scheduled(client, volume_name, &#34;reason&#34;,
                                        CONDITION_REASON_SCHEDULING_FAILURE)
    volume = wait_for_volume_degraded(client, volume_name)
    assert len(volume.replicas) == 3
    assert volume.ready
    for r in volume.replicas:
        assert r.name != failed_replica.name
        if r.name not in old_replicas:
            new_replica = r
            break
    assert new_replica
    assert not new_replica.running
    assert not new_replica.hostId

    data_path2 = &#34;/data/test2&#34;
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path2, DATA_SIZE_IN_MB_1)
    original_md5sum2 = get_pod_data_md5sum(core_api, test_pod_name, data_path2)

    delete_and_wait_pod(core_api, test_pod_name)
    wait_for_volume_detached(client, volume_name)
    volume = wait_for_volume_condition_scheduled(client, volume_name, &#34;status&#34;,
                                                 CONDITION_STATUS_TRUE)
    assert volume.ready
    # The scheduling failed replica will be removed
    # so that the volume can be reattached later.
    assert len(volume.replicas) == 2
    for r in volume.replicas:
        assert r.hostId != &#34;&#34;
        assert r.name != new_replica.name

    expanded_size = str(400 * Mi)
    volume.expand(size=expanded_size)
    wait_for_volume_expansion(client, volume_name)
    volume = client.by_id_volume(volume_name)
    assert volume.state == &#34;detached&#34;
    assert volume.size == expanded_size
    assert len(volume.replicas) == 2
    for r in volume.replicas:
        assert r.name in old_replicas

    create_and_wait_pod(core_api, pod)
    wait_for_volume_degraded(client, volume_name)

    md5sum1 = get_pod_data_md5sum(core_api, test_pod_name, data_path1)
    assert md5sum1 == original_md5sum1
    md5sum2 = get_pod_data_md5sum(core_api, test_pod_name, data_path2)
    assert md5sum2 == original_md5sum2

    # The data writing is fine
    data_path3 = &#34;/data/test3&#34;
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path3, DATA_SIZE_IN_MB_1)
    original_md5sum3 = get_pod_data_md5sum(core_api, test_pod_name, data_path3)

    node = client.by_id_node(failed_replica.hostId)
    set_node_scheduling(client, node, allowScheduling=True)
    wait_for_volume_healthy(client, volume_name)

    md5sum3 = get_pod_data_md5sum(core_api, test_pod_name, data_path3)
    assert md5sum3 == original_md5sum3

    delete_and_wait_pod(core_api, test_pod_name)
    delete_and_wait_pvc(core_api, test_pvc_name)
    delete_and_wait_pv(core_api, test_pv_name)


def test_backup_lock_deletion_during_restoration(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test backup locks
    Context:
    To test the locking mechanism that utilizes the backupstore,
    to prevent the following case of concurrent operations.
    - prevent backup deletion during backup restoration

    steps:
    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Take a backup.
    5. Wait for the backup to be completed.
    6. Start backup restoration for the backup creation.
    7. Wait for restoration to be in progress.
    8. Delete the backup from the backup store.
    9. Wait for the restoration to be completed.
    10. Assert the data from the restored volume with md5sum.
    11. Assert the backup count in the backup store with 0.
    &#34;&#34;&#34;
    backupstore_cleanup(client)
    std_volume_name = volume_name + &#34;-std&#34;
    restore_volume_name = volume_name + &#34;-restore&#34;
    _, _, _, std_md5sum = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, std_volume_name,
            data_size_in_mb=DATA_SIZE_IN_MB_2)
    std_volume = client.by_id_volume(std_volume_name)
    snap1 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap1.name)
    wait_for_backup_completion(client, std_volume_name, snap1.name)

    _, b = common.find_backup(client, std_volume_name, snap1.name)
    client.create_volume(name=restore_volume_name, fromBackup=b.url)
    wait_for_volume_restoration_start(client, restore_volume_name, b.name)

    backup_volume = client.by_id_backupVolume(std_volume_name)
    backup_volume.backupDelete(name=b.name)

    wait_for_volume_restoration_completed(client, restore_volume_name)
    wait_for_backup_delete(client, std_volume_name, b.name)
    restore_volume = wait_for_volume_detached(client, restore_volume_name)
    assert len(restore_volume.replicas) == 3

    restore_pod_name = restore_volume_name + &#34;-pod&#34;
    restore_pv_name = restore_volume_name + &#34;-pv&#34;
    restore_pvc_name = restore_volume_name + &#34;-pvc&#34;
    restore_pod = pod_make(name=restore_pod_name)
    create_pv_for_volume(client, core_api, restore_volume, restore_pv_name)
    create_pvc_for_volume(client, core_api, restore_volume, restore_pvc_name)
    restore_pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(restore_pvc_name)]
    create_and_wait_pod(core_api, restore_pod)

    restore_volume = client.by_id_volume(restore_volume_name)
    assert restore_volume[VOLUME_FIELD_ROBUSTNESS] == VOLUME_ROBUSTNESS_HEALTHY

    md5sum = get_pod_data_md5sum(core_api, restore_pod_name, &#34;/data/test&#34;)
    assert std_md5sum == md5sum

    try:
        _, b = common.find_backup(client, std_volume_name, snap1.name)
    except AssertionError:
        b = None
    assert b is None


def test_backup_lock_deletion_during_backup(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test backup locks
    Context:
    To test the locking mechanism that utilizes the backupstore,
    to prevent the following case of concurrent operations.
    - prevent backup deletion while a backup is in progress

    steps:
    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Take a backup.
    5. Wait for the backup to be completed.
    6. Write more data into the volume and compute md5sum.
    7. Take another backup of the volume.
    8. While backup is in progress, delete the older backup up.
    9. Wait for the backup creation in progress to be completed.
    10. Check the backup store, there should be 1 backup.
    11. Restore the latest backup.
    12. Wait for the restoration to be completed. Assert md5sum from step 6.
    &#34;&#34;&#34;
    backupstore_cleanup(client)
    std_volume_name = volume_name + &#34;-std&#34;
    restore_volume_name_1 = volume_name + &#34;-restore-1&#34;

    std_pod_name, _, _, std_md5sum1 = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, std_volume_name)
    std_volume = client.by_id_volume(std_volume_name)
    snap1 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap1.name)
    wait_for_backup_completion(client, std_volume_name, snap1.name)
    _, b1 = common.find_backup(client, std_volume_name, snap1.name)

    write_pod_volume_random_data(core_api, std_pod_name, &#34;/data/test&#34;,
                                 DATA_SIZE_IN_MB_3)

    std_md5sum2 = get_pod_data_md5sum(core_api, std_pod_name, &#34;/data/test&#34;)
    snap2 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap2.name)
    wait_for_backup_to_start(client, std_volume_name, snapshot_name=snap2.name)

    backup_volume = client.by_id_backupVolume(std_volume_name)
    backup_volume.backupDelete(name=b1.name)

    wait_for_backup_completion(client, std_volume_name, snap2.name,
                               retry_count=600)
    wait_for_backup_delete(client, std_volume_name, b1.name)

    _, b2 = common.find_backup(client, std_volume_name, snap2.name)
    assert b2 is not None

    try:
        _, b1 = common.find_backup(client, std_volume_name, snap1.name)
    except AssertionError:
        b1 = None
    assert b1 is None

    client.create_volume(name=restore_volume_name_1, fromBackup=b2.url)

    wait_for_volume_restoration_completed(client, restore_volume_name_1)
    restore_volume_1 = wait_for_volume_detached(client, restore_volume_name_1)
    assert len(restore_volume_1.replicas) == 3

    restore_pod_name_1 = restore_volume_name_1 + &#34;-pod&#34;
    restore_pv_name_1 = restore_volume_name_1 + &#34;-pv&#34;
    restore_pvc_name_1 = restore_volume_name_1 + &#34;-pvc&#34;
    restore_pod_1 = pod_make(name=restore_pod_name_1)
    create_pv_for_volume(client, core_api, restore_volume_1, restore_pv_name_1)
    create_pvc_for_volume(client, core_api, restore_volume_1,
                          restore_pvc_name_1)
    restore_pod_1[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(restore_pvc_name_1)]
    create_and_wait_pod(core_api, restore_pod_1)

    md5sum2 = get_pod_data_md5sum(core_api, restore_pod_name_1, &#34;/data/test&#34;)

    assert std_md5sum2 == md5sum2


def test_backup_lock_creation_during_deletion(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test backup locks
    Context:
    To test the locking mechanism that utilizes the backupstore,
    to prevent the following case of concurrent operations.
    - prevent backup creation during backup deletion

    steps:
    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data (DATA_SIZE_IN_MB_2) to the pod volume and get the md5sum.
    4. Take a backup.
    5. Wait for the backup to be completed.
    6. Delete the backup.
    7. Create another backup of the same volume.
    8. Wait for the delete backup to be completed.
    8. Wait for the backup to be completed.
    9. Assert there is 1 backup in the backup store.
    &#34;&#34;&#34;
    backupstore_cleanup(client)
    std_volume_name = volume_name + &#34;-std&#34;

    std_pod_name, _, _, std_md5sum1 = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, std_volume_name,
            data_size_in_mb=DATA_SIZE_IN_MB_1)
    std_volume = client.by_id_volume(std_volume_name)
    snap1 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap1.name)
    wait_for_backup_completion(client, std_volume_name, snap1.name)
    _, b1 = common.find_backup(client, std_volume_name, snap1.name)

    write_pod_volume_random_data(core_api, std_pod_name,
                                 &#34;/data/test2&#34;, DATA_SIZE_IN_MB_1)

    backup_volume = client.by_id_backupVolume(std_volume_name)
    backup_volume.backupDelete(name=b1.name)

    snap2 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap2.name)

    wait_for_backup_delete(client, volume_name, b1.name)
    wait_for_backup_completion(client, std_volume_name, snap2.name)

    try:
        _, b1 = common.find_backup(client, std_volume_name, snap1.name)
    except AssertionError:
        b1 = None
    assert b1 is None
    _, b2 = common.find_backup(client, std_volume_name, snap2.name)


@pytest.mark.skip(reason=&#34;This test takes more than 20 mins to run&#34;)  # NOQA
def test_backup_lock_restoration_during_deletion(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test backup locks
    Context:
    To test the locking mechanism that utilizes the backupstore,
    to prevent the following case of concurrent operations.
    - prevent backup restoration during backup deletion

    steps:
    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Take a backup.
    5. Wait for the backup to be completed.
    6. Write more data (1.5 Gi) to the volume and take another backup.
    7. Wait for the 2nd backup to be completed.
    8. Delete the 2nd backup.
    9. Without waiting for the backup deletion completion, restore the 1st
       backup from the backup store.
    10. Verify the restored volume become faulted.
    11. Wait for the 2nd backup deletion and assert the count of the backups
       with 1 in the backup store.
    &#34;&#34;&#34;
    backupstore_cleanup(client)
    std_volume_name = volume_name + &#34;-std&#34;
    restore_volume_name = volume_name + &#34;-restore&#34;
    std_pod_name, _, _, std_md5sum1 = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, std_volume_name,
            volume_size=str(3*Gi), data_size_in_mb=DATA_SIZE_IN_MB_1)
    std_volume = client.by_id_volume(std_volume_name)
    snap1 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap1.name)
    wait_for_backup_completion(client, std_volume_name, snap1.name)
    std_volume.snapshotBackup(name=snap1.name)
    backup_volume = client.by_id_backupVolume(std_volume_name)
    _, b1 = common.find_backup(client, std_volume_name, snap1.name)

    write_pod_volume_random_data(core_api, std_pod_name,
                                 &#34;/data/test2&#34;, 1500)
    snap2 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap2.name)
    wait_for_backup_completion(client, std_volume_name, snap2.name,
                               retry_count=1200)
    _, b2 = common.find_backup(client, std_volume_name, snap2.name)

    backup_volume.backupDelete(name=b2.name)

    client.create_volume(name=restore_volume_name, fromBackup=b1.url)
    wait_for_volume_detached(client, restore_volume_name)
    restore_volume = client.by_id_volume(restore_volume_name)
    assert restore_volume[VOLUME_FIELD_ROBUSTNESS] == VOLUME_ROBUSTNESS_FAULTED

    wait_for_backup_delete(client, volume_name, b2.name)

    _, b1 = common.find_backup(client, std_volume_name, snap1.name)
    assert b1 is not None

    try:
        _, b2 = common.find_backup(client, std_volume_name, snap2.name)
    except AssertionError:
        b2 = None
    assert b2 is None


@pytest.mark.coretest  # NOQA
def test_allow_volume_creation_with_degraded_availability(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test Allow Volume Creation with Degraded Availability (API)

    Requirement:
    1. Set `allow-volume-creation-with-degraded-availability` to true.
    2. `node-level-soft-anti-affinity` to false.

    Steps:
    (degraded availablity)
    1. Disable scheduling for node 2 and 3.
    2. Create a volume with three replicas.
        1. Volume should be `ready` after creation and `Scheduled` is true.
        2. One replica schedule succeed. Two other replicas failed scheduling.
    3. Enable the scheduling of node 2.
        1. One additional replica of the volume will become scheduled.
        2. The other replica is still failed to schedule.
        3. Scheduled condition is still true.
    4. Attach the volume.
        1. After the volume is attached, scheduled condition become false.
    5. Write data to the volume.
    6. Detach the volume.
        1. Scheduled condition should become true.
    7. Reattach the volume to verify the data.
        1. Scheduled condition should become false.
    8. Enable the scheduling for the node 3.
    9. Wait for the scheduling condition to become true.
    10. Detach and reattach the volume to verify the data.
    &#34;&#34;&#34;
    # enable volume create with degraded availability
    degraded_availability_setting = \
        client.by_id_setting(common.SETTING_DEGRADED_AVAILABILITY)
    client.update(degraded_availability_setting, value=&#34;true&#34;)

    # disable node level soft anti-affinity
    replica_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_soft_anti_affinity_setting, value=&#34;false&#34;)

    nodes = client.list_node()
    node1 = nodes[0]
    node2 = nodes[1]
    node3 = nodes[2]

    # disable node 2 and 3 to schedule to node 1
    client.update(node2, allowScheduling=False)
    client.update(node3, allowScheduling=False)

    # create volume
    volume = create_and_check_volume(client, volume_name, num_of_replicas=3)
    assert volume.ready
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # check only 1 replica scheduled successfully
    common.wait_for_replica_scheduled(client, volume_name,
                                      to_nodes=[node1.name],
                                      expect_success=1, expect_fail=2,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)

    # enable node 2 to schedule to node 1 and 2
    client.update(node2, allowScheduling=True)

    # check 2 replicas scheduled successfully
    common.wait_for_replica_scheduled(client, volume_name,
                                      to_nodes=[node1.name, node2.name],
                                      expect_success=2, expect_fail=1,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)

    volume = client.by_id_volume(volume_name)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # attach volume
    self_host = get_self_host_id()
    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_degraded(client, volume_name)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;False&#34;

    data = write_volume_random_data(volume, {})

    # detach volume
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # re-attach volume to verify the data
    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_degraded(client, volume_name)
    check_volume_data(volume, data)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;False&#34;

    # enable node 3 to schedule to node 1, 2 and 3
    client.update(node3, allowScheduling=True)
    common.wait_for_volume_condition_scheduled(client, volume_name,
                                               &#34;status&#34;, &#34;True&#34;)

    # detach and re-attach the volume to verify the data
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data)


@pytest.mark.coretest  # NOQA
def test_allow_volume_creation_with_degraded_availability_error(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test Allow Volume Creation with Degraded Availability (API)

    Requirement:
    1. Set `allow-volume-creation-with-degraded-availability` to true.
    2. `node-level-soft-anti-affinity` to false.

    Steps:
    (no availability)
    1. Disable all nodes&#39; scheduling.
    2. Create a volume with three replicas.
        1. Volume should be NotReady after creation.
        2. Scheduled condition should become false.
    3. Attaching the volume should result in error.
    4. Enable one node&#39;s scheduling.
        1. Volume should become Ready soon.
        2. Scheduled condition should become true.
    5. Attach the volume. Write data. Detach and reattach to verify the data.
    &#34;&#34;&#34;
    # enable volume create with degraded availability
    degraded_availability_setting = \
        client.by_id_setting(common.SETTING_DEGRADED_AVAILABILITY)
    client.update(degraded_availability_setting, value=&#34;true&#34;)

    # disable node level soft anti-affinity
    replica_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_soft_anti_affinity_setting, value=&#34;false&#34;)

    nodes = client.list_node()
    node1 = nodes[0]
    node2 = nodes[1]
    node3 = nodes[2]

    # disable node 1, 2 and 3 to make 0 available node
    client.update(node1, allowScheduling=False)
    client.update(node2, allowScheduling=False)
    client.update(node3, allowScheduling=False)

    # create volume
    volume = create_and_check_volume(client, volume_name, num_of_replicas=3)
    assert not volume.ready
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;False&#34;

    # attach the volume
    self_host = get_self_host_id()
    with pytest.raises(Exception) as e:
        volume.attach(hostId=self_host)
    assert &#34;unable to attach volume&#34; in str(e.value)

    # enable node 1
    client.update(node1, allowScheduling=True)

    # check only 1 replica scheduled successfully
    common.wait_for_replica_scheduled(client, volume_name,
                                      to_nodes=[node1.name],
                                      expect_success=1, expect_fail=2,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)
    volume = common.wait_for_volume_status(client, volume_name,
                                           VOLUME_FIELD_READY, True)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # attach the volume and write some data
    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_degraded(client, volume_name)
    data = write_volume_random_data(volume, {})

    # detach and re-attach the volume to verify the data
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_degraded(client, volume_name)
    check_volume_data(volume, data)


def test_multiple_volumes_creation_with_degraded_availability(set_random_backupstore, client, core_api, apps_api, storage_class, statefulset):  # NOQA
    &#34;&#34;&#34;
    Scenario: verify multiple volumes with degraded availability can be
              created, attached, detached, and deleted at nearly the same time.

    Given new StorageClass created with `numberOfReplicas=5`.

    When set `allow-volume-creation-with-degraded-availability` to `True`.
    And deploy this StatefulSet:
        https://github.com/longhorn/longhorn/issues/2073#issuecomment-742948726
    Then all 10 volumes are healthy in 1 minute.

    When delete the StatefulSet.
    then all 10 volumes are detached in 1 minute.

    When find and delete the PVC of the 10 volumes.
    Then all 10 volumes are deleted in 1 minute.
    &#34;&#34;&#34;
    storage_class[&#39;parameters&#39;][&#39;numberOfReplicas&#39;] = &#34;5&#34;
    create_storage_class(storage_class)

    common.update_setting(client,
                          common.SETTING_DEGRADED_AVAILABILITY, &#34;true&#34;)

    sts_spec = statefulset[&#39;spec&#39;]
    sts_spec[&#39;podManagementPolicy&#39;] = &#34;Parallel&#34;
    sts_spec[&#39;replicas&#39;] = 10
    sts_spec[&#39;volumeClaimTemplates&#39;][0][&#39;spec&#39;][&#39;storageClassName&#39;] = \
        storage_class[&#39;metadata&#39;][&#39;name&#39;]
    statefulset[&#39;spec&#39;] = sts_spec
    common.create_and_wait_statefulset(statefulset)
    pod_list = common.get_statefulset_pod_info(core_api, statefulset)
    retry_counts = int(60 / RETRY_INTERVAL)
    common.wait_for_pods_volume_state(
        client, pod_list,
        common.VOLUME_FIELD_ROBUSTNESS,
        common.VOLUME_ROBUSTNESS_HEALTHY,
        retry_counts=retry_counts
    )

    apps_api.delete_namespaced_stateful_set(
        name=statefulset[&#39;metadata&#39;][&#39;name&#39;],
        namespace=statefulset[&#39;metadata&#39;][&#39;namespace&#39;],
        body=k8sclient.V1DeleteOptions()
    )
    common.wait_for_pods_volume_state(
        client, pod_list,
        common.VOLUME_FIELD_STATE,
        common.VOLUME_STATE_DETACHED,
        retry_counts=retry_counts
    )

    for p in pod_list:
        common.delete_and_wait_pvc(core_api, p[&#39;pvc_name&#39;],
                                   retry_counts=retry_counts)
    common.wait_for_pods_volume_delete(client, pod_list,
                                       retry_counts=retry_counts)


def test_allow_volume_creation_with_degraded_availability_restore(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test Allow Volume Creation with Degraded Availability (Restore)

    Requirement:
    1. Set `allow-volume-creation-with-degraded-availability` to true.
    2. `node-level-soft-anti-affinity` to false.
    3. `replica-replenishment-wait-interval` to 0.
    4. Create a backup of 800MB.

    Steps:
    (restore)
    1. Disable scheduling for node 2 and 3.
    2. Restore a volume with 3 replicas.
        1. The scheduled condition is true.
        2. Only node 1 replica become scheduled.
    3. Enable scheduling for node 2.
    4. Wait for the restore to complete and volume detach automatically.
       Then check the scheduled condition still true.
    5. Attach and wait for the volume.
        1. Replicas scheduling to node 1 and 2 success.
           Replica scheduling to node 3 fail.
        2. The scheduled condition becomes false.
        3. Verify the data.
    &#34;&#34;&#34;
    # enable volume create with degraded availability
    degraded_availability_setting = \
        client.by_id_setting(common.SETTING_DEGRADED_AVAILABILITY)
    client.update(degraded_availability_setting, value=&#34;true&#34;)

    # disable node level soft anti-affinity
    replica_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_soft_anti_affinity_setting, value=&#34;false&#34;)

    replenish_wait_setting = \
        client.by_id_setting(SETTING_REPLICA_REPLENISHMENT_WAIT_INTERVAL)
    client.update(replenish_wait_setting, value=&#34;0&#34;)

    # create a backup
    backupstore_cleanup(client)

    data_path = &#34;/data/test&#34;
    src_vol_name = generate_volume_name()
    _, _, _, src_md5sum = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, src_vol_name,
            data_path=data_path, data_size_in_mb=common.DATA_SIZE_IN_MB_4)

    src_vol = client.by_id_volume(src_vol_name)
    src_snap = create_snapshot(client, src_vol_name)
    src_vol.snapshotBackup(name=src_snap.name)
    wait_for_backup_completion(client, src_vol_name, src_snap.name,
                               retry_count=600)
    _, backup = find_backup(client, src_vol_name, src_snap.name)

    nodes = client.list_node()
    node1 = nodes[0]
    node2 = nodes[1]
    node3 = nodes[2]

    # disable node 2 and 3 to schedule to node 1
    client.update(node2, allowScheduling=False)
    client.update(node3, allowScheduling=False)

    # restore volume
    dst_vol_name = generate_volume_name()
    client.create_volume(name=dst_vol_name, size=str(1*Gi),
                         numberOfReplicas=3, fromBackup=backup.url)
    common.wait_for_volume_replica_count(client, dst_vol_name, 3)
    common.wait_for_volume_restoration_start(client, dst_vol_name, backup.name)
    wait_for_volume_condition_scheduled(client, dst_vol_name,
                                        &#34;status&#34;, &#34;True&#34;)

    # check only 1 replica scheduled successfully
    common.wait_for_replica_scheduled(client, dst_vol_name,
                                      to_nodes=[node1.name],
                                      expect_success=1,
                                      expect_fail=2,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)

    # Enable node 2 to schedule to node 1, 2
    client.update(node2, allowScheduling=True)

    # wait to complete restore
    common.wait_for_volume_restoration_completed(client, dst_vol_name)
    dst_vol = common.wait_for_volume_detached(client, dst_vol_name)
    assert dst_vol.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # attach the volume
    create_pv_for_volume(client, core_api, dst_vol, dst_vol_name)
    create_pvc_for_volume(client, core_api, dst_vol, dst_vol_name)

    dst_pod_name = dst_vol_name + &#34;-pod&#34;
    pod[&#39;metadata&#39;][&#39;name&#39;] = dst_vol_name + &#34;-pod&#34;
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: dst_vol_name,
        },
    }]
    create_and_wait_pod(core_api, pod)
    # check 2 replica scheduled successfully
    dst_vol = common.wait_for_replica_scheduled(client, dst_vol_name,
                                                to_nodes=[node1.name,
                                                          node2.name],
                                                expect_success=2,
                                                expect_fail=1,
                                                chk_vol_healthy=False,
                                                chk_replica_running=False)
    wait_for_volume_condition_scheduled(client, dst_vol_name,
                                        &#34;status&#34;, &#34;False&#34;)

    # verify the data
    dst_md5sum = get_pod_data_md5sum(core_api, dst_pod_name, data_path)
    assert src_md5sum == dst_md5sum


def test_allow_volume_creation_with_degraded_availability_dr(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test Allow Volume Creation with Degraded Availability (Restore)

    Requirement:
    1. Set `allow-volume-creation-with-degraded-availability` to true.
    2. `node-level-soft-anti-affinity` to false.
    3. Create a backup of 800MB.

    Steps:
    (DR volume)
    1. Disable scheduling for node 2 and 3.
    2. Create a DR volume from backup with 3 replicas.
        1. The scheduled condition is false.
        2. Only node 1 replica become scheduled.
    3. Enable scheduling for node 2 and 3.
        1. Replicas scheduling to node 1, 2, 3 success.
        2. Wait for restore progress to complete.
        3. The scheduled condition becomes true.
    4. Activate, attach the volume, and verify the data.
    &#34;&#34;&#34;
    # enable volume create with degraded availability
    degraded_availability_setting = \
        client.by_id_setting(common.SETTING_DEGRADED_AVAILABILITY)
    client.update(degraded_availability_setting, value=&#34;true&#34;)

    # disable node level soft anti-affinity
    replica_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_soft_anti_affinity_setting, value=&#34;false&#34;)

    # create a backup
    backupstore_cleanup(client)

    data_path = &#34;/data/test&#34;
    src_vol_name = generate_volume_name()
    _, _, _, src_md5sum = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, src_vol_name,
            data_path=data_path, data_size_in_mb=common.DATA_SIZE_IN_MB_4)

    src_vol = client.by_id_volume(src_vol_name)
    src_snap = create_snapshot(client, src_vol_name)
    src_vol.snapshotBackup(name=src_snap.name)
    wait_for_backup_completion(client, src_vol_name, src_snap.name,
                               retry_count=600)
    _, backup = find_backup(client, src_vol_name, src_snap.name)

    nodes = client.list_node()
    node1 = nodes[0]
    node2 = nodes[1]
    node3 = nodes[2]

    # disable node 2 and 3 to schedule to node 1
    client.update(node2, allowScheduling=False)
    client.update(node3, allowScheduling=False)

    # create DR volume
    dst_vol_name = generate_volume_name()
    dst_vol = client.create_volume(name=dst_vol_name, size=str(1*Gi),
                                   numberOfReplicas=3,
                                   fromBackup=backup.url,
                                   frontend=&#34;&#34;,
                                   standby=True)
    common.wait_for_volume_replica_count(client, dst_vol_name, 3)
    wait_for_volume_restoration_start(client, dst_vol_name, backup.name)
    wait_for_volume_condition_scheduled(client, dst_vol_name,
                                        &#34;status&#34;, &#34;False&#34;)

    # check only 1 replica scheduled successfully
    common.wait_for_replica_scheduled(client, dst_vol_name,
                                      to_nodes=[node1.name],
                                      expect_success=1,
                                      expect_fail=2,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)

    # Enable node 2, 3 to schedule to node 1,2,3
    client.update(node2, allowScheduling=True)
    client.update(node3, allowScheduling=True)

    common.wait_for_replica_scheduled(client, dst_vol_name,
                                      to_nodes=[node1.name,
                                                node2.name,
                                                node3.name],
                                      expect_success=3,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)
    common.monitor_restore_progress(client, dst_vol_name)

    wait_for_volume_condition_scheduled(client, dst_vol_name,
                                        &#34;status&#34;, &#34;True&#34;)

    # activate the volume
    activate_standby_volume(client, dst_vol_name)

    # attach the volume
    dst_vol = client.by_id_volume(dst_vol_name)
    create_pv_for_volume(client, core_api, dst_vol, dst_vol_name)
    create_pvc_for_volume(client, core_api, dst_vol, dst_vol_name)

    dst_pod_name = dst_vol_name + &#34;-pod&#34;
    pod[&#39;metadata&#39;][&#39;name&#39;] = dst_vol_name + &#34;-pod&#34;
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: dst_vol_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    # verify the data
    dst_md5sum = get_pod_data_md5sum(core_api, dst_pod_name, data_path)
    assert src_md5sum == dst_md5sum


def test_cleanup_system_generated_snapshots(client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test Cleanup System Generated Snapshots

    1. Enabled &#39;Auto Cleanup System Generated Snapshot&#39;.
    2. Create a volume and attach it to a node.
    3. Write some data to the volume and get the checksum of the data.
    4. Delete a random replica to trigger a system generated snapshot.
    5. Repeat Step 3 for 3 times, and make sure only one snapshot is left.
    6. Check the data with the saved checksum.
    &#34;&#34;&#34;

    pod_name, _, _, md5sum1 = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, volume_name)

    volume = client.by_id_volume(volume_name)

    for i in range(3):
        replica_name = volume[&#34;replicas&#34;][i][&#34;name&#34;]
        volume.replicaRemove(name=replica_name)
        wait_for_volume_degraded(client, volume_name)
        wait_for_volume_healthy(client, volume_name)

        volume = client.by_id_volume(volume_name)
        # For the below assertion, the number of snapshots is compared with 2
        # as the list of snapshot have the volume-head too.
        assert len(volume.snapshotList()) == 2

    read_md5sum1 = get_pod_data_md5sum(core_api, pod_name, &#34;/data/test&#34;)
    assert md5sum1 == read_md5sum1


def test_volume_toomanysnapshots_condition(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test Volume TooManySnapshots Condition

    1. Create a volume and attach it to a node.
    2. Check the &#39;TooManySnapshots&#39; condition is False.
    3. Writing data to this volume and meanwhile taking 100 snapshots.
    4. Check the &#39;TooManySnapshots&#39; condition is True.
    5. Take one more snapshot to make sure snapshots works fine.
    6. Delete 2 snapshots, and check the &#39;TooManySnapshots&#39; condition is
       False.
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name)
    self_hostId = get_self_host_id()
    volume = volume.attach(hostId=self_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    snap = {}
    max_count = 100
    for i in range(max_count):
        write_volume_random_data(volume, {})

        count = i + 1
        snap[count] = create_snapshot(client, volume_name)

        if count &lt; max_count:
            volume = client.by_id_volume(volume_name)
            assert volume.conditions.toomanysnapshots.status == &#34;False&#34;
        else:
            wait_for_volume_condition_toomanysnapshots(client, volume_name,
                                                       &#34;status&#34;, &#34;True&#34;)

    snap[max_count + 1] = create_snapshot(client, volume_name)
    wait_for_volume_condition_toomanysnapshots(client, volume_name,
                                               &#34;status&#34;, &#34;True&#34;)

    volume = client.by_id_volume(volume_name)
    volume.snapshotDelete(name=snap[100].name)
    volume.snapshotDelete(name=snap[99].name)

    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name,
                                     snap[100].name, snap[99].name)

    wait_for_volume_condition_toomanysnapshots(client, volume_name,
                                               &#34;status&#34;, &#34;False&#34;)


def test_expand_pvc_with_size_round_up(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    test expand longhorn volume with pvc

    1. Create LHV,PV,PVC with size &#39;1Gi&#39;
    2. Attach, write data, and detach
    3. Expand volume size to &#39;2000000000/2G&#39; and
        check if size round up &#39;2000683008/1908Mi&#39;
    4. Attach, write data, and detach
    5. Expand volume size to &#39;2Gi&#39; and check if size is &#39;2147483648&#39;
    6. Attach, write data, and detach
    &#34;&#34;&#34;

    static_sc_name = &#34;longhorn&#34;
    setting = client.by_id_setting(SETTING_DEFAULT_LONGHORN_STATIC_SC)
    setting = client.update(setting, value=static_sc_name)
    assert setting.value == static_sc_name

    volume = create_and_check_volume(client, volume_name, 2, str(1 * Gi))
    create_pv_for_volume(client, core_api, volume, volume_name)
    create_pvc_for_volume(client, core_api, volume, volume_name)

    self_hostId = get_self_host_id()
    volume.attach(hostId=self_hostId, disableFrontend=False)
    volume = wait_for_volume_healthy(client, volume_name)
    test_data = write_volume_random_data(volume)
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    volume.expand(size=&#34;2000000000&#34;)
    wait_for_volume_expansion(client, volume_name)

    for i in range(DEFAULT_POD_TIMEOUT):
        claim = core_api.read_namespaced_persistent_volume_claim(
            name=volume_name, namespace=&#39;default&#39;)
        if claim.spec.resources.requests[&#39;storage&#39;] == &#34;2000683008&#34; and \
                claim.status.capacity[&#39;storage&#39;] == &#34;1908Mi&#34;:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert claim.spec.resources.requests[&#39;storage&#39;] == &#34;2000683008&#34;
    assert claim.status.capacity[&#39;storage&#39;] == &#34;1908Mi&#34;

    volume = client.by_id_volume(volume_name)
    assert volume.size == &#34;2000683008&#34;
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    self_hostId = get_self_host_id()
    volume.attach(hostId=self_hostId, disableFrontend=False)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, test_data, False)
    test_data = write_volume_random_data(volume)
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    volume.expand(size=str(2 * Gi))
    wait_for_volume_expansion(client, volume_name)

    for i in range(DEFAULT_POD_TIMEOUT):
        claim = core_api.read_namespaced_persistent_volume_claim(
            name=volume_name, namespace=&#39;default&#39;)
        if claim.spec.resources.requests[&#39;storage&#39;] == &#34;2147483648&#34; and \
                claim.status.capacity[&#39;storage&#39;] == &#34;2Gi&#34;:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert claim.spec.resources.requests[&#39;storage&#39;] == &#34;2147483648&#34;
    assert claim.status.capacity[&#39;storage&#39;] == &#34;2Gi&#34;

    volume = client.by_id_volume(volume_name)
    assert volume.size == &#34;2147483648&#34;
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    self_hostId = get_self_host_id()
    volume.attach(hostId=self_hostId, disableFrontend=False)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, test_data, False)
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)


def test_workload_with_fsgroup(core_api, statefulset):  # NOQA
    &#34;&#34;&#34;
    1. Deploy a StatefulSet workload that uses Longhorn volume and has
       securityContext set:
       ```
        securityContext:
            runAsUser: 1000
            runAsGroup: 1000
            fsGroup: 1000
        ```
       See
       https://github.com/longhorn/longhorn/issues/2964#issuecomment-910117570
       for an example.
    2. Wait for the workload pod to be running
    3. Exec into the workload pod, cd into the mount point of the volume.
    4. Verify that the mount point has correct filesystem permission (e.g.,
       running `ls -l` on the mount point should return the permission in
       the format ****rw****
    5. Verify that we can read/write files.
    &#34;&#34;&#34;
    statefulset_name = &#39;statefulset-non-root-access&#39;
    pod_name = statefulset_name + &#39;-0&#39;

    statefulset[&#39;metadata&#39;][&#39;name&#39;] = \
        statefulset[&#39;spec&#39;][&#39;selector&#39;][&#39;matchLabels&#39;][&#39;app&#39;] = \
        statefulset[&#39;spec&#39;][&#39;serviceName&#39;] = \
        statefulset[&#39;spec&#39;][&#39;template&#39;][&#39;metadata&#39;][&#39;labels&#39;][&#39;app&#39;] = \
        statefulset_name
    statefulset[&#39;spec&#39;][&#39;replicas&#39;] = 1
    statefulset[&#39;spec&#39;][&#39;volumeClaimTemplates&#39;][0][&#39;spec&#39;][&#39;storageClassName&#39;]\
        = &#39;longhorn&#39;
    statefulset[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;securityContext&#39;] = {
        &#39;runAsUser&#39;: 1000,
        &#39;runAsGroup&#39;: 1000,
        &#39;fsGroup&#39;: 1000
    }

    create_and_wait_statefulset(statefulset)

    write_pod_volume_random_data(core_api, pod_name, &#34;/data/test&#34;,
                                 DATA_SIZE_IN_MB_1)
    get_pod_data_md5sum(core_api, pod_name, &#34;/data/test&#34;)

# After introducing the gRPC proxy, the backup target controller is relying on
# the gRPC server availability instead of the engine binary availability.
# https://github.com/longhorn/longhorn/issues/4033
@pytest.mark.skip(reason=&#34;TODO&#34;)  # NOQA
def test_backuptarget_available_during_engine_image_not_ready(client, apps_api, request):  # NOQA
    &#34;&#34;&#34;
    Test backup target available during engine image not ready

    1. Set backup target URL to S3 and NFS respectively
    2. Set poll interval to 0 and 300 respectively
    3. Scale down the engine image DaemonSet
    4. Check engine image in deploying state
    5. Configures backup target during engine image in not ready state
    6. Check backup target status.available=false
    7. Scale up the engine image DaemonSet
    8. Check backup target status.available=true
    9. Reset backup target setting
    10. Check backup target status.available=false
    &#34;&#34;&#34;
    def finalizer():
        default_img = common.get_default_engine_image(client)
        ds_name = &#34;engine-image-&#34; + default_img.name
        body = [{
            &#34;op&#34;: &#34;remove&#34;,
            &#34;path&#34;: &#34;/spec/template/spec/nodeSelector/foo&#34;
        }]
        apps_api.patch_namespaced_daemon_set(
            name=ds_name, namespace=&#39;longhorn-system&#39;, body=body)

    request.addfinalizer(finalizer)

    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        url = &#34;&#34;
        cred_secret = &#34;&#34;
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            url = backupsettings[0]
            cred_secret = backupsettings[1]
        elif common.is_backupTarget_nfs(backupstore):
            url = backupstore
            cred_secret = &#34;&#34;
        else:
            raise NotImplementedError

        poll_intervals = [&#34;0&#34;, &#34;300&#34;]
        for poll_interval in poll_intervals:
            set_backupstore_poll_interval(client, poll_interval)

            default_img = common.get_default_engine_image(client)
            ds_name = &#34;engine-image-&#34; + default_img.name

            # Scale down the engine image DaemonSet
            daemonset = apps_api.read_namespaced_daemon_set(
                name=ds_name, namespace=&#39;longhorn-system&#39;)
            daemonset.spec.template.spec.node_selector = {&#39;foo&#39;: &#39;bar&#39;}
            apps_api.patch_namespaced_daemon_set(
                name=ds_name, namespace=&#39;longhorn-system&#39;, body=daemonset)

            # Check engine image in deploying state
            common.wait_for_engine_image_state(
                client, default_img.name, &#34;deploying&#34;)
            deploying = False
            for _ in range(RETRY_COUNTS):
                default_img = client.by_id_engine_image(default_img.name)
                if not any(default_img.nodeDeploymentMap.values()):
                    deploying = True
                    break
                time.sleep(RETRY_INTERVAL)
            assert deploying is True

            # Set valid backup target during
            # the engine image in not ready state
            set_backupstore_url(client, url)
            set_backupstore_credential_secret(client, cred_secret)
            common.wait_for_backup_target_available(client, False)

            # Scale up the engine image DaemonSet
            body = [{&#34;op&#34;: &#34;remove&#34;,
                     &#34;path&#34;: &#34;/spec/template/spec/nodeSelector/foo&#34;}]
            apps_api.patch_namespaced_daemon_set(
                name=ds_name, namespace=&#39;longhorn-system&#39;, body=body)
            common.wait_for_backup_target_available(client, True)

            # Sleep 1 second to prevent the same time
            # of BackupTarget CR spec.SyncRequestedAt
            time.sleep(1)

            # Reset backup store setting
            reset_backupstore_setting(client)
            common.wait_for_backup_target_available(client, False)


@pytest.mark.skipif(&#39;s3&#39; not in BACKUPSTORE, reason=&#39;This test is only applicable for s3&#39;)  # NOQA
def test_aws_iam_role_arn(client, core_api):  # NOQA
    &#34;&#34;&#34;
    Test AWS IAM Role ARN

    1. Set backup target to S3
    2. Check longhorn manager and replica instance manager Pods
       without &#39;iam.amazonaws.com/role&#39; annotation
    3. Add AWS_IAM_ROLE_ARN to secret
    4. Check longhorn manager and replica instance manager Pods
       with &#39;iam.amazonaws.com/role&#39; annotation
       and matches to AWS_IAM_ROLE_ARN in secret
    5. Update AWS_IAM_ROLE_ARN from secret
    6. Check longhorn manager and replica instance manager Pods
       with &#39;iam.amazonaws.com/role&#39; annotation
       and matches to AWS_IAM_ROLE_ARN in secret
    7. Remove AWS_IAM_ROLE_ARN from secret
    8. Check longhorn manager and replica instance manager Pods
       without &#39;iam.amazonaws.com/role&#39; annotation
    &#34;&#34;&#34;
    set_backupstore_s3(client)

    lh_label = &#39;app=longhorn-manager&#39;
    replica_im_label = &#39;longhorn.io/instance-manager-type=replica&#39;
    anno_key = &#39;iam.amazonaws.com/role&#39;
    secret_name = backupstore_get_secret(client)

    common.wait_for_pod_annotation(
        core_api, lh_label, anno_key, None)
    common.wait_for_pod_annotation(
        core_api, replica_im_label, anno_key, None)

    # Add secret key AWS_IAM_ROLE_ARN value test-aws-iam-role-arn
    secret = core_api.read_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;)
    secret.data[&#39;AWS_IAM_ROLE_ARN&#39;] = &#39;dGVzdC1hd3MtaWFtLXJvbGUtYXJu&#39;
    core_api.patch_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;, body=secret)

    common.wait_for_pod_annotation(
        core_api, lh_label, anno_key, &#39;test-aws-iam-role-arn&#39;)
    common.wait_for_pod_annotation(
        core_api, replica_im_label, anno_key, &#39;test-aws-iam-role-arn&#39;)

    # Update secret key AWS_IAM_ROLE_ARN value test-aws-iam-role-arn-2
    secret = core_api.read_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;)
    secret.data[&#39;AWS_IAM_ROLE_ARN&#39;] = &#39;dGVzdC1hd3MtaWFtLXJvbGUtYXJuLTI=&#39;
    core_api.patch_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;, body=secret)

    common.wait_for_pod_annotation(
        core_api, lh_label, anno_key, &#39;test-aws-iam-role-arn-2&#39;)
    common.wait_for_pod_annotation(
        core_api, replica_im_label, anno_key, &#39;test-aws-iam-role-arn-2&#39;)

    # Remove secret key AWS_IAM_ROLE_ARN
    body = [{&#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/data/AWS_IAM_ROLE_ARN&#34;}]
    core_api.patch_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;, body=body)

    common.wait_for_pod_annotation(
        core_api, lh_label, anno_key, None)
    common.wait_for_pod_annotation(
        core_api, replica_im_label, anno_key, None)


@pytest.mark.coretest   # NOQA
def test_restore_basic(set_random_backupstore, client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Steps:
    1. Create a volume and attach to a pod.
    2. Write some data into the volume and compute the checksum m1.
    3. Create a backup say b1.
    4. Write some more data into the volume and compute the checksum m2.
    5. Create a backup say b2.
    6. Delete all the data from the volume.
    7. Write some more data into the volume and compute the checksum m3.
    8. Create a backup say b3.
    9. Restore backup b1 and verify the data with m1.
    10. Restore backup b2 and verify the data with m1 and m2.
    11. Restore backup b3 and verify the data with m3.
    12. Delete the backup b2.
    13. restore the backup b3 and verify the data with m3.
    &#34;&#34;&#34;

    test_pv_name = &#34;pv-&#34; + volume_name
    test_pvc_name = &#34;pvc-&#34; + volume_name
    test_pod_name = &#34;pod-&#34; + volume_name

    volume = create_and_check_volume(client, volume_name, size=str(1 * Gi))
    create_pv_for_volume(client, core_api, volume, test_pv_name)
    create_pvc_for_volume(client, core_api, volume, test_pvc_name)
    pod[&#39;metadata&#39;][&#39;name&#39;] = test_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: test_pvc_name,
        },
    }]
    create_and_wait_pod(core_api, pod)
    wait_for_volume_healthy(client, volume_name)

    # Write 1st data and take backup
    backup_volume, backup1, data_checksum_1 = \
        create_backup_from_volume_attached_to_pod(client, core_api,
                                                  volume_name, test_pod_name,
                                                  data_path=&#39;/data/test1&#39;)

    # Write 2nd data and take backup
    backup_volume, backup2, data_checksum_2 = \
        create_backup_from_volume_attached_to_pod(client, core_api,
                                                  volume_name, test_pod_name,
                                                  data_path=&#39;/data/test2&#39;)

    # Remove test1 and test2 files
    command = &#39;rm /data/test1 /data/test2&#39;
    exec_command_in_pod(core_api, command, test_pod_name, &#39;default&#39;)

    # Write 3rd data and take backup
    backup_volume, backup3, data_checksum_3 = \
        create_backup_from_volume_attached_to_pod(client, core_api,
                                                  volume_name, test_pod_name,
                                                  data_path=&#39;/data/test3&#39;)

    delete_and_wait_pod(core_api, test_pod_name)

    # restore 1st backup and assert with data_checksum_1
    restored_data_checksum1, output, restore_pod_name = \
        restore_backup_and_get_data_checksum(client, core_api, backup1, pod,
                                             file_name=&#39;test1&#39;)
    assert data_checksum_1 == restored_data_checksum1[&#39;test1&#39;]

    delete_and_wait_pod(core_api, restore_pod_name)

    # restore 2nd backup and assert with data_checksum_1 and data_checksum_2
    restored_data_checksum2, output, restore_pod_name = \
        restore_backup_and_get_data_checksum(client, core_api, backup2, pod)
    assert data_checksum_1 == restored_data_checksum2[&#39;test1&#39;]
    assert data_checksum_2 == restored_data_checksum2[&#39;test2&#39;]

    delete_and_wait_pod(core_api, restore_pod_name)

    # restore 3rd backup and assert with data_checksum_3
    restored_data_checksum3, output, restore_pod_name = \
        restore_backup_and_get_data_checksum(client, core_api, backup3, pod,
                                             file_name=&#39;test3&#39;,
                                             command=r&#34;ls /data | grep &#39;test1\|test2&#39;&#34;)  # NOQA
    assert data_checksum_3 == restored_data_checksum3[&#39;test3&#39;]
    assert output == &#39;&#39;

    delete_and_wait_pod(core_api, restore_pod_name)

    # Delete the 2nd backup
    delete_backup(client, backup_volume.name, backup2.name)

    # restore 3rd backup again
    restored_data_checksum3, output, restore_pod_name = \
        restore_backup_and_get_data_checksum(client, core_api, backup3, pod,
                                             file_name=&#39;test3&#39;,
                                             command=r&#34;ls /data | grep &#39;test1\|test2&#39;&#34;)  # NOQA
    assert data_checksum_3 == restored_data_checksum3[&#39;test3&#39;]
    assert output == &#39;&#39;


@pytest.mark.coretest   # NOQA
def test_default_storage_class_syncup(core_api, request):  # NOQA
    &#34;&#34;&#34;
    Steps:
    1. Record the current Longhorn-StorageClass-related ConfigMap
       `longhorn-storageclass`.
    2. Modify the default Longhorn StorageClass `longhorn`.
       e.g., update `reclaimPolicy` from `Delete` to `Retain`.
    3. Verify that the change is reverted immediately and the manifest is the
       same as the record in ConfigMap `longhorn-storageclass`.
    4. Delete the default Longhorn StorageClass `longhorn`.
    5. Verify that the StorageClass is recreated immediately with the manifest
       the same as the record in ConfigMap `longhorn-storageclass`.
    6. Modify the content of ConfigMap `longhorn-storageclass`.
    7. Verify that the modifications will be applied to the default Longhorn
       StorageClass `longhorn` immediately.
    8. Revert the modifications of the ConfigMaps. Then wait for the
       StorageClass sync-up.
    &#34;&#34;&#34;
    def edit_configmap_allow_vol_exp(allow_exp):
        &#34;&#34;&#34;
        allow_exp : bool, set allowVolumeExpansion
        &#34;&#34;&#34;
        config_map = core_api.read_namespaced_config_map(
                                                &#34;longhorn-storageclass&#34;,
                                                &#34;longhorn-system&#34;)
        config_map_data = yaml.safe_load(config_map.data[&#34;storageclass.yaml&#34;])
        config_map_data[&#34;allowVolumeExpansion&#34;] = allow_exp
        config_map.data[&#34;storageclass.yaml&#34;] = yaml.dump(config_map_data)
        core_api.patch_namespaced_config_map(&#34;longhorn-storageclass&#34;,
                                             &#34;longhorn-system&#34;,
                                             config_map)

        for i in range(RETRY_COMMAND_COUNT):
            try:
                longhorn_storage_class = storage_api.read_storage_class(
                                                     &#34;longhorn&#34;)
                assert longhorn_storage_class.allow_volume_expansion is allow_exp # NOQA
                break
            except Exception as e:
                print(e)
            finally:
                time.sleep(RETRY_INTERVAL)
        longhorn_storage_class = storage_api.read_storage_class(&#34;longhorn&#34;)
        assert longhorn_storage_class.allow_volume_expansion is allow_exp

    def finalizer():

        edit_configmap_allow_vol_exp(True)

    request.addfinalizer(finalizer)

    # step 1
    storage_api = common.get_storage_api_client()
    config_map = core_api.read_namespaced_config_map(&#34;longhorn-storageclass&#34;,
                                                     &#34;longhorn-system&#34;)
    config_map_data = yaml.safe_load(config_map.data[&#34;storageclass.yaml&#34;])

    # step 2
    longhorn_storage_class = storage_api.read_storage_class(&#34;longhorn&#34;)
    storage_class_data = \
        yaml.safe_load(longhorn_storage_class.
                       metadata.
                       annotations[&#34;longhorn.io/last-applied-configmap&#34;])
    storage_class_data[&#34;reclaimPolicy&#34;] = &#34;Retain&#34;
    longhorn_storage_class.\
        metadata.annotations[&#34;longhorn.io/last-applied-configmap&#34;] =\
        yaml.dump(storage_class_data)

    storage_api.patch_storage_class(&#34;longhorn&#34;, longhorn_storage_class)

    # step 3
    for i in range(RETRY_COMMAND_COUNT):
        longhorn_storage_class = storage_api.read_storage_class(&#34;longhorn&#34;)
        storage_class_data = \
            yaml.safe_load(longhorn_storage_class.
                           metadata.
                           annotations[&#34;longhorn.io/last-applied-configmap&#34;])

        if storage_class_data[&#34;reclaimPolicy&#34;] == \
                config_map_data[&#34;reclaimPolicy&#34;]:
            break

        time.sleep(RETRY_INTERVAL)

    assert storage_class_data[&#34;reclaimPolicy&#34;] == \
        config_map_data[&#34;reclaimPolicy&#34;]

    # step 4
    storage_api.delete_storage_class(&#34;longhorn&#34;)
    for item in storage_api.list_storage_class().items:
        assert item.metadata.name != &#34;longhorn&#34;

    # step 5
    storage_class_recreated = False
    for i in range(RETRY_COMMAND_COUNT):
        for item in storage_api.list_storage_class().items:
            if item.metadata.name == &#34;longhorn&#34;:
                storage_class_recreated = True
                break
        time.sleep(RETRY_INTERVAL)
    assert storage_class_recreated is True

    # step 6, 7
    edit_configmap_allow_vol_exp(False)

    # step 8 in finalizer


@pytest.mark.coretest   # NOQA
def test_snapshot_prune(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    &#34;&#34;&#34;
    Test removing the snapshot directly behinds the volume head would trigger
    snapshot prune. Snapshot pruning means removing the overlapping part from
    the snapshot based on the volume head content.


    1.  Create a volume and attach to the node
    2.  Generate and write data `snap1_data`, then create `snap1`
    3.  Generate and write data `snap2_data` with the same offset.
    4.  Mark `snap1` as removed.
        Make sure volume&#39;s data didn&#39;t change.
        But all data of the `snap1` will be pruned.
    5.  Detach and expand the volume, then wait for the expansion done.
        This will implicitly create a new snapshot `snap2`.
    6.  Attach the volume.
        Make sure there is a system snapshot with the old size.
    7.  Generate and write data `snap3_data` which is partially overlapped with
       `snap2_data`, plus one extra data chunk in the expanded part.
    8.  Mark `snap2` as removed then do snapshot purge.
        Make sure volume&#39;s data didn&#39;t change.
        But the overlapping part of `snap2` will be pruned.
    9.  Create `snap3`.
    10. Do snapshot purge for the volume. Make sure `snap2` will be removed.
    11. Generate and write data `snap4_data` which has no overlapping with
        `snap3_data`.
    12. Mark `snap3` as removed.
        Make sure volume&#39;s data didn&#39;t change.
        But there is no change for `snap3`.
    13. Create `snap4`.
    14. Generate and write data `snap5_data`, then create `snap5`.
    15. Detach and reattach the volume in maintenance mode.
    16. Make sure the volume frontend is still `blockdev` but disabled
    17. Revert to `snap4`
    18. Detach and reattach the volume with frontend enabled
    19. Make sure volume&#39;s data is correct.
    20. List snapshot. Make sure `volume-head` is now `snap4`&#39;s child
    &#34;&#34;&#34;
    snapshot_prune_test(client, volume_name, backing_image)


def snapshot_prune_test(client, volume_name, backing_image):  # NOQA
    # For this test, the fiemap size should not greater than 1Mi
    fiemap_max_size = 4 * Ki
    snap_data_size_in_mb = 4
    volume = create_and_check_volume(client, volume_name,
                                     backing_image=backing_image)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)
    volume_endpoint = get_volume_endpoint(volume)

    # Prepare and write snap1_data
    snap1_offset = 1
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap1_offset, snap_data_size_in_mb)
    snap1_before = create_snapshot(client, volume_name)

    # Prepare and write snap2_data,
    # which would completely overwrite snap1 content.
    snap2_offset = 1
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap2_offset, snap_data_size_in_mb)
    cksum_before = get_device_checksum(volume_endpoint)

    # All data in snap1 should be pruned.
    volume.snapshotDelete(name=snap1_before.name)
    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap1_before.name)
    snap1_after = volume.snapshotGet(name=snap1_before.name)
    volume_head1 = volume.snapshotGet(name=VOLUME_HEAD_NAME)
    snap1_after_size = int(snap1_after.size)
    cksum_after = get_device_checksum(volume_endpoint)
    assert snap1_after.removed
    assert snap1_after_size == 0
    assert cksum_before == cksum_after

    # Expansion will implicitly created a system snapshot `snap2`
    expand_attached_volume(client, volume_name)
    volume = client.by_id_volume(volume_name)
    for snap in volume.snapshotList(volume=volume_name):
        # In the future, there may be an automatic purge operation
        # after expansion
        if snap.name == snap1_before.name:
            assert snap.name == snap1_before.name
            assert snap.removed
            assert VOLUME_HEAD_NAME not in snap.children.keys()
            continue
        if snap.name != VOLUME_HEAD_NAME:
            snap2_before = snap
            assert not snap.usercreated
            assert snap.size == volume_head1.size
            assert snap.parent == snap1_before.name
            assert VOLUME_HEAD_NAME in snap.children.keys()
            continue
    assert snap2_before
    snap2_before_size = int(snap2_before.size)

    # Prepare and write snap3_data,
    # which would partially overwrite snap2 content, plus one extra data chunk
    # in the expanded part.
    snap3_offset1 = random.randrange(snap2_offset,
                                     snap2_offset + snap_data_size_in_mb, 1)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap3_offset1, snap_data_size_in_mb)
    snap3_offset2 = random.randrange(
        int(SIZE)/Mi + snap_data_size_in_mb,
        int(EXPAND_SIZE)/Mi - snap_data_size_in_mb, 1)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap3_offset2, snap_data_size_in_mb)
    cksum_before = get_device_checksum(volume_endpoint)

    # Pruning snap2 as well as coalescing snap1 with snap2
    volume.snapshotDelete(name=snap2_before.name)
    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap2_before.name)
    snap2_after = volume.snapshotGet(name=snap2_before.name)
    snap2_after_size = int(snap2_after.size)
    cksum_after = get_device_checksum(volume_endpoint)
    assert snap2_after.removed
    assert snap2_before_size &gt;= snap2_after_size - snap1_after_size - \
           fiemap_max_size + \
           (snap2_offset + snap_data_size_in_mb - snap3_offset1) * Mi

    assert cksum_before == cksum_after

    snap3_before = create_snapshot(client, volume_name)
    snap3_before_size = int(snap3_before.size)

    # Prepare and write snap4_data which has no overlapping part with snap3.
    snap4_offset = random.randrange(snap3_offset1 + snap_data_size_in_mb,
                                    int(SIZE)/Mi + snap_data_size_in_mb, 1)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap4_offset, snap_data_size_in_mb)
    cksum_before = get_device_checksum(volume_endpoint)
    # Pruning snap3 then coalescing snap2 with snap3
    volume.snapshotDelete(name=snap3_before.name)
    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap3_before.name)
    snap3_after = volume.snapshotGet(name=snap3_before.name)
    snap3_after_size = int(snap3_after.size)
    cksum_after = get_device_checksum(volume_endpoint)
    assert snap3_after.removed
    assert snap3_before_size &gt;= snap3_after_size - snap2_after_size - \
           fiemap_max_size
    assert cksum_before == cksum_after

    snap4 = create_snapshot(client, volume_name)

    # We don&#39;t care the exact content of snap5
    snap5_offset = random.randrange(
        0, int(EXPAND_SIZE)/Mi - snap_data_size_in_mb, 1)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap5_offset, snap_data_size_in_mb)
    create_snapshot(client, volume_name)

    # Prepare to do revert
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    assert volume.disableFrontend is True
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV
    check_volume_endpoint(volume)
    # Reverting to a removed snapshot should fail
    with pytest.raises(Exception):
        volume.snapshotRevert(name=snap3_after.name)
    # Reverting to snap4 should succeed
    volume.snapshotRevert(name=snap4.name)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    cksum_revert = get_device_checksum(volume_endpoint)
    assert cksum_before == cksum_revert

    cleanup_volume(client, volume)


@pytest.mark.coretest   # NOQA
def test_snapshot_prune_and_coalesce_simultaneously(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    &#34;&#34;&#34;
    Test the prune for the snapshot directly behinds the volume head would be
    handled after all snapshot coalescing done.


    1. Create a volume and attach to the node
    2. Generate and write 1st data chunk `snap1_data`, then create `snap1`
    3. Generate and write 2nd data chunk `snap2_data`, then create `snap2`
    4. Generate and write 3rd data chunk `snap3_data`, then create `snap3`
    5. Generate and write 4th data chunk `snap4_data`, then create `snap4`
    6. Overwrite all existing data chunks in the volume head.
    7. Mark all snapshots as `Removed`,
       then start snapshot purge and wait for complete.
    8. List snapshot.
       Make sure there are only 2 snapshots left: `volume-head` and `snap4`.
       And `snap4` is an empty snapshot.
    9. Make sure volume&#39;s data is correct.
    &#34;&#34;&#34;
    snapshot_prune_and_coalesce_simultaneously(
        client, volume_name, backing_image)


def snapshot_prune_and_coalesce_simultaneously(client, volume_name, backing_image):  # NOQA
    snap_data_size_in_mb = 2
    volume = create_and_check_volume(client, volume_name,
                                     backing_image=backing_image)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)
    volume_endpoint = get_volume_endpoint(volume)

    # Take snapshots4 without overlapping
    write_volume_dev_random_mb_data(
        volume_endpoint, 0*snap_data_size_in_mb, snap_data_size_in_mb)
    snap1 = create_snapshot(client, volume_name)

    write_volume_dev_random_mb_data(
        volume_endpoint, 1*snap_data_size_in_mb, snap_data_size_in_mb)
    snap2 = create_snapshot(client, volume_name)

    write_volume_dev_random_mb_data(
        volume_endpoint, 2*snap_data_size_in_mb, snap_data_size_in_mb)
    snap3 = create_snapshot(client, volume_name)

    write_volume_dev_random_mb_data(
        volume_endpoint, 3*snap_data_size_in_mb, snap_data_size_in_mb)
    snap4 = create_snapshot(client, volume_name)

    # Overwrite the existing data in the volume head
    write_volume_dev_random_mb_data(
        volume_endpoint, 0*snap_data_size_in_mb, 4*snap_data_size_in_mb)
    cksum_before = get_device_checksum(volume_endpoint)
    volume_head_before = volume.snapshotGet(name=VOLUME_HEAD_NAME)

    # Simultaneous snapshot coalescing &amp; pruning
    volume.snapshotDelete(name=snap1.name)
    volume.snapshotDelete(name=snap2.name)
    volume.snapshotDelete(name=snap3.name)
    volume.snapshotDelete(name=snap4.name)
    volume.snapshotPurge()
    wait_for_snapshot_purge(client, volume_name, snap1.name)
    wait_for_snapshot_purge(client, volume_name, snap2.name)
    wait_for_snapshot_purge(client, volume_name, snap3.name)
    wait_for_snapshot_purge(client, volume_name, snap4.name)

    # List and validate snap info
    volume = client.by_id_volume(volume_name)
    snaps = volume.snapshotList(volume=volume_name)
    assert len(snaps) == 2
    for snap in snaps:
        if snap.name == VOLUME_HEAD_NAME:
            assert snap.size == volume_head_before.size
            assert snap.parent == snap4.name
        else:
            assert snap.name == snap4.name
            assert int(snap.size) == 0
            assert snap.parent == &#34;&#34;
            assert VOLUME_HEAD_NAME in snap.children.keys()
            continue

    # Verify the data
    cksum_after = get_device_checksum(volume_endpoint)
    assert cksum_after == cksum_before


@pytest.mark.skip(reason=&#34;TODO&#34;)  # NOQA
def test_space_usage_for_rebuilding_only_volume():  # NOQA
    &#34;&#34;&#34;
    Test the space usage of a volume with rebuilding only.

    Prepare:
    1. Create a 7Gi volume and attach to the node.
    2. Make a filesystem then mount this volume.
    3. Make this volume as a disk of the node, and disable the scheduling for
       the default disk.

    Case 1: the worst scenario
    1. Create a new volume with 2Gi spec size.
    2. Write 2Gi data (using `dd`) to the volume.
    3. Take a snapshot then mark this snapshot as Removed.
       (this snapshot won&#39;t be deleted immediately.)
    4. Write 2Gi data (using `dd`) to the volume again.
    5. Delete a random replica to trigger the rebuilding.
    6. Write 2Gi data once the rebuilding is trigger (new replica is created).
    7. Wait for the rebuilding complete. And verify the volume actual size
       won&#39;t be greater than 3x of the volume spec size.
    8. Delete the volume.

    Case 2: the normal scenario
    1. Create a new volume with 3Gi spec size.
    2. Write 3Gi data (using `dd`) to the volume.
    3. Take a snapshot then mark this snapshot as Removed.
       (this snapshot won&#39;t be deleted immediately.)
    4. Write 3Gi data (using `dd`) to the volume again.
    5. Delete a random replica to trigger the rebuilding.
    6. Wait for the rebuilding complete. And verify the volume actual size
       won&#39;t be greater than 2x of the volume spec size.
    7. Delete the volume.

    &#34;&#34;&#34;
    pass


def backup_failed_cleanup(client, core_api, volume_name, volume_size,  # NOQA
                          failed_backup_ttl=&#34;3&#34;):  # NOQA
    &#34;&#34;&#34;
    Setup the failed backup cleanup
    &#34;&#34;&#34;
    bt_poll_interval = &#34;60&#34;

    # set backupstore poll interval to 60 sec
    set_backupstore_poll_interval(client, bt_poll_interval)

    # set failed backup time to live, default: 3 min, disabled: 0
    failed_backup_ttl_setting = client.by_id_setting(SETTING_FAILED_BACKUP_TTL)
    new_failed_backup_ttl_setting = client.update(
        failed_backup_ttl_setting, value=failed_backup_ttl)
    assert new_failed_backup_ttl_setting.value == failed_backup_ttl

    backupstore_cleanup(client)

    # create a volume
    vol = create_and_check_volume(client, volume_name,
                                  num_of_replicas=2, size=str(volume_size))

    # attach the volume
    vol.attach(hostId=get_self_host_id())
    vol = wait_for_volume_healthy(client, volume_name)

    # create a snapshot and
    # a successful backup to make sure the backup volume created
    snap = create_snapshot(client, volume_name)
    vol.snapshotBackup(name=snap.name)

    # write some data to the volume
    data = {
        &#39;pos&#39;: 0,
        &#39;content&#39;: common.generate_random_data(volume_size),
    }
    write_volume_data(vol, data)

    # create a snapshot and a backup
    snap = create_snapshot(client, volume_name)
    vol.snapshotBackup(name=snap.name)

    # check backup status is in an InProgress state
    _, backup = find_backup(client, volume_name, snap.name)
    backup_id = backup.id
    backup_name = backup.name

    def backup_inprogress_predicate(b):
        return b.id == backup_id and &#34;InProgress&#34; in b.state
    common.wait_for_backup_state(client, volume_name,
                                 backup_inprogress_predicate)

    # crash all replicas of the volume
    try:
        crash_replica_processes(client, core_api, volume_name)
    except AssertionError:
        crash_replica_processes(client, core_api, volume_name)

    # backup status should be in an Error state and with an error message
    def backup_failure_predicate(b):
        return b.id == backup_id and &#34;Error&#34; in b.state and b.error != &#34;&#34;
    common.wait_for_backup_state(client, volume_name,
                                 backup_failure_predicate)

    return backup_name


@pytest.mark.coretest  # NOQA
def test_backup_failed_enable_auto_cleanup(set_random_backupstore,  # NOQA
                                        client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test the failed backup would be automatically deleted.

    1.   Set the default setting `backupstore-poll-interval` to 60 (seconds)
    2.   Set the default setting `failed-backup-ttl` to 3 (minutes)
    3.   Create a volume and attach to the current node
    4.   Create a empty backup for creating the backup volume
    5.   Write some data to the volume
    6.   Create a backup of the volume
    7.   Crash all replicas
    8.   Wait and check if the backup failed
    9.   Wait and check if the backup was deleted automatically
    10.  Cleanup
    &#34;&#34;&#34;
    backup_name = backup_failed_cleanup(client, core_api, volume_name, 256*Mi)

    # wait in 5 minutes for automatic failed backup cleanup
    wait_for_backup_delete(client, volume_name, backup_name)


@pytest.mark.coretest  # NOQA
def test_backup_failed_disable_auto_cleanup(set_random_backupstore,  # NOQA
                                        client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test the failed backup would be automatically deleted.

    1.    Set the default setting `backupstore-poll-interval` to 60 (seconds)
    2.    Set the default setting `failed-backup-ttl` to 0
    3.    Create a volume and attach to the current node
    4.    Create a empty backup for creating the backup volume
    5.    Write some data to the volume
    6.    Create a backup of the volume
    7.    Crash all replicas
    8.    Wait and check if the backup failed
    9.    Wait and check if the backup was not deleted.
    10.   Cleanup
    &#34;&#34;&#34;
    backup_name = backup_failed_cleanup(client, core_api, volume_name, 256*Mi,
                                        failed_backup_ttl=&#34;0&#34;)

    # wait for 5 minutes to check if the failed backup exists
    try:
        wait_for_backup_delete(client, volume_name, backup_name)
        assert False, &#34;backup &#34; + backup_name + &#34; for volume &#34; \
                      + volume_name + &#34; is deleted&#34;
    except AssertionError:
        pass


@pytest.mark.parametrize(
    &#34;access_mode,overridden_restored_access_mode&#34;,
    [
        pytest.param(&#34;rwx&#34;, &#34;rwo&#34;),
        pytest.param(&#34;rwo&#34;, &#34;rwx&#34;)
    ],
)
def test_backup_volume_restore_with_access_mode(set_random_backupstore, # NOQA
                                                client, # NOQA
                                                access_mode, # NOQA
                                       overridden_restored_access_mode): # NOQA
    &#34;&#34;&#34;
    Test the backup w/ the volume access mode, then restore a volume w/ the
     original access mode or being overridden.

    1. Prepare a healthy volume
    2. Create a backup for the volume
    3. Restore a volume from the backup w/o specifying the access mode
       =&gt; Validate the access mode should be the same the volume
    4. Restore a volume from the backup w/ specifying the access mode
       =&gt; Validate the access mode should be the same as the specified
    &#34;&#34;&#34;
    # Step 1
    test_volume_name = generate_volume_name()
    client.create_volume(name=test_volume_name,
                         size=str(DEFAULT_VOLUME_SIZE * Gi),
                         numberOfReplicas=2,
                         accessMode=access_mode)
    wait_for_volume_creation(client, test_volume_name)
    volume = wait_for_volume_detached(client, test_volume_name)
    volume.attach(hostId=common.get_self_host_id())
    volume = common.wait_for_volume_healthy(client, test_volume_name)

    # Step 2
    _, b, _, _ = create_backup(client, test_volume_name)

    # Step 3
    volume_name_ori_access_mode = test_volume_name + &#39;-default-access&#39;
    client.create_volume(name=volume_name_ori_access_mode,
                         size=str(DEFAULT_VOLUME_SIZE * Gi),
                         numberOfReplicas=2,
                         fromBackup=b.url)
    volume_ori_access_mode = client.by_id_volume(volume_name_ori_access_mode)
    assert volume_ori_access_mode.accessMode == access_mode

    # Step 4
    volume_name_sp_access_mode = test_volume_name + &#39;-specified-access&#39;
    client.create_volume(name=volume_name_sp_access_mode,
                         size=str(DEFAULT_VOLUME_SIZE * Gi),
                         numberOfReplicas=2,
                         accessMode=overridden_restored_access_mode,
                         fromBackup=b.url)
    volume_sp_access_mode = client.by_id_volume(volume_name_sp_access_mode)
    assert volume_sp_access_mode.accessMode == overridden_restored_access_mode</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_basic.backup_failed_cleanup"><code class="name flex">
<span>def <span class="ident">backup_failed_cleanup</span></span>(<span>client, core_api, volume_name, volume_size, failed_backup_ttl='3')</span>
</code></dt>
<dd>
<div class="desc"><p>Setup the failed backup cleanup</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup_failed_cleanup(client, core_api, volume_name, volume_size,  # NOQA
                          failed_backup_ttl=&#34;3&#34;):  # NOQA
    &#34;&#34;&#34;
    Setup the failed backup cleanup
    &#34;&#34;&#34;
    bt_poll_interval = &#34;60&#34;

    # set backupstore poll interval to 60 sec
    set_backupstore_poll_interval(client, bt_poll_interval)

    # set failed backup time to live, default: 3 min, disabled: 0
    failed_backup_ttl_setting = client.by_id_setting(SETTING_FAILED_BACKUP_TTL)
    new_failed_backup_ttl_setting = client.update(
        failed_backup_ttl_setting, value=failed_backup_ttl)
    assert new_failed_backup_ttl_setting.value == failed_backup_ttl

    backupstore_cleanup(client)

    # create a volume
    vol = create_and_check_volume(client, volume_name,
                                  num_of_replicas=2, size=str(volume_size))

    # attach the volume
    vol.attach(hostId=get_self_host_id())
    vol = wait_for_volume_healthy(client, volume_name)

    # create a snapshot and
    # a successful backup to make sure the backup volume created
    snap = create_snapshot(client, volume_name)
    vol.snapshotBackup(name=snap.name)

    # write some data to the volume
    data = {
        &#39;pos&#39;: 0,
        &#39;content&#39;: common.generate_random_data(volume_size),
    }
    write_volume_data(vol, data)

    # create a snapshot and a backup
    snap = create_snapshot(client, volume_name)
    vol.snapshotBackup(name=snap.name)

    # check backup status is in an InProgress state
    _, backup = find_backup(client, volume_name, snap.name)
    backup_id = backup.id
    backup_name = backup.name

    def backup_inprogress_predicate(b):
        return b.id == backup_id and &#34;InProgress&#34; in b.state
    common.wait_for_backup_state(client, volume_name,
                                 backup_inprogress_predicate)

    # crash all replicas of the volume
    try:
        crash_replica_processes(client, core_api, volume_name)
    except AssertionError:
        crash_replica_processes(client, core_api, volume_name)

    # backup status should be in an Error state and with an error message
    def backup_failure_predicate(b):
        return b.id == backup_id and &#34;Error&#34; in b.state and b.error != &#34;&#34;
    common.wait_for_backup_state(client, volume_name,
                                 backup_failure_predicate)

    return backup_name</code></pre>
</details>
</dd>
<dt id="tests.test_basic.backup_labels_test"><code class="name flex">
<span>def <span class="ident">backup_labels_test</span></span>(<span>client, random_labels, volume_name, size='16777216', backing_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup_labels_test(client, random_labels, volume_name, size=SIZE, backing_image=&#34;&#34;):  # NOQA
    host_id = get_self_host_id()

    volume = create_and_check_volume(client, volume_name, 2, size,
                                     backing_image)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    bv, b, _, _ = create_backup(client, volume_name, labels=random_labels)
    # If we&#39;re running the test with a BackingImage,
    # check field `volumeBackingImageName` is set properly.
    backup = bv.backupGet(name=b.name)
    # Longhorn will automatically add a label `longhorn.io/volume-access-mode`
    # to a newly created backup
    assert len(backup.labels) == len(random_labels) + 1
    assert random_labels[&#34;key&#34;] == backup.labels[&#34;key&#34;]
    assert &#34;longhorn.io/volume-access-mode&#34; in backup.labels.keys()
    wait_for_backup_volume(client, volume_name, backing_image)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.backup_status_for_unavailable_replicas_test"><code class="name flex">
<span>def <span class="ident">backup_status_for_unavailable_replicas_test</span></span>(<span>client, volume_name, size, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup_status_for_unavailable_replicas_test(client, volume_name,  # NOQA
                                                size, backing_image=&#34;&#34;):  # NOQA
    volume = create_and_check_volume(client, volume_name, 2, size,
                                     backing_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    # write data to the volume
    data = {
        &#39;pos&#39;: 0,
        &#39;content&#39;: common.generate_random_data(int(size)),
    }
    write_volume_data(volume, data)

    # create a snapshot and backup
    snap = create_snapshot(client, volume_name)
    volume.snapshotBackup(name=snap.name)
    bv, b = find_backup(client, volume_name, snap.name)
    backup_id = b.id

    # find the replica for this backup
    replica_name = find_replica_for_backup(client, volume_name, backup_id)

    # disable scheduling on that node
    volume = client.by_id_volume(volume_name)
    for r in volume.replicas:
        if r.name == replica_name:
            node = client.by_id_node(r.hostId)
            node = set_node_scheduling(client, node, allowScheduling=False)
            common.wait_for_node_update(client, node.id,
                                        &#34;allowScheduling&#34;, False)
    assert node

    # remove the replica with the backup
    volume.replicaRemove(name=replica_name)
    volume = common.wait_for_volume_degraded(client, volume_name)

    # now the backup status should in an Error state and with an error message
    def backup_failure_predicate(b):
        return b.id == backup_id and &#34;Error&#34; in b.state and b.error != &#34;&#34;
    volume = common.wait_for_backup_state(client, volume_name,
                                          backup_failure_predicate)

    # re enable scheduling on the previously disabled node
    node = client.by_id_node(node.id)
    node = set_node_scheduling(client, node, allowScheduling=True)
    common.wait_for_node_update(client, node.id,
                                &#34;allowScheduling&#34;, True)

    # delete the old backup
    delete_backup(client, bv.name, b.name)
    volume = wait_for_volume_status(client, volume_name,
                                    &#34;lastBackup&#34;, &#34;&#34;)
    assert volume.lastBackupAt == &#34;&#34;

    # check that we can create another successful backup
    bv, b, _, _ = create_backup(client, volume_name)

    # delete the new backup
    delete_backup(client, bv.name, b.name)
    volume = wait_for_volume_status(client, volume_name, &#34;lastBackup&#34;, &#34;&#34;)
    assert volume.lastBackupAt == &#34;&#34;</code></pre>
</details>
</dd>
<dt id="tests.test_basic.backup_test"><code class="name flex">
<span>def <span class="ident">backup_test</span></span>(<span>client, volume_name, size, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup_test(client, volume_name, size, backing_image=&#34;&#34;):  # NOQA
    volume = create_and_check_volume(client, volume_name, 2, size,
                                     backing_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    backupstore_test(client, lht_hostId, volume_name, size)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.backupstore_test"><code class="name flex">
<span>def <span class="ident">backupstore_test</span></span>(<span>client, host_id, volname, size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backupstore_test(client, host_id, volname, size):  # NOQA
    bv, b, snap2, data = create_backup(client, volname)

    # test restore
    restore_name = generate_volume_name()
    volume = client.create_volume(name=restore_name, size=size,
                                  numberOfReplicas=2,
                                  fromBackup=b.url)

    volume = common.wait_for_volume_restoration_completed(client, restore_name)
    volume = common.wait_for_volume_detached(client, restore_name)
    assert volume.name == restore_name
    assert volume.size == size
    assert volume.numberOfReplicas == 2
    assert volume.state == &#34;detached&#34;
    assert volume.restoreRequired is False

    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, restore_name)
    check_volume_data(volume, data)
    volume = volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, restore_name)

    delete_backup(client, bv.name, b.name)
    volume = wait_for_volume_status(client, volume.name,
                                    &#34;lastBackup&#34;, &#34;&#34;)
    assert volume.lastBackupAt == &#34;&#34;

    client.delete(volume)
    volume = wait_for_volume_delete(client, restore_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.restore_inc_test"><code class="name flex">
<span>def <span class="ident">restore_inc_test</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restore_inc_test(client, core_api, volume_name, pod):  # NOQA
    std_volume = create_and_check_volume(client, volume_name, 2, SIZE)
    lht_host_id = get_self_host_id()
    std_volume.attach(hostId=lht_host_id)
    std_volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        std_volume.activate(frontend=VOLUME_FRONTEND_BLOCKDEV)
        assert &#34;already in active mode&#34; in str(e.value)

    data0 = {&#39;len&#39;: 2 * BACKUP_BLOCK_SIZE, &#39;pos&#39;: 0,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}
    _, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    sb_volume0_name = &#34;sb-0-&#34; + volume_name
    sb_volume1_name = &#34;sb-1-&#34; + volume_name
    sb_volume2_name = &#34;sb-2-&#34; + volume_name
    client.create_volume(name=sb_volume0_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=sb_volume1_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=sb_volume2_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    wait_for_backup_restore_completed(client, sb_volume0_name, backup0.name)
    wait_for_backup_restore_completed(client, sb_volume1_name, backup0.name)
    wait_for_backup_restore_completed(client, sb_volume2_name, backup0.name)

    sb_volume0 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume0_name)
    sb_volume1 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume1_name)
    sb_volume2 = common.wait_for_volume_healthy_no_frontend(client,
                                                            sb_volume2_name)

    for _ in range(RETRY_COUNTS):
        client.list_backupVolume()
        sb_volume0 = client.by_id_volume(sb_volume0_name)
        sb_volume1 = client.by_id_volume(sb_volume1_name)
        sb_volume2 = client.by_id_volume(sb_volume2_name)
        sb_engine0 = get_volume_engine(sb_volume0)
        sb_engine1 = get_volume_engine(sb_volume1)
        sb_engine2 = get_volume_engine(sb_volume2)
        if sb_volume0.restoreRequired is False or \
           sb_volume1.restoreRequired is False or \
           sb_volume2.restoreRequired is False or \
                not sb_engine0.lastRestoredBackup or \
                not sb_engine1.lastRestoredBackup or \
                not sb_engine2.lastRestoredBackup:
            time.sleep(RETRY_INTERVAL)
        else:
            break
    assert sb_volume0.standby is True
    assert sb_volume0.lastBackup == backup0.name
    assert sb_volume0.frontend == &#34;&#34;
    assert sb_volume0.restoreRequired is True
    sb_engine0 = get_volume_engine(sb_volume0)
    assert sb_engine0.lastRestoredBackup == backup0.name
    assert sb_engine0.requestedBackupRestore == backup0.name
    assert sb_volume1.standby is True
    assert sb_volume1.lastBackup == backup0.name
    assert sb_volume1.frontend == &#34;&#34;
    assert sb_volume1.restoreRequired is True
    sb_engine1 = get_volume_engine(sb_volume1)
    assert sb_engine1.lastRestoredBackup == backup0.name
    assert sb_engine1.requestedBackupRestore == backup0.name
    assert sb_volume2.standby is True
    assert sb_volume2.lastBackup == backup0.name
    assert sb_volume2.frontend == &#34;&#34;
    assert sb_volume2.restoreRequired is True
    sb_engine2 = get_volume_engine(sb_volume2)
    assert sb_engine2.lastRestoredBackup == backup0.name
    assert sb_engine2.requestedBackupRestore == backup0.name

    sb0_snaps = sb_volume0.snapshotList()
    assert len(sb0_snaps) == 2
    for s in sb0_snaps:
        if s.name != &#34;volume-head&#34;:
            sb0_snap = s
    assert sb0_snaps
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotCreate()
        assert &#34;cannot create snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotRevert(name=sb0_snap.name)
        assert &#34;cannot revert snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotDelete(name=sb0_snap.name)
        assert &#34;cannot delete snapshot for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.snapshotBackup(name=sb0_snap.name)
        assert &#34;cannot create backup for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.pvCreate(pvName=sb_volume0_name)
        assert &#34;cannot create PV for standby volume&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.pvcCreate(pvcName=sb_volume0_name)
        assert &#34;cannot create PVC for standby volume&#34; in str(e.value)
    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    with pytest.raises(Exception) as e:
        client.update(setting, value=&#34;random.backup.target&#34;)
        assert &#34;cannot modify BackupTarget &#34; \
               &#34;since there are existing standby volumes&#34; in str(e.value)
    with pytest.raises(Exception) as e:
        sb_volume0.activate(frontend=&#34;wrong_frontend&#34;)
        assert &#34;invalid frontend&#34; in str(e.value)

    activate_standby_volume(client, sb_volume0_name)
    sb_volume0 = client.by_id_volume(sb_volume0_name)
    sb_volume0.attach(hostId=lht_host_id)
    sb_volume0 = common.wait_for_volume_healthy(client, sb_volume0_name)
    check_volume_data(sb_volume0, data0, False)

    zero_string = b&#39;\x00&#39;.decode(&#39;utf-8&#39;)
    _, backup1, _, data1 = create_backup(
        client, volume_name,
        {&#39;len&#39;: BACKUP_BLOCK_SIZE, &#39;pos&#39;: 0,
         &#39;content&#39;: zero_string * BACKUP_BLOCK_SIZE})
    check_volume_last_backup(client, sb_volume1_name, backup1.name)
    activate_standby_volume(client, sb_volume1_name)
    sb_volume1 = client.by_id_volume(sb_volume1_name)
    sb_volume1.attach(hostId=lht_host_id)
    sb_volume1 = common.wait_for_volume_healthy(client, sb_volume1_name)
    data0_modified1 = {
        &#39;len&#39;: data0[&#39;len&#39;],
        &#39;pos&#39;: 0,
        &#39;content&#39;: data1[&#39;content&#39;] + data0[&#39;content&#39;][data1[&#39;len&#39;]:],
    }
    check_volume_data(sb_volume1, data0_modified1, False)

    data2_len = int(BACKUP_BLOCK_SIZE/2)
    data2 = {&#39;len&#39;: data2_len, &#39;pos&#39;: 0,
             &#39;content&#39;: common.generate_random_data(data2_len)}
    _, backup2, _, data2 = create_backup(client, volume_name, data2)

    check_volume_last_backup(client, sb_volume2_name, backup2.name)
    activate_standby_volume(client, sb_volume2_name)
    sb_volume2 = client.by_id_volume(sb_volume2_name)
    sb_volume2.attach(hostId=lht_host_id)
    sb_volume2 = common.wait_for_volume_healthy(client, sb_volume2_name)
    data0_modified2 = {
        &#39;len&#39;: data0[&#39;len&#39;],
        &#39;pos&#39;: 0,
        &#39;content&#39;:
            data2[&#39;content&#39;] +
            data1[&#39;content&#39;][data2[&#39;len&#39;]:] +
            data0[&#39;content&#39;][data1[&#39;len&#39;]:],
    }
    check_volume_data(sb_volume2, data0_modified2, False)

    # allocated this active volume to a pod
    sb_volume2.detach(hostId=&#34;&#34;)
    sb_volume2 = common.wait_for_volume_detached(client, sb_volume2_name)

    create_pv_for_volume(client, core_api, sb_volume2, sb_volume2_name)
    create_pvc_for_volume(client, core_api, sb_volume2, sb_volume2_name)

    sb_volume2_pod_name = &#34;pod-&#34; + sb_volume2_name
    pod[&#39;metadata&#39;][&#39;name&#39;] = sb_volume2_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: sb_volume2_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    sb_volume2 = client.by_id_volume(sb_volume2_name)
    k_status = sb_volume2.kubernetesStatus
    workloads = k_status.workloadsStatus
    assert k_status.pvName == sb_volume2_name
    assert k_status.pvStatus == &#39;Bound&#39;
    assert len(workloads) == 1
    for i in range(RETRY_COUNTS):
        if workloads[0].podStatus == &#39;Running&#39;:
            break
        time.sleep(RETRY_INTERVAL)
        sb_volume2 = client.by_id_volume(sb_volume2_name)
        k_status = sb_volume2.kubernetesStatus
        workloads = k_status.workloadsStatus
        assert len(workloads) == 1
    assert workloads[0].podName == sb_volume2_pod_name
    assert workloads[0].podStatus == &#39;Running&#39;
    assert not workloads[0].workloadName
    assert not workloads[0].workloadType
    assert k_status.namespace == &#39;default&#39;
    assert k_status.pvcName == sb_volume2_name
    assert not k_status.lastPVCRefAt
    assert not k_status.lastPodRefAt

    delete_and_wait_pod(core_api, sb_volume2_pod_name)
    delete_and_wait_pvc(core_api, sb_volume2_name)
    delete_and_wait_pv(core_api, sb_volume2_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.snapshot_prune_and_coalesce_simultaneously"><code class="name flex">
<span>def <span class="ident">snapshot_prune_and_coalesce_simultaneously</span></span>(<span>client, volume_name, backing_image)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def snapshot_prune_and_coalesce_simultaneously(client, volume_name, backing_image):  # NOQA
    snap_data_size_in_mb = 2
    volume = create_and_check_volume(client, volume_name,
                                     backing_image=backing_image)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)
    volume_endpoint = get_volume_endpoint(volume)

    # Take snapshots4 without overlapping
    write_volume_dev_random_mb_data(
        volume_endpoint, 0*snap_data_size_in_mb, snap_data_size_in_mb)
    snap1 = create_snapshot(client, volume_name)

    write_volume_dev_random_mb_data(
        volume_endpoint, 1*snap_data_size_in_mb, snap_data_size_in_mb)
    snap2 = create_snapshot(client, volume_name)

    write_volume_dev_random_mb_data(
        volume_endpoint, 2*snap_data_size_in_mb, snap_data_size_in_mb)
    snap3 = create_snapshot(client, volume_name)

    write_volume_dev_random_mb_data(
        volume_endpoint, 3*snap_data_size_in_mb, snap_data_size_in_mb)
    snap4 = create_snapshot(client, volume_name)

    # Overwrite the existing data in the volume head
    write_volume_dev_random_mb_data(
        volume_endpoint, 0*snap_data_size_in_mb, 4*snap_data_size_in_mb)
    cksum_before = get_device_checksum(volume_endpoint)
    volume_head_before = volume.snapshotGet(name=VOLUME_HEAD_NAME)

    # Simultaneous snapshot coalescing &amp; pruning
    volume.snapshotDelete(name=snap1.name)
    volume.snapshotDelete(name=snap2.name)
    volume.snapshotDelete(name=snap3.name)
    volume.snapshotDelete(name=snap4.name)
    volume.snapshotPurge()
    wait_for_snapshot_purge(client, volume_name, snap1.name)
    wait_for_snapshot_purge(client, volume_name, snap2.name)
    wait_for_snapshot_purge(client, volume_name, snap3.name)
    wait_for_snapshot_purge(client, volume_name, snap4.name)

    # List and validate snap info
    volume = client.by_id_volume(volume_name)
    snaps = volume.snapshotList(volume=volume_name)
    assert len(snaps) == 2
    for snap in snaps:
        if snap.name == VOLUME_HEAD_NAME:
            assert snap.size == volume_head_before.size
            assert snap.parent == snap4.name
        else:
            assert snap.name == snap4.name
            assert int(snap.size) == 0
            assert snap.parent == &#34;&#34;
            assert VOLUME_HEAD_NAME in snap.children.keys()
            continue

    # Verify the data
    cksum_after = get_device_checksum(volume_endpoint)
    assert cksum_after == cksum_before</code></pre>
</details>
</dd>
<dt id="tests.test_basic.snapshot_prune_test"><code class="name flex">
<span>def <span class="ident">snapshot_prune_test</span></span>(<span>client, volume_name, backing_image)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def snapshot_prune_test(client, volume_name, backing_image):  # NOQA
    # For this test, the fiemap size should not greater than 1Mi
    fiemap_max_size = 4 * Ki
    snap_data_size_in_mb = 4
    volume = create_and_check_volume(client, volume_name,
                                     backing_image=backing_image)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)
    volume_endpoint = get_volume_endpoint(volume)

    # Prepare and write snap1_data
    snap1_offset = 1
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap1_offset, snap_data_size_in_mb)
    snap1_before = create_snapshot(client, volume_name)

    # Prepare and write snap2_data,
    # which would completely overwrite snap1 content.
    snap2_offset = 1
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap2_offset, snap_data_size_in_mb)
    cksum_before = get_device_checksum(volume_endpoint)

    # All data in snap1 should be pruned.
    volume.snapshotDelete(name=snap1_before.name)
    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap1_before.name)
    snap1_after = volume.snapshotGet(name=snap1_before.name)
    volume_head1 = volume.snapshotGet(name=VOLUME_HEAD_NAME)
    snap1_after_size = int(snap1_after.size)
    cksum_after = get_device_checksum(volume_endpoint)
    assert snap1_after.removed
    assert snap1_after_size == 0
    assert cksum_before == cksum_after

    # Expansion will implicitly created a system snapshot `snap2`
    expand_attached_volume(client, volume_name)
    volume = client.by_id_volume(volume_name)
    for snap in volume.snapshotList(volume=volume_name):
        # In the future, there may be an automatic purge operation
        # after expansion
        if snap.name == snap1_before.name:
            assert snap.name == snap1_before.name
            assert snap.removed
            assert VOLUME_HEAD_NAME not in snap.children.keys()
            continue
        if snap.name != VOLUME_HEAD_NAME:
            snap2_before = snap
            assert not snap.usercreated
            assert snap.size == volume_head1.size
            assert snap.parent == snap1_before.name
            assert VOLUME_HEAD_NAME in snap.children.keys()
            continue
    assert snap2_before
    snap2_before_size = int(snap2_before.size)

    # Prepare and write snap3_data,
    # which would partially overwrite snap2 content, plus one extra data chunk
    # in the expanded part.
    snap3_offset1 = random.randrange(snap2_offset,
                                     snap2_offset + snap_data_size_in_mb, 1)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap3_offset1, snap_data_size_in_mb)
    snap3_offset2 = random.randrange(
        int(SIZE)/Mi + snap_data_size_in_mb,
        int(EXPAND_SIZE)/Mi - snap_data_size_in_mb, 1)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap3_offset2, snap_data_size_in_mb)
    cksum_before = get_device_checksum(volume_endpoint)

    # Pruning snap2 as well as coalescing snap1 with snap2
    volume.snapshotDelete(name=snap2_before.name)
    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap2_before.name)
    snap2_after = volume.snapshotGet(name=snap2_before.name)
    snap2_after_size = int(snap2_after.size)
    cksum_after = get_device_checksum(volume_endpoint)
    assert snap2_after.removed
    assert snap2_before_size &gt;= snap2_after_size - snap1_after_size - \
           fiemap_max_size + \
           (snap2_offset + snap_data_size_in_mb - snap3_offset1) * Mi

    assert cksum_before == cksum_after

    snap3_before = create_snapshot(client, volume_name)
    snap3_before_size = int(snap3_before.size)

    # Prepare and write snap4_data which has no overlapping part with snap3.
    snap4_offset = random.randrange(snap3_offset1 + snap_data_size_in_mb,
                                    int(SIZE)/Mi + snap_data_size_in_mb, 1)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap4_offset, snap_data_size_in_mb)
    cksum_before = get_device_checksum(volume_endpoint)
    # Pruning snap3 then coalescing snap2 with snap3
    volume.snapshotDelete(name=snap3_before.name)
    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap3_before.name)
    snap3_after = volume.snapshotGet(name=snap3_before.name)
    snap3_after_size = int(snap3_after.size)
    cksum_after = get_device_checksum(volume_endpoint)
    assert snap3_after.removed
    assert snap3_before_size &gt;= snap3_after_size - snap2_after_size - \
           fiemap_max_size
    assert cksum_before == cksum_after

    snap4 = create_snapshot(client, volume_name)

    # We don&#39;t care the exact content of snap5
    snap5_offset = random.randrange(
        0, int(EXPAND_SIZE)/Mi - snap_data_size_in_mb, 1)
    write_volume_dev_random_mb_data(volume_endpoint,
                                    snap5_offset, snap_data_size_in_mb)
    create_snapshot(client, volume_name)

    # Prepare to do revert
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    assert volume.disableFrontend is True
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV
    check_volume_endpoint(volume)
    # Reverting to a removed snapshot should fail
    with pytest.raises(Exception):
        volume.snapshotRevert(name=snap3_after.name)
    # Reverting to snap4 should succeed
    volume.snapshotRevert(name=snap4.name)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    cksum_revert = get_device_checksum(volume_endpoint)
    assert cksum_before == cksum_revert

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.snapshot_test"><code class="name flex">
<span>def <span class="ident">snapshot_test</span></span>(<span>client, volume_name, backing_image)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def snapshot_test(client, volume_name, backing_image):  # NOQA
    volume = create_and_check_volume(client, volume_name,
                                     backing_image=backing_image)

    lht_hostId = get_self_host_id()
    volume = volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    positions = {}

    snap1 = create_snapshot(client, volume_name)

    snap2_data = write_volume_random_data(volume, positions)
    snap2 = create_snapshot(client, volume_name)

    snap3_data = write_volume_random_data(volume, positions)
    snap3 = create_snapshot(client, volume_name)

    snapshots = volume.snapshotList()
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert snapMap[snap3.name].removed is False

    volume.snapshotDelete(name=snap3.name)
    check_volume_data(volume, snap3_data)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert len(snapMap[snap3.name].children) == 1
    assert &#34;volume-head&#34; in snapMap[snap3.name].children.keys()
    assert snapMap[snap3.name].removed is True

    snap = volume.snapshotGet(name=snap3.name)
    assert snap.name == snap3.name
    assert snap.parent == snap3.parent
    assert len(snap3.children) == 1
    assert len(snap.children) == 1
    assert &#34;volume-head&#34; in snap3.children.keys()
    assert &#34;volume-head&#34; in snap.children.keys()
    assert snap.removed is True

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    common.wait_for_volume_healthy_no_frontend(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is True
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV
    check_volume_endpoint(volume)

    volume.snapshotRevert(name=snap2.name)

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    check_volume_data(volume, snap2_data)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap

    assert snapMap[snap1.name].name == snap1.name
    assert snapMap[snap1.name].removed is False
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == snap1.name
    assert &#34;volume-head&#34; in snapMap[snap2.name].children.keys()
    assert snap3.name in snapMap[snap2.name].children.keys()
    assert snapMap[snap2.name].removed is False
    assert snapMap[snap3.name].name == snap3.name
    assert snapMap[snap3.name].parent == snap2.name
    assert len(snapMap[snap3.name].children) == 0
    assert snapMap[snap3.name].removed is True

    volume.snapshotDelete(name=snap1.name)
    volume.snapshotDelete(name=snap2.name)

    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name, snap1.name,
                                     snap3.name)

    snapshots = volume.snapshotList(volume=volume_name)
    snapMap = {}
    for snap in snapshots:
        snapMap[snap.name] = snap
    assert snap1.name not in snapMap
    assert snap3.name not in snapMap

    # it&#39;s the parent of volume-head, so it cannot be purged at this time
    assert snapMap[snap2.name].name == snap2.name
    assert snapMap[snap2.name].parent == &#34;&#34;
    assert &#34;volume-head&#34; in snapMap[snap2.name].children.keys()
    assert snapMap[snap2.name].removed is True
    check_volume_data(volume, snap2_data)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_allow_volume_creation_with_degraded_availability"><code class="name flex">
<span>def <span class="ident">test_allow_volume_creation_with_degraded_availability</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test Allow Volume Creation with Degraded Availability (API)</p>
<p>Requirement:
1. Set <code>allow-volume-creation-with-degraded-availability</code> to true.
2. <code>node-level-soft-anti-affinity</code> to false.</p>
<p>Steps:
(degraded availablity)
1. Disable scheduling for node 2 and 3.
2. Create a volume with three replicas.
1. Volume should be <code>ready</code> after creation and <code>Scheduled</code> is true.
2. One replica schedule succeed. Two other replicas failed scheduling.
3. Enable the scheduling of node 2.
1. One additional replica of the volume will become scheduled.
2. The other replica is still failed to schedule.
3. Scheduled condition is still true.
4. Attach the volume.
1. After the volume is attached, scheduled condition become false.
5. Write data to the volume.
6. Detach the volume.
1. Scheduled condition should become true.
7. Reattach the volume to verify the data.
1. Scheduled condition should become false.
8. Enable the scheduling for the node 3.
9. Wait for the scheduling condition to become true.
10. Detach and reattach the volume to verify the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_allow_volume_creation_with_degraded_availability(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test Allow Volume Creation with Degraded Availability (API)

    Requirement:
    1. Set `allow-volume-creation-with-degraded-availability` to true.
    2. `node-level-soft-anti-affinity` to false.

    Steps:
    (degraded availablity)
    1. Disable scheduling for node 2 and 3.
    2. Create a volume with three replicas.
        1. Volume should be `ready` after creation and `Scheduled` is true.
        2. One replica schedule succeed. Two other replicas failed scheduling.
    3. Enable the scheduling of node 2.
        1. One additional replica of the volume will become scheduled.
        2. The other replica is still failed to schedule.
        3. Scheduled condition is still true.
    4. Attach the volume.
        1. After the volume is attached, scheduled condition become false.
    5. Write data to the volume.
    6. Detach the volume.
        1. Scheduled condition should become true.
    7. Reattach the volume to verify the data.
        1. Scheduled condition should become false.
    8. Enable the scheduling for the node 3.
    9. Wait for the scheduling condition to become true.
    10. Detach and reattach the volume to verify the data.
    &#34;&#34;&#34;
    # enable volume create with degraded availability
    degraded_availability_setting = \
        client.by_id_setting(common.SETTING_DEGRADED_AVAILABILITY)
    client.update(degraded_availability_setting, value=&#34;true&#34;)

    # disable node level soft anti-affinity
    replica_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_soft_anti_affinity_setting, value=&#34;false&#34;)

    nodes = client.list_node()
    node1 = nodes[0]
    node2 = nodes[1]
    node3 = nodes[2]

    # disable node 2 and 3 to schedule to node 1
    client.update(node2, allowScheduling=False)
    client.update(node3, allowScheduling=False)

    # create volume
    volume = create_and_check_volume(client, volume_name, num_of_replicas=3)
    assert volume.ready
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # check only 1 replica scheduled successfully
    common.wait_for_replica_scheduled(client, volume_name,
                                      to_nodes=[node1.name],
                                      expect_success=1, expect_fail=2,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)

    # enable node 2 to schedule to node 1 and 2
    client.update(node2, allowScheduling=True)

    # check 2 replicas scheduled successfully
    common.wait_for_replica_scheduled(client, volume_name,
                                      to_nodes=[node1.name, node2.name],
                                      expect_success=2, expect_fail=1,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)

    volume = client.by_id_volume(volume_name)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # attach volume
    self_host = get_self_host_id()
    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_degraded(client, volume_name)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;False&#34;

    data = write_volume_random_data(volume, {})

    # detach volume
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # re-attach volume to verify the data
    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_degraded(client, volume_name)
    check_volume_data(volume, data)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;False&#34;

    # enable node 3 to schedule to node 1, 2 and 3
    client.update(node3, allowScheduling=True)
    common.wait_for_volume_condition_scheduled(client, volume_name,
                                               &#34;status&#34;, &#34;True&#34;)

    # detach and re-attach the volume to verify the data
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, data)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_allow_volume_creation_with_degraded_availability_dr"><code class="name flex">
<span>def <span class="ident">test_allow_volume_creation_with_degraded_availability_dr</span></span>(<span>set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod, pod_make)</span>
</code></dt>
<dd>
<div class="desc"><p>Test Allow Volume Creation with Degraded Availability (Restore)</p>
<p>Requirement:
1. Set <code>allow-volume-creation-with-degraded-availability</code> to true.
2. <code>node-level-soft-anti-affinity</code> to false.
3. Create a backup of 800MB.</p>
<p>Steps:
(DR volume)
1. Disable scheduling for node 2 and 3.
2. Create a DR volume from backup with 3 replicas.
1. The scheduled condition is false.
2. Only node 1 replica become scheduled.
3. Enable scheduling for node 2 and 3.
1. Replicas scheduling to node 1, 2, 3 success.
2. Wait for restore progress to complete.
3. The scheduled condition becomes true.
4. Activate, attach the volume, and verify the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_allow_volume_creation_with_degraded_availability_dr(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test Allow Volume Creation with Degraded Availability (Restore)

    Requirement:
    1. Set `allow-volume-creation-with-degraded-availability` to true.
    2. `node-level-soft-anti-affinity` to false.
    3. Create a backup of 800MB.

    Steps:
    (DR volume)
    1. Disable scheduling for node 2 and 3.
    2. Create a DR volume from backup with 3 replicas.
        1. The scheduled condition is false.
        2. Only node 1 replica become scheduled.
    3. Enable scheduling for node 2 and 3.
        1. Replicas scheduling to node 1, 2, 3 success.
        2. Wait for restore progress to complete.
        3. The scheduled condition becomes true.
    4. Activate, attach the volume, and verify the data.
    &#34;&#34;&#34;
    # enable volume create with degraded availability
    degraded_availability_setting = \
        client.by_id_setting(common.SETTING_DEGRADED_AVAILABILITY)
    client.update(degraded_availability_setting, value=&#34;true&#34;)

    # disable node level soft anti-affinity
    replica_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_soft_anti_affinity_setting, value=&#34;false&#34;)

    # create a backup
    backupstore_cleanup(client)

    data_path = &#34;/data/test&#34;
    src_vol_name = generate_volume_name()
    _, _, _, src_md5sum = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, src_vol_name,
            data_path=data_path, data_size_in_mb=common.DATA_SIZE_IN_MB_4)

    src_vol = client.by_id_volume(src_vol_name)
    src_snap = create_snapshot(client, src_vol_name)
    src_vol.snapshotBackup(name=src_snap.name)
    wait_for_backup_completion(client, src_vol_name, src_snap.name,
                               retry_count=600)
    _, backup = find_backup(client, src_vol_name, src_snap.name)

    nodes = client.list_node()
    node1 = nodes[0]
    node2 = nodes[1]
    node3 = nodes[2]

    # disable node 2 and 3 to schedule to node 1
    client.update(node2, allowScheduling=False)
    client.update(node3, allowScheduling=False)

    # create DR volume
    dst_vol_name = generate_volume_name()
    dst_vol = client.create_volume(name=dst_vol_name, size=str(1*Gi),
                                   numberOfReplicas=3,
                                   fromBackup=backup.url,
                                   frontend=&#34;&#34;,
                                   standby=True)
    common.wait_for_volume_replica_count(client, dst_vol_name, 3)
    wait_for_volume_restoration_start(client, dst_vol_name, backup.name)
    wait_for_volume_condition_scheduled(client, dst_vol_name,
                                        &#34;status&#34;, &#34;False&#34;)

    # check only 1 replica scheduled successfully
    common.wait_for_replica_scheduled(client, dst_vol_name,
                                      to_nodes=[node1.name],
                                      expect_success=1,
                                      expect_fail=2,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)

    # Enable node 2, 3 to schedule to node 1,2,3
    client.update(node2, allowScheduling=True)
    client.update(node3, allowScheduling=True)

    common.wait_for_replica_scheduled(client, dst_vol_name,
                                      to_nodes=[node1.name,
                                                node2.name,
                                                node3.name],
                                      expect_success=3,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)
    common.monitor_restore_progress(client, dst_vol_name)

    wait_for_volume_condition_scheduled(client, dst_vol_name,
                                        &#34;status&#34;, &#34;True&#34;)

    # activate the volume
    activate_standby_volume(client, dst_vol_name)

    # attach the volume
    dst_vol = client.by_id_volume(dst_vol_name)
    create_pv_for_volume(client, core_api, dst_vol, dst_vol_name)
    create_pvc_for_volume(client, core_api, dst_vol, dst_vol_name)

    dst_pod_name = dst_vol_name + &#34;-pod&#34;
    pod[&#39;metadata&#39;][&#39;name&#39;] = dst_vol_name + &#34;-pod&#34;
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: dst_vol_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    # verify the data
    dst_md5sum = get_pod_data_md5sum(core_api, dst_pod_name, data_path)
    assert src_md5sum == dst_md5sum</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_allow_volume_creation_with_degraded_availability_error"><code class="name flex">
<span>def <span class="ident">test_allow_volume_creation_with_degraded_availability_error</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test Allow Volume Creation with Degraded Availability (API)</p>
<p>Requirement:
1. Set <code>allow-volume-creation-with-degraded-availability</code> to true.
2. <code>node-level-soft-anti-affinity</code> to false.</p>
<p>Steps:
(no availability)
1. Disable all nodes' scheduling.
2. Create a volume with three replicas.
1. Volume should be NotReady after creation.
2. Scheduled condition should become false.
3. Attaching the volume should result in error.
4. Enable one node's scheduling.
1. Volume should become Ready soon.
2. Scheduled condition should become true.
5. Attach the volume. Write data. Detach and reattach to verify the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_allow_volume_creation_with_degraded_availability_error(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test Allow Volume Creation with Degraded Availability (API)

    Requirement:
    1. Set `allow-volume-creation-with-degraded-availability` to true.
    2. `node-level-soft-anti-affinity` to false.

    Steps:
    (no availability)
    1. Disable all nodes&#39; scheduling.
    2. Create a volume with three replicas.
        1. Volume should be NotReady after creation.
        2. Scheduled condition should become false.
    3. Attaching the volume should result in error.
    4. Enable one node&#39;s scheduling.
        1. Volume should become Ready soon.
        2. Scheduled condition should become true.
    5. Attach the volume. Write data. Detach and reattach to verify the data.
    &#34;&#34;&#34;
    # enable volume create with degraded availability
    degraded_availability_setting = \
        client.by_id_setting(common.SETTING_DEGRADED_AVAILABILITY)
    client.update(degraded_availability_setting, value=&#34;true&#34;)

    # disable node level soft anti-affinity
    replica_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_soft_anti_affinity_setting, value=&#34;false&#34;)

    nodes = client.list_node()
    node1 = nodes[0]
    node2 = nodes[1]
    node3 = nodes[2]

    # disable node 1, 2 and 3 to make 0 available node
    client.update(node1, allowScheduling=False)
    client.update(node2, allowScheduling=False)
    client.update(node3, allowScheduling=False)

    # create volume
    volume = create_and_check_volume(client, volume_name, num_of_replicas=3)
    assert not volume.ready
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;False&#34;

    # attach the volume
    self_host = get_self_host_id()
    with pytest.raises(Exception) as e:
        volume.attach(hostId=self_host)
    assert &#34;unable to attach volume&#34; in str(e.value)

    # enable node 1
    client.update(node1, allowScheduling=True)

    # check only 1 replica scheduled successfully
    common.wait_for_replica_scheduled(client, volume_name,
                                      to_nodes=[node1.name],
                                      expect_success=1, expect_fail=2,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)
    volume = common.wait_for_volume_status(client, volume_name,
                                           VOLUME_FIELD_READY, True)
    assert volume.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # attach the volume and write some data
    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_degraded(client, volume_name)
    data = write_volume_random_data(volume, {})

    # detach and re-attach the volume to verify the data
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=self_host)
    volume = common.wait_for_volume_degraded(client, volume_name)
    check_volume_data(volume, data)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_allow_volume_creation_with_degraded_availability_restore"><code class="name flex">
<span>def <span class="ident">test_allow_volume_creation_with_degraded_availability_restore</span></span>(<span>set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod, pod_make)</span>
</code></dt>
<dd>
<div class="desc"><p>Test Allow Volume Creation with Degraded Availability (Restore)</p>
<p>Requirement:
1. Set <code>allow-volume-creation-with-degraded-availability</code> to true.
2. <code>node-level-soft-anti-affinity</code> to false.
3. <code>replica-replenishment-wait-interval</code> to 0.
4. Create a backup of 800MB.</p>
<p>Steps:
(restore)
1. Disable scheduling for node 2 and 3.
2. Restore a volume with 3 replicas.
1. The scheduled condition is true.
2. Only node 1 replica become scheduled.
3. Enable scheduling for node 2.
4. Wait for the restore to complete and volume detach automatically.
Then check the scheduled condition still true.
5. Attach and wait for the volume.
1. Replicas scheduling to node 1 and 2 success.
Replica scheduling to node 3 fail.
2. The scheduled condition becomes false.
3. Verify the data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_allow_volume_creation_with_degraded_availability_restore(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test Allow Volume Creation with Degraded Availability (Restore)

    Requirement:
    1. Set `allow-volume-creation-with-degraded-availability` to true.
    2. `node-level-soft-anti-affinity` to false.
    3. `replica-replenishment-wait-interval` to 0.
    4. Create a backup of 800MB.

    Steps:
    (restore)
    1. Disable scheduling for node 2 and 3.
    2. Restore a volume with 3 replicas.
        1. The scheduled condition is true.
        2. Only node 1 replica become scheduled.
    3. Enable scheduling for node 2.
    4. Wait for the restore to complete and volume detach automatically.
       Then check the scheduled condition still true.
    5. Attach and wait for the volume.
        1. Replicas scheduling to node 1 and 2 success.
           Replica scheduling to node 3 fail.
        2. The scheduled condition becomes false.
        3. Verify the data.
    &#34;&#34;&#34;
    # enable volume create with degraded availability
    degraded_availability_setting = \
        client.by_id_setting(common.SETTING_DEGRADED_AVAILABILITY)
    client.update(degraded_availability_setting, value=&#34;true&#34;)

    # disable node level soft anti-affinity
    replica_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_soft_anti_affinity_setting, value=&#34;false&#34;)

    replenish_wait_setting = \
        client.by_id_setting(SETTING_REPLICA_REPLENISHMENT_WAIT_INTERVAL)
    client.update(replenish_wait_setting, value=&#34;0&#34;)

    # create a backup
    backupstore_cleanup(client)

    data_path = &#34;/data/test&#34;
    src_vol_name = generate_volume_name()
    _, _, _, src_md5sum = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, src_vol_name,
            data_path=data_path, data_size_in_mb=common.DATA_SIZE_IN_MB_4)

    src_vol = client.by_id_volume(src_vol_name)
    src_snap = create_snapshot(client, src_vol_name)
    src_vol.snapshotBackup(name=src_snap.name)
    wait_for_backup_completion(client, src_vol_name, src_snap.name,
                               retry_count=600)
    _, backup = find_backup(client, src_vol_name, src_snap.name)

    nodes = client.list_node()
    node1 = nodes[0]
    node2 = nodes[1]
    node3 = nodes[2]

    # disable node 2 and 3 to schedule to node 1
    client.update(node2, allowScheduling=False)
    client.update(node3, allowScheduling=False)

    # restore volume
    dst_vol_name = generate_volume_name()
    client.create_volume(name=dst_vol_name, size=str(1*Gi),
                         numberOfReplicas=3, fromBackup=backup.url)
    common.wait_for_volume_replica_count(client, dst_vol_name, 3)
    common.wait_for_volume_restoration_start(client, dst_vol_name, backup.name)
    wait_for_volume_condition_scheduled(client, dst_vol_name,
                                        &#34;status&#34;, &#34;True&#34;)

    # check only 1 replica scheduled successfully
    common.wait_for_replica_scheduled(client, dst_vol_name,
                                      to_nodes=[node1.name],
                                      expect_success=1,
                                      expect_fail=2,
                                      chk_vol_healthy=False,
                                      chk_replica_running=False)

    # Enable node 2 to schedule to node 1, 2
    client.update(node2, allowScheduling=True)

    # wait to complete restore
    common.wait_for_volume_restoration_completed(client, dst_vol_name)
    dst_vol = common.wait_for_volume_detached(client, dst_vol_name)
    assert dst_vol.conditions[VOLUME_CONDITION_SCHEDULED][&#39;status&#39;] == &#34;True&#34;

    # attach the volume
    create_pv_for_volume(client, core_api, dst_vol, dst_vol_name)
    create_pvc_for_volume(client, core_api, dst_vol, dst_vol_name)

    dst_pod_name = dst_vol_name + &#34;-pod&#34;
    pod[&#39;metadata&#39;][&#39;name&#39;] = dst_vol_name + &#34;-pod&#34;
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: dst_vol_name,
        },
    }]
    create_and_wait_pod(core_api, pod)
    # check 2 replica scheduled successfully
    dst_vol = common.wait_for_replica_scheduled(client, dst_vol_name,
                                                to_nodes=[node1.name,
                                                          node2.name],
                                                expect_success=2,
                                                expect_fail=1,
                                                chk_vol_healthy=False,
                                                chk_replica_running=False)
    wait_for_volume_condition_scheduled(client, dst_vol_name,
                                        &#34;status&#34;, &#34;False&#34;)

    # verify the data
    dst_md5sum = get_pod_data_md5sum(core_api, dst_pod_name, data_path)
    assert src_md5sum == dst_md5sum</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_attach_without_frontend"><code class="name flex">
<span>def <span class="ident">test_attach_without_frontend</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test attach in maintenance mode (without frontend)</p>
<ol>
<li>Create a volume and attach to the current node with enabled frontend</li>
<li>Check volume has <code>blockdev</code></li>
<li>Write <code>snap1_data</code> into volume and create snapshot <code>snap1</code></li>
<li>Write more random data into volume and create another anspshot</li>
<li>Detach the volume and reattach with disabled frontend</li>
<li>Check volume still has <code>blockdev</code> as frontend but no endpoint</li>
<li>Revert back to <code>snap1</code></li>
<li>Detach and reattach the volume with enabled frontend</li>
<li>Check volume contains data <code>snap1_data</code></li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_attach_without_frontend(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test attach in maintenance mode (without frontend)

    1. Create a volume and attach to the current node with enabled frontend
    2. Check volume has `blockdev`
    3. Write `snap1_data` into volume and create snapshot `snap1`
    4. Write more random data into volume and create another anspshot
    5. Detach the volume and reattach with disabled frontend
    6. Check volume still has `blockdev` as frontend but no endpoint
    7. Revert back to `snap1`
    8. Detach and reattach the volume with enabled frontend
    9. Check volume contains data `snap1_data`
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    snap1_data = write_volume_random_data(volume)
    snap1 = create_snapshot(client, volume_name)

    write_volume_random_data(volume)
    create_snapshot(client, volume_name)

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    common.wait_for_volume_healthy_no_frontend(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is True
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV
    check_volume_endpoint(volume)

    volume.snapshotRevert(name=snap1.name)

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    check_volume_data(volume, snap1_data)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_aws_iam_role_arn"><code class="name flex">
<span>def <span class="ident">test_aws_iam_role_arn</span></span>(<span>client, core_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Test AWS IAM Role ARN</p>
<ol>
<li>Set backup target to S3</li>
<li>Check longhorn manager and replica instance manager Pods
without 'iam.amazonaws.com/role' annotation</li>
<li>Add AWS_IAM_ROLE_ARN to secret</li>
<li>Check longhorn manager and replica instance manager Pods
with 'iam.amazonaws.com/role' annotation
and matches to AWS_IAM_ROLE_ARN in secret</li>
<li>Update AWS_IAM_ROLE_ARN from secret</li>
<li>Check longhorn manager and replica instance manager Pods
with 'iam.amazonaws.com/role' annotation
and matches to AWS_IAM_ROLE_ARN in secret</li>
<li>Remove AWS_IAM_ROLE_ARN from secret</li>
<li>Check longhorn manager and replica instance manager Pods
without 'iam.amazonaws.com/role' annotation</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skipif(&#39;s3&#39; not in BACKUPSTORE, reason=&#39;This test is only applicable for s3&#39;)  # NOQA
def test_aws_iam_role_arn(client, core_api):  # NOQA
    &#34;&#34;&#34;
    Test AWS IAM Role ARN

    1. Set backup target to S3
    2. Check longhorn manager and replica instance manager Pods
       without &#39;iam.amazonaws.com/role&#39; annotation
    3. Add AWS_IAM_ROLE_ARN to secret
    4. Check longhorn manager and replica instance manager Pods
       with &#39;iam.amazonaws.com/role&#39; annotation
       and matches to AWS_IAM_ROLE_ARN in secret
    5. Update AWS_IAM_ROLE_ARN from secret
    6. Check longhorn manager and replica instance manager Pods
       with &#39;iam.amazonaws.com/role&#39; annotation
       and matches to AWS_IAM_ROLE_ARN in secret
    7. Remove AWS_IAM_ROLE_ARN from secret
    8. Check longhorn manager and replica instance manager Pods
       without &#39;iam.amazonaws.com/role&#39; annotation
    &#34;&#34;&#34;
    set_backupstore_s3(client)

    lh_label = &#39;app=longhorn-manager&#39;
    replica_im_label = &#39;longhorn.io/instance-manager-type=replica&#39;
    anno_key = &#39;iam.amazonaws.com/role&#39;
    secret_name = backupstore_get_secret(client)

    common.wait_for_pod_annotation(
        core_api, lh_label, anno_key, None)
    common.wait_for_pod_annotation(
        core_api, replica_im_label, anno_key, None)

    # Add secret key AWS_IAM_ROLE_ARN value test-aws-iam-role-arn
    secret = core_api.read_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;)
    secret.data[&#39;AWS_IAM_ROLE_ARN&#39;] = &#39;dGVzdC1hd3MtaWFtLXJvbGUtYXJu&#39;
    core_api.patch_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;, body=secret)

    common.wait_for_pod_annotation(
        core_api, lh_label, anno_key, &#39;test-aws-iam-role-arn&#39;)
    common.wait_for_pod_annotation(
        core_api, replica_im_label, anno_key, &#39;test-aws-iam-role-arn&#39;)

    # Update secret key AWS_IAM_ROLE_ARN value test-aws-iam-role-arn-2
    secret = core_api.read_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;)
    secret.data[&#39;AWS_IAM_ROLE_ARN&#39;] = &#39;dGVzdC1hd3MtaWFtLXJvbGUtYXJuLTI=&#39;
    core_api.patch_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;, body=secret)

    common.wait_for_pod_annotation(
        core_api, lh_label, anno_key, &#39;test-aws-iam-role-arn-2&#39;)
    common.wait_for_pod_annotation(
        core_api, replica_im_label, anno_key, &#39;test-aws-iam-role-arn-2&#39;)

    # Remove secret key AWS_IAM_ROLE_ARN
    body = [{&#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/data/AWS_IAM_ROLE_ARN&#34;}]
    core_api.patch_namespaced_secret(
        name=secret_name, namespace=&#39;longhorn-system&#39;, body=body)

    common.wait_for_pod_annotation(
        core_api, lh_label, anno_key, None)
    common.wait_for_pod_annotation(
        core_api, replica_im_label, anno_key, None)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup"><code class="name flex">
<span>def <span class="ident">test_backup</span></span>(<span>set_random_backupstore, client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test basic backup</p>
<p>Setup:</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Run the test for all the available backupstores.</li>
</ol>
<p>Steps:</p>
<ol>
<li>Create a backup of volume</li>
<li>Restore the backup to a new volume</li>
<li>Attach the new volume and make sure the data is the same as the old one</li>
<li>Detach the volume and delete the backup.</li>
<li>Wait for the restored volume's <code>lastBackup</code> to be cleaned (due to remove
the backup)</li>
<li>Delete the volume</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_backup(set_random_backupstore, client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic backup

    Setup:

    1. Create a volume and attach to the current node
    2. Run the test for all the available backupstores.

    Steps:

    1. Create a backup of volume
    2. Restore the backup to a new volume
    3. Attach the new volume and make sure the data is the same as the old one
    4. Detach the volume and delete the backup.
    5. Wait for the restored volume&#39;s `lastBackup` to be cleaned (due to remove
    the backup)
    6. Delete the volume
    &#34;&#34;&#34;
    backup_test(client, volume_name, SIZE)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_block_deletion"><code class="name flex">
<span>def <span class="ident">test_backup_block_deletion</span></span>(<span>set_random_backupstore, client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup block deletion</p>
<p>Context:</p>
<p>We want to make sure that we only delete non referenced backup blocks,
we also don't want to delete blocks while there other backups in progress.
The reason for this is that we don't yet know which blocks are required by
the in progress backup, so blocks deletion could lead to a faulty backup.</p>
<p>Setup:</p>
<ol>
<li>Setup minio as S3 backupstore</li>
</ol>
<p>Steps:</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Write 4 MB to the beginning of the volume (2 x 2MB backup blocks)</li>
<li>Create backup(1) of the volume</li>
<li>Overwrite the first of the backup blocks of data on the volume</li>
<li>Create backup(2) of the volume</li>
<li>Overwrite the first of the backup blocks of data on the volume</li>
<li>Create backup(3) of the volume</li>
<li>Verify backup block count == 4
assert volume["DataStored"] == str(BLOCK_SIZE * expected_count)
assert count of *.blk files for that volume == expected_count</li>
<li>Create an artificial in progress backup.cfg file
json.dumps({"Name": name, "VolumeName": volume, "CreatedTime": ""})</li>
<li>Delete backup(2)</li>
<li>Verify backup block count == 4 (because of the in progress backup)</li>
<li>Delete the artificial in progress backup.cfg file</li>
<li>Delete backup(1)</li>
<li>Verify backup block count == 2</li>
<li>Delete backup(3)</li>
<li>Verify backup block count == 0</li>
<li>Delete the backup volume</li>
<li>Cleanup the volume</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_backup_block_deletion(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test backup block deletion

    Context:

    We want to make sure that we only delete non referenced backup blocks,
    we also don&#39;t want to delete blocks while there other backups in progress.
    The reason for this is that we don&#39;t yet know which blocks are required by
    the in progress backup, so blocks deletion could lead to a faulty backup.

    Setup:

    1. Setup minio as S3 backupstore

    Steps:

    1.  Create a volume and attach to the current node
    2.  Write 4 MB to the beginning of the volume (2 x 2MB backup blocks)
    3.  Create backup(1) of the volume
    4.  Overwrite the first of the backup blocks of data on the volume
    5.  Create backup(2) of the volume
    6.  Overwrite the first of the backup blocks of data on the volume
    7.  Create backup(3) of the volume
    8.  Verify backup block count == 4
        assert volume[&#34;DataStored&#34;] == str(BLOCK_SIZE * expected_count)
        assert count of *.blk files for that volume == expected_count
    9.  Create an artificial in progress backup.cfg file
        json.dumps({&#34;Name&#34;: name, &#34;VolumeName&#34;: volume, &#34;CreatedTime&#34;: &#34;&#34;})
    10. Delete backup(2)
    11. Verify backup block count == 4 (because of the in progress backup)
    12. Delete the artificial in progress backup.cfg file
    13. Delete backup(1)
    14. Verify backup block count == 2
    15. Delete backup(3)
    16. Verify backup block count == 0
    17. Delete the backup volume
    18. Cleanup the volume
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    volume = create_and_check_volume(client, volume_name)
    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    data0 = {&#39;pos&#39;: 0,
             &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}

    bv0, backup0, _, _ = create_backup(client, volume_name, data0)

    data1 = {&#39;pos&#39;: 0,
             &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}

    bv1, backup1, _, _ = create_backup(client, volume_name, data1)

    data2 = {&#39;pos&#39;: 0,
             &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}

    bv2, backup2, _, _ = create_backup(client, volume_name, data2)

    backup_blocks_count = backupstore_count_backup_block_files(client,
                                                               core_api,
                                                               volume_name)
    assert backup_blocks_count == 4

    bvs = client.list_backupVolume()

    for bv in bvs:
        if bv[&#39;name&#39;] == volume_name:
            assert bv[&#39;dataStored&#39;] == \
                str(backup_blocks_count * BACKUP_BLOCK_SIZE)

    backupstore_create_dummy_in_progress_backup(client, core_api, volume_name)
    delete_backup(client, volume_name, backup1.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 4

    backupstore_delete_dummy_in_progress_backup(client, core_api, volume_name)

    delete_backup(client, volume_name, backup0.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 2

    delete_backup(client, volume_name, backup2.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 0

    delete_backup_volume(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_failed_disable_auto_cleanup"><code class="name flex">
<span>def <span class="ident">test_backup_failed_disable_auto_cleanup</span></span>(<span>set_random_backupstore, client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the failed backup would be automatically deleted.</p>
<ol>
<li>Set the default setting <code>backupstore-poll-interval</code> to 60 (seconds)</li>
<li>Set the default setting <code>failed-backup-ttl</code> to 0</li>
<li>Create a volume and attach to the current node</li>
<li>Create a empty backup for creating the backup volume</li>
<li>Write some data to the volume</li>
<li>Create a backup of the volume</li>
<li>Crash all replicas</li>
<li>Wait and check if the backup failed</li>
<li>Wait and check if the backup was not deleted.</li>
<li>Cleanup</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_backup_failed_disable_auto_cleanup(set_random_backupstore,  # NOQA
                                        client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test the failed backup would be automatically deleted.

    1.    Set the default setting `backupstore-poll-interval` to 60 (seconds)
    2.    Set the default setting `failed-backup-ttl` to 0
    3.    Create a volume and attach to the current node
    4.    Create a empty backup for creating the backup volume
    5.    Write some data to the volume
    6.    Create a backup of the volume
    7.    Crash all replicas
    8.    Wait and check if the backup failed
    9.    Wait and check if the backup was not deleted.
    10.   Cleanup
    &#34;&#34;&#34;
    backup_name = backup_failed_cleanup(client, core_api, volume_name, 256*Mi,
                                        failed_backup_ttl=&#34;0&#34;)

    # wait for 5 minutes to check if the failed backup exists
    try:
        wait_for_backup_delete(client, volume_name, backup_name)
        assert False, &#34;backup &#34; + backup_name + &#34; for volume &#34; \
                      + volume_name + &#34; is deleted&#34;
    except AssertionError:
        pass</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_failed_enable_auto_cleanup"><code class="name flex">
<span>def <span class="ident">test_backup_failed_enable_auto_cleanup</span></span>(<span>set_random_backupstore, client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the failed backup would be automatically deleted.</p>
<ol>
<li>Set the default setting <code>backupstore-poll-interval</code> to 60 (seconds)</li>
<li>Set the default setting <code>failed-backup-ttl</code> to 3 (minutes)</li>
<li>Create a volume and attach to the current node</li>
<li>Create a empty backup for creating the backup volume</li>
<li>Write some data to the volume</li>
<li>Create a backup of the volume</li>
<li>Crash all replicas</li>
<li>Wait and check if the backup failed</li>
<li>Wait and check if the backup was deleted automatically</li>
<li>Cleanup</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_backup_failed_enable_auto_cleanup(set_random_backupstore,  # NOQA
                                        client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test the failed backup would be automatically deleted.

    1.   Set the default setting `backupstore-poll-interval` to 60 (seconds)
    2.   Set the default setting `failed-backup-ttl` to 3 (minutes)
    3.   Create a volume and attach to the current node
    4.   Create a empty backup for creating the backup volume
    5.   Write some data to the volume
    6.   Create a backup of the volume
    7.   Crash all replicas
    8.   Wait and check if the backup failed
    9.   Wait and check if the backup was deleted automatically
    10.  Cleanup
    &#34;&#34;&#34;
    backup_name = backup_failed_cleanup(client, core_api, volume_name, 256*Mi)

    # wait in 5 minutes for automatic failed backup cleanup
    wait_for_backup_delete(client, volume_name, backup_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_labels"><code class="name flex">
<span>def <span class="ident">test_backup_labels</span></span>(<span>set_random_backupstore, client, random_labels, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test that the proper Labels are applied when creating a Backup manually.</p>
<ol>
<li>Create a volume</li>
<li>Run the following steps on all backupstores</li>
<li>Create a backup with some random labels</li>
<li>Get backup from backupstore, verify the labels are set on the backups</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_backup_labels(set_random_backupstore, client, random_labels, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test that the proper Labels are applied when creating a Backup manually.

    1. Create a volume
    2. Run the following steps on all backupstores
    3. Create a backup with some random labels
    4. Get backup from backupstore, verify the labels are set on the backups
    &#34;&#34;&#34;
    backup_labels_test(client, random_labels, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_lock_creation_during_deletion"><code class="name flex">
<span>def <span class="ident">test_backup_lock_creation_during_deletion</span></span>(<span>set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup locks
Context:
To test the locking mechanism that utilizes the backupstore,
to prevent the following case of concurrent operations.
- prevent backup creation during backup deletion</p>
<p>steps:
1. Create a volume, then create the corresponding PV, PVC and Pod.
2. Wait for the pod running and the volume healthy.
3. Write data (DATA_SIZE_IN_MB_2) to the pod volume and get the md5sum.
4. Take a backup.
5. Wait for the backup to be completed.
6. Delete the backup.
7. Create another backup of the same volume.
8. Wait for the delete backup to be completed.
8. Wait for the backup to be completed.
9. Assert there is 1 backup in the backup store.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_backup_lock_creation_during_deletion(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test backup locks
    Context:
    To test the locking mechanism that utilizes the backupstore,
    to prevent the following case of concurrent operations.
    - prevent backup creation during backup deletion

    steps:
    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data (DATA_SIZE_IN_MB_2) to the pod volume and get the md5sum.
    4. Take a backup.
    5. Wait for the backup to be completed.
    6. Delete the backup.
    7. Create another backup of the same volume.
    8. Wait for the delete backup to be completed.
    8. Wait for the backup to be completed.
    9. Assert there is 1 backup in the backup store.
    &#34;&#34;&#34;
    backupstore_cleanup(client)
    std_volume_name = volume_name + &#34;-std&#34;

    std_pod_name, _, _, std_md5sum1 = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, std_volume_name,
            data_size_in_mb=DATA_SIZE_IN_MB_1)
    std_volume = client.by_id_volume(std_volume_name)
    snap1 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap1.name)
    wait_for_backup_completion(client, std_volume_name, snap1.name)
    _, b1 = common.find_backup(client, std_volume_name, snap1.name)

    write_pod_volume_random_data(core_api, std_pod_name,
                                 &#34;/data/test2&#34;, DATA_SIZE_IN_MB_1)

    backup_volume = client.by_id_backupVolume(std_volume_name)
    backup_volume.backupDelete(name=b1.name)

    snap2 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap2.name)

    wait_for_backup_delete(client, volume_name, b1.name)
    wait_for_backup_completion(client, std_volume_name, snap2.name)

    try:
        _, b1 = common.find_backup(client, std_volume_name, snap1.name)
    except AssertionError:
        b1 = None
    assert b1 is None
    _, b2 = common.find_backup(client, std_volume_name, snap2.name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_lock_deletion_during_backup"><code class="name flex">
<span>def <span class="ident">test_backup_lock_deletion_during_backup</span></span>(<span>set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup locks
Context:
To test the locking mechanism that utilizes the backupstore,
to prevent the following case of concurrent operations.
- prevent backup deletion while a backup is in progress</p>
<p>steps:
1. Create a volume, then create the corresponding PV, PVC and Pod.
2. Wait for the pod running and the volume healthy.
3. Write data to the pod volume and get the md5sum.
4. Take a backup.
5. Wait for the backup to be completed.
6. Write more data into the volume and compute md5sum.
7. Take another backup of the volume.
8. While backup is in progress, delete the older backup up.
9. Wait for the backup creation in progress to be completed.
10. Check the backup store, there should be 1 backup.
11. Restore the latest backup.
12. Wait for the restoration to be completed. Assert md5sum from step 6.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_backup_lock_deletion_during_backup(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test backup locks
    Context:
    To test the locking mechanism that utilizes the backupstore,
    to prevent the following case of concurrent operations.
    - prevent backup deletion while a backup is in progress

    steps:
    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Take a backup.
    5. Wait for the backup to be completed.
    6. Write more data into the volume and compute md5sum.
    7. Take another backup of the volume.
    8. While backup is in progress, delete the older backup up.
    9. Wait for the backup creation in progress to be completed.
    10. Check the backup store, there should be 1 backup.
    11. Restore the latest backup.
    12. Wait for the restoration to be completed. Assert md5sum from step 6.
    &#34;&#34;&#34;
    backupstore_cleanup(client)
    std_volume_name = volume_name + &#34;-std&#34;
    restore_volume_name_1 = volume_name + &#34;-restore-1&#34;

    std_pod_name, _, _, std_md5sum1 = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, std_volume_name)
    std_volume = client.by_id_volume(std_volume_name)
    snap1 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap1.name)
    wait_for_backup_completion(client, std_volume_name, snap1.name)
    _, b1 = common.find_backup(client, std_volume_name, snap1.name)

    write_pod_volume_random_data(core_api, std_pod_name, &#34;/data/test&#34;,
                                 DATA_SIZE_IN_MB_3)

    std_md5sum2 = get_pod_data_md5sum(core_api, std_pod_name, &#34;/data/test&#34;)
    snap2 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap2.name)
    wait_for_backup_to_start(client, std_volume_name, snapshot_name=snap2.name)

    backup_volume = client.by_id_backupVolume(std_volume_name)
    backup_volume.backupDelete(name=b1.name)

    wait_for_backup_completion(client, std_volume_name, snap2.name,
                               retry_count=600)
    wait_for_backup_delete(client, std_volume_name, b1.name)

    _, b2 = common.find_backup(client, std_volume_name, snap2.name)
    assert b2 is not None

    try:
        _, b1 = common.find_backup(client, std_volume_name, snap1.name)
    except AssertionError:
        b1 = None
    assert b1 is None

    client.create_volume(name=restore_volume_name_1, fromBackup=b2.url)

    wait_for_volume_restoration_completed(client, restore_volume_name_1)
    restore_volume_1 = wait_for_volume_detached(client, restore_volume_name_1)
    assert len(restore_volume_1.replicas) == 3

    restore_pod_name_1 = restore_volume_name_1 + &#34;-pod&#34;
    restore_pv_name_1 = restore_volume_name_1 + &#34;-pv&#34;
    restore_pvc_name_1 = restore_volume_name_1 + &#34;-pvc&#34;
    restore_pod_1 = pod_make(name=restore_pod_name_1)
    create_pv_for_volume(client, core_api, restore_volume_1, restore_pv_name_1)
    create_pvc_for_volume(client, core_api, restore_volume_1,
                          restore_pvc_name_1)
    restore_pod_1[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(restore_pvc_name_1)]
    create_and_wait_pod(core_api, restore_pod_1)

    md5sum2 = get_pod_data_md5sum(core_api, restore_pod_name_1, &#34;/data/test&#34;)

    assert std_md5sum2 == md5sum2</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_lock_deletion_during_restoration"><code class="name flex">
<span>def <span class="ident">test_backup_lock_deletion_during_restoration</span></span>(<span>set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup locks
Context:
To test the locking mechanism that utilizes the backupstore,
to prevent the following case of concurrent operations.
- prevent backup deletion during backup restoration</p>
<p>steps:
1. Create a volume, then create the corresponding PV, PVC and Pod.
2. Wait for the pod running and the volume healthy.
3. Write data to the pod volume and get the md5sum.
4. Take a backup.
5. Wait for the backup to be completed.
6. Start backup restoration for the backup creation.
7. Wait for restoration to be in progress.
8. Delete the backup from the backup store.
9. Wait for the restoration to be completed.
10. Assert the data from the restored volume with md5sum.
11. Assert the backup count in the backup store with 0.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_backup_lock_deletion_during_restoration(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test backup locks
    Context:
    To test the locking mechanism that utilizes the backupstore,
    to prevent the following case of concurrent operations.
    - prevent backup deletion during backup restoration

    steps:
    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Take a backup.
    5. Wait for the backup to be completed.
    6. Start backup restoration for the backup creation.
    7. Wait for restoration to be in progress.
    8. Delete the backup from the backup store.
    9. Wait for the restoration to be completed.
    10. Assert the data from the restored volume with md5sum.
    11. Assert the backup count in the backup store with 0.
    &#34;&#34;&#34;
    backupstore_cleanup(client)
    std_volume_name = volume_name + &#34;-std&#34;
    restore_volume_name = volume_name + &#34;-restore&#34;
    _, _, _, std_md5sum = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, std_volume_name,
            data_size_in_mb=DATA_SIZE_IN_MB_2)
    std_volume = client.by_id_volume(std_volume_name)
    snap1 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap1.name)
    wait_for_backup_completion(client, std_volume_name, snap1.name)

    _, b = common.find_backup(client, std_volume_name, snap1.name)
    client.create_volume(name=restore_volume_name, fromBackup=b.url)
    wait_for_volume_restoration_start(client, restore_volume_name, b.name)

    backup_volume = client.by_id_backupVolume(std_volume_name)
    backup_volume.backupDelete(name=b.name)

    wait_for_volume_restoration_completed(client, restore_volume_name)
    wait_for_backup_delete(client, std_volume_name, b.name)
    restore_volume = wait_for_volume_detached(client, restore_volume_name)
    assert len(restore_volume.replicas) == 3

    restore_pod_name = restore_volume_name + &#34;-pod&#34;
    restore_pv_name = restore_volume_name + &#34;-pv&#34;
    restore_pvc_name = restore_volume_name + &#34;-pvc&#34;
    restore_pod = pod_make(name=restore_pod_name)
    create_pv_for_volume(client, core_api, restore_volume, restore_pv_name)
    create_pvc_for_volume(client, core_api, restore_volume, restore_pvc_name)
    restore_pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(restore_pvc_name)]
    create_and_wait_pod(core_api, restore_pod)

    restore_volume = client.by_id_volume(restore_volume_name)
    assert restore_volume[VOLUME_FIELD_ROBUSTNESS] == VOLUME_ROBUSTNESS_HEALTHY

    md5sum = get_pod_data_md5sum(core_api, restore_pod_name, &#34;/data/test&#34;)
    assert std_md5sum == md5sum

    try:
        _, b = common.find_backup(client, std_volume_name, snap1.name)
    except AssertionError:
        b = None
    assert b is None</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_lock_restoration_during_deletion"><code class="name flex">
<span>def <span class="ident">test_backup_lock_restoration_during_deletion</span></span>(<span>set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup locks
Context:
To test the locking mechanism that utilizes the backupstore,
to prevent the following case of concurrent operations.
- prevent backup restoration during backup deletion</p>
<p>steps:
1. Create a volume, then create the corresponding PV, PVC and Pod.
2. Wait for the pod running and the volume healthy.
3. Write data to the pod volume and get the md5sum.
4. Take a backup.
5. Wait for the backup to be completed.
6. Write more data (1.5 Gi) to the volume and take another backup.
7. Wait for the 2nd backup to be completed.
8. Delete the 2nd backup.
9. Without waiting for the backup deletion completion, restore the 1st
backup from the backup store.
10. Verify the restored volume become faulted.
11. Wait for the 2nd backup deletion and assert the count of the backups
with 1 in the backup store.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;This test takes more than 20 mins to run&#34;)  # NOQA
def test_backup_lock_restoration_during_deletion(set_random_backupstore, client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test backup locks
    Context:
    To test the locking mechanism that utilizes the backupstore,
    to prevent the following case of concurrent operations.
    - prevent backup restoration during backup deletion

    steps:
    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Take a backup.
    5. Wait for the backup to be completed.
    6. Write more data (1.5 Gi) to the volume and take another backup.
    7. Wait for the 2nd backup to be completed.
    8. Delete the 2nd backup.
    9. Without waiting for the backup deletion completion, restore the 1st
       backup from the backup store.
    10. Verify the restored volume become faulted.
    11. Wait for the 2nd backup deletion and assert the count of the backups
       with 1 in the backup store.
    &#34;&#34;&#34;
    backupstore_cleanup(client)
    std_volume_name = volume_name + &#34;-std&#34;
    restore_volume_name = volume_name + &#34;-restore&#34;
    std_pod_name, _, _, std_md5sum1 = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, std_volume_name,
            volume_size=str(3*Gi), data_size_in_mb=DATA_SIZE_IN_MB_1)
    std_volume = client.by_id_volume(std_volume_name)
    snap1 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap1.name)
    wait_for_backup_completion(client, std_volume_name, snap1.name)
    std_volume.snapshotBackup(name=snap1.name)
    backup_volume = client.by_id_backupVolume(std_volume_name)
    _, b1 = common.find_backup(client, std_volume_name, snap1.name)

    write_pod_volume_random_data(core_api, std_pod_name,
                                 &#34;/data/test2&#34;, 1500)
    snap2 = create_snapshot(client, std_volume_name)
    std_volume.snapshotBackup(name=snap2.name)
    wait_for_backup_completion(client, std_volume_name, snap2.name,
                               retry_count=1200)
    _, b2 = common.find_backup(client, std_volume_name, snap2.name)

    backup_volume.backupDelete(name=b2.name)

    client.create_volume(name=restore_volume_name, fromBackup=b1.url)
    wait_for_volume_detached(client, restore_volume_name)
    restore_volume = client.by_id_volume(restore_volume_name)
    assert restore_volume[VOLUME_FIELD_ROBUSTNESS] == VOLUME_ROBUSTNESS_FAULTED

    wait_for_backup_delete(client, volume_name, b2.name)

    _, b1 = common.find_backup(client, std_volume_name, snap1.name)
    assert b1 is not None

    try:
        _, b2 = common.find_backup(client, std_volume_name, snap2.name)
    except AssertionError:
        b2 = None
    assert b2 is None</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_metadata_deletion"><code class="name flex">
<span>def <span class="ident">test_backup_metadata_deletion</span></span>(<span>set_random_backupstore, client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup metadata deletion</p>
<p>Context:</p>
<p>We want to be able to delete the metadata (.cfg) files,
even if they are corrupt or in a bad state (missing volume.cfg).</p>
<p>Setup:</p>
<ol>
<li>Setup minio as S3 backupstore</li>
<li>Cleanup backupstore</li>
</ol>
<p>Steps:</p>
<ol>
<li>Create volume(1,2) and attach to the current node</li>
<li>write some data to volume(1,2)</li>
<li>Create backup(1,2) of volume(1,2)</li>
<li>request a backup list</li>
<li>verify backup list contains no error messages for volume(1,2)</li>
<li>verify backup list contains backup(1,2) information for volume(1,2)</li>
<li>delete backup(1) of volume(1,2)</li>
<li>request a backup list</li>
<li>verify backup list contains no error messages for volume(1,2)</li>
<li>verify backup list only contains backup(2) information for volume(1,2)</li>
<li>delete volume.cfg of volume(2)</li>
<li>request backup volume deletion for volume(2)</li>
<li>verify that volume(2) has been deleted in the backupstore.</li>
<li>request a backup list</li>
<li>verify backup list only contains volume(1) and no errors</li>
<li>verify backup list only contains backup(2) information for volume(1)</li>
<li>delete backup volume(1)</li>
<li>verify that volume(1) has been deleted in the backupstore.</li>
<li>cleanup</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_backup_metadata_deletion(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test backup metadata deletion

    Context:

    We want to be able to delete the metadata (.cfg) files,
    even if they are corrupt or in a bad state (missing volume.cfg).

    Setup:

    1. Setup minio as S3 backupstore
    2. Cleanup backupstore

    Steps:

    1.  Create volume(1,2) and attach to the current node
    2.  write some data to volume(1,2)
    3.  Create backup(1,2) of volume(1,2)
    4.  request a backup list
    5.  verify backup list contains no error messages for volume(1,2)
    6.  verify backup list contains backup(1,2) information for volume(1,2)
    7.  delete backup(1) of volume(1,2)
    8.  request a backup list
    9.  verify backup list contains no error messages for volume(1,2)
    10. verify backup list only contains backup(2) information for volume(1,2)
    11. delete volume.cfg of volume(2)
    12. request backup volume deletion for volume(2)
    13. verify that volume(2) has been deleted in the backupstore.
    14. request a backup list
    15. verify backup list only contains volume(1) and no errors
    16. verify backup list only contains backup(2) information for volume(1)
    17. delete backup volume(1)
    18. verify that volume(1) has been deleted in the backupstore.
    19. cleanup
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    volume1_name = volume_name + &#34;-1&#34;
    volume2_name = volume_name + &#34;-2&#34;

    host_id = get_self_host_id()

    volume1 = create_and_check_volume(client, volume1_name)
    volume2 = create_and_check_volume(client, volume2_name)

    volume1.attach(hostId=host_id)
    volume2.attach(hostId=host_id)

    volume1 = wait_for_volume_healthy(client, volume1_name)
    volume2 = wait_for_volume_healthy(client, volume2_name)

    v1bv, v1b1, _, _ = create_backup(client, volume1_name)
    v2bv, v2b1, _, _ = create_backup(client, volume2_name)
    _, v1b2, _, _ = create_backup(client, volume1_name)
    _, v2b2, _, _ = create_backup(client, volume2_name)

    for i in range(RETRY_COUNTS):
        found1 = found2 = found3 = found4 = False

        bvs = client.list_backupVolume()
        for bv in bvs:
            backups = bv.backupList()
            for b in backups:
                if b.name == v1b1.name:
                    found1 = True
                elif b.name == v1b2.name:
                    found2 = True
                elif b.name == v2b1.name:
                    found3 = True
                elif b.name == v2b2.name:
                    found4 = True
        if found1 &amp; found2 &amp; found3 &amp; found4:
            break
        time.sleep(RETRY_INTERVAL)
    assert found1 &amp; found2 &amp; found3 &amp; found4

    v1b2_new = v1bv.backupGet(name=v1b2.name)
    assert_backup_state(v1b2, v1b2_new)

    v2b1_new = v2bv.backupGet(name=v2b1.name)
    assert_backup_state(v2b1, v2b1_new)

    v2b2_new = v2bv.backupGet(name=v2b2.name)
    assert_backup_state(v2b2, v2b2_new)

    delete_backup(client, volume1_name, v1b1.name)
    delete_backup(client, volume2_name, v2b1.name)

    for i in range(RETRY_COUNTS):
        found1 = found2 = found3 = found4 = False

        bvs = client.list_backupVolume()
        for bv in bvs:
            backups = bv.backupList()
            for b in backups:
                if b.name == v1b1.name:
                    found1 = True
                elif b.name == v1b2.name:
                    found2 = True
                elif b.name == v2b1.name:
                    found3 = True
                elif b.name == v2b2.name:
                    found4 = True
        if (not found1) &amp; found2 &amp; (not found3) &amp; found4:
            break
        time.sleep(RETRY_INTERVAL)
    assert (not found1) &amp; found2 &amp; (not found3) &amp; found4

    assert len(v1bv.backupList()) == 1
    assert len(v2bv.backupList()) == 1
    assert v1bv.backupList()[0].name == v1b2.name
    assert v2bv.backupList()[0].name == v2b2.name

    backupstore_delete_volume_cfg_file(client, core_api, volume2_name)

    delete_backup(client, volume2_name, v2b2.name)
    assert len(v2bv.backupList()) == 0

    delete_backup_volume(client, v2bv.name)
    for i in range(RETRY_COUNTS):
        if backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume2_name) == 0:
            break
        time.sleep(RETRY_INTERVAL)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume2_name) == 0

    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()

        found1 = found2 = found3 = found4 = False
        for bv in bvs:
            backups = bv.backupList()
            for b in backups:
                if b.name == v1b1.name:
                    found1 = True
                elif b.name == v1b2.name:
                    found2 = True
                elif b.name == v2b1.name:
                    found3 = True
                elif b.name == v2b2.name:
                    found4 = True
        if (not found1) &amp; found2 &amp; (not found3) &amp; (not found4):
            break
        time.sleep(RETRY_INTERVAL)
    assert (not found1) &amp; found2 &amp; (not found3) &amp; (not found4)

    v1b2_new = v1bv.backupGet(name=v1b2.name)
    assert_backup_state(v1b2, v1b2_new)
    assert v1b2_new.messages == v1b2.messages is None

    delete_backup(client, volume1_name, v1b2.name)
    for i in range(RETRY_COUNTS):
        if backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume1_name) == 0:
            break
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume1_name) == 0

    for i in range(RETRY_COUNTS):
        found1 = found2 = found3 = found4 = False

        bvs = client.list_backupVolume()
        for bv in bvs:
            backups = bv.backupList()
            for b in backups:
                if b.name == v1b1.name:
                    found1 = True
                elif b.name == v1b2.name:
                    found2 = True
                elif b.name == v2b1.name:
                    found3 = True
                elif b.name == v2b2.name:
                    found4 = True
        if (not found1) &amp; (not found2) &amp; (not found3) &amp; (not found4):
            break
        time.sleep(RETRY_INTERVAL)
    assert (not found1) &amp; (not found2) &amp; (not found3) &amp; (not found4)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_status_for_unavailable_replicas"><code class="name flex">
<span>def <span class="ident">test_backup_status_for_unavailable_replicas</span></span>(<span>set_random_backupstore, client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup status for unavailable replicas</p>
<p>Context:</p>
<p>We want to make sure that during the backup creation, once the responsible
replica gone, the backup should in Error state and with the error message.</p>
<p>Setup:</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Run the test for all the available backupstores</li>
</ol>
<p>Steps:</p>
<ol>
<li>Create a backup of volume</li>
<li>Find the replica for that backup</li>
<li>Disable scheduling on the node of that replica</li>
<li>Delete the replica</li>
<li>Verify backup status with Error state and with an error message</li>
<li>Create a new backup</li>
<li>Verify new backup was successful</li>
<li>Cleanup (delete backups, delete volume)</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_backup_status_for_unavailable_replicas(set_random_backupstore, client, volume_name):    # NOQA
    &#34;&#34;&#34;
    Test backup status for unavailable replicas

    Context:

    We want to make sure that during the backup creation, once the responsible
    replica gone, the backup should in Error state and with the error message.

    Setup:

    1. Create a volume and attach to the current node
    2. Run the test for all the available backupstores

    Steps:

    1. Create a backup of volume
    2. Find the replica for that backup
    3. Disable scheduling on the node of that replica
    4. Delete the replica
    5. Verify backup status with Error state and with an error message
    6. Create a new backup
    7. Verify new backup was successful
    8. Cleanup (delete backups, delete volume)
    &#34;&#34;&#34;
    backup_status_for_unavailable_replicas_test(
        client, volume_name, size=str(512 * Mi))</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_volume_list"><code class="name flex">
<span>def <span class="ident">test_backup_volume_list</span></span>(<span>set_random_backupstore, client, core_api)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup volume list
Context:
We want to make sure that an error when listing a single backup volume
does not stop us from listing all the other backup volumes. Otherwise a
single faulty backup can block the retrieval of all known backup volumes.
Setup:
1. Setup minio as S3 backupstore
Steps:
1.
Create a volume(1,2) and attach to the current node
2.
write some data to volume(1,2)
3.
Create a backup(1) of volume(1,2)
4.
request a backup list
5.
verify backup list contains no error messages for volume(1,2)
6.
verify backup list contains backup(1) for volume(1,2)
7.
place a file named "backup_1234@failure.cfg"
into the backups folder of volume(1)
8.
request a backup list
9.
verify backup list contains no error messages for volume(1,2)
10. verify backup list contains backup(1) for volume(1,2)
11. delete backup volumes(1 &amp; 2)
12. cleanup</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_backup_volume_list(set_random_backupstore, client, core_api):  # NOQA
    &#34;&#34;&#34;
    Test backup volume list
    Context:
    We want to make sure that an error when listing a single backup volume
    does not stop us from listing all the other backup volumes. Otherwise a
    single faulty backup can block the retrieval of all known backup volumes.
    Setup:
    1. Setup minio as S3 backupstore
    Steps:
    1.  Create a volume(1,2) and attach to the current node
    2.  write some data to volume(1,2)
    3.  Create a backup(1) of volume(1,2)
    4.  request a backup list
    5.  verify backup list contains no error messages for volume(1,2)
    6.  verify backup list contains backup(1) for volume(1,2)
    7.  place a file named &#34;backup_1234@failure.cfg&#34;
        into the backups folder of volume(1)
    8.  request a backup list
    9.  verify backup list contains no error messages for volume(1,2)
    10. verify backup list contains backup(1) for volume(1,2)
    11. delete backup volumes(1 &amp; 2)
    12. cleanup
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    # create 2 volumes.
    volume1_name, volume2_name = generate_volume_name(), generate_volume_name()

    volume1 = create_and_check_volume(client, volume1_name)
    volume2 = create_and_check_volume(client, volume2_name)

    host_id = get_self_host_id()
    volume1 = volume1.attach(hostId=host_id)
    volume1 = common.wait_for_volume_healthy(client, volume1_name)
    volume2 = volume2.attach(hostId=host_id)
    volume2 = common.wait_for_volume_healthy(client, volume2_name)

    bv1, backup1, snap1, _ = create_backup(client, volume1_name)
    bv2, backup2, snap2, _ = create_backup(client, volume2_name)

    def verify_no_err():
        &#39;&#39;&#39;
        request a backup list
        verify backup list contains no error messages for volume(1,2)
        verify backup list contains backup(1) for volume(1,2)
        &#39;&#39;&#39;
        for _ in range(RETRY_COUNTS):
            verified_bvs = set()
            backup_volume_list = client.list_backupVolume()
            for bv in backup_volume_list:
                if bv.name in (volume1_name, volume2_name):
                    assert not bv[&#39;messages&#39;]
                    for b in bv.backupList().data:
                        if bv.name == volume1_name \
                                and b.name == backup1.name \
                                or bv.name == volume2_name \
                                and b.name == backup2.name:
                            verified_bvs.add(bv.name)
            if len(verified_bvs) == 2:
                break
            time.sleep(RETRY_INTERVAL)
        assert len(verified_bvs) == 2

    verify_no_err()

    # place a bad named file into the backups folder of volume(1)
    prefix = \
        backupstore_get_backup_volume_prefix(client, volume1_name) + &#34;/backups&#34;
    backupstore_create_file(client,
                            core_api,
                            prefix + &#34;/backup_1234@failure.cfg&#34;)

    verify_no_err()

    backupstore_delete_file(client,
                            core_api,
                            prefix + &#34;/backup_1234@failure.cfg&#34;)

    backupstore_cleanup(client)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backup_volume_restore_with_access_mode"><code class="name flex">
<span>def <span class="ident">test_backup_volume_restore_with_access_mode</span></span>(<span>set_random_backupstore, client, access_mode, overridden_restored_access_mode)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the backup w/ the volume access mode, then restore a volume w/ the
original access mode or being overridden.</p>
<ol>
<li>Prepare a healthy volume</li>
<li>Create a backup for the volume</li>
<li>Restore a volume from the backup w/o specifying the access mode
=&gt; Validate the access mode should be the same the volume</li>
<li>Restore a volume from the backup w/ specifying the access mode
=&gt; Validate the access mode should be the same as the specified</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.parametrize(
    &#34;access_mode,overridden_restored_access_mode&#34;,
    [
        pytest.param(&#34;rwx&#34;, &#34;rwo&#34;),
        pytest.param(&#34;rwo&#34;, &#34;rwx&#34;)
    ],
)
def test_backup_volume_restore_with_access_mode(set_random_backupstore, # NOQA
                                                client, # NOQA
                                                access_mode, # NOQA
                                       overridden_restored_access_mode): # NOQA
    &#34;&#34;&#34;
    Test the backup w/ the volume access mode, then restore a volume w/ the
     original access mode or being overridden.

    1. Prepare a healthy volume
    2. Create a backup for the volume
    3. Restore a volume from the backup w/o specifying the access mode
       =&gt; Validate the access mode should be the same the volume
    4. Restore a volume from the backup w/ specifying the access mode
       =&gt; Validate the access mode should be the same as the specified
    &#34;&#34;&#34;
    # Step 1
    test_volume_name = generate_volume_name()
    client.create_volume(name=test_volume_name,
                         size=str(DEFAULT_VOLUME_SIZE * Gi),
                         numberOfReplicas=2,
                         accessMode=access_mode)
    wait_for_volume_creation(client, test_volume_name)
    volume = wait_for_volume_detached(client, test_volume_name)
    volume.attach(hostId=common.get_self_host_id())
    volume = common.wait_for_volume_healthy(client, test_volume_name)

    # Step 2
    _, b, _, _ = create_backup(client, test_volume_name)

    # Step 3
    volume_name_ori_access_mode = test_volume_name + &#39;-default-access&#39;
    client.create_volume(name=volume_name_ori_access_mode,
                         size=str(DEFAULT_VOLUME_SIZE * Gi),
                         numberOfReplicas=2,
                         fromBackup=b.url)
    volume_ori_access_mode = client.by_id_volume(volume_name_ori_access_mode)
    assert volume_ori_access_mode.accessMode == access_mode

    # Step 4
    volume_name_sp_access_mode = test_volume_name + &#39;-specified-access&#39;
    client.create_volume(name=volume_name_sp_access_mode,
                         size=str(DEFAULT_VOLUME_SIZE * Gi),
                         numberOfReplicas=2,
                         accessMode=overridden_restored_access_mode,
                         fromBackup=b.url)
    volume_sp_access_mode = client.by_id_volume(volume_name_sp_access_mode)
    assert volume_sp_access_mode.accessMode == overridden_restored_access_mode</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_backuptarget_available_during_engine_image_not_ready"><code class="name flex">
<span>def <span class="ident">test_backuptarget_available_during_engine_image_not_ready</span></span>(<span>client, apps_api, request)</span>
</code></dt>
<dd>
<div class="desc"><p>Test backup target available during engine image not ready</p>
<ol>
<li>Set backup target URL to S3 and NFS respectively</li>
<li>Set poll interval to 0 and 300 respectively</li>
<li>Scale down the engine image DaemonSet</li>
<li>Check engine image in deploying state</li>
<li>Configures backup target during engine image in not ready state</li>
<li>Check backup target status.available=false</li>
<li>Scale up the engine image DaemonSet</li>
<li>Check backup target status.available=true</li>
<li>Reset backup target setting</li>
<li>Check backup target status.available=false</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;TODO&#34;)  # NOQA
def test_backuptarget_available_during_engine_image_not_ready(client, apps_api, request):  # NOQA
    &#34;&#34;&#34;
    Test backup target available during engine image not ready

    1. Set backup target URL to S3 and NFS respectively
    2. Set poll interval to 0 and 300 respectively
    3. Scale down the engine image DaemonSet
    4. Check engine image in deploying state
    5. Configures backup target during engine image in not ready state
    6. Check backup target status.available=false
    7. Scale up the engine image DaemonSet
    8. Check backup target status.available=true
    9. Reset backup target setting
    10. Check backup target status.available=false
    &#34;&#34;&#34;
    def finalizer():
        default_img = common.get_default_engine_image(client)
        ds_name = &#34;engine-image-&#34; + default_img.name
        body = [{
            &#34;op&#34;: &#34;remove&#34;,
            &#34;path&#34;: &#34;/spec/template/spec/nodeSelector/foo&#34;
        }]
        apps_api.patch_namespaced_daemon_set(
            name=ds_name, namespace=&#39;longhorn-system&#39;, body=body)

    request.addfinalizer(finalizer)

    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        url = &#34;&#34;
        cred_secret = &#34;&#34;
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            url = backupsettings[0]
            cred_secret = backupsettings[1]
        elif common.is_backupTarget_nfs(backupstore):
            url = backupstore
            cred_secret = &#34;&#34;
        else:
            raise NotImplementedError

        poll_intervals = [&#34;0&#34;, &#34;300&#34;]
        for poll_interval in poll_intervals:
            set_backupstore_poll_interval(client, poll_interval)

            default_img = common.get_default_engine_image(client)
            ds_name = &#34;engine-image-&#34; + default_img.name

            # Scale down the engine image DaemonSet
            daemonset = apps_api.read_namespaced_daemon_set(
                name=ds_name, namespace=&#39;longhorn-system&#39;)
            daemonset.spec.template.spec.node_selector = {&#39;foo&#39;: &#39;bar&#39;}
            apps_api.patch_namespaced_daemon_set(
                name=ds_name, namespace=&#39;longhorn-system&#39;, body=daemonset)

            # Check engine image in deploying state
            common.wait_for_engine_image_state(
                client, default_img.name, &#34;deploying&#34;)
            deploying = False
            for _ in range(RETRY_COUNTS):
                default_img = client.by_id_engine_image(default_img.name)
                if not any(default_img.nodeDeploymentMap.values()):
                    deploying = True
                    break
                time.sleep(RETRY_INTERVAL)
            assert deploying is True

            # Set valid backup target during
            # the engine image in not ready state
            set_backupstore_url(client, url)
            set_backupstore_credential_secret(client, cred_secret)
            common.wait_for_backup_target_available(client, False)

            # Scale up the engine image DaemonSet
            body = [{&#34;op&#34;: &#34;remove&#34;,
                     &#34;path&#34;: &#34;/spec/template/spec/nodeSelector/foo&#34;}]
            apps_api.patch_namespaced_daemon_set(
                name=ds_name, namespace=&#39;longhorn-system&#39;, body=body)
            common.wait_for_backup_target_available(client, True)

            # Sleep 1 second to prevent the same time
            # of BackupTarget CR spec.SyncRequestedAt
            time.sleep(1)

            # Reset backup store setting
            reset_backupstore_setting(client)
            common.wait_for_backup_target_available(client, False)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_cleanup_system_generated_snapshots"><code class="name flex">
<span>def <span class="ident">test_cleanup_system_generated_snapshots</span></span>(<span>client, core_api, volume_name, csi_pv, pvc, pod_make)</span>
</code></dt>
<dd>
<div class="desc"><p>Test Cleanup System Generated Snapshots</p>
<ol>
<li>Enabled 'Auto Cleanup System Generated Snapshot'.</li>
<li>Create a volume and attach it to a node.</li>
<li>Write some data to the volume and get the checksum of the data.</li>
<li>Delete a random replica to trigger a system generated snapshot.</li>
<li>Repeat Step 3 for 3 times, and make sure only one snapshot is left.</li>
<li>Check the data with the saved checksum.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_cleanup_system_generated_snapshots(client, core_api, volume_name, csi_pv, pvc, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test Cleanup System Generated Snapshots

    1. Enabled &#39;Auto Cleanup System Generated Snapshot&#39;.
    2. Create a volume and attach it to a node.
    3. Write some data to the volume and get the checksum of the data.
    4. Delete a random replica to trigger a system generated snapshot.
    5. Repeat Step 3 for 3 times, and make sure only one snapshot is left.
    6. Check the data with the saved checksum.
    &#34;&#34;&#34;

    pod_name, _, _, md5sum1 = \
        prepare_pod_with_data_in_mb(
            client, core_api, csi_pv, pvc, pod_make, volume_name)

    volume = client.by_id_volume(volume_name)

    for i in range(3):
        replica_name = volume[&#34;replicas&#34;][i][&#34;name&#34;]
        volume.replicaRemove(name=replica_name)
        wait_for_volume_degraded(client, volume_name)
        wait_for_volume_healthy(client, volume_name)

        volume = client.by_id_volume(volume_name)
        # For the below assertion, the number of snapshots is compared with 2
        # as the list of snapshot have the volume-head too.
        assert len(volume.snapshotList()) == 2

    read_md5sum1 = get_pod_data_md5sum(core_api, pod_name, &#34;/data/test&#34;)
    assert md5sum1 == read_md5sum1</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_default_storage_class_syncup"><code class="name flex">
<span>def <span class="ident">test_default_storage_class_syncup</span></span>(<span>core_api, request)</span>
</code></dt>
<dd>
<div class="desc"><p>Steps:
1. Record the current Longhorn-StorageClass-related ConfigMap
<code>longhorn-storageclass</code>.
2. Modify the default Longhorn StorageClass <code>longhorn</code>.
e.g., update <code>reclaimPolicy</code> from <code>Delete</code> to <code>Retain</code>.
3. Verify that the change is reverted immediately and the manifest is the
same as the record in ConfigMap <code>longhorn-storageclass</code>.
4. Delete the default Longhorn StorageClass <code>longhorn</code>.
5. Verify that the StorageClass is recreated immediately with the manifest
the same as the record in ConfigMap <code>longhorn-storageclass</code>.
6. Modify the content of ConfigMap <code>longhorn-storageclass</code>.
7. Verify that the modifications will be applied to the default Longhorn
StorageClass <code>longhorn</code> immediately.
8. Revert the modifications of the ConfigMaps. Then wait for the
StorageClass sync-up.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_default_storage_class_syncup(core_api, request):  # NOQA
    &#34;&#34;&#34;
    Steps:
    1. Record the current Longhorn-StorageClass-related ConfigMap
       `longhorn-storageclass`.
    2. Modify the default Longhorn StorageClass `longhorn`.
       e.g., update `reclaimPolicy` from `Delete` to `Retain`.
    3. Verify that the change is reverted immediately and the manifest is the
       same as the record in ConfigMap `longhorn-storageclass`.
    4. Delete the default Longhorn StorageClass `longhorn`.
    5. Verify that the StorageClass is recreated immediately with the manifest
       the same as the record in ConfigMap `longhorn-storageclass`.
    6. Modify the content of ConfigMap `longhorn-storageclass`.
    7. Verify that the modifications will be applied to the default Longhorn
       StorageClass `longhorn` immediately.
    8. Revert the modifications of the ConfigMaps. Then wait for the
       StorageClass sync-up.
    &#34;&#34;&#34;
    def edit_configmap_allow_vol_exp(allow_exp):
        &#34;&#34;&#34;
        allow_exp : bool, set allowVolumeExpansion
        &#34;&#34;&#34;
        config_map = core_api.read_namespaced_config_map(
                                                &#34;longhorn-storageclass&#34;,
                                                &#34;longhorn-system&#34;)
        config_map_data = yaml.safe_load(config_map.data[&#34;storageclass.yaml&#34;])
        config_map_data[&#34;allowVolumeExpansion&#34;] = allow_exp
        config_map.data[&#34;storageclass.yaml&#34;] = yaml.dump(config_map_data)
        core_api.patch_namespaced_config_map(&#34;longhorn-storageclass&#34;,
                                             &#34;longhorn-system&#34;,
                                             config_map)

        for i in range(RETRY_COMMAND_COUNT):
            try:
                longhorn_storage_class = storage_api.read_storage_class(
                                                     &#34;longhorn&#34;)
                assert longhorn_storage_class.allow_volume_expansion is allow_exp # NOQA
                break
            except Exception as e:
                print(e)
            finally:
                time.sleep(RETRY_INTERVAL)
        longhorn_storage_class = storage_api.read_storage_class(&#34;longhorn&#34;)
        assert longhorn_storage_class.allow_volume_expansion is allow_exp

    def finalizer():

        edit_configmap_allow_vol_exp(True)

    request.addfinalizer(finalizer)

    # step 1
    storage_api = common.get_storage_api_client()
    config_map = core_api.read_namespaced_config_map(&#34;longhorn-storageclass&#34;,
                                                     &#34;longhorn-system&#34;)
    config_map_data = yaml.safe_load(config_map.data[&#34;storageclass.yaml&#34;])

    # step 2
    longhorn_storage_class = storage_api.read_storage_class(&#34;longhorn&#34;)
    storage_class_data = \
        yaml.safe_load(longhorn_storage_class.
                       metadata.
                       annotations[&#34;longhorn.io/last-applied-configmap&#34;])
    storage_class_data[&#34;reclaimPolicy&#34;] = &#34;Retain&#34;
    longhorn_storage_class.\
        metadata.annotations[&#34;longhorn.io/last-applied-configmap&#34;] =\
        yaml.dump(storage_class_data)

    storage_api.patch_storage_class(&#34;longhorn&#34;, longhorn_storage_class)

    # step 3
    for i in range(RETRY_COMMAND_COUNT):
        longhorn_storage_class = storage_api.read_storage_class(&#34;longhorn&#34;)
        storage_class_data = \
            yaml.safe_load(longhorn_storage_class.
                           metadata.
                           annotations[&#34;longhorn.io/last-applied-configmap&#34;])

        if storage_class_data[&#34;reclaimPolicy&#34;] == \
                config_map_data[&#34;reclaimPolicy&#34;]:
            break

        time.sleep(RETRY_INTERVAL)

    assert storage_class_data[&#34;reclaimPolicy&#34;] == \
        config_map_data[&#34;reclaimPolicy&#34;]

    # step 4
    storage_api.delete_storage_class(&#34;longhorn&#34;)
    for item in storage_api.list_storage_class().items:
        assert item.metadata.name != &#34;longhorn&#34;

    # step 5
    storage_class_recreated = False
    for i in range(RETRY_COMMAND_COUNT):
        for item in storage_api.list_storage_class().items:
            if item.metadata.name == &#34;longhorn&#34;:
                storage_class_recreated = True
                break
        time.sleep(RETRY_INTERVAL)
    assert storage_class_recreated is True

    # step 6, 7
    edit_configmap_allow_vol_exp(False)

    # step 8 in finalizer</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_deleting_backup_volume"><code class="name flex">
<span>def <span class="ident">test_deleting_backup_volume</span></span>(<span>set_random_backupstore, client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test deleting backup volumes</p>
<ol>
<li>Create volume and create backup</li>
<li>Delete the backup and make sure it's gone in the backupstore</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_deleting_backup_volume(set_random_backupstore, client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test deleting backup volumes

    1. Create volume and create backup
    2. Delete the backup and make sure it&#39;s gone in the backupstore
    &#34;&#34;&#34;
    lht_host_id = get_self_host_id()
    volume = create_and_check_volume(client, volume_name)

    volume.attach(hostId=lht_host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    create_backup(client, volume_name)
    create_backup(client, volume_name)

    delete_backup_volume(client, volume_name)
    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_dr_volume_with_all_backup_blocks_deleted"><code class="name flex">
<span>def <span class="ident">test_dr_volume_with_all_backup_blocks_deleted</span></span>(<span>set_random_backupstore, client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test DR volume can be activate after delete all backups.</p>
<p>Context:</p>
<p>We want to make sure that DR volume can activate after delete all backups.</p>
<p>Steps:</p>
<ol>
<li>Create a volume and attach to the current node.</li>
<li>Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).</li>
<li>Create backup(0) of the volume.</li>
<li>Verify backup block count == 2.</li>
<li>Create DR volume from backup(0).</li>
<li>Verify DR volume last backup is backup(0).</li>
<li>Delete backup(0).</li>
<li>Verify backup block count == 0.</li>
<li>Verify DR volume last backup is empty.</li>
<li>Activate and verify DR volume data is data(0).</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_dr_volume_with_all_backup_blocks_deleted(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test DR volume can be activate after delete all backups.

    Context:

    We want to make sure that DR volume can activate after delete all backups.

    Steps:

    1.  Create a volume and attach to the current node.
    2.  Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).
    3.  Create backup(0) of the volume.
    6.  Verify backup block count == 2.
    7.  Create DR volume from backup(0).
    8.  Verify DR volume last backup is backup(0).
    9.  Delete backup(0).
    10. Verify backup block count == 0.
    11. Verify DR volume last backup is empty.
    15. Activate and verify DR volume data is data(0).
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    host_id = get_self_host_id()

    vol = create_and_check_volume(client, volume_name, 2, SIZE)
    vol.attach(hostId=host_id)
    vol = common.wait_for_volume_healthy(client, volume_name)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}
    _, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    backup_blocks_count = backupstore_count_backup_block_files(client,
                                                               core_api,
                                                               volume_name)
    assert backup_blocks_count == 2

    dr_vol_name = &#34;dr-&#34; + volume_name
    client.create_volume(name=dr_vol_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    check_volume_last_backup(client, dr_vol_name, backup0.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup0.name)

    delete_backup(client, volume_name, backup0.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 0
    check_volume_last_backup(client, dr_vol_name, &#34;&#34;)

    activate_standby_volume(client, dr_vol_name)
    dr_vol = client.by_id_volume(dr_vol_name)
    dr_vol.attach(hostId=host_id)
    dr_vol = common.wait_for_volume_healthy(client, dr_vol_name)
    check_volume_data(dr_vol, data0, False)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_dr_volume_with_backup_block_deletion"><code class="name flex">
<span>def <span class="ident">test_dr_volume_with_backup_block_deletion</span></span>(<span>set_random_backupstore, client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test DR volume last backup after block deletion.</p>
<p>Context:</p>
<p>We want to make sure that when the block is delete, the DR volume picks up
the correct last backup.</p>
<p>Steps:</p>
<ol>
<li>Create a volume and attach to the current node.</li>
<li>Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).</li>
<li>Create backup(0) of the volume.</li>
<li>Overwrite backup(0) 1st blocks of data on the volume.
(Since backup(0) contains 2 blocks of data, the updated data is
data1["content"] + data0["content"][BACKUP_BLOCK_SIZE:])</li>
<li>Create backup(1) of the volume.</li>
<li>Verify backup block count == 3.</li>
<li>Create DR volume from backup(1).</li>
<li>Verify DR volume last backup is backup(1).</li>
<li>Delete backup(1).</li>
<li>Verify backup block count == 2.</li>
<li>Verify DR volume last backup is backup(0).</li>
<li>Overwrite backup(0) 1st blocks of data on the volume.
(Since backup(0) contains 2 blocks of data, the updated data is
data2["content"] + data0["content"][BACKUP_BLOCK_SIZE:])</li>
<li>Create backup(2) of the volume.</li>
<li>Verify DR volume last backup is backup(2).</li>
<li>Activate and verify DR volume data is
data2["content"] + data0["content"][BACKUP_BLOCK_SIZE:].</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_dr_volume_with_backup_block_deletion(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test DR volume last backup after block deletion.

    Context:

    We want to make sure that when the block is delete, the DR volume picks up
    the correct last backup.

    Steps:

    1.  Create a volume and attach to the current node.
    2.  Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).
    3.  Create backup(0) of the volume.
    4.  Overwrite backup(0) 1st blocks of data on the volume.
        (Since backup(0) contains 2 blocks of data, the updated data is
        data1[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:])
    5.  Create backup(1) of the volume.
    6.  Verify backup block count == 3.
    7.  Create DR volume from backup(1).
    8.  Verify DR volume last backup is backup(1).
    9.  Delete backup(1).
    10. Verify backup block count == 2.
    11. Verify DR volume last backup is backup(0).
    12. Overwrite backup(0) 1st blocks of data on the volume.
        (Since backup(0) contains 2 blocks of data, the updated data is
        data2[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:])
    13. Create backup(2) of the volume.
    14. Verify DR volume last backup is backup(2).
    15. Activate and verify DR volume data is
        data2[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:].
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    host_id = get_self_host_id()

    vol = create_and_check_volume(client, volume_name, 2, SIZE)
    vol.attach(hostId=host_id)
    vol = common.wait_for_volume_healthy(client, volume_name)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}
    _, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    data1 = {&#39;pos&#39;: 0, &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}
    _, backup1, _, data1 = create_backup(
        client, volume_name, data1)

    backup_blocks_count = backupstore_count_backup_block_files(client,
                                                               core_api,
                                                               volume_name)
    assert backup_blocks_count == 3

    dr_vol_name = &#34;dr-&#34; + volume_name
    client.create_volume(name=dr_vol_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup1.url,
                         frontend=&#34;&#34;, standby=True)
    check_volume_last_backup(client, dr_vol_name, backup1.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup1.name)

    delete_backup(client, volume_name, backup1.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 2
    check_volume_last_backup(client, dr_vol_name, backup0.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup0.name)

    data2 = {&#39;pos&#39;: 0,
             &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}
    _, backup2, _, _ = create_backup(client, volume_name, data2)

    check_volume_last_backup(client, dr_vol_name, backup2.name)
    wait_for_volume_restoration_start(client, dr_vol_name, backup2.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup2.name)

    activate_standby_volume(client, dr_vol_name)
    dr_vol = client.by_id_volume(dr_vol_name)
    dr_vol.attach(hostId=host_id)
    dr_vol = common.wait_for_volume_healthy(client, dr_vol_name)
    final_data = {
        &#39;pos&#39;: 0,
        &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
        &#39;content&#39;: data2[&#39;content&#39;] + data0[&#39;content&#39;][BACKUP_BLOCK_SIZE:],
    }
    check_volume_data(dr_vol, final_data, False)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_dr_volume_with_backup_block_deletion_abort_during_backup_in_progress"><code class="name flex">
<span>def <span class="ident">test_dr_volume_with_backup_block_deletion_abort_during_backup_in_progress</span></span>(<span>set_random_backupstore, client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test DR volume last backup after block deletion aborted. This will set the
last backup to be empty.</p>
<p>Context:</p>
<p>We want to make sure that when the block deletion for the last backup is
aborted by operations such as backups in progress, the DR volume will still
pick up the correct last backup.</p>
<p>Steps:</p>
<ol>
<li>Create a volume and attach to the current node.</li>
<li>Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).</li>
<li>Create backup(0) of the volume.</li>
<li>Overwrite backup(0) 1st blocks of data on the volume.
(Since backup(0) contains 2 blocks of data, the updated data is
data1["content"] + data0["content"][BACKUP_BLOCK_SIZE:])</li>
<li>Create backup(1) of the volume.</li>
<li>Verify backup block count == 3.</li>
<li>Create DR volume from backup(1).</li>
<li>Verify DR volume last backup is backup(1).</li>
<li>Create an artificial in progress backup.cfg file.
This cfg file will convince the longhorn manager that there is a
backup being created. Then all subsequent backup block cleanup will be
skipped.</li>
<li>Delete backup(1).</li>
<li>Verify backup block count == 3 (because of the in progress backup).</li>
<li>Verify DR volume last backup is empty.</li>
<li>Delete the artificial in progress backup.cfg file.</li>
<li>Overwrite backup(0) 1st blocks of data on the volume.
(Since backup(0) contains 2 blocks of data, the updated data is
data2["content"] + data0["content"][BACKUP_BLOCK_SIZE:])</li>
<li>Create backup(2) of the volume.</li>
<li>Verify DR volume last backup is backup(2).</li>
<li>Activate and verify DR volume data is
data2["content"] + data0["content"][BACKUP_BLOCK_SIZE:].</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_dr_volume_with_backup_block_deletion_abort_during_backup_in_progress(set_random_backupstore, client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test DR volume last backup after block deletion aborted. This will set the
    last backup to be empty.

    Context:

    We want to make sure that when the block deletion for the last backup is
    aborted by operations such as backups in progress, the DR volume will still
    pick up the correct last backup.

    Steps:

    1.  Create a volume and attach to the current node.
    2.  Write 4 MB to the beginning of the volume (2 x 2MB backup blocks).
    3.  Create backup(0) of the volume.
    4.  Overwrite backup(0) 1st blocks of data on the volume.
        (Since backup(0) contains 2 blocks of data, the updated data is
        data1[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:])
    5.  Create backup(1) of the volume.
    6.  Verify backup block count == 3.
    7.  Create DR volume from backup(1).
    8.  Verify DR volume last backup is backup(1).
    9.  Create an artificial in progress backup.cfg file.
        This cfg file will convince the longhorn manager that there is a
        backup being created. Then all subsequent backup block cleanup will be
        skipped.
    10. Delete backup(1).
    11. Verify backup block count == 3 (because of the in progress backup).
    12. Verify DR volume last backup is empty.
    13. Delete the artificial in progress backup.cfg file.
    14. Overwrite backup(0) 1st blocks of data on the volume.
        (Since backup(0) contains 2 blocks of data, the updated data is
        data2[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:])
    15. Create backup(2) of the volume.
    16. Verify DR volume last backup is backup(2).
    17. Activate and verify DR volume data is
        data2[&#34;content&#34;] + data0[&#34;content&#34;][BACKUP_BLOCK_SIZE:].
    &#34;&#34;&#34;
    backupstore_cleanup(client)

    host_id = get_self_host_id()

    vol = create_and_check_volume(client, volume_name, 2, SIZE)
    vol.attach(hostId=host_id)
    vol = common.wait_for_volume_healthy(client, volume_name)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(2 * BACKUP_BLOCK_SIZE)}
    create_backup(client, volume_name, data0)

    data1 = {&#39;pos&#39;: 0, &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}
    _, backup1, _, data1 = create_backup(
        client, volume_name, data1)

    backup_blocks_count = backupstore_count_backup_block_files(client,
                                                               core_api,
                                                               volume_name)
    assert backup_blocks_count == 3

    dr_vol_name = &#34;dr-&#34; + volume_name
    client.create_volume(name=dr_vol_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup1.url,
                         frontend=&#34;&#34;, standby=True)
    check_volume_last_backup(client, dr_vol_name, backup1.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup1.name)

    backupstore_create_dummy_in_progress_backup(client, core_api, volume_name)
    delete_backup(client, volume_name, backup1.name)
    assert backupstore_count_backup_block_files(client,
                                                core_api,
                                                volume_name) == 3
    check_volume_last_backup(client, dr_vol_name, &#34;&#34;)
    backupstore_delete_dummy_in_progress_backup(client, core_api, volume_name)

    data2 = {&#39;pos&#39;: 0,
             &#39;len&#39;: BACKUP_BLOCK_SIZE,
             &#39;content&#39;: common.generate_random_data(BACKUP_BLOCK_SIZE)}
    _, backup2, _, _ = create_backup(client, volume_name, data2)

    check_volume_last_backup(client, dr_vol_name, backup2.name)
    wait_for_volume_restoration_start(client, dr_vol_name, backup2.name)
    wait_for_backup_restore_completed(client, dr_vol_name, backup2.name)

    activate_standby_volume(client, dr_vol_name)
    dr_vol = client.by_id_volume(dr_vol_name)
    dr_vol.attach(hostId=host_id)
    dr_vol = common.wait_for_volume_healthy(client, dr_vol_name)
    final_data = {
        &#39;pos&#39;: 0,
        &#39;len&#39;: 2 * BACKUP_BLOCK_SIZE,
        &#39;content&#39;: data2[&#39;content&#39;] + data0[&#39;content&#39;][BACKUP_BLOCK_SIZE:],
    }
    check_volume_data(dr_vol, final_data, False)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_engine_image_daemonset_restart"><code class="name flex">
<span>def <span class="ident">test_engine_image_daemonset_restart</span></span>(<span>client, apps_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test restarting engine image daemonset</p>
<ol>
<li>Get the default engine image</li>
<li>Create a volume and attach to the current node</li>
<li>Write random data to the volume and create a snapshot</li>
<li>Delete the engine image daemonset</li>
<li>Engine image daemonset should be recreated</li>
<li>In the meantime, validate the volume data to prove it's still functional</li>
<li>Wait for the engine image to become <code>ready</code> again</li>
<li>Check the volume data again.</li>
<li>Write some data and create a new snapshot.<ol>
<li>Since create snapshot will use engine image binary.</li>
</ol>
</li>
<li>Check the volume data again</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_engine_image_daemonset_restart(client, apps_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test restarting engine image daemonset

    1. Get the default engine image
    2. Create a volume and attach to the current node
    3. Write random data to the volume and create a snapshot
    4. Delete the engine image daemonset
    5. Engine image daemonset should be recreated
    6. In the meantime, validate the volume data to prove it&#39;s still functional
    7. Wait for the engine image to become `ready` again
    8. Check the volume data again.
    9. Write some data and create a new snapshot.
        1. Since create snapshot will use engine image binary.
    10. Check the volume data again
    &#34;&#34;&#34;
    default_img = common.get_default_engine_image(client)
    ds_name = &#34;engine-image-&#34; + default_img.name

    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    volume = common.wait_for_volume_healthy(client, volume_name)
    snap1_data = write_volume_random_data(volume)
    create_snapshot(client, volume_name)

    # The engine image DaemonSet will be recreated/restarted automatically
    apps_api.delete_namespaced_daemon_set(ds_name, common.LONGHORN_NAMESPACE)

    # Let DaemonSet really restarted
    common.wait_for_engine_image_condition(client, default_img.name, &#34;False&#34;)

    # The Longhorn volume is still available
    # during the engine image DaemonSet restarting
    check_volume_data(volume, snap1_data)

    # Wait for the restart complete
    common.wait_for_engine_image_condition(client, default_img.name, &#34;True&#34;)

    # Longhorn is still able to use the corresponding engine binary to
    # operate snapshot
    check_volume_data(volume, snap1_data)
    snap2_data = write_volume_random_data(volume)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap2_data)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_expand_pvc_with_size_round_up"><code class="name flex">
<span>def <span class="ident">test_expand_pvc_with_size_round_up</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>test expand longhorn volume with pvc</p>
<ol>
<li>Create LHV,PV,PVC with size '1Gi'</li>
<li>Attach, write data, and detach</li>
<li>Expand volume size to '2000000000/2G' and
check if size round up '2000683008/1908Mi'</li>
<li>Attach, write data, and detach</li>
<li>Expand volume size to '2Gi' and check if size is '2147483648'</li>
<li>Attach, write data, and detach</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_expand_pvc_with_size_round_up(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    test expand longhorn volume with pvc

    1. Create LHV,PV,PVC with size &#39;1Gi&#39;
    2. Attach, write data, and detach
    3. Expand volume size to &#39;2000000000/2G&#39; and
        check if size round up &#39;2000683008/1908Mi&#39;
    4. Attach, write data, and detach
    5. Expand volume size to &#39;2Gi&#39; and check if size is &#39;2147483648&#39;
    6. Attach, write data, and detach
    &#34;&#34;&#34;

    static_sc_name = &#34;longhorn&#34;
    setting = client.by_id_setting(SETTING_DEFAULT_LONGHORN_STATIC_SC)
    setting = client.update(setting, value=static_sc_name)
    assert setting.value == static_sc_name

    volume = create_and_check_volume(client, volume_name, 2, str(1 * Gi))
    create_pv_for_volume(client, core_api, volume, volume_name)
    create_pvc_for_volume(client, core_api, volume, volume_name)

    self_hostId = get_self_host_id()
    volume.attach(hostId=self_hostId, disableFrontend=False)
    volume = wait_for_volume_healthy(client, volume_name)
    test_data = write_volume_random_data(volume)
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    volume.expand(size=&#34;2000000000&#34;)
    wait_for_volume_expansion(client, volume_name)

    for i in range(DEFAULT_POD_TIMEOUT):
        claim = core_api.read_namespaced_persistent_volume_claim(
            name=volume_name, namespace=&#39;default&#39;)
        if claim.spec.resources.requests[&#39;storage&#39;] == &#34;2000683008&#34; and \
                claim.status.capacity[&#39;storage&#39;] == &#34;1908Mi&#34;:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert claim.spec.resources.requests[&#39;storage&#39;] == &#34;2000683008&#34;
    assert claim.status.capacity[&#39;storage&#39;] == &#34;1908Mi&#34;

    volume = client.by_id_volume(volume_name)
    assert volume.size == &#34;2000683008&#34;
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    self_hostId = get_self_host_id()
    volume.attach(hostId=self_hostId, disableFrontend=False)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, test_data, False)
    test_data = write_volume_random_data(volume)
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    volume.expand(size=str(2 * Gi))
    wait_for_volume_expansion(client, volume_name)

    for i in range(DEFAULT_POD_TIMEOUT):
        claim = core_api.read_namespaced_persistent_volume_claim(
            name=volume_name, namespace=&#39;default&#39;)
        if claim.spec.resources.requests[&#39;storage&#39;] == &#34;2147483648&#34; and \
                claim.status.capacity[&#39;storage&#39;] == &#34;2Gi&#34;:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert claim.spec.resources.requests[&#39;storage&#39;] == &#34;2147483648&#34;
    assert claim.status.capacity[&#39;storage&#39;] == &#34;2Gi&#34;

    volume = client.by_id_volume(volume_name)
    assert volume.size == &#34;2147483648&#34;
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    self_hostId = get_self_host_id()
    volume.attach(hostId=self_hostId, disableFrontend=False)
    volume = wait_for_volume_healthy(client, volume_name)
    check_volume_data(volume, test_data, False)
    volume.detach(hostId=&#34;&#34;)
    volume = wait_for_volume_detached(client, volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_expansion_basic"><code class="name flex">
<span>def <span class="ident">test_expansion_basic</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test volume expansion using Longhorn API</p>
<ol>
<li>Create volume and attach to the current node</li>
<li>Generate data <code>snap1_data</code> and write it to the volume</li>
<li>Create snapshot <code>snap1</code></li>
<li>Expand the volume (volume will be detached, expanded, then attached)</li>
<li>Verify the volume has been expanded</li>
<li>Generate data <code>snap2_data</code> and write it to the volume</li>
<li>Create snapshot <code>snap2</code></li>
<li>Gerneate data <code>snap3_data</code> and write it after the original size</li>
<li>Create snapshot <code>snap3</code> and verify the <code>snap3_data</code> with location</li>
<li>Detach and reattach the volume.</li>
<li>Verify the volume is still expanded, and <code>snap3_data</code> remain valid</li>
<li>Detach the volume.</li>
<li>Reattach the volume in maintence mode</li>
<li>Revert to <code>snap2</code> and detach.</li>
<li>Attach the volume and check data <code>snap2_data</code></li>
<li>Generate <code>snap4_data</code> and write it after the original size</li>
<li>Create snapshot <code>snap4</code> and verify <code>snap4_data</code>.</li>
<li>Detach the volume and revert to <code>snap1</code></li>
<li>Validate <code>snap1_data</code></li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_expansion_basic(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test volume expansion using Longhorn API

    1. Create volume and attach to the current node
    2. Generate data `snap1_data` and write it to the volume
    3. Create snapshot `snap1`
    4. Expand the volume (volume will be detached, expanded, then attached)
    5. Verify the volume has been expanded
    6. Generate data `snap2_data` and write it to the volume
    7. Create snapshot `snap2`
    8. Gerneate data `snap3_data` and write it after the original size
    9. Create snapshot `snap3` and verify the `snap3_data` with location
    10. Detach and reattach the volume.
    11. Verify the volume is still expanded, and `snap3_data` remain valid
    12. Detach the volume.
    13. Reattach the volume in maintence mode
    14. Revert to `snap2` and detach.
    15. Attach the volume and check data `snap2_data`
    16. Generate `snap4_data` and write it after the original size
    17. Create snapshot `snap4` and verify `snap4_data`.
    18. Detach the volume and revert to `snap1`
    19. Validate `snap1_data`
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)
    assert volume.disableFrontend is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    snap1_data = write_volume_random_data(volume)
    snap1 = create_snapshot(client, volume_name)

    expand_attached_volume(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))

    snap2_data = write_volume_random_data(volume)
    snap2 = create_snapshot(client, volume_name)

    snap3_data = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    snap3_data = write_volume_data(volume, snap3_data)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap3_data)

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))
    check_volume_data(volume, snap3_data)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    assert volume.disableFrontend is True
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV
    check_volume_endpoint(volume)
    volume.snapshotRevert(name=snap2.name)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_volume_data(volume, snap2_data, False)
    snap4_data = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    snap4_data = write_volume_data(volume, snap4_data)
    create_snapshot(client, volume_name)
    check_volume_data(volume, snap4_data)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    volume.attach(hostId=lht_hostId, disableFrontend=True)
    volume = common.wait_for_volume_healthy_no_frontend(client, volume_name)
    volume.snapshotRevert(name=snap1.name)
    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=lht_hostId, disableFrontend=False)
    common.wait_for_volume_healthy(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_volume_data(volume, snap1_data, False)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_expansion_canceling"><code class="name flex">
<span>def <span class="ident">test_expansion_canceling</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"><p>Test expansion canceling</p>
<ol>
<li>Create a volume, then create the corresponding PV, PVC and Pod.</li>
<li>Generate <code>test_data</code> and write to the pod</li>
<li>Create an empty directory with expansion snapshot tmp meta file path
so that the following expansion will fail</li>
<li>Delete the pod and wait for volume detachment</li>
<li>Try to expand the volume using Longhorn API</li>
<li>Wait for expansion failure then use Longhorn API to cancel it</li>
<li>Create a new pod and validate the volume content,
then re-write random data to the pod</li>
<li>Delete the pod and wait for volume detachment</li>
<li>Retry expansion then verify the expansion done using Longhorn API</li>
<li>Create a new pod</li>
<li>Validate the volume content, then check if data writing looks fine</li>
<li>Clean up pod, PVC, and PV</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_expansion_canceling(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test expansion canceling

    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Generate `test_data` and write to the pod
    3. Create an empty directory with expansion snapshot tmp meta file path
       so that the following expansion will fail
    4. Delete the pod and wait for volume detachment
    5. Try to expand the volume using Longhorn API
    6. Wait for expansion failure then use Longhorn API to cancel it
    7. Create a new pod and validate the volume content,
       then re-write random data to the pod
    8. Delete the pod and wait for volume detachment
    9. Retry expansion then verify the expansion done using Longhorn API
    10. Create a new pod
    11. Validate the volume content, then check if data writing looks fine
    12. Clean up pod, PVC, and PV
    &#34;&#34;&#34;
    expansion_pvc_name = &#34;pvc-&#34; + volume_name
    expansion_pv_name = &#34;pv-&#34; + volume_name
    pod_name = &#34;pod-&#34; + volume_name
    volume = create_and_check_volume(client, volume_name, 2, SIZE)
    create_pv_for_volume(client, core_api, volume, expansion_pv_name)
    create_pvc_for_volume(client, core_api, volume, expansion_pvc_name)
    pod[&#39;metadata&#39;][&#39;name&#39;] = pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: expansion_pvc_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    volume = client.by_id_volume(volume_name)
    replicas = volume.replicas
    fail_replica_expansion(client, core_api,
                           volume_name, EXPAND_SIZE, replicas)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)
    write_pod_volume_data(core_api, pod_name, test_data)

    delete_and_wait_pod(core_api, pod_name)
    volume = wait_for_volume_detached(client, volume_name)

    volume.expand(size=EXPAND_SIZE)
    wait_for_expansion_failure(client, volume_name)
    volume = client.by_id_volume(volume_name)
    volume.cancelExpansion()
    wait_for_volume_expansion(client, volume_name)
    volume = client.by_id_volume(volume_name)
    assert volume.state == &#34;detached&#34;
    assert volume.size == SIZE

    # check if the volume still works fine
    create_and_wait_pod(core_api, pod)
    resp = read_volume_data(core_api, pod_name)
    assert resp == test_data
    test_data = generate_random_data(VOLUME_RWTEST_SIZE)
    write_pod_volume_data(core_api, pod_name, test_data)

    # retry expansion
    delete_and_wait_pod(core_api, pod_name)
    volume = wait_for_volume_detached(client, volume_name)
    volume.expand(size=EXPAND_SIZE)
    wait_for_volume_expansion(client, volume_name)
    volume = client.by_id_volume(volume_name)
    assert volume.state == &#34;detached&#34;
    assert volume.size == str(EXPAND_SIZE)

    create_and_wait_pod(core_api, pod)
    volume = client.by_id_volume(volume_name)
    engine = get_volume_engine(volume)
    assert volume.size == EXPAND_SIZE
    assert volume.size == engine.size
    resp = read_volume_data(core_api, pod_name)
    assert resp == test_data
    write_pod_volume_data(core_api, pod_name, test_data)
    resp = read_volume_data(core_api, pod_name)
    assert resp == test_data

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, expansion_pvc_name)
    delete_and_wait_pv(core_api, expansion_pv_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_expansion_with_scheduling_failure"><code class="name flex">
<span>def <span class="ident">test_expansion_with_scheduling_failure</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"><p>Test if the running volume with scheduling failure
can be expanded after the detachment.</p>
<p>Prerequisite:
Setting "soft anti-affinity" is false.</p>
<ol>
<li>Create a volume, then create the corresponding PV, PVC and Pod.</li>
<li>Wait for the pod running and the volume healthy.</li>
<li>Write data to the pod volume and get the md5sum.</li>
<li>Disable the scheduling for a node contains a running replica.</li>
<li>Crash the replica on the scheduling disabled node for the volume.
Then delete the failed replica so that it won't be reused.</li>
<li>Wait for the scheduling failure which is caused
by the new replica creation.</li>
<li>Verify:
7.1. <code>volume.ready == True</code>.
7.2. <code>volume.conditions[scheduled].status == False</code>.
7.3. the volume is Degraded.
7.4. the new replica is created but it is not running.</li>
<li>Write more data to the volume and get the md5sum</li>
<li>Delete the pod and wait for the volume detached.</li>
<li>Verify the scheduling failed replica is removed.</li>
<li>Verify:
11.1. <code>volume.ready == True</code>.
11.2. <code>volume.conditions[scheduled].status == True</code></li>
<li>Expand the volume and wait for the expansion succeeds.</li>
<li>Verify there is no rebuild replica after the expansion.</li>
<li>Recreate a new pod for the volume and wait for the pod running.</li>
<li>Validate the volume content.</li>
<li>Verify the expanded part can be read/written correctly.</li>
<li>Enable the node scheduling.</li>
<li>Wait for the volume rebuild succeeds.</li>
<li>Verify the data written in the expanded part.</li>
<li>Clean up pod, PVC, and PV.</li>
</ol>
<p>Notice that the step 1 to step 11 is identical with
those of the case test_running_volume_with_scheduling_failure().</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_expansion_with_scheduling_failure(
        client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test if the running volume with scheduling failure
    can be expanded after the detachment.

    Prerequisite:
    Setting &#34;soft anti-affinity&#34; is false.

    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Disable the scheduling for a node contains a running replica.
    5. Crash the replica on the scheduling disabled node for the volume.
       Then delete the failed replica so that it won&#39;t be reused.
    6. Wait for the scheduling failure which is caused
       by the new replica creation.
    7. Verify:
      7.1. `volume.ready == True`.
      7.2. `volume.conditions[scheduled].status == False`.
      7.3. the volume is Degraded.
      7.4. the new replica is created but it is not running.
    8. Write more data to the volume and get the md5sum
    9. Delete the pod and wait for the volume detached.
    10. Verify the scheduling failed replica is removed.
    11. Verify:
      11.1. `volume.ready == True`.
      11.2. `volume.conditions[scheduled].status == True`
    12. Expand the volume and wait for the expansion succeeds.
    13. Verify there is no rebuild replica after the expansion.
    14. Recreate a new pod for the volume and wait for the pod running.
    15. Validate the volume content.
    16. Verify the expanded part can be read/written correctly.
    17. Enable the node scheduling.
    18. Wait for the volume rebuild succeeds.
    19. Verify the data written in the expanded part.
    20. Clean up pod, PVC, and PV.

    Notice that the step 1 to step 11 is identical with
    those of the case test_running_volume_with_scheduling_failure().
    &#34;&#34;&#34;
    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_node_soft_anti_affinity_setting, value=&#34;false&#34;)

    data_path1 = &#34;/data/test1&#34;
    test_pv_name = &#34;pv-&#34; + volume_name
    test_pvc_name = &#34;pvc-&#34; + volume_name
    test_pod_name = &#34;pod-&#34; + volume_name

    volume = create_and_check_volume(client, volume_name, size=str(300 * Mi))
    create_pv_for_volume(client, core_api, volume, test_pv_name)
    create_pvc_for_volume(client, core_api, volume, test_pvc_name)

    pod[&#39;metadata&#39;][&#39;name&#39;] = test_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: test_pvc_name,
        },
    }]
    create_and_wait_pod(core_api, pod)
    wait_for_volume_healthy(client, volume_name)
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path1, DATA_SIZE_IN_MB_1)
    original_md5sum1 = get_pod_data_md5sum(core_api, test_pod_name,
                                           data_path1)

    volume = client.by_id_volume(volume_name)
    old_replicas = {}
    for r in volume.replicas:
        old_replicas[r.name] = r
    failed_replica = volume.replicas[0]
    node = client.by_id_node(failed_replica.hostId)
    node = set_node_scheduling(client, node, allowScheduling=False)
    common.wait_for_node_update(client, node.id,
                                &#34;allowScheduling&#34;, False)

    crash_replica_processes(client, core_api, volume_name,
                            replicas=[failed_replica],
                            wait_to_fail=False)

    # Remove the failed replica so that it won&#39;t be reused later
    volume = wait_for_volume_degraded(client, volume_name)
    volume.replicaRemove(name=failed_replica.name)

    # Wait for scheduling failure.
    # It means the new replica is created but fails to be scheduled.
    wait_for_volume_condition_scheduled(client, volume_name, &#34;status&#34;,
                                        CONDITION_STATUS_FALSE)
    wait_for_volume_condition_scheduled(client, volume_name, &#34;reason&#34;,
                                        CONDITION_REASON_SCHEDULING_FAILURE)
    volume = wait_for_volume_degraded(client, volume_name)
    assert len(volume.replicas) == 3
    assert volume.ready
    for r in volume.replicas:
        assert r.name != failed_replica.name
        if r.name not in old_replicas:
            new_replica = r
            break
    assert new_replica
    assert not new_replica.running
    assert not new_replica.hostId

    data_path2 = &#34;/data/test2&#34;
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path2, DATA_SIZE_IN_MB_1)
    original_md5sum2 = get_pod_data_md5sum(core_api, test_pod_name, data_path2)

    delete_and_wait_pod(core_api, test_pod_name)
    wait_for_volume_detached(client, volume_name)
    volume = wait_for_volume_condition_scheduled(client, volume_name, &#34;status&#34;,
                                                 CONDITION_STATUS_TRUE)
    assert volume.ready
    # The scheduling failed replica will be removed
    # so that the volume can be reattached later.
    assert len(volume.replicas) == 2
    for r in volume.replicas:
        assert r.hostId != &#34;&#34;
        assert r.name != new_replica.name

    expanded_size = str(400 * Mi)
    volume.expand(size=expanded_size)
    wait_for_volume_expansion(client, volume_name)
    volume = client.by_id_volume(volume_name)
    assert volume.state == &#34;detached&#34;
    assert volume.size == expanded_size
    assert len(volume.replicas) == 2
    for r in volume.replicas:
        assert r.name in old_replicas

    create_and_wait_pod(core_api, pod)
    wait_for_volume_degraded(client, volume_name)

    md5sum1 = get_pod_data_md5sum(core_api, test_pod_name, data_path1)
    assert md5sum1 == original_md5sum1
    md5sum2 = get_pod_data_md5sum(core_api, test_pod_name, data_path2)
    assert md5sum2 == original_md5sum2

    # The data writing is fine
    data_path3 = &#34;/data/test3&#34;
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path3, DATA_SIZE_IN_MB_1)
    original_md5sum3 = get_pod_data_md5sum(core_api, test_pod_name, data_path3)

    node = client.by_id_node(failed_replica.hostId)
    set_node_scheduling(client, node, allowScheduling=True)
    wait_for_volume_healthy(client, volume_name)

    md5sum3 = get_pod_data_md5sum(core_api, test_pod_name, data_path3)
    assert md5sum3 == original_md5sum3

    delete_and_wait_pod(core_api, test_pod_name)
    delete_and_wait_pvc(core_api, test_pvc_name)
    delete_and_wait_pv(core_api, test_pv_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_hosts"><code class="name flex">
<span>def <span class="ident">test_hosts</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"><p>Check node name and IP</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_hosts(client):  # NOQA
    &#34;&#34;&#34;
    Check node name and IP
    &#34;&#34;&#34;
    hosts = client.list_node()
    for host in hosts:
        assert host.name is not None
        assert host.address is not None

    host_id = []
    for i in range(0, len(hosts)):
        host_id.append(hosts.data[i].name)

    host0_from_i = {}
    for i in range(0, len(hosts)):
        if len(host0_from_i) == 0:
            host0_from_i = client.by_id_node(host_id[0])
        else:
            assert host0_from_i.name == \
                client.by_id_node(host_id[0]).name
            assert host0_from_i.address == \
                client.by_id_node(host_id[0]).address</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_listing_backup_volume"><code class="name flex">
<span>def <span class="ident">test_listing_backup_volume</span></span>(<span>client, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"><p>Test listing backup volumes</p>
<ol>
<li>Create three volumes: <code>volume1/2/3</code></li>
<li>Setup NFS backupstore since we can manipulate the content easily</li>
<li>Create multiple snapshots for all three volumes</li>
<li>Rename <code>volume1</code>'s <code>volume.cfg</code> to <code>volume.cfg.tmp</code> in backupstore</li>
<li>List backup volumes. Make sure <code>volume1</code> errors out but found other two</li>
<li>Restore <code>volume1</code>'s <code>volume.cfg</code>.</li>
<li>Make sure now backup volume <code>volume1</code> can be found</li>
<li>Delete backups for <code>volume1/2</code>, make sure they cannot be found later</li>
<li>Corrupt a backup.cfg on volume3</li>
<li>Check that the backup is listed with the other backups of volume3</li>
<li>Verify that the corrupted backup has Messages of type error</li>
<li>Check that backup inspection for the previously corrupted backup fails</li>
<li>Delete backups for <code>volume3</code>, make sure they cannot be found later</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
@pytest.mark.skipif(&#39;nfs&#39; not in BACKUPSTORE, reason=&#39;This test is only applicable for nfs&#39;)  # NOQA
def test_listing_backup_volume(client, backing_image=&#34;&#34;):   # NOQA
    &#34;&#34;&#34;
    Test listing backup volumes

    1. Create three volumes: `volume1/2/3`
    2. Setup NFS backupstore since we can manipulate the content easily
    3. Create multiple snapshots for all three volumes
    4. Rename `volume1`&#39;s `volume.cfg` to `volume.cfg.tmp` in backupstore
    5. List backup volumes. Make sure `volume1` errors out but found other two
    6. Restore `volume1`&#39;s `volume.cfg`.
    7. Make sure now backup volume `volume1` can be found
    8. Delete backups for `volume1/2`, make sure they cannot be found later
    9. Corrupt a backup.cfg on volume3
    11. Check that the backup is listed with the other backups of volume3
    12. Verify that the corrupted backup has Messages of type error
    13. Check that backup inspection for the previously corrupted backup fails
    14. Delete backups for `volume3`, make sure they cannot be found later
    &#34;&#34;&#34;
    lht_hostId = get_self_host_id()

    # create 3 volumes.
    volume1_name = generate_volume_name()
    volume2_name = generate_volume_name()
    volume3_name = generate_volume_name()

    volume1 = create_and_check_volume(client, volume1_name)
    volume2 = create_and_check_volume(client, volume2_name)
    volume3 = create_and_check_volume(client, volume3_name)

    volume1.attach(hostId=lht_hostId)
    volume1 = common.wait_for_volume_healthy(client, volume1_name)
    volume2.attach(hostId=lht_hostId)
    volume2 = common.wait_for_volume_healthy(client, volume2_name)
    volume3.attach(hostId=lht_hostId)
    volume3 = common.wait_for_volume_healthy(client, volume3_name)

    # we only test NFS here.
    # Since it is difficult to directly remove volume.cfg from s3 buckets
    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_nfs(backupstore):
            updated = False
            for i in range(RETRY_COMMAND_COUNT):
                nfs_url = backupstore.strip(&#34;nfs://&#34;)
                setting = client.update(setting, value=backupstore)
                assert setting.value == backupstore
                setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
                if &#34;nfs&#34; in setting.value:
                    updated = True
                    break
            assert updated

    _, _, snap1, _ = create_backup(client, volume1_name)
    _, _, snap2, _ = create_backup(client, volume2_name)
    _, _, snap3, _ = create_backup(client, volume3_name)
    subprocess.check_output([&#34;sync&#34;])
    _, _, snap4, _ = create_backup(client, volume3_name)
    subprocess.check_output([&#34;sync&#34;])
    _, _, snap5, _ = create_backup(client, volume3_name)
    subprocess.check_output([&#34;sync&#34;])

    # invalidate backup volume 1 by renaming volume.cfg to volume.cfg.tmp
    cmd = [&#34;mkdir&#34;, &#34;-p&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;mount&#34;, &#34;-t&#34;, &#34;nfs4&#34;, nfs_url, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;find&#34;, &#34;/mnt/nfs&#34;, &#34;-type&#34;, &#34;d&#34;, &#34;-name&#34;, volume1_name]
    volume1_backup_volume_path = \
        subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)

    cmd = [&#34;find&#34;, volume1_backup_volume_path, &#34;-name&#34;, &#34;volume.cfg&#34;]
    volume1_backup_volume_cfg_path = \
        subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)
    cmd = [&#34;mv&#34;, volume1_backup_volume_cfg_path,
           volume1_backup_volume_cfg_path + &#34;.tmp&#34;]
    subprocess.check_output(cmd)
    subprocess.check_output([&#34;sync&#34;])

    found1 = True
    found2 = found3 = False
    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()
        if volume1_name not in bvs:
            found1 = False
        if volume2_name in bvs:
            found2 = True
        if volume3_name in bvs:
            found3 = True
        if not found1 &amp; found2 &amp; found3:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found1 &amp; found2 &amp; found3

    cmd = [&#34;mv&#34;, volume1_backup_volume_cfg_path + &#34;.tmp&#34;,
           volume1_backup_volume_cfg_path]
    subprocess.check_output(cmd)
    subprocess.check_output([&#34;sync&#34;])

    bv1, b1 = common.find_backup(client, volume1_name, snap1.name)
    common.delete_backup(client, volume1_name, b1.name)

    bv2, b2 = common.find_backup(client, volume2_name, snap2.name)
    common.delete_backup(client, volume2_name, b2.name)

    # corrupt backup for snap4
    bv4, b4 = common.find_backup(client, volume3_name, snap4.name)
    b4_cfg_name = &#34;backup_&#34; + b4[&#34;name&#34;] + &#34;.cfg&#34;
    cmd = [&#34;find&#34;, &#34;/mnt/nfs&#34;, &#34;-type&#34;, &#34;d&#34;, &#34;-name&#34;, volume3_name]
    v3_backup_path = subprocess.check_output(cmd).strip().decode(&#39;utf-8&#39;)
    b4_cfg_path = os.path.join(v3_backup_path, &#34;backups&#34;, b4_cfg_name)
    assert os.path.exists(b4_cfg_path)
    b4_tmp_cfg_path = os.path.join(v3_backup_path, b4_cfg_name)
    os.rename(b4_cfg_path, b4_tmp_cfg_path)
    assert os.path.exists(b4_tmp_cfg_path)

    corrupt_backup = open(b4_cfg_path, &#34;w&#34;)
    assert corrupt_backup
    assert corrupt_backup.write(&#34;{corrupt: definitely&#34;) &gt; 0
    corrupt_backup.close()
    subprocess.check_output([&#34;sync&#34;])

    # a corrupt backup cannot provide information about the snapshot
    found = True
    for i in range(RETRY_COMMAND_COUNT):
        if b4[&#34;name&#34;] not in bv4.backupList():
            found = False
            break
    assert not found

    # cleanup b4
    os.remove(b4_cfg_path)
    os.rename(b4_tmp_cfg_path, b4_cfg_path)
    subprocess.check_output([&#34;sync&#34;])

    bv3, b3 = common.find_backup(client, volume3_name, snap3.name)
    common.delete_backup(client, volume3_name, b3.name)
    bv4, b4 = common.find_backup(client, volume3_name, snap4.name)
    common.delete_backup(client, volume3_name, b4.name)
    bv5, b5 = common.find_backup(client, volume3_name, snap5.name)
    common.delete_backup(client, volume3_name, b5.name)

    common.delete_backup_volume(client, volume3_name)
    common.wait_for_backup_volume_delete(client, volume3_name)

    volume1.detach(hostId=&#34;&#34;)
    volume1 = common.wait_for_volume_detached(client, volume1_name)
    client.delete(volume1)
    wait_for_volume_delete(client, volume1_name)

    volume2.detach(hostId=&#34;&#34;)
    volume2 = common.wait_for_volume_detached(client, volume2_name)
    client.delete(volume2)
    wait_for_volume_delete(client, volume2_name)

    volume3.detach(hostId=&#34;&#34;)
    volume3 = common.wait_for_volume_detached(client, volume3_name)
    client.delete(volume3)
    wait_for_volume_delete(client, volume3_name)

    volumes = client.list_volume()
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_multiple_volumes_creation_with_degraded_availability"><code class="name flex">
<span>def <span class="ident">test_multiple_volumes_creation_with_degraded_availability</span></span>(<span>set_random_backupstore, client, core_api, apps_api, storage_class, statefulset)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: verify multiple volumes with degraded availability can be
created, attached, detached, and deleted at nearly the same time.</p>
<p>Given new StorageClass created with <code>numberOfReplicas=5</code>.</p>
<p>When set <code>allow-volume-creation-with-degraded-availability</code> to <code>True</code>.
And deploy this StatefulSet:
<a href="https://github.com/longhorn/longhorn/issues/2073#issuecomment-742948726">https://github.com/longhorn/longhorn/issues/2073#issuecomment-742948726</a>
Then all 10 volumes are healthy in 1 minute.</p>
<p>When delete the StatefulSet.
then all 10 volumes are detached in 1 minute.</p>
<p>When find and delete the PVC of the 10 volumes.
Then all 10 volumes are deleted in 1 minute.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_multiple_volumes_creation_with_degraded_availability(set_random_backupstore, client, core_api, apps_api, storage_class, statefulset):  # NOQA
    &#34;&#34;&#34;
    Scenario: verify multiple volumes with degraded availability can be
              created, attached, detached, and deleted at nearly the same time.

    Given new StorageClass created with `numberOfReplicas=5`.

    When set `allow-volume-creation-with-degraded-availability` to `True`.
    And deploy this StatefulSet:
        https://github.com/longhorn/longhorn/issues/2073#issuecomment-742948726
    Then all 10 volumes are healthy in 1 minute.

    When delete the StatefulSet.
    then all 10 volumes are detached in 1 minute.

    When find and delete the PVC of the 10 volumes.
    Then all 10 volumes are deleted in 1 minute.
    &#34;&#34;&#34;
    storage_class[&#39;parameters&#39;][&#39;numberOfReplicas&#39;] = &#34;5&#34;
    create_storage_class(storage_class)

    common.update_setting(client,
                          common.SETTING_DEGRADED_AVAILABILITY, &#34;true&#34;)

    sts_spec = statefulset[&#39;spec&#39;]
    sts_spec[&#39;podManagementPolicy&#39;] = &#34;Parallel&#34;
    sts_spec[&#39;replicas&#39;] = 10
    sts_spec[&#39;volumeClaimTemplates&#39;][0][&#39;spec&#39;][&#39;storageClassName&#39;] = \
        storage_class[&#39;metadata&#39;][&#39;name&#39;]
    statefulset[&#39;spec&#39;] = sts_spec
    common.create_and_wait_statefulset(statefulset)
    pod_list = common.get_statefulset_pod_info(core_api, statefulset)
    retry_counts = int(60 / RETRY_INTERVAL)
    common.wait_for_pods_volume_state(
        client, pod_list,
        common.VOLUME_FIELD_ROBUSTNESS,
        common.VOLUME_ROBUSTNESS_HEALTHY,
        retry_counts=retry_counts
    )

    apps_api.delete_namespaced_stateful_set(
        name=statefulset[&#39;metadata&#39;][&#39;name&#39;],
        namespace=statefulset[&#39;metadata&#39;][&#39;namespace&#39;],
        body=k8sclient.V1DeleteOptions()
    )
    common.wait_for_pods_volume_state(
        client, pod_list,
        common.VOLUME_FIELD_STATE,
        common.VOLUME_STATE_DETACHED,
        retry_counts=retry_counts
    )

    for p in pod_list:
        common.delete_and_wait_pvc(core_api, p[&#39;pvc_name&#39;],
                                   retry_counts=retry_counts)
    common.wait_for_pods_volume_delete(client, pod_list,
                                       retry_counts=retry_counts)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_restore_basic"><code class="name flex">
<span>def <span class="ident">test_restore_basic</span></span>(<span>set_random_backupstore, client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"><p>Steps:
1. Create a volume and attach to a pod.
2. Write some data into the volume and compute the checksum m1.
3. Create a backup say b1.
4. Write some more data into the volume and compute the checksum m2.
5. Create a backup say b2.
6. Delete all the data from the volume.
7. Write some more data into the volume and compute the checksum m3.
8. Create a backup say b3.
9. Restore backup b1 and verify the data with m1.
10. Restore backup b2 and verify the data with m1 and m2.
11. Restore backup b3 and verify the data with m3.
12. Delete the backup b2.
13. restore the backup b3 and verify the data with m3.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_restore_basic(set_random_backupstore, client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Steps:
    1. Create a volume and attach to a pod.
    2. Write some data into the volume and compute the checksum m1.
    3. Create a backup say b1.
    4. Write some more data into the volume and compute the checksum m2.
    5. Create a backup say b2.
    6. Delete all the data from the volume.
    7. Write some more data into the volume and compute the checksum m3.
    8. Create a backup say b3.
    9. Restore backup b1 and verify the data with m1.
    10. Restore backup b2 and verify the data with m1 and m2.
    11. Restore backup b3 and verify the data with m3.
    12. Delete the backup b2.
    13. restore the backup b3 and verify the data with m3.
    &#34;&#34;&#34;

    test_pv_name = &#34;pv-&#34; + volume_name
    test_pvc_name = &#34;pvc-&#34; + volume_name
    test_pod_name = &#34;pod-&#34; + volume_name

    volume = create_and_check_volume(client, volume_name, size=str(1 * Gi))
    create_pv_for_volume(client, core_api, volume, test_pv_name)
    create_pvc_for_volume(client, core_api, volume, test_pvc_name)
    pod[&#39;metadata&#39;][&#39;name&#39;] = test_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: test_pvc_name,
        },
    }]
    create_and_wait_pod(core_api, pod)
    wait_for_volume_healthy(client, volume_name)

    # Write 1st data and take backup
    backup_volume, backup1, data_checksum_1 = \
        create_backup_from_volume_attached_to_pod(client, core_api,
                                                  volume_name, test_pod_name,
                                                  data_path=&#39;/data/test1&#39;)

    # Write 2nd data and take backup
    backup_volume, backup2, data_checksum_2 = \
        create_backup_from_volume_attached_to_pod(client, core_api,
                                                  volume_name, test_pod_name,
                                                  data_path=&#39;/data/test2&#39;)

    # Remove test1 and test2 files
    command = &#39;rm /data/test1 /data/test2&#39;
    exec_command_in_pod(core_api, command, test_pod_name, &#39;default&#39;)

    # Write 3rd data and take backup
    backup_volume, backup3, data_checksum_3 = \
        create_backup_from_volume_attached_to_pod(client, core_api,
                                                  volume_name, test_pod_name,
                                                  data_path=&#39;/data/test3&#39;)

    delete_and_wait_pod(core_api, test_pod_name)

    # restore 1st backup and assert with data_checksum_1
    restored_data_checksum1, output, restore_pod_name = \
        restore_backup_and_get_data_checksum(client, core_api, backup1, pod,
                                             file_name=&#39;test1&#39;)
    assert data_checksum_1 == restored_data_checksum1[&#39;test1&#39;]

    delete_and_wait_pod(core_api, restore_pod_name)

    # restore 2nd backup and assert with data_checksum_1 and data_checksum_2
    restored_data_checksum2, output, restore_pod_name = \
        restore_backup_and_get_data_checksum(client, core_api, backup2, pod)
    assert data_checksum_1 == restored_data_checksum2[&#39;test1&#39;]
    assert data_checksum_2 == restored_data_checksum2[&#39;test2&#39;]

    delete_and_wait_pod(core_api, restore_pod_name)

    # restore 3rd backup and assert with data_checksum_3
    restored_data_checksum3, output, restore_pod_name = \
        restore_backup_and_get_data_checksum(client, core_api, backup3, pod,
                                             file_name=&#39;test3&#39;,
                                             command=r&#34;ls /data | grep &#39;test1\|test2&#39;&#34;)  # NOQA
    assert data_checksum_3 == restored_data_checksum3[&#39;test3&#39;]
    assert output == &#39;&#39;

    delete_and_wait_pod(core_api, restore_pod_name)

    # Delete the 2nd backup
    delete_backup(client, backup_volume.name, backup2.name)

    # restore 3rd backup again
    restored_data_checksum3, output, restore_pod_name = \
        restore_backup_and_get_data_checksum(client, core_api, backup3, pod,
                                             file_name=&#39;test3&#39;,
                                             command=r&#34;ls /data | grep &#39;test1\|test2&#39;&#34;)  # NOQA
    assert data_checksum_3 == restored_data_checksum3[&#39;test3&#39;]
    assert output == &#39;&#39;</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_restore_inc"><code class="name flex">
<span>def <span class="ident">test_restore_inc</span></span>(<span>set_random_backupstore, client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"><p>Test restore from disaster recovery volume (incremental restore)</p>
<p>Run test against all the backupstores</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate <code>data0</code>, write to the volume, make a backup <code>backup0</code></li>
<li>Create three DR(standby) volumes from the backup: <code>sb_volume0/1/2</code></li>
<li>Wait for all three DR volumes to start the initial restoration</li>
<li>Verify DR volumes's <code>lastBackup</code> is <code>backup0</code></li>
<li>Verify snapshot/pv/pvc/change backup target are not allowed as long
as the DR volume exists</li>
<li>Activate standby <code>sb_volume0</code> and attach it to check the volume data</li>
<li>Generate <code>data1</code> and write to the original volume and create <code>backup1</code></li>
<li>Make sure <code>sb_volume1</code>'s <code>lastBackup</code> field has been updated to
<code>backup1</code></li>
<li>Wait for <code>sb_volume1</code> to finish incremental restoration then activate</li>
<li>Attach and check <code>sb_volume1</code>'s data</li>
<li>Generate <code>data2</code> and write to the original volume and create <code>backup2</code></li>
<li>Make sure <code>sb_volume2</code>'s <code>lastBackup</code> field has been updated to
<code>backup1</code></li>
<li>Wait for <code>sb_volume2</code> to finish incremental restoration then activate</li>
<li>Attach and check <code>sb_volume2</code>'s data</li>
<li>Create PV, PVC and Pod to use <code>sb_volume2</code>, check PV/PVC/POD are good</li>
</ol>
<p>FIXME: Step 16 works because the disk will be treated as a unformatted disk</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_restore_inc(set_random_backupstore, client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test restore from disaster recovery volume (incremental restore)

    Run test against all the backupstores

    1. Create a volume and attach to the current node
    2. Generate `data0`, write to the volume, make a backup `backup0`
    3. Create three DR(standby) volumes from the backup: `sb_volume0/1/2`
    4. Wait for all three DR volumes to start the initial restoration
    5. Verify DR volumes&#39;s `lastBackup` is `backup0`
    6. Verify snapshot/pv/pvc/change backup target are not allowed as long
    as the DR volume exists
    7. Activate standby `sb_volume0` and attach it to check the volume data
    8. Generate `data1` and write to the original volume and create `backup1`
    9. Make sure `sb_volume1`&#39;s `lastBackup` field has been updated to
    `backup1`
    10. Wait for `sb_volume1` to finish incremental restoration then activate
    11. Attach and check `sb_volume1`&#39;s data
    12. Generate `data2` and write to the original volume and create `backup2`
    13. Make sure `sb_volume2`&#39;s `lastBackup` field has been updated to
    `backup1`
    14. Wait for `sb_volume2` to finish incremental restoration then activate
    15. Attach and check `sb_volume2`&#39;s data
    16. Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good

    FIXME: Step 16 works because the disk will be treated as a unformatted disk
    &#34;&#34;&#34;
    restore_inc_test(client, core_api, volume_name, pod)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_restore_inc_with_expansion"><code class="name flex">
<span>def <span class="ident">test_restore_inc_with_expansion</span></span>(<span>set_random_backupstore, client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"><p>Test restore from disaster recovery volume with volume expansion</p>
<p>Run test against a random backupstores</p>
<ol>
<li>Create a volume and attach to the current node</li>
<li>Generate <code>data0</code>, write to the volume, make a backup <code>backup0</code></li>
<li>Create three DR(standby) volumes from the backup: <code>dr_volume0/1/2</code></li>
<li>Wait for all three DR volumes to start the initial restoration</li>
<li>Verify DR volumes's <code>lastBackup</code> is <code>backup0</code></li>
<li>Verify snapshot/pv/pvc/change backup target are not allowed as long
as the DR volume exists</li>
<li>Activate standby <code>dr_volume0</code> and attach it to check the volume data</li>
<li>Expand the original volume. Make sure the expansion is successful.</li>
<li>Generate <code>data1</code> and write to the original volume and create <code>backup1</code></li>
<li>Make sure <code>dr_volume1</code>'s <code>lastBackup</code> field has been updated to
<code>backup1</code></li>
<li>Activate <code>dr_volume1</code> and check data <code>data0</code> and <code>data1</code></li>
<li>Generate <code>data2</code> and write to the original volume after original SIZE</li>
<li>Create <code>backup2</code></li>
<li>Wait for <code>dr_volume2</code> to finish expansion, show <code>backup2</code> as latest</li>
<li>Activate <code>dr_volume2</code> and verify <code>data2</code></li>
<li>Detach <code>dr_volume2</code></li>
<li>Create PV, PVC and Pod to use <code>sb_volume2</code>, check PV/PVC/POD are good</li>
</ol>
<p>FIXME: Step 16 works because the disk will be treated as a unformatted disk</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_restore_inc_with_expansion(set_random_backupstore, client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test restore from disaster recovery volume with volume expansion

    Run test against a random backupstores

    1. Create a volume and attach to the current node
    2. Generate `data0`, write to the volume, make a backup `backup0`
    3. Create three DR(standby) volumes from the backup: `dr_volume0/1/2`
    4. Wait for all three DR volumes to start the initial restoration
    5. Verify DR volumes&#39;s `lastBackup` is `backup0`
    6. Verify snapshot/pv/pvc/change backup target are not allowed as long
    as the DR volume exists
    7. Activate standby `dr_volume0` and attach it to check the volume data
    8. Expand the original volume. Make sure the expansion is successful.
    8. Generate `data1` and write to the original volume and create `backup1`
    9. Make sure `dr_volume1`&#39;s `lastBackup` field has been updated to
    `backup1`
    10. Activate `dr_volume1` and check data `data0` and `data1`
    11. Generate `data2` and write to the original volume after original SIZE
    12. Create `backup2`
    13. Wait for `dr_volume2` to finish expansion, show `backup2` as latest
    14. Activate `dr_volume2` and verify `data2`
    15. Detach `dr_volume2`
    16. Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good

    FIXME: Step 16 works because the disk will be treated as a unformatted disk
    &#34;&#34;&#34;
    lht_host_id = get_self_host_id()

    std_volume = create_and_check_volume(client, volume_name, 2, SIZE)
    std_volume.attach(hostId=lht_host_id)
    std_volume = common.wait_for_volume_healthy(client, volume_name)

    with pytest.raises(Exception) as e:
        std_volume.activate(frontend=VOLUME_FRONTEND_BLOCKDEV)
        assert &#34;already in active mode&#34; in str(e.value)

    data0 = {&#39;pos&#39;: 0, &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup0, _, data0 = create_backup(
        client, volume_name, data0)

    dr_volume0_name = &#34;dr-expand-0-&#34; + volume_name
    dr_volume1_name = &#34;dr-expand-1-&#34; + volume_name
    dr_volume2_name = &#34;dr-expand-2-&#34; + volume_name
    client.create_volume(name=dr_volume0_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=dr_volume1_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    client.create_volume(name=dr_volume2_name, size=SIZE,
                         numberOfReplicas=2, fromBackup=backup0.url,
                         frontend=&#34;&#34;, standby=True)
    wait_for_backup_restore_completed(client, dr_volume0_name, backup0.name)
    wait_for_backup_restore_completed(client, dr_volume1_name, backup0.name)
    wait_for_backup_restore_completed(client, dr_volume2_name, backup0.name)

    dr_volume0 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume0_name)
    dr_volume1 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume1_name)
    dr_volume2 = common.wait_for_volume_healthy_no_frontend(client,
                                                            dr_volume2_name)

    for i in range(RETRY_COUNTS):
        client.list_backupVolume()
        dr_volume0 = client.by_id_volume(dr_volume0_name)
        dr_volume1 = client.by_id_volume(dr_volume1_name)
        dr_volume2 = client.by_id_volume(dr_volume2_name)
        dr_engine0 = get_volume_engine(dr_volume0)
        dr_engine1 = get_volume_engine(dr_volume1)
        dr_engine2 = get_volume_engine(dr_volume2)
        if dr_volume0.restoreRequired is False or \
                dr_volume1.restoreRequired is False or \
                dr_volume2.restoreRequired is False or \
                not dr_engine0.lastRestoredBackup or \
                not dr_engine1.lastRestoredBackup or \
                not dr_engine2.lastRestoredBackup:
            time.sleep(RETRY_INTERVAL)
        else:
            break
    assert dr_volume0.standby is True
    assert dr_volume0.lastBackup == backup0.name
    assert dr_volume0.frontend == &#34;&#34;
    assert dr_volume0.restoreRequired is True
    dr_engine0 = get_volume_engine(dr_volume0)
    assert dr_engine0.lastRestoredBackup == backup0.name
    assert dr_engine0.requestedBackupRestore == backup0.name
    assert dr_volume1.standby is True
    assert dr_volume1.lastBackup == backup0.name
    assert dr_volume1.frontend == &#34;&#34;
    assert dr_volume1.restoreRequired is True
    dr_engine1 = get_volume_engine(dr_volume1)
    assert dr_engine1.lastRestoredBackup == backup0.name
    assert dr_engine1.requestedBackupRestore == backup0.name
    assert dr_volume2.standby is True
    assert dr_volume2.lastBackup == backup0.name
    assert dr_volume2.frontend == &#34;&#34;
    assert dr_volume2.restoreRequired is True
    dr_engine2 = get_volume_engine(dr_volume2)
    assert dr_engine2.lastRestoredBackup == backup0.name
    assert dr_engine2.requestedBackupRestore == backup0.name

    dr0_snaps = dr_volume0.snapshotList()
    assert len(dr0_snaps) == 2

    activate_standby_volume(client, dr_volume0_name)
    dr_volume0 = client.by_id_volume(dr_volume0_name)
    dr_volume0.attach(hostId=lht_host_id)
    dr_volume0 = common.wait_for_volume_healthy(client, dr_volume0_name)
    check_volume_data(dr_volume0, data0, False)

    expand_attached_volume(client, volume_name)
    std_volume = client.by_id_volume(volume_name)
    check_block_device_size(std_volume, int(EXPAND_SIZE))

    data1 = {&#39;pos&#39;: VOLUME_RWTEST_SIZE, &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup1, _, data1 = create_backup(
        client, volume_name, data1)

    check_volume_last_backup(client, dr_volume1_name, backup1.name)
    activate_standby_volume(client, dr_volume1_name)
    dr_volume1 = client.by_id_volume(dr_volume1_name)
    dr_volume1.attach(hostId=lht_host_id)
    dr_volume1 = common.wait_for_volume_healthy(client, dr_volume1_name)
    check_volume_data(dr_volume1, data0, False)
    check_volume_data(dr_volume1, data1, False)

    data2 = {&#39;pos&#39;: int(SIZE), &#39;len&#39;: VOLUME_RWTEST_SIZE,
             &#39;content&#39;: common.generate_random_data(VOLUME_RWTEST_SIZE)}
    bv, backup2, _, data2 = create_backup(
        client, volume_name, data2)
    assert backup2.volumeSize == EXPAND_SIZE

    wait_for_dr_volume_expansion(client, dr_volume2_name, EXPAND_SIZE)
    check_volume_last_backup(client, dr_volume2_name, backup2.name)
    activate_standby_volume(client, dr_volume2_name)
    dr_volume2 = client.by_id_volume(dr_volume2_name)
    dr_volume2.attach(hostId=lht_host_id)
    dr_volume2 = common.wait_for_volume_healthy(client, dr_volume2_name)
    check_volume_data(dr_volume2, data2)

    # allocated this active volume to a pod
    dr_volume2.detach(hostId=&#34;&#34;)
    dr_volume2 = common.wait_for_volume_detached(client, dr_volume2_name)

    create_pv_for_volume(client, core_api, dr_volume2, dr_volume2_name)
    create_pvc_for_volume(client, core_api, dr_volume2, dr_volume2_name)

    dr_volume2_pod_name = &#34;pod-&#34; + dr_volume2_name
    pod[&#39;metadata&#39;][&#39;name&#39;] = dr_volume2_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: dr_volume2_name,
        },
    }]
    create_and_wait_pod(core_api, pod)

    dr_volume2 = client.by_id_volume(dr_volume2_name)
    k_status = dr_volume2.kubernetesStatus
    workloads = k_status.workloadsStatus
    assert k_status.pvName == dr_volume2_name
    assert k_status.pvStatus == &#39;Bound&#39;
    assert len(workloads) == 1
    for i in range(RETRY_COUNTS):
        if workloads[0].podStatus == &#39;Running&#39;:
            break
        time.sleep(RETRY_INTERVAL)
        dr_volume2 = client.by_id_volume(dr_volume2_name)
        k_status = dr_volume2.kubernetesStatus
        workloads = k_status.workloadsStatus
        assert len(workloads) == 1
    assert workloads[0].podName == dr_volume2_pod_name
    assert workloads[0].podStatus == &#39;Running&#39;
    assert not workloads[0].workloadName
    assert not workloads[0].workloadType
    assert k_status.namespace == &#39;default&#39;
    assert k_status.pvcName == dr_volume2_name
    assert not k_status.lastPVCRefAt
    assert not k_status.lastPodRefAt

    delete_and_wait_pod(core_api, dr_volume2_pod_name)
    delete_and_wait_pvc(core_api, dr_volume2_name)
    delete_and_wait_pv(core_api, dr_volume2_name)

    # cleanup
    std_volume.detach(hostId=&#34;&#34;)
    dr_volume0.detach(hostId=&#34;&#34;)
    dr_volume1.detach(hostId=&#34;&#34;)
    std_volume = common.wait_for_volume_detached(client, volume_name)
    dr_volume0 = common.wait_for_volume_detached(client, dr_volume0_name)
    dr_volume1 = common.wait_for_volume_detached(client, dr_volume1_name)
    dr_volume2 = common.wait_for_volume_detached(client, dr_volume2_name)

    backupstore_cleanup(client)

    client.delete(std_volume)
    client.delete(dr_volume0)
    client.delete(dr_volume1)
    client.delete(dr_volume2)

    wait_for_volume_delete(client, volume_name)
    wait_for_volume_delete(client, dr_volume0_name)
    wait_for_volume_delete(client, dr_volume1_name)
    wait_for_volume_delete(client, dr_volume2_name)

    volumes = client.list_volume().data
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_running_volume_with_scheduling_failure"><code class="name flex">
<span>def <span class="ident">test_running_volume_with_scheduling_failure</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"><p>Test if the running volume still work fine
when there is a scheduling failed replica</p>
<p>Prerequisite:
Setting "soft anti-affinity" is false.
Setting "replica-replenishment-wait-interval" is 0</p>
<ol>
<li>Create a volume, then create the corresponding PV, PVC and Pod.</li>
<li>Wait for the pod running and the volume healthy.</li>
<li>Write data to the pod volume and get the md5sum.</li>
<li>Disable the scheduling for a node contains a running replica.</li>
<li>Crash the replica on the scheduling disabled node for the volume.</li>
<li>Wait for the scheduling failure which is caused
by the new replica creation.</li>
<li>Verify:
7.1. <code>volume.ready == True</code>.
7.2. <code>volume.conditions[scheduled].status == False</code>.
7.3. the volume is Degraded.
7.4. the new replica is created but it is not running.</li>
<li>Write more data to the volume and get the md5sum</li>
<li>Delete the pod and wait for the volume detached.</li>
<li>Verify the scheduling failed replica is removed.</li>
<li>Verify:
11.1. <code>volume.ready == True</code>.
11.2. <code>volume.conditions[scheduled].status == True</code></li>
<li>Recreate a new pod for the volume and wait for the pod running.</li>
<li>Validate the volume content, then check if data writing looks fine.</li>
<li>Clean up pod, PVC, and PV.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_running_volume_with_scheduling_failure(
        client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Test if the running volume still work fine
    when there is a scheduling failed replica

    Prerequisite:
    Setting &#34;soft anti-affinity&#34; is false.
    Setting &#34;replica-replenishment-wait-interval&#34; is 0

    1. Create a volume, then create the corresponding PV, PVC and Pod.
    2. Wait for the pod running and the volume healthy.
    3. Write data to the pod volume and get the md5sum.
    4. Disable the scheduling for a node contains a running replica.
    5. Crash the replica on the scheduling disabled node for the volume.
    6. Wait for the scheduling failure which is caused
       by the new replica creation.
    7. Verify:
      7.1. `volume.ready == True`.
      7.2. `volume.conditions[scheduled].status == False`.
      7.3. the volume is Degraded.
      7.4. the new replica is created but it is not running.
    8. Write more data to the volume and get the md5sum
    9. Delete the pod and wait for the volume detached.
    10. Verify the scheduling failed replica is removed.
    11. Verify:
      11.1. `volume.ready == True`.
      11.2. `volume.conditions[scheduled].status == True`
    12. Recreate a new pod for the volume and wait for the pod running.
    13. Validate the volume content, then check if data writing looks fine.
    14. Clean up pod, PVC, and PV.
    &#34;&#34;&#34;

    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_node_soft_anti_affinity_setting, value=&#34;false&#34;)

    replenish_wait_setting = \
        client.by_id_setting(SETTING_REPLICA_REPLENISHMENT_WAIT_INTERVAL)
    client.update(replenish_wait_setting, value=&#34;0&#34;)

    data_path1 = &#34;/data/test1&#34;
    test_pv_name = &#34;pv-&#34; + volume_name
    test_pvc_name = &#34;pvc-&#34; + volume_name
    test_pod_name = &#34;pod-&#34; + volume_name

    volume = create_and_check_volume(client, volume_name, size=str(1 * Gi))
    create_pv_for_volume(client, core_api, volume, test_pv_name)
    create_pvc_for_volume(client, core_api, volume, test_pvc_name)

    pod[&#39;metadata&#39;][&#39;name&#39;] = test_pod_name
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#39;name&#39;: pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;volumeMounts&#39;][0][&#39;name&#39;],
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: test_pvc_name,
        },
    }]
    create_and_wait_pod(core_api, pod)
    wait_for_volume_healthy(client, volume_name)
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path1, DATA_SIZE_IN_MB_1)
    original_md5sum1 = get_pod_data_md5sum(core_api, test_pod_name,
                                           data_path1)

    volume = client.by_id_volume(volume_name)
    existing_replicas = {}
    for r in volume.replicas:
        existing_replicas[r.name] = r
    node = client.by_id_node(volume.replicas[0].hostId)
    node = set_node_scheduling(client, node, allowScheduling=False)
    common.wait_for_node_update(client, node.id,
                                &#34;allowScheduling&#34;, False)

    crash_replica_processes(client, core_api, volume_name,
                            replicas=[volume.replicas[0]],
                            wait_to_fail=False)

    # Wait for scheduling failure.
    # It means the new replica is created but fails to be scheduled.
    wait_for_volume_condition_scheduled(client, volume_name, &#34;status&#34;,
                                        CONDITION_STATUS_FALSE)
    wait_for_volume_condition_scheduled(client, volume_name, &#34;reason&#34;,
                                        CONDITION_REASON_SCHEDULING_FAILURE)
    volume = wait_for_volume_degraded(client, volume_name)
    assert len(volume.replicas) == 4
    assert volume.ready
    for r in volume.replicas:
        if r.name not in existing_replicas:
            new_replica = r
            break
    assert new_replica
    assert not new_replica.running
    assert not new_replica.hostId

    data_path2 = &#34;/data/test2&#34;
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path2, DATA_SIZE_IN_MB_1)
    original_md5sum2 = get_pod_data_md5sum(core_api, test_pod_name, data_path2)

    delete_and_wait_pod(core_api, test_pod_name)
    wait_for_volume_detached(client, volume_name)
    volume = wait_for_volume_condition_scheduled(client, volume_name, &#34;status&#34;,
                                                 CONDITION_STATUS_TRUE)
    assert volume.ready
    # The scheduling failed replica will be removed
    # so that the volume can be reattached later.
    assert len(volume.replicas) == 3
    for r in volume.replicas:
        assert r.hostId != &#34;&#34;
        assert r.name != new_replica.name

    create_and_wait_pod(core_api, pod)
    wait_for_volume_degraded(client, volume_name)

    md5sum1 = get_pod_data_md5sum(core_api, test_pod_name, data_path1)
    assert md5sum1 == original_md5sum1
    md5sum2 = get_pod_data_md5sum(core_api, test_pod_name, data_path2)
    assert md5sum2 == original_md5sum2

    # The data writing is fine
    data_path3 = &#34;/data/test3&#34;
    write_pod_volume_random_data(core_api, test_pod_name,
                                 data_path3, DATA_SIZE_IN_MB_1)
    get_pod_data_md5sum(core_api, test_pod_name, data_path3)

    delete_and_wait_pod(core_api, test_pod_name)
    delete_and_wait_pvc(core_api, test_pvc_name)
    delete_and_wait_pv(core_api, test_pv_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_setting_default_replica_count"><code class="name flex">
<span>def <span class="ident">test_setting_default_replica_count</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test <code>Default Replica Count</code> setting</p>
<ol>
<li>Set default replica count in the global settings to 5</li>
<li>Create a volume without specify the replica count</li>
<li>The volume should have 5 replicas (instead of the previous default 3)</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_setting_default_replica_count(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test `Default Replica Count` setting

    1. Set default replica count in the global settings to 5
    2. Create a volume without specify the replica count
    3. The volume should have 5 replicas (instead of the previous default 3)
    &#34;&#34;&#34;
    setting = client.by_id_setting(common.SETTING_DEFAULT_REPLICA_COUNT)
    old_value = setting.value
    setting = client.update(setting, value=&#34;5&#34;)

    volume = client.create_volume(name=volume_name, size=SIZE)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert len(volume.replicas) == int(setting.value)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    setting = client.update(setting, value=old_value)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_settings"><code class="name flex">
<span>def <span class="ident">test_settings</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"><p>Check input for settings</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_settings(client):  # NOQA
    &#34;&#34;&#34;
    Check input for settings
    &#34;&#34;&#34;

    setting_names = [common.SETTING_BACKUP_TARGET,
                     common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET,
                     common.SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE,
                     common.SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE,
                     common.SETTING_DEFAULT_REPLICA_COUNT]
    settings = client.list_setting()

    settingMap = {}
    for setting in settings:
        settingMap[setting.name] = setting

    for name in setting_names:
        assert settingMap[name] is not None
        assert settingMap[name].definition.description is not None

    for name in setting_names:
        setting = client.by_id_setting(name)
        assert settingMap[name].value == setting.value

        old_value = setting.value

        if name == common.SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-100&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            setting = client.update(setting, value=&#34;200&#34;)
            assert setting.value == &#34;200&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;200&#34;
        elif name == common.SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;300&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-30&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            setting = client.update(setting, value=&#34;30&#34;)
            assert setting.value == &#34;30&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;30&#34;
        elif name == common.SETTING_BACKUP_TARGET:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue$test&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            setting = client.update(setting, value=&#34;nfs://test&#34;)
            assert setting.value == &#34;nfs://test&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;nfs://test&#34;
        elif name == common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET:
            setting = client.update(setting, value=&#34;testvalue&#34;)
            assert setting.value == &#34;testvalue&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;testvalue&#34;
        elif name == common.SETTING_DEFAULT_REPLICA_COUNT:
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;-1&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;testvalue&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            with pytest.raises(Exception) as e:
                client.update(setting, value=&#34;21&#34;)
            assert name+&#34; with invalid value &#34; in \
                   str(e.value)
            setting = client.update(setting, value=&#34;2&#34;)
            assert setting.value == &#34;2&#34;
            setting = client.by_id_setting(name)
            assert setting.value == &#34;2&#34;

        setting = client.update(setting, value=old_value)
        assert setting.value == old_value</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_snapshot"><code class="name flex">
<span>def <span class="ident">test_snapshot</span></span>(<span>client, volume_name, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"><p>Test snapshot operations</p>
<ol>
<li>Create a volume and attach to the node</li>
<li>Create the empty snapshot <code>snap1</code></li>
<li>Generate and write data <code>snap2_data</code>, then create <code>snap2</code></li>
<li>Generate and write data <code>snap3_data</code>, then create <code>snap3</code></li>
<li>List snapshot. Validate the snapshot chain relationship</li>
<li>Mark <code>snap3</code> as removed. Make sure volume's data didn't change</li>
<li>List snapshot. Make sure <code>snap3</code> is marked as removed</li>
<li>Detach and reattach the volume in maintenance mode.</li>
<li>Make sure the volume frontend is still <code>blockdev</code> but disabled</li>
<li>Revert to <code>snap2</code></li>
<li>Detach and reattach the volume with frontend enabled</li>
<li>Make sure volume's data is <code>snap2_data</code></li>
<li>List snapshot. Make sure <code>volume-head</code> is now <code>snap2</code>'s child</li>
<li>Delete <code>snap1</code> and <code>snap2</code></li>
<li>Purge the snapshot.</li>
<li>List the snapshot, make sure <code>snap1</code> and <code>snap3</code>
are gone. <code>snap2</code> is marked as removed.</li>
<li>Check volume data, make sure it's still <code>snap2_data</code>.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_snapshot(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    &#34;&#34;&#34;
    Test snapshot operations

    1. Create a volume and attach to the node
    2. Create the empty snapshot `snap1`
    3. Generate and write data `snap2_data`, then create `snap2`
    4. Generate and write data `snap3_data`, then create `snap3`
    5. List snapshot. Validate the snapshot chain relationship
    6. Mark `snap3` as removed. Make sure volume&#39;s data didn&#39;t change
    7. List snapshot. Make sure `snap3` is marked as removed
    8. Detach and reattach the volume in maintenance mode.
    9. Make sure the volume frontend is still `blockdev` but disabled
    10. Revert to `snap2`
    11. Detach and reattach the volume with frontend enabled
    12. Make sure volume&#39;s data is `snap2_data`
    13. List snapshot. Make sure `volume-head` is now `snap2`&#39;s child
    14. Delete `snap1` and `snap2`
    15. Purge the snapshot.
    16. List the snapshot, make sure `snap1` and `snap3`
    are gone. `snap2` is marked as removed.
    17. Check volume data, make sure it&#39;s still `snap2_data`.
    &#34;&#34;&#34;
    snapshot_test(client, volume_name, backing_image)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_snapshot_prune"><code class="name flex">
<span>def <span class="ident">test_snapshot_prune</span></span>(<span>client, volume_name, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"><p>Test removing the snapshot directly behinds the volume head would trigger
snapshot prune. Snapshot pruning means removing the overlapping part from
the snapshot based on the volume head content.</p>
<ol>
<li>Create a volume and attach to the node</li>
<li>Generate and write data <code>snap1_data</code>, then create <code>snap1</code></li>
<li>Generate and write data <code>snap2_data</code> with the same offset.</li>
<li>Mark <code>snap1</code> as removed.
Make sure volume's data didn't change.
But all data of the <code>snap1</code> will be pruned.</li>
<li>Detach and expand the volume, then wait for the expansion done.
This will implicitly create a new snapshot <code>snap2</code>.</li>
<li>Attach the volume.
Make sure there is a system snapshot with the old size.</li>
<li>Generate and write data <code>snap3_data</code> which is partially overlapped with
<code>snap2_data</code>, plus one extra data chunk in the expanded part.</li>
<li>Mark <code>snap2</code> as removed then do snapshot purge.
Make sure volume's data didn't change.
But the overlapping part of <code>snap2</code> will be pruned.</li>
<li>Create <code>snap3</code>.</li>
<li>Do snapshot purge for the volume. Make sure <code>snap2</code> will be removed.</li>
<li>Generate and write data <code>snap4_data</code> which has no overlapping with
<code>snap3_data</code>.</li>
<li>Mark <code>snap3</code> as removed.
Make sure volume's data didn't change.
But there is no change for <code>snap3</code>.</li>
<li>Create <code>snap4</code>.</li>
<li>Generate and write data <code>snap5_data</code>, then create <code>snap5</code>.</li>
<li>Detach and reattach the volume in maintenance mode.</li>
<li>Make sure the volume frontend is still <code>blockdev</code> but disabled</li>
<li>Revert to <code>snap4</code></li>
<li>Detach and reattach the volume with frontend enabled</li>
<li>Make sure volume's data is correct.</li>
<li>List snapshot. Make sure <code>volume-head</code> is now <code>snap4</code>'s child</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_snapshot_prune(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    &#34;&#34;&#34;
    Test removing the snapshot directly behinds the volume head would trigger
    snapshot prune. Snapshot pruning means removing the overlapping part from
    the snapshot based on the volume head content.


    1.  Create a volume and attach to the node
    2.  Generate and write data `snap1_data`, then create `snap1`
    3.  Generate and write data `snap2_data` with the same offset.
    4.  Mark `snap1` as removed.
        Make sure volume&#39;s data didn&#39;t change.
        But all data of the `snap1` will be pruned.
    5.  Detach and expand the volume, then wait for the expansion done.
        This will implicitly create a new snapshot `snap2`.
    6.  Attach the volume.
        Make sure there is a system snapshot with the old size.
    7.  Generate and write data `snap3_data` which is partially overlapped with
       `snap2_data`, plus one extra data chunk in the expanded part.
    8.  Mark `snap2` as removed then do snapshot purge.
        Make sure volume&#39;s data didn&#39;t change.
        But the overlapping part of `snap2` will be pruned.
    9.  Create `snap3`.
    10. Do snapshot purge for the volume. Make sure `snap2` will be removed.
    11. Generate and write data `snap4_data` which has no overlapping with
        `snap3_data`.
    12. Mark `snap3` as removed.
        Make sure volume&#39;s data didn&#39;t change.
        But there is no change for `snap3`.
    13. Create `snap4`.
    14. Generate and write data `snap5_data`, then create `snap5`.
    15. Detach and reattach the volume in maintenance mode.
    16. Make sure the volume frontend is still `blockdev` but disabled
    17. Revert to `snap4`
    18. Detach and reattach the volume with frontend enabled
    19. Make sure volume&#39;s data is correct.
    20. List snapshot. Make sure `volume-head` is now `snap4`&#39;s child
    &#34;&#34;&#34;
    snapshot_prune_test(client, volume_name, backing_image)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_snapshot_prune_and_coalesce_simultaneously"><code class="name flex">
<span>def <span class="ident">test_snapshot_prune_and_coalesce_simultaneously</span></span>(<span>client, volume_name, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"><p>Test the prune for the snapshot directly behinds the volume head would be
handled after all snapshot coalescing done.</p>
<ol>
<li>Create a volume and attach to the node</li>
<li>Generate and write 1st data chunk <code>snap1_data</code>, then create <code>snap1</code></li>
<li>Generate and write 2nd data chunk <code>snap2_data</code>, then create <code>snap2</code></li>
<li>Generate and write 3rd data chunk <code>snap3_data</code>, then create <code>snap3</code></li>
<li>Generate and write 4th data chunk <code>snap4_data</code>, then create <code>snap4</code></li>
<li>Overwrite all existing data chunks in the volume head.</li>
<li>Mark all snapshots as <code>Removed</code>,
then start snapshot purge and wait for complete.</li>
<li>List snapshot.
Make sure there are only 2 snapshots left: <code>volume-head</code> and <code>snap4</code>.
And <code>snap4</code> is an empty snapshot.</li>
<li>Make sure volume's data is correct.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_snapshot_prune_and_coalesce_simultaneously(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    &#34;&#34;&#34;
    Test the prune for the snapshot directly behinds the volume head would be
    handled after all snapshot coalescing done.


    1. Create a volume and attach to the node
    2. Generate and write 1st data chunk `snap1_data`, then create `snap1`
    3. Generate and write 2nd data chunk `snap2_data`, then create `snap2`
    4. Generate and write 3rd data chunk `snap3_data`, then create `snap3`
    5. Generate and write 4th data chunk `snap4_data`, then create `snap4`
    6. Overwrite all existing data chunks in the volume head.
    7. Mark all snapshots as `Removed`,
       then start snapshot purge and wait for complete.
    8. List snapshot.
       Make sure there are only 2 snapshots left: `volume-head` and `snap4`.
       And `snap4` is an empty snapshot.
    9. Make sure volume&#39;s data is correct.
    &#34;&#34;&#34;
    snapshot_prune_and_coalesce_simultaneously(
        client, volume_name, backing_image)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_space_usage_for_rebuilding_only_volume"><code class="name flex">
<span>def <span class="ident">test_space_usage_for_rebuilding_only_volume</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the space usage of a volume with rebuilding only.</p>
<p>Prepare:
1. Create a 7Gi volume and attach to the node.
2. Make a filesystem then mount this volume.
3. Make this volume as a disk of the node, and disable the scheduling for
the default disk.</p>
<p>Case 1: the worst scenario
1. Create a new volume with 2Gi spec size.
2. Write 2Gi data (using <code>dd</code>) to the volume.
3. Take a snapshot then mark this snapshot as Removed.
(this snapshot won't be deleted immediately.)
4. Write 2Gi data (using <code>dd</code>) to the volume again.
5. Delete a random replica to trigger the rebuilding.
6. Write 2Gi data once the rebuilding is trigger (new replica is created).
7. Wait for the rebuilding complete. And verify the volume actual size
won't be greater than 3x of the volume spec size.
8. Delete the volume.</p>
<p>Case 2: the normal scenario
1. Create a new volume with 3Gi spec size.
2. Write 3Gi data (using <code>dd</code>) to the volume.
3. Take a snapshot then mark this snapshot as Removed.
(this snapshot won't be deleted immediately.)
4. Write 3Gi data (using <code>dd</code>) to the volume again.
5. Delete a random replica to trigger the rebuilding.
6. Wait for the rebuilding complete. And verify the volume actual size
won't be greater than 2x of the volume spec size.
7. Delete the volume.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;TODO&#34;)  # NOQA
def test_space_usage_for_rebuilding_only_volume():  # NOQA
    &#34;&#34;&#34;
    Test the space usage of a volume with rebuilding only.

    Prepare:
    1. Create a 7Gi volume and attach to the node.
    2. Make a filesystem then mount this volume.
    3. Make this volume as a disk of the node, and disable the scheduling for
       the default disk.

    Case 1: the worst scenario
    1. Create a new volume with 2Gi spec size.
    2. Write 2Gi data (using `dd`) to the volume.
    3. Take a snapshot then mark this snapshot as Removed.
       (this snapshot won&#39;t be deleted immediately.)
    4. Write 2Gi data (using `dd`) to the volume again.
    5. Delete a random replica to trigger the rebuilding.
    6. Write 2Gi data once the rebuilding is trigger (new replica is created).
    7. Wait for the rebuilding complete. And verify the volume actual size
       won&#39;t be greater than 3x of the volume spec size.
    8. Delete the volume.

    Case 2: the normal scenario
    1. Create a new volume with 3Gi spec size.
    2. Write 3Gi data (using `dd`) to the volume.
    3. Take a snapshot then mark this snapshot as Removed.
       (this snapshot won&#39;t be deleted immediately.)
    4. Write 3Gi data (using `dd`) to the volume again.
    5. Delete a random replica to trigger the rebuilding.
    6. Wait for the rebuilding complete. And verify the volume actual size
       won&#39;t be greater than 2x of the volume spec size.
    7. Delete the volume.

    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_storage_class_from_backup"><code class="name flex">
<span>def <span class="ident">test_storage_class_from_backup</span></span>(<span>set_random_backupstore, volume_name, pvc_name, storage_class, client, core_api, pod_make)</span>
</code></dt>
<dd>
<div class="desc"><p>Test restore backup using StorageClass</p>
<ol>
<li>Create volume and PV/PVC/POD</li>
<li>Write <code>test_data</code> into pod</li>
<li>Create a snapshot and back it up. Get the backup URL</li>
<li>Create a new StorageClass <code>longhorn-from-backup</code> and set backup URL.</li>
<li>Use <code>longhorn-from-backup</code> to create a new PVC</li>
<li>Wait for the volume to be created and complete the restoration.</li>
<li>Create the pod using the PVC. Verify the data</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_storage_class_from_backup(set_random_backupstore, volume_name, pvc_name, storage_class, client, core_api, pod_make):  # NOQA
    &#34;&#34;&#34;
    Test restore backup using StorageClass

    1. Create volume and PV/PVC/POD
    2. Write `test_data` into pod
    3. Create a snapshot and back it up. Get the backup URL
    4. Create a new StorageClass `longhorn-from-backup` and set backup URL.
    5. Use `longhorn-from-backup` to create a new PVC
    6. Wait for the volume to be created and complete the restoration.
    7. Create the pod using the PVC. Verify the data
    &#34;&#34;&#34;
    VOLUME_SIZE = str(DEFAULT_VOLUME_SIZE * Gi)

    pv_name = pvc_name

    volume = create_and_check_volume(
        client,
        volume_name,
        size=VOLUME_SIZE
    )

    wait_for_volume_detached(client, volume_name)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)

    pod_manifest = pod_make()
    pod_manifest[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    pod_name = pod_manifest[&#39;metadata&#39;][&#39;name&#39;]
    create_and_wait_pod(core_api, pod_manifest)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)
    write_pod_volume_data(core_api, pod_name, test_data)

    volume_id = client.by_id_volume(volume_name)
    snapshot = volume_id.snapshotCreate()

    volume_id.snapshotBackup(name=snapshot.name)
    wait_for_backup_completion(client, volume_name, snapshot.name)
    bv, b = find_backup(client, volume_name, snapshot.name)

    backup_url = b.url

    storage_class[&#39;metadata&#39;][&#39;name&#39;] = &#34;longhorn-from-backup&#34;
    storage_class[&#39;parameters&#39;][&#39;fromBackup&#39;] = backup_url

    create_storage_class(storage_class)

    backup_pvc_name = generate_volume_name()

    backup_pvc_spec = {
        &#34;apiVersion&#34;: &#34;v1&#34;,
        &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;,
        &#34;metadata&#34;: {
                &#34;name&#34;: backup_pvc_name,
        },
        &#34;spec&#34;: {
            &#34;accessModes&#34;: [
                &#34;ReadWriteOnce&#34;
            ],
            &#34;storageClassName&#34;: storage_class[&#39;metadata&#39;][&#39;name&#39;],
            &#34;resources&#34;: {
                &#34;requests&#34;: {
                    &#34;storage&#34;: VOLUME_SIZE
                }
            }
        }
    }

    volume_count = len(client.list_volume())

    core_api.create_namespaced_persistent_volume_claim(
        &#39;default&#39;,
        backup_pvc_spec
    )

    backup_volume_created = False

    for i in range(RETRY_COUNTS):
        if len(client.list_volume()) == volume_count + 1:
            backup_volume_created = True
            break
        time.sleep(RETRY_INTERVAL)

    assert backup_volume_created

    for i in range(RETRY_COUNTS):
        pvc_status = core_api.read_namespaced_persistent_volume_claim_status(
            name=backup_pvc_name,
            namespace=&#39;default&#39;
        )

        if pvc_status.status.phase == &#39;Bound&#39;:
            break
        time.sleep(RETRY_INTERVAL)

    found = False
    for i in range(RETRY_COUNTS):
        volumes = client.list_volume()
        for volume in volumes:
            if volume.kubernetesStatus.pvcName == backup_pvc_name:
                backup_volume_name = volume.name
                found = True
                break
        if found:
            break
        time.sleep(RETRY_INTERVAL)
    assert found

    wait_for_volume_restoration_completed(client, backup_volume_name)
    wait_for_volume_detached(client, backup_volume_name)

    backup_pod_manifest = pod_make(name=&#34;backup-pod&#34;)
    backup_pod_manifest[&#39;spec&#39;][&#39;volumes&#39;] = \
        [create_pvc_spec(backup_pvc_name)]
    backup_pod_name = backup_pod_manifest[&#39;metadata&#39;][&#39;name&#39;]
    create_and_wait_pod(core_api, backup_pod_manifest)

    restored_data = read_volume_data(core_api, backup_pod_name)
    assert test_data == restored_data</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_basic"><code class="name flex">
<span>def <span class="ident">test_volume_basic</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test basic volume operations:</p>
<ol>
<li>Check volume name and parameter</li>
<li>Create a volume and attach to the current node, then check volume states</li>
<li>Check soft anti-affinity rule</li>
<li>Write then read back to check volume data</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_volume_basic(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic volume operations:

    1. Check volume name and parameter
    2. Create a volume and attach to the current node, then check volume states
    3. Check soft anti-affinity rule
    4. Write then read back to check volume data
    &#34;&#34;&#34;
    volume_basic_test(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_iscsi_basic"><code class="name flex">
<span>def <span class="ident">test_volume_iscsi_basic</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test basic volume operations with iscsi frontend</p>
<ol>
<li>Create and attach a volume with iscsi frontend</li>
<li>Check the volume endpoint and connect it using the iscsi
initator on the node.</li>
<li>Write then read back volume data for validation</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_volume_iscsi_basic(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test basic volume operations with iscsi frontend

    1. Create and attach a volume with iscsi frontend
    2. Check the volume endpoint and connect it using the iscsi
    initator on the node.
    3. Write then read back volume data for validation

    &#34;&#34;&#34;
    volume_iscsi_basic_test(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_multinode"><code class="name flex">
<span>def <span class="ident">test_volume_multinode</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test the volume can be attached on multiple nodes</p>
<ol>
<li>Create one volume</li>
<li>Attach it on every node once, verify the state, then detach it</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_volume_multinode(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test the volume can be attached on multiple nodes

    1. Create one volume
    2. Attach it on every node once, verify the state, then detach it
    &#34;&#34;&#34;
    hosts = [node[&#39;name&#39;] for node in client.list_node()]

    volume = client.create_volume(name=volume_name,
                                  size=SIZE,
                                  numberOfReplicas=2)
    volume = common.wait_for_volume_detached(client,
                                             volume_name)

    for host_id in hosts:
        volume = volume.attach(hostId=host_id)
        volume = common.wait_for_volume_healthy(client,
                                                volume_name)
        engine = get_volume_engine(volume)
        assert engine.hostId == host_id
        volume = volume.detach(hostId=&#34;&#34;)
        volume = common.wait_for_volume_detached(client,
                                                 volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    volumes = client.list_volume()
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_scheduling_failure"><code class="name flex">
<span>def <span class="ident">test_volume_scheduling_failure</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test fail to schedule by disable scheduling for all the nodes</p>
<p>Also test cannot attach a scheduling failed volume</p>
<ol>
<li>Disable <code>allowScheduling</code> for all nodes</li>
<li>Create a volume.</li>
<li>Verify the volume condition <code>Scheduled</code> is false</li>
<li>Verify the volume is not ready for workloads</li>
<li>Verify attaching the volume will result in error</li>
<li>Enable <code>allowScheduling</code> for all nodes</li>
<li>Volume should be automatically scheduled (condition become true)</li>
<li>Volume can be attached now</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest  # NOQA
def test_volume_scheduling_failure(client, volume_name):  # NOQA
    &#39;&#39;&#39;
    Test fail to schedule by disable scheduling for all the nodes

    Also test cannot attach a scheduling failed volume

    1. Disable `allowScheduling` for all nodes
    2. Create a volume.
    3. Verify the volume condition `Scheduled` is false
    4. Verify the volume is not ready for workloads
    5. Verify attaching the volume will result in error
    6. Enable `allowScheduling` for all nodes
    7. Volume should be automatically scheduled (condition become true)
    8. Volume can be attached now
    &#39;&#39;&#39;
    nodes = client.list_node()
    assert len(nodes) &gt; 0

    for node in nodes:
        node = set_node_scheduling(client, node, allowScheduling=False)
        node = common.wait_for_node_update(client, node.id,
                                           &#34;allowScheduling&#34;, False)

    volume = client.create_volume(name=volume_name, size=SIZE,
                                  numberOfReplicas=3)

    volume = common.wait_for_volume_condition_scheduled(client, volume_name,
                                                        &#34;status&#34;,
                                                        CONDITION_STATUS_FALSE)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert not volume.ready
    self_node = get_self_host_id()
    with pytest.raises(Exception) as e:
        volume.attach(hostId=self_node)
    assert &#34;unable to attach volume&#34; in str(e.value)

    for node in nodes:
        node = set_node_scheduling(client, node, allowScheduling=True)
        node = common.wait_for_node_update(client, node.id,
                                           &#34;allowScheduling&#34;, True)

    volume = common.wait_for_volume_condition_scheduled(client, volume_name,
                                                        &#34;status&#34;,
                                                        CONDITION_STATUS_TRUE)
    volume = common.wait_for_volume_detached(client, volume_name)
    volume = volume.attach(hostId=self_node)
    volume = common.wait_for_volume_healthy(client, volume_name)
    endpoint = get_volume_endpoint(volume)
    volume_rw_test(endpoint)

    volume = volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_toomanysnapshots_condition"><code class="name flex">
<span>def <span class="ident">test_volume_toomanysnapshots_condition</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test Volume TooManySnapshots Condition</p>
<ol>
<li>Create a volume and attach it to a node.</li>
<li>Check the 'TooManySnapshots' condition is False.</li>
<li>Writing data to this volume and meanwhile taking 100 snapshots.</li>
<li>Check the 'TooManySnapshots' condition is True.</li>
<li>Take one more snapshot to make sure snapshots works fine.</li>
<li>Delete 2 snapshots, and check the 'TooManySnapshots' condition is
False.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_volume_toomanysnapshots_condition(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test Volume TooManySnapshots Condition

    1. Create a volume and attach it to a node.
    2. Check the &#39;TooManySnapshots&#39; condition is False.
    3. Writing data to this volume and meanwhile taking 100 snapshots.
    4. Check the &#39;TooManySnapshots&#39; condition is True.
    5. Take one more snapshot to make sure snapshots works fine.
    6. Delete 2 snapshots, and check the &#39;TooManySnapshots&#39; condition is
       False.
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name)
    self_hostId = get_self_host_id()
    volume = volume.attach(hostId=self_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)

    snap = {}
    max_count = 100
    for i in range(max_count):
        write_volume_random_data(volume, {})

        count = i + 1
        snap[count] = create_snapshot(client, volume_name)

        if count &lt; max_count:
            volume = client.by_id_volume(volume_name)
            assert volume.conditions.toomanysnapshots.status == &#34;False&#34;
        else:
            wait_for_volume_condition_toomanysnapshots(client, volume_name,
                                                       &#34;status&#34;, &#34;True&#34;)

    snap[max_count + 1] = create_snapshot(client, volume_name)
    wait_for_volume_condition_toomanysnapshots(client, volume_name,
                                               &#34;status&#34;, &#34;True&#34;)

    volume = client.by_id_volume(volume_name)
    volume.snapshotDelete(name=snap[100].name)
    volume.snapshotDelete(name=snap[99].name)

    volume.snapshotPurge()
    volume = wait_for_snapshot_purge(client, volume_name,
                                     snap[100].name, snap[99].name)

    wait_for_volume_condition_toomanysnapshots(client, volume_name,
                                               &#34;status&#34;, &#34;False&#34;)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_volume_update_replica_count"><code class="name flex">
<span>def <span class="ident">test_volume_update_replica_count</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Test updating volume's replica count</p>
<ol>
<li>Create a volume with 2 replicas</li>
<li>Attach the volume</li>
<li>Increase the replica to 3.</li>
<li>Volume will become degraded and start rebuilding</li>
<li>Wait for rebuilding to complete</li>
<li>Update the replica count to 2. Volume should remain healthy</li>
<li>Remove 1 replicas, so there will be 2 replicas in the volume</li>
<li>Verify the volume is still healthy</li>
</ol>
<p>Volume should always be healthy even only with 2 replicas.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_volume_update_replica_count(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test updating volume&#39;s replica count

    1. Create a volume with 2 replicas
    2. Attach the volume
    3. Increase the replica to 3.
    4. Volume will become degraded and start rebuilding
    5. Wait for rebuilding to complete
    6. Update the replica count to 2. Volume should remain healthy
    7. Remove 1 replicas, so there will be 2 replicas in the volume
    8. Verify the volume is still healthy

    Volume should always be healthy even only with 2 replicas.
    &#34;&#34;&#34;
    host_id = get_self_host_id()

    replica_count = 2
    volume = create_and_check_volume(client, volume_name, replica_count)

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    replica_count = 3
    volume = volume.updateReplicaCount(replicaCount=replica_count)
    volume = common.wait_for_volume_degraded(client, volume_name)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == replica_count

    old_replica_count = replica_count
    replica_count = 2
    volume = volume.updateReplicaCount(replicaCount=replica_count)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == old_replica_count

    volume.replicaRemove(name=volume.replicas[0].name)

    volume = common.wait_for_volume_replica_count(client, volume_name,
                                                  replica_count)
    assert volume.robustness == &#34;healthy&#34;
    assert len(volume.replicas) == replica_count

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.test_workload_with_fsgroup"><code class="name flex">
<span>def <span class="ident">test_workload_with_fsgroup</span></span>(<span>core_api, statefulset)</span>
</code></dt>
<dd>
<div class="desc"><ol>
<li>Deploy a StatefulSet workload that uses Longhorn volume and has
securityContext set:
<code>securityContext:
runAsUser: 1000
runAsGroup: 1000
fsGroup: 1000</code>
See
<a href="https://github.com/longhorn/longhorn/issues/2964#issuecomment-910117570">https://github.com/longhorn/longhorn/issues/2964#issuecomment-910117570</a>
for an example.</li>
<li>Wait for the workload pod to be running</li>
<li>Exec into the workload pod, cd into the mount point of the volume.</li>
<li>Verify that the mount point has correct filesystem permission (e.g.,
running <code>ls -l</code> on the mount point should return the permission in
the format <strong><em>*rw</em></strong>*</li>
<li>Verify that we can read/write files.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_workload_with_fsgroup(core_api, statefulset):  # NOQA
    &#34;&#34;&#34;
    1. Deploy a StatefulSet workload that uses Longhorn volume and has
       securityContext set:
       ```
        securityContext:
            runAsUser: 1000
            runAsGroup: 1000
            fsGroup: 1000
        ```
       See
       https://github.com/longhorn/longhorn/issues/2964#issuecomment-910117570
       for an example.
    2. Wait for the workload pod to be running
    3. Exec into the workload pod, cd into the mount point of the volume.
    4. Verify that the mount point has correct filesystem permission (e.g.,
       running `ls -l` on the mount point should return the permission in
       the format ****rw****
    5. Verify that we can read/write files.
    &#34;&#34;&#34;
    statefulset_name = &#39;statefulset-non-root-access&#39;
    pod_name = statefulset_name + &#39;-0&#39;

    statefulset[&#39;metadata&#39;][&#39;name&#39;] = \
        statefulset[&#39;spec&#39;][&#39;selector&#39;][&#39;matchLabels&#39;][&#39;app&#39;] = \
        statefulset[&#39;spec&#39;][&#39;serviceName&#39;] = \
        statefulset[&#39;spec&#39;][&#39;template&#39;][&#39;metadata&#39;][&#39;labels&#39;][&#39;app&#39;] = \
        statefulset_name
    statefulset[&#39;spec&#39;][&#39;replicas&#39;] = 1
    statefulset[&#39;spec&#39;][&#39;volumeClaimTemplates&#39;][0][&#39;spec&#39;][&#39;storageClassName&#39;]\
        = &#39;longhorn&#39;
    statefulset[&#39;spec&#39;][&#39;template&#39;][&#39;spec&#39;][&#39;securityContext&#39;] = {
        &#39;runAsUser&#39;: 1000,
        &#39;runAsGroup&#39;: 1000,
        &#39;fsGroup&#39;: 1000
    }

    create_and_wait_statefulset(statefulset)

    write_pod_volume_random_data(core_api, pod_name, &#34;/data/test&#34;,
                                 DATA_SIZE_IN_MB_1)
    get_pod_data_md5sum(core_api, pod_name, &#34;/data/test&#34;)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.volume_basic_test"><code class="name flex">
<span>def <span class="ident">volume_basic_test</span></span>(<span>client, volume_name, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_basic_test(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    num_hosts = len(client.list_node())
    num_replicas = 3

    with pytest.raises(Exception):
        volume = client.create_volume(name=&#34;wrong_volume-name-1.0&#34;, size=SIZE,
                                      numberOfReplicas=2)
        volume = client.create_volume(name=&#34;wrong_volume-name&#34;, size=SIZE,
                                      numberOfReplicas=2)
        volume = client.create_volume(name=&#34;wrong_volume-name&#34;, size=SIZE,
                                      numberOfReplicas=2,
                                      frontend=&#34;invalid_frontend&#34;)

    volume = create_and_check_volume(client, volume_name, num_replicas, SIZE,
                                     backing_image)
    assert volume.restoreRequired is False

    def validate_volume_basic(expected, actual):
        assert actual.name == expected.name
        assert actual.size == expected.size
        assert actual.numberOfReplicas == expected.numberOfReplicas
        assert actual.frontend == VOLUME_FRONTEND_BLOCKDEV
        assert actual.backingImage == backing_image
        assert actual.state == expected.state
        assert actual.created == expected.created

    volumes = client.list_volume().data
    assert len(volumes) == 1
    validate_volume_basic(volume, volumes[0])

    volumeByName = client.by_id_volume(volume_name)
    validate_volume_basic(volume, volumeByName)

    lht_hostId = get_self_host_id()
    volume.attach(hostId=lht_hostId)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert volume.restoreRequired is False

    volumeByName = client.by_id_volume(volume_name)
    validate_volume_basic(volume, volumeByName)
    check_volume_endpoint(volumeByName)

    # validate soft anti-affinity
    hosts = {}
    for replica in volume.replicas:
        id = replica.hostId
        assert id != &#34;&#34;
        hosts[id] = True
    if num_hosts &gt;= num_replicas:
        assert len(hosts) == num_replicas
    else:
        assert len(hosts) == num_hosts

    volumes = client.list_volume().data
    assert len(volumes) == 1
    assert volumes[0].name == volume.name
    assert volumes[0].size == volume.size
    assert volumes[0].numberOfReplicas == volume.numberOfReplicas
    assert volumes[0].state == volume.state
    assert volumes[0].created == volume.created
    check_volume_endpoint(volumes[0])

    volume = client.by_id_volume(volume_name)
    volume_rw_test(get_volume_endpoint(volume))

    volume.detach(hostId=&#34;&#34;)
    volume = common.wait_for_volume_detached(client, volume_name)
    assert volume.restoreRequired is False

    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    volumes = client.list_volume().data
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.test_basic.volume_iscsi_basic_test"><code class="name flex">
<span>def <span class="ident">volume_iscsi_basic_test</span></span>(<span>client, volume_name, backing_image='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_iscsi_basic_test(client, volume_name, backing_image=&#34;&#34;):  # NOQA
    host_id = get_self_host_id()
    volume = create_and_check_volume(client, volume_name, 3, SIZE,
                                     backing_image, VOLUME_FRONTEND_ISCSI)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    volumes = client.list_volume().data
    assert len(volumes) == 1
    assert volumes[0].name == volume.name
    assert volumes[0].size == volume.size
    assert volumes[0].numberOfReplicas == volume.numberOfReplicas
    assert volumes[0].state == volume.state
    assert volumes[0].created == volume.created
    assert volumes[0].frontend == VOLUME_FRONTEND_ISCSI
    assert volumes[0].backingImage == volume.backingImage
    endpoint = get_volume_endpoint(volumes[0])

    try:
        dev = iscsi_login(endpoint)
        volume_rw_test(dev)
    finally:
        iscsi_logout(endpoint)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_basic.volume_rw_test"><code class="name flex">
<span>def <span class="ident">volume_rw_test</span></span>(<span>dev)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_rw_test(dev):
    assert volume_valid(dev)
    data = write_device_random_data(dev)
    check_device_data(dev, data)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_basic.backup_failed_cleanup" href="#tests.test_basic.backup_failed_cleanup">backup_failed_cleanup</a></code></li>
<li><code><a title="tests.test_basic.backup_labels_test" href="#tests.test_basic.backup_labels_test">backup_labels_test</a></code></li>
<li><code><a title="tests.test_basic.backup_status_for_unavailable_replicas_test" href="#tests.test_basic.backup_status_for_unavailable_replicas_test">backup_status_for_unavailable_replicas_test</a></code></li>
<li><code><a title="tests.test_basic.backup_test" href="#tests.test_basic.backup_test">backup_test</a></code></li>
<li><code><a title="tests.test_basic.backupstore_test" href="#tests.test_basic.backupstore_test">backupstore_test</a></code></li>
<li><code><a title="tests.test_basic.restore_inc_test" href="#tests.test_basic.restore_inc_test">restore_inc_test</a></code></li>
<li><code><a title="tests.test_basic.snapshot_prune_and_coalesce_simultaneously" href="#tests.test_basic.snapshot_prune_and_coalesce_simultaneously">snapshot_prune_and_coalesce_simultaneously</a></code></li>
<li><code><a title="tests.test_basic.snapshot_prune_test" href="#tests.test_basic.snapshot_prune_test">snapshot_prune_test</a></code></li>
<li><code><a title="tests.test_basic.snapshot_test" href="#tests.test_basic.snapshot_test">snapshot_test</a></code></li>
<li><code><a title="tests.test_basic.test_allow_volume_creation_with_degraded_availability" href="#tests.test_basic.test_allow_volume_creation_with_degraded_availability">test_allow_volume_creation_with_degraded_availability</a></code></li>
<li><code><a title="tests.test_basic.test_allow_volume_creation_with_degraded_availability_dr" href="#tests.test_basic.test_allow_volume_creation_with_degraded_availability_dr">test_allow_volume_creation_with_degraded_availability_dr</a></code></li>
<li><code><a title="tests.test_basic.test_allow_volume_creation_with_degraded_availability_error" href="#tests.test_basic.test_allow_volume_creation_with_degraded_availability_error">test_allow_volume_creation_with_degraded_availability_error</a></code></li>
<li><code><a title="tests.test_basic.test_allow_volume_creation_with_degraded_availability_restore" href="#tests.test_basic.test_allow_volume_creation_with_degraded_availability_restore">test_allow_volume_creation_with_degraded_availability_restore</a></code></li>
<li><code><a title="tests.test_basic.test_attach_without_frontend" href="#tests.test_basic.test_attach_without_frontend">test_attach_without_frontend</a></code></li>
<li><code><a title="tests.test_basic.test_aws_iam_role_arn" href="#tests.test_basic.test_aws_iam_role_arn">test_aws_iam_role_arn</a></code></li>
<li><code><a title="tests.test_basic.test_backup" href="#tests.test_basic.test_backup">test_backup</a></code></li>
<li><code><a title="tests.test_basic.test_backup_block_deletion" href="#tests.test_basic.test_backup_block_deletion">test_backup_block_deletion</a></code></li>
<li><code><a title="tests.test_basic.test_backup_failed_disable_auto_cleanup" href="#tests.test_basic.test_backup_failed_disable_auto_cleanup">test_backup_failed_disable_auto_cleanup</a></code></li>
<li><code><a title="tests.test_basic.test_backup_failed_enable_auto_cleanup" href="#tests.test_basic.test_backup_failed_enable_auto_cleanup">test_backup_failed_enable_auto_cleanup</a></code></li>
<li><code><a title="tests.test_basic.test_backup_labels" href="#tests.test_basic.test_backup_labels">test_backup_labels</a></code></li>
<li><code><a title="tests.test_basic.test_backup_lock_creation_during_deletion" href="#tests.test_basic.test_backup_lock_creation_during_deletion">test_backup_lock_creation_during_deletion</a></code></li>
<li><code><a title="tests.test_basic.test_backup_lock_deletion_during_backup" href="#tests.test_basic.test_backup_lock_deletion_during_backup">test_backup_lock_deletion_during_backup</a></code></li>
<li><code><a title="tests.test_basic.test_backup_lock_deletion_during_restoration" href="#tests.test_basic.test_backup_lock_deletion_during_restoration">test_backup_lock_deletion_during_restoration</a></code></li>
<li><code><a title="tests.test_basic.test_backup_lock_restoration_during_deletion" href="#tests.test_basic.test_backup_lock_restoration_during_deletion">test_backup_lock_restoration_during_deletion</a></code></li>
<li><code><a title="tests.test_basic.test_backup_metadata_deletion" href="#tests.test_basic.test_backup_metadata_deletion">test_backup_metadata_deletion</a></code></li>
<li><code><a title="tests.test_basic.test_backup_status_for_unavailable_replicas" href="#tests.test_basic.test_backup_status_for_unavailable_replicas">test_backup_status_for_unavailable_replicas</a></code></li>
<li><code><a title="tests.test_basic.test_backup_volume_list" href="#tests.test_basic.test_backup_volume_list">test_backup_volume_list</a></code></li>
<li><code><a title="tests.test_basic.test_backup_volume_restore_with_access_mode" href="#tests.test_basic.test_backup_volume_restore_with_access_mode">test_backup_volume_restore_with_access_mode</a></code></li>
<li><code><a title="tests.test_basic.test_backuptarget_available_during_engine_image_not_ready" href="#tests.test_basic.test_backuptarget_available_during_engine_image_not_ready">test_backuptarget_available_during_engine_image_not_ready</a></code></li>
<li><code><a title="tests.test_basic.test_cleanup_system_generated_snapshots" href="#tests.test_basic.test_cleanup_system_generated_snapshots">test_cleanup_system_generated_snapshots</a></code></li>
<li><code><a title="tests.test_basic.test_default_storage_class_syncup" href="#tests.test_basic.test_default_storage_class_syncup">test_default_storage_class_syncup</a></code></li>
<li><code><a title="tests.test_basic.test_deleting_backup_volume" href="#tests.test_basic.test_deleting_backup_volume">test_deleting_backup_volume</a></code></li>
<li><code><a title="tests.test_basic.test_dr_volume_with_all_backup_blocks_deleted" href="#tests.test_basic.test_dr_volume_with_all_backup_blocks_deleted">test_dr_volume_with_all_backup_blocks_deleted</a></code></li>
<li><code><a title="tests.test_basic.test_dr_volume_with_backup_block_deletion" href="#tests.test_basic.test_dr_volume_with_backup_block_deletion">test_dr_volume_with_backup_block_deletion</a></code></li>
<li><code><a title="tests.test_basic.test_dr_volume_with_backup_block_deletion_abort_during_backup_in_progress" href="#tests.test_basic.test_dr_volume_with_backup_block_deletion_abort_during_backup_in_progress">test_dr_volume_with_backup_block_deletion_abort_during_backup_in_progress</a></code></li>
<li><code><a title="tests.test_basic.test_engine_image_daemonset_restart" href="#tests.test_basic.test_engine_image_daemonset_restart">test_engine_image_daemonset_restart</a></code></li>
<li><code><a title="tests.test_basic.test_expand_pvc_with_size_round_up" href="#tests.test_basic.test_expand_pvc_with_size_round_up">test_expand_pvc_with_size_round_up</a></code></li>
<li><code><a title="tests.test_basic.test_expansion_basic" href="#tests.test_basic.test_expansion_basic">test_expansion_basic</a></code></li>
<li><code><a title="tests.test_basic.test_expansion_canceling" href="#tests.test_basic.test_expansion_canceling">test_expansion_canceling</a></code></li>
<li><code><a title="tests.test_basic.test_expansion_with_scheduling_failure" href="#tests.test_basic.test_expansion_with_scheduling_failure">test_expansion_with_scheduling_failure</a></code></li>
<li><code><a title="tests.test_basic.test_hosts" href="#tests.test_basic.test_hosts">test_hosts</a></code></li>
<li><code><a title="tests.test_basic.test_listing_backup_volume" href="#tests.test_basic.test_listing_backup_volume">test_listing_backup_volume</a></code></li>
<li><code><a title="tests.test_basic.test_multiple_volumes_creation_with_degraded_availability" href="#tests.test_basic.test_multiple_volumes_creation_with_degraded_availability">test_multiple_volumes_creation_with_degraded_availability</a></code></li>
<li><code><a title="tests.test_basic.test_restore_basic" href="#tests.test_basic.test_restore_basic">test_restore_basic</a></code></li>
<li><code><a title="tests.test_basic.test_restore_inc" href="#tests.test_basic.test_restore_inc">test_restore_inc</a></code></li>
<li><code><a title="tests.test_basic.test_restore_inc_with_expansion" href="#tests.test_basic.test_restore_inc_with_expansion">test_restore_inc_with_expansion</a></code></li>
<li><code><a title="tests.test_basic.test_running_volume_with_scheduling_failure" href="#tests.test_basic.test_running_volume_with_scheduling_failure">test_running_volume_with_scheduling_failure</a></code></li>
<li><code><a title="tests.test_basic.test_setting_default_replica_count" href="#tests.test_basic.test_setting_default_replica_count">test_setting_default_replica_count</a></code></li>
<li><code><a title="tests.test_basic.test_settings" href="#tests.test_basic.test_settings">test_settings</a></code></li>
<li><code><a title="tests.test_basic.test_snapshot" href="#tests.test_basic.test_snapshot">test_snapshot</a></code></li>
<li><code><a title="tests.test_basic.test_snapshot_prune" href="#tests.test_basic.test_snapshot_prune">test_snapshot_prune</a></code></li>
<li><code><a title="tests.test_basic.test_snapshot_prune_and_coalesce_simultaneously" href="#tests.test_basic.test_snapshot_prune_and_coalesce_simultaneously">test_snapshot_prune_and_coalesce_simultaneously</a></code></li>
<li><code><a title="tests.test_basic.test_space_usage_for_rebuilding_only_volume" href="#tests.test_basic.test_space_usage_for_rebuilding_only_volume">test_space_usage_for_rebuilding_only_volume</a></code></li>
<li><code><a title="tests.test_basic.test_storage_class_from_backup" href="#tests.test_basic.test_storage_class_from_backup">test_storage_class_from_backup</a></code></li>
<li><code><a title="tests.test_basic.test_volume_basic" href="#tests.test_basic.test_volume_basic">test_volume_basic</a></code></li>
<li><code><a title="tests.test_basic.test_volume_iscsi_basic" href="#tests.test_basic.test_volume_iscsi_basic">test_volume_iscsi_basic</a></code></li>
<li><code><a title="tests.test_basic.test_volume_multinode" href="#tests.test_basic.test_volume_multinode">test_volume_multinode</a></code></li>
<li><code><a title="tests.test_basic.test_volume_scheduling_failure" href="#tests.test_basic.test_volume_scheduling_failure">test_volume_scheduling_failure</a></code></li>
<li><code><a title="tests.test_basic.test_volume_toomanysnapshots_condition" href="#tests.test_basic.test_volume_toomanysnapshots_condition">test_volume_toomanysnapshots_condition</a></code></li>
<li><code><a title="tests.test_basic.test_volume_update_replica_count" href="#tests.test_basic.test_volume_update_replica_count">test_volume_update_replica_count</a></code></li>
<li><code><a title="tests.test_basic.test_workload_with_fsgroup" href="#tests.test_basic.test_workload_with_fsgroup">test_workload_with_fsgroup</a></code></li>
<li><code><a title="tests.test_basic.volume_basic_test" href="#tests.test_basic.volume_basic_test">volume_basic_test</a></code></li>
<li><code><a title="tests.test_basic.volume_iscsi_basic_test" href="#tests.test_basic.volume_iscsi_basic_test">volume_iscsi_basic_test</a></code></li>
<li><code><a title="tests.test_basic.volume_rw_test" href="#tests.test_basic.volume_rw_test">volume_rw_test</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>