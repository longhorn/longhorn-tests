<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>tests.test_zone API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_zone</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pytest
import time

from random import randrange

import common
from common import client # NOQA
from common import core_api  # NOQA
from common import pvc, pod  # NOQA
from common import volume_name # NOQA
from common import create_and_check_volume
from common import get_self_host_id
from common import wait_for_volume_degraded
from common import wait_for_volume_healthy
from common import wait_for_volume_replica_count
from common import get_k8s_zone_label
from common import set_k8s_node_zone_label
from common import wait_for_volume_condition_scheduled
from common import wait_for_volume_delete
from common import RETRY_COUNTS
from common import RETRY_INTERVAL
from common import RETRY_INTERVAL_LONG
from common import CONDITION_STATUS_TRUE
from common import SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY
from common import SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY
from common import SETTING_REPLICA_AUTO_BALANCE
from common import SETTING_DEFAULT_DATA_LOCALITY

from test_scheduling import wait_new_replica_ready

ZONE1 = &#34;lh-zone1&#34;
ZONE2 = &#34;lh-zone2&#34;
ZONE3 = &#34;lh-zone3&#34;


@pytest.fixture
def k8s_node_zone_tags(client, core_api):  # NOQA

    k8s_zone_label = get_k8s_zone_label()
    lh_nodes = client.list_node()

    node_index = 0
    for node in lh_nodes:
        node_name = node.name

        if node_index % 2 == 0:
            zone = ZONE1
        else:
            zone = ZONE2

        payload = {
            &#34;metadata&#34;: {
                &#34;labels&#34;: {
                    k8s_zone_label: zone}
            }
        }

        core_api.patch_node(node_name, body=payload)
        node_index += 1

    yield

    lh_nodes = client.list_node()

    node_index = 0
    for node in lh_nodes:
        node_name = node.name

        payload = {
            &#34;metadata&#34;: {
                &#34;labels&#34;: {
                    k8s_zone_label: None}
            }
        }

        core_api.patch_node(node_name, body=payload)


def wait_longhorn_node_zone_updated(client): # NOQA

    lh_nodes = client.list_node()
    node_names = map(lambda node: node.name, lh_nodes)

    for node_name in node_names:
        for j in range(RETRY_COUNTS):
            lh_node = client.by_id_node(node_name)
            if lh_node.zone != &#39;&#39;:
                break
            time.sleep(RETRY_INTERVAL)

        assert lh_node.zone != &#39;&#39;


def get_zone_replica_count(client, volume_name, zone_name, chk_running=False): # NOQA
    volume = client.by_id_volume(volume_name)

    zone_replica_count = 0
    for replica in volume.replicas:
        if chk_running and not replica.running:
            continue
        replica_host_id = replica.hostId
        replica_host_zone = client.by_id_node(replica_host_id).zone
        if replica_host_zone == zone_name:
            zone_replica_count += 1
    return zone_replica_count


def test_zone_tags(client, core_api, volume_name, k8s_node_zone_tags):  # NOQA
    &#34;&#34;&#34;
    Test anti affinity zone feature

    1. Add Kubernetes zone labels to the nodes
        1. Only two zones now: zone1 and zone2
    2. Create a volume with two replicas
    3. Verify zone1 and zone2 either has one replica.
    4. Remove a random replica and wait for volume to finish rebuild
    5. Verify zone1 and zone2 either has one replica.
    6. Repeat step 4-5 a few times.
    7. Update volume to 3 replicas, make sure they&#39;re scheduled on 3 nodes
    8. Remove a random replica and wait for volume to finish rebuild
    9. Make sure replicas are on different nodes
    10. Repeat step 8-9 a few times
    &#34;&#34;&#34;

    wait_longhorn_node_zone_updated(client)

    volume = create_and_check_volume(client, volume_name, num_of_replicas=2)

    host_id = get_self_host_id()

    volume.attach(hostId=host_id)

    volume = wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)

    zone1_replica_count = get_zone_replica_count(client, volume_name, ZONE1)
    zone2_replica_count = get_zone_replica_count(client, volume_name, ZONE2)

    assert zone1_replica_count == zone2_replica_count

    for i in range(randrange(3, 5)):
        volume = client.by_id_volume(volume_name)

        replica_count = len(volume.replicas)
        assert replica_count == 2

        replica_id = randrange(0, replica_count)

        replica_name = volume.replicas[replica_id].name

        volume.replicaRemove(name=replica_name)

        wait_for_volume_degraded(client, volume_name)

        wait_for_volume_healthy(client, volume_name)

        wait_for_volume_replica_count(client, volume_name, replica_count)

        volume = client.by_id_volume(volume_name)

        replica_names = map(lambda replica: replica.name, volume[&#34;replicas&#34;])

        wait_new_replica_ready(client, volume_name, replica_names)

        zone1_replica_count = \
            get_zone_replica_count(client, volume_name, ZONE1)
        zone2_replica_count = \
            get_zone_replica_count(client, volume_name, ZONE2)

        assert zone1_replica_count == zone2_replica_count

    volume.updateReplicaCount(replicaCount=3)

    wait_for_volume_degraded(client, volume_name)

    wait_for_volume_replica_count(client, volume_name, 3)

    wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)

    lh_node_names = list(map(lambda node: node.name, client.list_node()))

    for replica in volume.replicas:
        lh_node_names.remove(replica.hostId)

    assert lh_node_names == []

    for i in range(randrange(3, 5)):
        volume = client.by_id_volume(volume_name)

        replica_count = len(volume.replicas)
        assert replica_count == 3

        replica_id = randrange(0, replica_count)

        replica_name = volume.replicas[replica_id].name

        volume.replicaRemove(name=replica_name)

        wait_for_volume_degraded(client, volume_name)

        wait_for_volume_healthy(client, volume_name)

        wait_for_volume_replica_count(client, volume_name, replica_count)

        volume = client.by_id_volume(volume_name)

        lh_node_names = list(map(lambda node: node.name, client.list_node()))

        for replica in volume.replicas:
            lh_node_names.remove(replica.hostId)

        assert lh_node_names == []


@pytest.mark.node  # NOQA
def test_replica_zone_anti_affinity(client, core_api, volume_name, k8s_node_zone_tags):  # NOQA
    &#34;&#34;&#34;
    Test replica scheduler with zone anti-affinity

    1. Set zone anti-affinity to hard.
    2. Label nodes 1 &amp; 2 with same zone label &#34;zone1&#34;.
    Label node 3 with zone label &#34;zone2&#34;.
    3. Create a volume with 3 replicas.
    4. Wait for volume condition `scheduled` to be false.
    5. Label node 2 with zone label &#34;zone3&#34;.
    6. Wait for volume condition `scheduled` to be success.
    7. Clear the volume.
    8. Set zone anti-affinity to soft.
    9. Change the zone labels on node 1 &amp; 2 &amp; 3 to &#34;zone1&#34;.
    10. Create a volume.
    11. Wait for volume condition `scheduled` to be success.
    12. Clean up the replica count, the zone labels and the volume.
    &#34;&#34;&#34;

    wait_longhorn_node_zone_updated(client)

    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_node_soft_anti_affinity_setting, value=&#34;false&#34;)

    replica_zone_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY)
    client.update(replica_zone_soft_anti_affinity_setting, value=&#34;false&#34;)

    volume = create_and_check_volume(client, volume_name)

    lh_nodes = client.list_node()

    count = 0
    for node in lh_nodes:
        count += 1
        set_k8s_node_zone_label(core_api, node.name, &#34;lh-zone&#34; + str(count))

    wait_longhorn_node_zone_updated(client)

    wait_for_volume_condition_scheduled(client, volume_name,
                                        &#34;status&#34;,
                                        CONDITION_STATUS_TRUE)

    replica_zone_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY)
    client.update(replica_zone_soft_anti_affinity_setting, value=&#34;true&#34;)

    volume = client.by_id_volume(volume_name)
    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    for node in lh_nodes:
        set_k8s_node_zone_label(core_api, node.name, &#34;lh-zone1&#34;)

    wait_longhorn_node_zone_updated(client)

    volume = create_and_check_volume(client, volume_name)
    wait_for_volume_condition_scheduled(client, volume_name,
                                        &#34;status&#34;,
                                        CONDITION_STATUS_TRUE)


def test_replica_auto_balance_zone_least_effort(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance zones with least-effort.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `least-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
        set node-3 to zone-3.
    And disable scheduling for node-2.
        disable scheduling for node-3.
    And create a volume with 6 replicas.
    And attach the volume to self-node.
    And 6 replicas running in zone-1.
        0 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-2.
    Then count replicas running on each node.
    And zone-1 replica count != zone-2 replica count.
        zone-2 replica count != 0.
        zone-3 replica count == 0.

    When enable scheduling for node-3.
    Then count replicas running on each node.
    And zone-1 replica count != zone-3 replica count.
        zone-2 replica count != 0.
        zone-3 replica count != 0.
    &#34;&#34;&#34;
    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;least-effort&#34;)

    n1, n2, n3 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE2)
    set_k8s_node_zone_label(core_api, n3.name, ZONE3)
    wait_longhorn_node_zone_updated(client)

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)

    n_replicas = 6
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == 6 and z2_r_count == z3_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count == 6
    assert z2_r_count == 0
    assert z3_r_count == 0

    client.update(n2, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        all_r_count = z1_r_count + z2_r_count + z3_r_count
        if z2_r_count != 0 and all_r_count == n_replicas:
            break

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count != z2_r_count
    assert z2_r_count != 0
    assert z3_r_count == 0

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        all_r_count = z1_r_count + z2_r_count + z3_r_count
        if z3_r_count != 0 and all_r_count == n_replicas:
            break

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count != z3_r_count
    assert z2_r_count != 0
    assert z3_r_count != 0


def test_replica_auto_balance_zone_best_effort(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance zones with best-effort.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
        set node-3 to zone-3.
    And disable scheduling for node-2.
        disable scheduling for node-3.
    And create a volume with 6 replicas.
    And attach the volume to self-node.
    And 6 replicas running in zone-1.
        0 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-2.
    Then count replicas running on each node.
    And 3 replicas running in zone-1.
        3 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-3.
    Then count replicas running on each node.
    And 2 replicas running in zone-1.
        2 replicas running in zone-2.
        2 replicas running in zone-3.
    &#34;&#34;&#34;

    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE2)
    set_k8s_node_zone_label(core_api, n3.name, ZONE3)
    wait_longhorn_node_zone_updated(client)

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)

    n_replicas = 6
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == 6 and z2_r_count == z3_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count == 6
    assert z2_r_count == 0
    assert z3_r_count == 0

    client.update(n2, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == z2_r_count == 3 and z3_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL_LONG)
    assert z1_r_count == 3
    assert z2_r_count == 3
    assert z3_r_count == 0

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == z2_r_count == z3_r_count == 2:
            break

        time.sleep(RETRY_INTERVAL_LONG)
    assert z1_r_count == 2
    assert z2_r_count == 2
    assert z3_r_count == 2


def test_replica_auto_balance_zone_best_effort_with_data_locality(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Background:
    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set `default-data-locality` to `best-effort`.
    And set `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-1.
        set node-3 to zone-2.
    And create volume with 2 replicas.
    And create pv for volume.
    And create pvc for volume.

    Scenario Outline: replica auto-balance zones with best-effort should
                      not remove pod local replicas when data locality is
                      enabled (best-effort).

    Given create and wait pod on &lt;pod-node&gt;.
    And disable scheduling and evict node-3.
    And count replicas on each nodes.
    And 1 replica running on &lt;pod-node&gt;.
        1 replica running on &lt;duplicate-node&gt;.
        0 replica running on node-3.

    When enable scheduling for node-3.
    Then count replicas on each nodes.
    And 1 replica running on &lt;pod-node&gt;.
        0 replica running on &lt;duplicate-node&gt;.
        1 replica running on node-3.
    And count replicas in each zones.
    And 1 replica running in zone-1.
        1 replica running in zone-2.
    And loop 3 times with each wait 5 seconds and count replicas on each nodes.
        To ensure no addition scheduling is happening.
        1 replica running on &lt;pod-node&gt;.
        0 replica running on &lt;duplicate-node&gt;.
        1 replica running on node-3.

    And delete pod.

    Examples:
        | pod-node | duplicate-node |
        | node-1   | node-2         |
        | node-2   | node-1         |
        | node-1   | node-2         |
    &#34;&#34;&#34;

    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_DEFAULT_DATA_LOCALITY, &#34;best-effort&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE1)
    set_k8s_node_zone_label(core_api, n3.name, ZONE2)
    wait_longhorn_node_zone_updated(client)

    n_replicas = 2
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    common.create_pv_for_volume(client, core_api, volume, volume_name)
    common.create_pvc_for_volume(client, core_api, volume, volume_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#34;name&#34;: &#34;pod-data&#34;,
        &#34;persistentVolumeClaim&#34;: {
            &#34;claimName&#34;: volume_name
        }
    }]

    for i in range(1, 4):
        pod_node_name = n2.name if i % 2 == 0 else n1.name
        pod[&#39;spec&#39;][&#39;nodeSelector&#39;] = {
            &#34;kubernetes.io/hostname&#34;: pod_node_name
        }
        common.create_and_wait_pod(core_api, pod)

        client.update(n3, allowScheduling=False, evictionRequested=True)

        duplicate_node = [n1.name, n2.name]
        duplicate_node.remove(pod_node_name)
        for _ in range(RETRY_COUNTS):
            pod_node_r_count = common.get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            duplicate_node_r_count = common.get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=True)
            balance_node_r_count = common.get_host_replica_count(
                client, volume_name, n3.name, chk_running=False)

            if pod_node_r_count == duplicate_node_r_count == 1 and \
                    balance_node_r_count == 0:
                break

            time.sleep(RETRY_INTERVAL)
        assert pod_node_r_count == 1
        assert duplicate_node_r_count == 1
        assert balance_node_r_count == 0

        client.update(n3, allowScheduling=True)

        for _ in range(RETRY_COUNTS):
            pod_node_r_count = common.get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            duplicate_node_r_count = common.get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=False)
            balance_node_r_count = common.get_host_replica_count(
                client, volume_name, n3.name, chk_running=True)

            if pod_node_r_count == balance_node_r_count == 1 and \
                    duplicate_node_r_count == 0:
                break

            time.sleep(RETRY_INTERVAL)
        assert pod_node_r_count == 1
        assert duplicate_node_r_count == 0
        assert balance_node_r_count == 1

        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        assert z1_r_count == z2_r_count == 1

        # loop 3 times and each to wait 5 seconds to ensure there is no
        # re-scheduling happening.
        for _ in range(3):
            time.sleep(5)
            assert pod_node_r_count == common.get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            assert duplicate_node_r_count == common.get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=False)
            assert balance_node_r_count == common.get_host_replica_count(
                client, volume_name, n3.name, chk_running=True)

        common.delete_and_wait_pod(core_api, pod[&#39;metadata&#39;][&#39;name&#39;])


def test_replica_auto_balance_node_duplicates_in_multiple_zones(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance to nodes with duplicated replicas in the
              zone.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `least-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
    And disable scheduling for node-3.
    And create a volume with 3 replicas.
    And attach the volume to self-node.
    And zone-1 and zone-2 should contain 3 replica in total.

    When set node-3 to the zone with duplicated replicas.
    And enable scheduling for node-3.
    Then count replicas running on each node.
    And 1 replica running on node-1
        1 replica running on node-2
        1 replica running on node-3.
    And count replicas running in each zone.
    And total of 3 replicas running in zone-1 and zone-2.
    &#34;&#34;&#34;

    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;least-effort&#34;)

    n1, n2, n3 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE2)
    set_k8s_node_zone_label(core_api, n3.name, &#34;temp&#34;)
    wait_longhorn_node_zone_updated(client)

    client.update(n3, allowScheduling=False)

    n_replicas = 3
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())
    z1_r_count = get_zone_replica_count(client, volume_name, ZONE1)
    z2_r_count = get_zone_replica_count(client, volume_name, ZONE2)
    assert z1_r_count + z2_r_count == n_replicas

    if z1_r_count == 2:
        set_k8s_node_zone_label(core_api, n3.name, ZONE1)
    else:
        set_k8s_node_zone_label(core_api, n3.name, ZONE2)

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        n1_r_count = common.get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = common.get_host_replica_count(
            client, volume_name, n2.name, chk_running=True)
        n3_r_count = common.get_host_replica_count(
            client, volume_name, n3.name, chk_running=True)

        if n1_r_count == n2_r_count == n3_r_count == 1:
            break
        time.sleep(RETRY_INTERVAL)
    assert n1_r_count == 1
    assert n2_r_count == 1
    assert n3_r_count == 1

    z1_r_count = get_zone_replica_count(
        client, volume_name, ZONE1, chk_running=True)
    z2_r_count = get_zone_replica_count(
        client, volume_name, ZONE2, chk_running=True)
    assert z1_r_count + z2_r_count == n_replicas


@pytest.mark.skip(reason=&#34;REQUIRE_5_NODES&#34;)
def test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-1.
        set node-3 to zone-1.
        set node-4 to zone-2.
        set node-5 to zone-2.
    And disable scheduling for node-2.
        disable scheduling for node-3.
        disable scheduling for node-4.
        disable scheduling for node-5.
    And create volume with 4 replicas.
    And attach the volume to node-1.

    Scenario: replica auto-balance zones with best-effort should balance
              replicas in zone.

    Given 4 replica running on node-1.
          0 replica running on node-2.
          0 replica running on node-3.
          0 replica running on node-4.
          0 replica running on node-5.

    When enable scheduling for node-4.
    Then count replicas on each zones.
    And 2 replica running on zode-1.
        2 replica running on zode-2.

    When enable scheduling for node-2.
         enable scheduling for node-3.
    Then count replicas on each nodes.
    And 1 replica running on node-1.
        1 replica running on node-2.
        1 replica running on node-3.
        1 replica running on node-4.
        0 replica running on node-5.

    When enable scheduling for node-5.
    Then count replicas on each zones.
    And 2 replica running on zode-1.
        2 replica running on zode-2.
    &#34;&#34;&#34;

    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_DEFAULT_DATA_LOCALITY, &#34;best-effort&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3, n4, n5 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE1)
    set_k8s_node_zone_label(core_api, n3.name, ZONE1)
    set_k8s_node_zone_label(core_api, n4.name, ZONE2)
    set_k8s_node_zone_label(core_api, n5.name, ZONE2)
    wait_longhorn_node_zone_updated(client)

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)
    client.update(n4, allowScheduling=False)
    client.update(n5, allowScheduling=False)

    n_replicas = 4
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=n1.name)

    for _ in range(RETRY_COUNTS):
        n1_r_count = common.get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = common.get_host_replica_count(
            client, volume_name, n2.name, chk_running=False)
        n3_r_count = common.get_host_replica_count(
            client, volume_name, n3.name, chk_running=False)
        n4_r_count = common.get_host_replica_count(
            client, volume_name, n4.name, chk_running=False)
        n5_r_count = common.get_host_replica_count(
            client, volume_name, n5.name, chk_running=False)

        if n1_r_count == 4 and \
                n2_r_count == n3_r_count == n4_r_count == n5_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL)
    assert n1_r_count == 4
    assert n2_r_count == 0
    assert n3_r_count == 0
    assert n4_r_count == 0
    assert n5_r_count == 0

    client.update(n4, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)

        if z1_r_count == z2_r_count == 2:
            break

        time.sleep(RETRY_INTERVAL)

    assert z1_r_count == 2
    assert z2_r_count == 2

    client.update(n2, allowScheduling=True)
    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        n1_r_count = common.get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = common.get_host_replica_count(
            client, volume_name, n2.name, chk_running=True)
        n3_r_count = common.get_host_replica_count(
            client, volume_name, n3.name, chk_running=True)
        n4_r_count = common.get_host_replica_count(
            client, volume_name, n4.name, chk_running=True)
        n5_r_count = common.get_host_replica_count(
            client, volume_name, n5.name, chk_running=False)

        if n1_r_count == n2_r_count == n3_r_count == n4_r_count == 1 and \
                n5_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL)
    assert n1_r_count == 1
    assert n2_r_count == 1
    assert n3_r_count == 1
    assert n4_r_count == 1
    assert n5_r_count == 0

    client.update(n5, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)

        if z1_r_count == z2_r_count == 2:
            break

        time.sleep(RETRY_INTERVAL)

    assert z1_r_count == 2
    assert z2_r_count == 2</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_zone.get_zone_replica_count"><code class="name flex">
<span>def <span class="ident">get_zone_replica_count</span></span>(<span>client, volume_name, zone_name, chk_running=False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_zone_replica_count(client, volume_name, zone_name, chk_running=False): # NOQA
    volume = client.by_id_volume(volume_name)

    zone_replica_count = 0
    for replica in volume.replicas:
        if chk_running and not replica.running:
            continue
        replica_host_id = replica.hostId
        replica_host_zone = client.by_id_node(replica_host_id).zone
        if replica_host_zone == zone_name:
            zone_replica_count += 1
    return zone_replica_count</code></pre>
</details>
</dd>
<dt id="tests.test_zone.k8s_node_zone_tags"><code class="name flex">
<span>def <span class="ident">k8s_node_zone_tags</span></span>(<span>client, core_api)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def k8s_node_zone_tags(client, core_api):  # NOQA

    k8s_zone_label = get_k8s_zone_label()
    lh_nodes = client.list_node()

    node_index = 0
    for node in lh_nodes:
        node_name = node.name

        if node_index % 2 == 0:
            zone = ZONE1
        else:
            zone = ZONE2

        payload = {
            &#34;metadata&#34;: {
                &#34;labels&#34;: {
                    k8s_zone_label: zone}
            }
        }

        core_api.patch_node(node_name, body=payload)
        node_index += 1

    yield

    lh_nodes = client.list_node()

    node_index = 0
    for node in lh_nodes:
        node_name = node.name

        payload = {
            &#34;metadata&#34;: {
                &#34;labels&#34;: {
                    k8s_zone_label: None}
            }
        }

        core_api.patch_node(node_name, body=payload)</code></pre>
</details>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_node_duplicates_in_multiple_zones"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_node_duplicates_in_multiple_zones</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: replica auto-balance to nodes with duplicated replicas in the
zone.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set volume spec <code>replicaAutoBalance</code> to <code>least-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-2.
And disable scheduling for node-3.
And create a volume with 3 replicas.
And attach the volume to self-node.
And zone-1 and zone-2 should contain 3 replica in total.</p>
<p>When set node-3 to the zone with duplicated replicas.
And enable scheduling for node-3.
Then count replicas running on each node.
And 1 replica running on node-1
1 replica running on node-2
1 replica running on node-3.
And count replicas running in each zone.
And total of 3 replicas running in zone-1 and zone-2.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_replica_auto_balance_node_duplicates_in_multiple_zones(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance to nodes with duplicated replicas in the
              zone.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `least-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
    And disable scheduling for node-3.
    And create a volume with 3 replicas.
    And attach the volume to self-node.
    And zone-1 and zone-2 should contain 3 replica in total.

    When set node-3 to the zone with duplicated replicas.
    And enable scheduling for node-3.
    Then count replicas running on each node.
    And 1 replica running on node-1
        1 replica running on node-2
        1 replica running on node-3.
    And count replicas running in each zone.
    And total of 3 replicas running in zone-1 and zone-2.
    &#34;&#34;&#34;

    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;least-effort&#34;)

    n1, n2, n3 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE2)
    set_k8s_node_zone_label(core_api, n3.name, &#34;temp&#34;)
    wait_longhorn_node_zone_updated(client)

    client.update(n3, allowScheduling=False)

    n_replicas = 3
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())
    z1_r_count = get_zone_replica_count(client, volume_name, ZONE1)
    z2_r_count = get_zone_replica_count(client, volume_name, ZONE2)
    assert z1_r_count + z2_r_count == n_replicas

    if z1_r_count == 2:
        set_k8s_node_zone_label(core_api, n3.name, ZONE1)
    else:
        set_k8s_node_zone_label(core_api, n3.name, ZONE2)

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        n1_r_count = common.get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = common.get_host_replica_count(
            client, volume_name, n2.name, chk_running=True)
        n3_r_count = common.get_host_replica_count(
            client, volume_name, n3.name, chk_running=True)

        if n1_r_count == n2_r_count == n3_r_count == 1:
            break
        time.sleep(RETRY_INTERVAL)
    assert n1_r_count == 1
    assert n2_r_count == 1
    assert n3_r_count == 1

    z1_r_count = get_zone_replica_count(
        client, volume_name, ZONE1, chk_running=True)
    z2_r_count = get_zone_replica_count(
        client, volume_name, ZONE2, chk_running=True)
    assert z1_r_count + z2_r_count == n_replicas</code></pre>
</details>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_zone_best_effort"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_zone_best_effort</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: replica auto-balance zones with best-effort.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set volume spec <code>replicaAutoBalance</code> to <code>best-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-2.
set node-3 to zone-3.
And disable scheduling for node-2.
disable scheduling for node-3.
And create a volume with 6 replicas.
And attach the volume to self-node.
And 6 replicas running in zone-1.
0 replicas running in zone-2.
0 replicas running in zone-3.</p>
<p>When enable scheduling for node-2.
Then count replicas running on each node.
And 3 replicas running in zone-1.
3 replicas running in zone-2.
0 replicas running in zone-3.</p>
<p>When enable scheduling for node-3.
Then count replicas running on each node.
And 2 replicas running in zone-1.
2 replicas running in zone-2.
2 replicas running in zone-3.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_replica_auto_balance_zone_best_effort(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance zones with best-effort.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
        set node-3 to zone-3.
    And disable scheduling for node-2.
        disable scheduling for node-3.
    And create a volume with 6 replicas.
    And attach the volume to self-node.
    And 6 replicas running in zone-1.
        0 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-2.
    Then count replicas running on each node.
    And 3 replicas running in zone-1.
        3 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-3.
    Then count replicas running on each node.
    And 2 replicas running in zone-1.
        2 replicas running in zone-2.
        2 replicas running in zone-3.
    &#34;&#34;&#34;

    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE2)
    set_k8s_node_zone_label(core_api, n3.name, ZONE3)
    wait_longhorn_node_zone_updated(client)

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)

    n_replicas = 6
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == 6 and z2_r_count == z3_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count == 6
    assert z2_r_count == 0
    assert z3_r_count == 0

    client.update(n2, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == z2_r_count == 3 and z3_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL_LONG)
    assert z1_r_count == 3
    assert z2_r_count == 3
    assert z3_r_count == 0

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == z2_r_count == z3_r_count == 2:
            break

        time.sleep(RETRY_INTERVAL_LONG)
    assert z1_r_count == 2
    assert z2_r_count == 2
    assert z3_r_count == 2</code></pre>
</details>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_zone_best_effort_with_data_locality"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_zone_best_effort_with_data_locality</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"><p>Background:
Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set <code>default-data-locality</code> to <code>best-effort</code>.
And set <code>replicaAutoBalance</code> to <code>best-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-1.
set node-3 to zone-2.
And create volume with 2 replicas.
And create pv for volume.
And create pvc for volume.</p>
<p>Scenario Outline: replica auto-balance zones with best-effort should
not remove pod local replicas when data locality is
enabled (best-effort).</p>
<p>Given create and wait pod on <pod-node>.
And disable scheduling and evict node-3.
And count replicas on each nodes.
And 1 replica running on <pod-node>.
1 replica running on <duplicate-node>.
0 replica running on node-3.</p>
<p>When enable scheduling for node-3.
Then count replicas on each nodes.
And 1 replica running on <pod-node>.
0 replica running on <duplicate-node>.
1 replica running on node-3.
And count replicas in each zones.
And 1 replica running in zone-1.
1 replica running in zone-2.
And loop 3 times with each wait 5 seconds and count replicas on each nodes.
To ensure no addition scheduling is happening.
1 replica running on <pod-node>.
0 replica running on <duplicate-node>.
1 replica running on node-3.</p>
<p>And delete pod.</p>
<h2 id="examples">Examples</h2>
<p>| pod-node | duplicate-node |
| node-1
| node-2
|
| node-2
| node-1
|
| node-1
| node-2
|</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_replica_auto_balance_zone_best_effort_with_data_locality(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Background:
    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set `default-data-locality` to `best-effort`.
    And set `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-1.
        set node-3 to zone-2.
    And create volume with 2 replicas.
    And create pv for volume.
    And create pvc for volume.

    Scenario Outline: replica auto-balance zones with best-effort should
                      not remove pod local replicas when data locality is
                      enabled (best-effort).

    Given create and wait pod on &lt;pod-node&gt;.
    And disable scheduling and evict node-3.
    And count replicas on each nodes.
    And 1 replica running on &lt;pod-node&gt;.
        1 replica running on &lt;duplicate-node&gt;.
        0 replica running on node-3.

    When enable scheduling for node-3.
    Then count replicas on each nodes.
    And 1 replica running on &lt;pod-node&gt;.
        0 replica running on &lt;duplicate-node&gt;.
        1 replica running on node-3.
    And count replicas in each zones.
    And 1 replica running in zone-1.
        1 replica running in zone-2.
    And loop 3 times with each wait 5 seconds and count replicas on each nodes.
        To ensure no addition scheduling is happening.
        1 replica running on &lt;pod-node&gt;.
        0 replica running on &lt;duplicate-node&gt;.
        1 replica running on node-3.

    And delete pod.

    Examples:
        | pod-node | duplicate-node |
        | node-1   | node-2         |
        | node-2   | node-1         |
        | node-1   | node-2         |
    &#34;&#34;&#34;

    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_DEFAULT_DATA_LOCALITY, &#34;best-effort&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE1)
    set_k8s_node_zone_label(core_api, n3.name, ZONE2)
    wait_longhorn_node_zone_updated(client)

    n_replicas = 2
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    common.create_pv_for_volume(client, core_api, volume, volume_name)
    common.create_pvc_for_volume(client, core_api, volume, volume_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#34;name&#34;: &#34;pod-data&#34;,
        &#34;persistentVolumeClaim&#34;: {
            &#34;claimName&#34;: volume_name
        }
    }]

    for i in range(1, 4):
        pod_node_name = n2.name if i % 2 == 0 else n1.name
        pod[&#39;spec&#39;][&#39;nodeSelector&#39;] = {
            &#34;kubernetes.io/hostname&#34;: pod_node_name
        }
        common.create_and_wait_pod(core_api, pod)

        client.update(n3, allowScheduling=False, evictionRequested=True)

        duplicate_node = [n1.name, n2.name]
        duplicate_node.remove(pod_node_name)
        for _ in range(RETRY_COUNTS):
            pod_node_r_count = common.get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            duplicate_node_r_count = common.get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=True)
            balance_node_r_count = common.get_host_replica_count(
                client, volume_name, n3.name, chk_running=False)

            if pod_node_r_count == duplicate_node_r_count == 1 and \
                    balance_node_r_count == 0:
                break

            time.sleep(RETRY_INTERVAL)
        assert pod_node_r_count == 1
        assert duplicate_node_r_count == 1
        assert balance_node_r_count == 0

        client.update(n3, allowScheduling=True)

        for _ in range(RETRY_COUNTS):
            pod_node_r_count = common.get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            duplicate_node_r_count = common.get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=False)
            balance_node_r_count = common.get_host_replica_count(
                client, volume_name, n3.name, chk_running=True)

            if pod_node_r_count == balance_node_r_count == 1 and \
                    duplicate_node_r_count == 0:
                break

            time.sleep(RETRY_INTERVAL)
        assert pod_node_r_count == 1
        assert duplicate_node_r_count == 0
        assert balance_node_r_count == 1

        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        assert z1_r_count == z2_r_count == 1

        # loop 3 times and each to wait 5 seconds to ensure there is no
        # re-scheduling happening.
        for _ in range(3):
            time.sleep(5)
            assert pod_node_r_count == common.get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            assert duplicate_node_r_count == common.get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=False)
            assert balance_node_r_count == common.get_host_replica_count(
                client, volume_name, n3.name, chk_running=True)

        common.delete_and_wait_pod(core_api, pod[&#39;metadata&#39;][&#39;name&#39;])</code></pre>
</details>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<div class="desc"><p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set <code>replicaAutoBalance</code> to <code>best-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-1.
set node-3 to zone-1.
set node-4 to zone-2.
set node-5 to zone-2.
And disable scheduling for node-2.
disable scheduling for node-3.
disable scheduling for node-4.
disable scheduling for node-5.
And create volume with 4 replicas.
And attach the volume to node-1.</p>
<p>Scenario: replica auto-balance zones with best-effort should balance
replicas in zone.</p>
<p>Given 4 replica running on node-1.
0 replica running on node-2.
0 replica running on node-3.
0 replica running on node-4.
0 replica running on node-5.</p>
<p>When enable scheduling for node-4.
Then count replicas on each zones.
And 2 replica running on zode-1.
2 replica running on zode-2.</p>
<p>When enable scheduling for node-2.
enable scheduling for node-3.
Then count replicas on each nodes.
And 1 replica running on node-1.
1 replica running on node-2.
1 replica running on node-3.
1 replica running on node-4.
0 replica running on node-5.</p>
<p>When enable scheduling for node-5.
Then count replicas on each zones.
And 2 replica running on zode-1.
2 replica running on zode-2.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;REQUIRE_5_NODES&#34;)
def test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-1.
        set node-3 to zone-1.
        set node-4 to zone-2.
        set node-5 to zone-2.
    And disable scheduling for node-2.
        disable scheduling for node-3.
        disable scheduling for node-4.
        disable scheduling for node-5.
    And create volume with 4 replicas.
    And attach the volume to node-1.

    Scenario: replica auto-balance zones with best-effort should balance
              replicas in zone.

    Given 4 replica running on node-1.
          0 replica running on node-2.
          0 replica running on node-3.
          0 replica running on node-4.
          0 replica running on node-5.

    When enable scheduling for node-4.
    Then count replicas on each zones.
    And 2 replica running on zode-1.
        2 replica running on zode-2.

    When enable scheduling for node-2.
         enable scheduling for node-3.
    Then count replicas on each nodes.
    And 1 replica running on node-1.
        1 replica running on node-2.
        1 replica running on node-3.
        1 replica running on node-4.
        0 replica running on node-5.

    When enable scheduling for node-5.
    Then count replicas on each zones.
    And 2 replica running on zode-1.
        2 replica running on zode-2.
    &#34;&#34;&#34;

    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_DEFAULT_DATA_LOCALITY, &#34;best-effort&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3, n4, n5 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE1)
    set_k8s_node_zone_label(core_api, n3.name, ZONE1)
    set_k8s_node_zone_label(core_api, n4.name, ZONE2)
    set_k8s_node_zone_label(core_api, n5.name, ZONE2)
    wait_longhorn_node_zone_updated(client)

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)
    client.update(n4, allowScheduling=False)
    client.update(n5, allowScheduling=False)

    n_replicas = 4
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=n1.name)

    for _ in range(RETRY_COUNTS):
        n1_r_count = common.get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = common.get_host_replica_count(
            client, volume_name, n2.name, chk_running=False)
        n3_r_count = common.get_host_replica_count(
            client, volume_name, n3.name, chk_running=False)
        n4_r_count = common.get_host_replica_count(
            client, volume_name, n4.name, chk_running=False)
        n5_r_count = common.get_host_replica_count(
            client, volume_name, n5.name, chk_running=False)

        if n1_r_count == 4 and \
                n2_r_count == n3_r_count == n4_r_count == n5_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL)
    assert n1_r_count == 4
    assert n2_r_count == 0
    assert n3_r_count == 0
    assert n4_r_count == 0
    assert n5_r_count == 0

    client.update(n4, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)

        if z1_r_count == z2_r_count == 2:
            break

        time.sleep(RETRY_INTERVAL)

    assert z1_r_count == 2
    assert z2_r_count == 2

    client.update(n2, allowScheduling=True)
    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        n1_r_count = common.get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = common.get_host_replica_count(
            client, volume_name, n2.name, chk_running=True)
        n3_r_count = common.get_host_replica_count(
            client, volume_name, n3.name, chk_running=True)
        n4_r_count = common.get_host_replica_count(
            client, volume_name, n4.name, chk_running=True)
        n5_r_count = common.get_host_replica_count(
            client, volume_name, n5.name, chk_running=False)

        if n1_r_count == n2_r_count == n3_r_count == n4_r_count == 1 and \
                n5_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL)
    assert n1_r_count == 1
    assert n2_r_count == 1
    assert n3_r_count == 1
    assert n4_r_count == 1
    assert n5_r_count == 0

    client.update(n5, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)

        if z1_r_count == z2_r_count == 2:
            break

        time.sleep(RETRY_INTERVAL)

    assert z1_r_count == 2
    assert z2_r_count == 2</code></pre>
</details>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_zone_least_effort"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_zone_least_effort</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Scenario: replica auto-balance zones with least-effort.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set volume spec <code>replicaAutoBalance</code> to <code>least-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-2.
set node-3 to zone-3.
And disable scheduling for node-2.
disable scheduling for node-3.
And create a volume with 6 replicas.
And attach the volume to self-node.
And 6 replicas running in zone-1.
0 replicas running in zone-2.
0 replicas running in zone-3.</p>
<p>When enable scheduling for node-2.
Then count replicas running on each node.
And zone-1 replica count != zone-2 replica count.
zone-2 replica count != 0.
zone-3 replica count == 0.</p>
<p>When enable scheduling for node-3.
Then count replicas running on each node.
And zone-1 replica count != zone-3 replica count.
zone-2 replica count != 0.
zone-3 replica count != 0.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_replica_auto_balance_zone_least_effort(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance zones with least-effort.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `least-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
        set node-3 to zone-3.
    And disable scheduling for node-2.
        disable scheduling for node-3.
    And create a volume with 6 replicas.
    And attach the volume to self-node.
    And 6 replicas running in zone-1.
        0 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-2.
    Then count replicas running on each node.
    And zone-1 replica count != zone-2 replica count.
        zone-2 replica count != 0.
        zone-3 replica count == 0.

    When enable scheduling for node-3.
    Then count replicas running on each node.
    And zone-1 replica count != zone-3 replica count.
        zone-2 replica count != 0.
        zone-3 replica count != 0.
    &#34;&#34;&#34;
    common.update_setting(client,
                          SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    common.update_setting(client,
                          SETTING_REPLICA_AUTO_BALANCE, &#34;least-effort&#34;)

    n1, n2, n3 = client.list_node()

    set_k8s_node_zone_label(core_api, n1.name, ZONE1)
    set_k8s_node_zone_label(core_api, n2.name, ZONE2)
    set_k8s_node_zone_label(core_api, n3.name, ZONE3)
    wait_longhorn_node_zone_updated(client)

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)

    n_replicas = 6
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == 6 and z2_r_count == z3_r_count == 0:
            break

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count == 6
    assert z2_r_count == 0
    assert z3_r_count == 0

    client.update(n2, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        all_r_count = z1_r_count + z2_r_count + z3_r_count
        if z2_r_count != 0 and all_r_count == n_replicas:
            break

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count != z2_r_count
    assert z2_r_count != 0
    assert z3_r_count == 0

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        all_r_count = z1_r_count + z2_r_count + z3_r_count
        if z3_r_count != 0 and all_r_count == n_replicas:
            break

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count != z3_r_count
    assert z2_r_count != 0
    assert z3_r_count != 0</code></pre>
</details>
</dd>
<dt id="tests.test_zone.test_replica_zone_anti_affinity"><code class="name flex">
<span>def <span class="ident">test_replica_zone_anti_affinity</span></span>(<span>client, core_api, volume_name, k8s_node_zone_tags)</span>
</code></dt>
<dd>
<div class="desc"><p>Test replica scheduler with zone anti-affinity</p>
<ol>
<li>Set zone anti-affinity to hard.</li>
<li>Label nodes 1 &amp; 2 with same zone label "zone1".
Label node 3 with zone label "zone2".</li>
<li>Create a volume with 3 replicas.</li>
<li>Wait for volume condition <code>scheduled</code> to be false.</li>
<li>Label node 2 with zone label "zone3".</li>
<li>Wait for volume condition <code>scheduled</code> to be success.</li>
<li>Clear the volume.</li>
<li>Set zone anti-affinity to soft.</li>
<li>Change the zone labels on node 1 &amp; 2 &amp; 3 to "zone1".</li>
<li>Create a volume.</li>
<li>Wait for volume condition <code>scheduled</code> to be success.</li>
<li>Clean up the replica count, the zone labels and the volume.</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.node  # NOQA
def test_replica_zone_anti_affinity(client, core_api, volume_name, k8s_node_zone_tags):  # NOQA
    &#34;&#34;&#34;
    Test replica scheduler with zone anti-affinity

    1. Set zone anti-affinity to hard.
    2. Label nodes 1 &amp; 2 with same zone label &#34;zone1&#34;.
    Label node 3 with zone label &#34;zone2&#34;.
    3. Create a volume with 3 replicas.
    4. Wait for volume condition `scheduled` to be false.
    5. Label node 2 with zone label &#34;zone3&#34;.
    6. Wait for volume condition `scheduled` to be success.
    7. Clear the volume.
    8. Set zone anti-affinity to soft.
    9. Change the zone labels on node 1 &amp; 2 &amp; 3 to &#34;zone1&#34;.
    10. Create a volume.
    11. Wait for volume condition `scheduled` to be success.
    12. Clean up the replica count, the zone labels and the volume.
    &#34;&#34;&#34;

    wait_longhorn_node_zone_updated(client)

    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_node_soft_anti_affinity_setting, value=&#34;false&#34;)

    replica_zone_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY)
    client.update(replica_zone_soft_anti_affinity_setting, value=&#34;false&#34;)

    volume = create_and_check_volume(client, volume_name)

    lh_nodes = client.list_node()

    count = 0
    for node in lh_nodes:
        count += 1
        set_k8s_node_zone_label(core_api, node.name, &#34;lh-zone&#34; + str(count))

    wait_longhorn_node_zone_updated(client)

    wait_for_volume_condition_scheduled(client, volume_name,
                                        &#34;status&#34;,
                                        CONDITION_STATUS_TRUE)

    replica_zone_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY)
    client.update(replica_zone_soft_anti_affinity_setting, value=&#34;true&#34;)

    volume = client.by_id_volume(volume_name)
    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    for node in lh_nodes:
        set_k8s_node_zone_label(core_api, node.name, &#34;lh-zone1&#34;)

    wait_longhorn_node_zone_updated(client)

    volume = create_and_check_volume(client, volume_name)
    wait_for_volume_condition_scheduled(client, volume_name,
                                        &#34;status&#34;,
                                        CONDITION_STATUS_TRUE)</code></pre>
</details>
</dd>
<dt id="tests.test_zone.test_zone_tags"><code class="name flex">
<span>def <span class="ident">test_zone_tags</span></span>(<span>client, core_api, volume_name, k8s_node_zone_tags)</span>
</code></dt>
<dd>
<div class="desc"><p>Test anti affinity zone feature</p>
<ol>
<li>Add Kubernetes zone labels to the nodes<ol>
<li>Only two zones now: zone1 and zone2</li>
</ol>
</li>
<li>Create a volume with two replicas</li>
<li>Verify zone1 and zone2 either has one replica.</li>
<li>Remove a random replica and wait for volume to finish rebuild</li>
<li>Verify zone1 and zone2 either has one replica.</li>
<li>Repeat step 4-5 a few times.</li>
<li>Update volume to 3 replicas, make sure they're scheduled on 3 nodes</li>
<li>Remove a random replica and wait for volume to finish rebuild</li>
<li>Make sure replicas are on different nodes</li>
<li>Repeat step 8-9 a few times</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_zone_tags(client, core_api, volume_name, k8s_node_zone_tags):  # NOQA
    &#34;&#34;&#34;
    Test anti affinity zone feature

    1. Add Kubernetes zone labels to the nodes
        1. Only two zones now: zone1 and zone2
    2. Create a volume with two replicas
    3. Verify zone1 and zone2 either has one replica.
    4. Remove a random replica and wait for volume to finish rebuild
    5. Verify zone1 and zone2 either has one replica.
    6. Repeat step 4-5 a few times.
    7. Update volume to 3 replicas, make sure they&#39;re scheduled on 3 nodes
    8. Remove a random replica and wait for volume to finish rebuild
    9. Make sure replicas are on different nodes
    10. Repeat step 8-9 a few times
    &#34;&#34;&#34;

    wait_longhorn_node_zone_updated(client)

    volume = create_and_check_volume(client, volume_name, num_of_replicas=2)

    host_id = get_self_host_id()

    volume.attach(hostId=host_id)

    volume = wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)

    zone1_replica_count = get_zone_replica_count(client, volume_name, ZONE1)
    zone2_replica_count = get_zone_replica_count(client, volume_name, ZONE2)

    assert zone1_replica_count == zone2_replica_count

    for i in range(randrange(3, 5)):
        volume = client.by_id_volume(volume_name)

        replica_count = len(volume.replicas)
        assert replica_count == 2

        replica_id = randrange(0, replica_count)

        replica_name = volume.replicas[replica_id].name

        volume.replicaRemove(name=replica_name)

        wait_for_volume_degraded(client, volume_name)

        wait_for_volume_healthy(client, volume_name)

        wait_for_volume_replica_count(client, volume_name, replica_count)

        volume = client.by_id_volume(volume_name)

        replica_names = map(lambda replica: replica.name, volume[&#34;replicas&#34;])

        wait_new_replica_ready(client, volume_name, replica_names)

        zone1_replica_count = \
            get_zone_replica_count(client, volume_name, ZONE1)
        zone2_replica_count = \
            get_zone_replica_count(client, volume_name, ZONE2)

        assert zone1_replica_count == zone2_replica_count

    volume.updateReplicaCount(replicaCount=3)

    wait_for_volume_degraded(client, volume_name)

    wait_for_volume_replica_count(client, volume_name, 3)

    wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)

    lh_node_names = list(map(lambda node: node.name, client.list_node()))

    for replica in volume.replicas:
        lh_node_names.remove(replica.hostId)

    assert lh_node_names == []

    for i in range(randrange(3, 5)):
        volume = client.by_id_volume(volume_name)

        replica_count = len(volume.replicas)
        assert replica_count == 3

        replica_id = randrange(0, replica_count)

        replica_name = volume.replicas[replica_id].name

        volume.replicaRemove(name=replica_name)

        wait_for_volume_degraded(client, volume_name)

        wait_for_volume_healthy(client, volume_name)

        wait_for_volume_replica_count(client, volume_name, replica_count)

        volume = client.by_id_volume(volume_name)

        lh_node_names = list(map(lambda node: node.name, client.list_node()))

        for replica in volume.replicas:
            lh_node_names.remove(replica.hostId)

        assert lh_node_names == []</code></pre>
</details>
</dd>
<dt id="tests.test_zone.wait_longhorn_node_zone_updated"><code class="name flex">
<span>def <span class="ident">wait_longhorn_node_zone_updated</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_longhorn_node_zone_updated(client): # NOQA

    lh_nodes = client.list_node()
    node_names = map(lambda node: node.name, lh_nodes)

    for node_name in node_names:
        for j in range(RETRY_COUNTS):
            lh_node = client.by_id_node(node_name)
            if lh_node.zone != &#39;&#39;:
                break
            time.sleep(RETRY_INTERVAL)

        assert lh_node.zone != &#39;&#39;</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_zone.get_zone_replica_count" href="#tests.test_zone.get_zone_replica_count">get_zone_replica_count</a></code></li>
<li><code><a title="tests.test_zone.k8s_node_zone_tags" href="#tests.test_zone.k8s_node_zone_tags">k8s_node_zone_tags</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_node_duplicates_in_multiple_zones" href="#tests.test_zone.test_replica_auto_balance_node_duplicates_in_multiple_zones">test_replica_auto_balance_node_duplicates_in_multiple_zones</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_zone_best_effort" href="#tests.test_zone.test_replica_auto_balance_zone_best_effort">test_replica_auto_balance_zone_best_effort</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_zone_best_effort_with_data_locality" href="#tests.test_zone.test_replica_auto_balance_zone_best_effort_with_data_locality">test_replica_auto_balance_zone_best_effort_with_data_locality</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones" href="#tests.test_zone.test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones">test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_zone_least_effort" href="#tests.test_zone.test_replica_auto_balance_zone_least_effort">test_replica_auto_balance_zone_least_effort</a></code></li>
<li><code><a title="tests.test_zone.test_replica_zone_anti_affinity" href="#tests.test_zone.test_replica_zone_anti_affinity">test_replica_zone_anti_affinity</a></code></li>
<li><code><a title="tests.test_zone.test_zone_tags" href="#tests.test_zone.test_zone_tags">test_zone_tags</a></code></li>
<li><code><a title="tests.test_zone.wait_longhorn_node_zone_updated" href="#tests.test_zone.wait_longhorn_node_zone_updated">wait_longhorn_node_zone_updated</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>