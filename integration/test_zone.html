<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>tests.test_zone API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_zone</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_zone.get_zone_replica_count"><code class="name flex">
<span>def <span class="ident">get_zone_replica_count</span></span>(<span>client, volume_name, zone_name, chk_running=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_zone_replica_count(client, volume_name, zone_name, chk_running=False): # NOQA
    volume = client.by_id_volume(volume_name)

    zone_replica_count = 0
    for replica in volume.replicas:
        if chk_running and not replica.running:
            continue
        replica_host_id = replica.hostId
        replica_host_zone = client.by_id_node(replica_host_id).zone
        if replica_host_zone == zone_name:
            zone_replica_count += 1
    return zone_replica_count</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_zone.k8s_node_zone_tags"><code class="name flex">
<span>def <span class="ident">k8s_node_zone_tags</span></span>(<span>client, core_api)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def k8s_node_zone_tags(client, core_api):  # NOQA

    k8s_zone_label = get_k8s_zone_label()
    lh_nodes = client.list_node()

    node_index = 0
    for node in lh_nodes:
        node_name = node.name

        if node_index % 2 == 0:
            zone = ZONE1
        else:
            zone = ZONE2

        payload = {
            &#34;metadata&#34;: {
                &#34;labels&#34;: {
                    k8s_zone_label: zone}
            }
        }

        core_api.patch_node(node_name, body=payload)
        node_index += 1

    yield

    lh_nodes = client.list_node()

    node_index = 0
    for node in lh_nodes:
        node_name = node.name

        payload = {
            &#34;metadata&#34;: {
                &#34;labels&#34;: {
                    k8s_zone_label: None}
            }
        }

        core_api.patch_node(node_name, body=payload)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_node_duplicates_in_multiple_zones"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_node_duplicates_in_multiple_zones</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_replica_auto_balance_node_duplicates_in_multiple_zones(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance to nodes with duplicated replicas in the
              zone.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `least-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
    And disable scheduling for node-3.
    And create a volume with 3 replicas.
    And attach the volume to self-node.
    And zone-1 and zone-2 should contain 3 replica in total.

    When set node-3 to the zone with duplicated replicas.
    And enable scheduling for node-3.
    Then count replicas running on each node.
    And 1 replica running on node-1
        1 replica running on node-2
        1 replica running on node-3.
    And count replicas running in each zone.
    And total of 3 replicas running in zone-1 and zone-2.
    &#34;&#34;&#34;

    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;least-effort&#34;)

    n1, n2, n3 = client.list_node()

    node_zone_map = {
        n1.name: ZONE1,
        n2.name: ZONE2,
        n3.name: &#34;temp&#34;
    }
    set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    client.update(n3, allowScheduling=False)

    n_replicas = 3
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())
    z1_r_count = get_zone_replica_count(client, volume_name, ZONE1)
    z2_r_count = get_zone_replica_count(client, volume_name, ZONE2)
    assert z1_r_count + z2_r_count == n_replicas

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {}
        if z1_r_count == 2:
            node_zone_map = {
                n1.name: ZONE1,
                n2.name: ZONE2,
                n3.name: ZONE1
            }
        else:
            node_zone_map = {
                n1.name: ZONE1,
                n2.name: ZONE2,
                n3.name: ZONE2
            }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        n1_r_count = get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = get_host_replica_count(
            client, volume_name, n2.name, chk_running=True)
        n3_r_count = get_host_replica_count(
            client, volume_name, n3.name, chk_running=True)

        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)

        if n1_r_count == n2_r_count == n3_r_count == 1 and \
                z1_r_count + z2_r_count == n_replicas:
            break
        time.sleep(RETRY_INTERVAL)

    assert n1_r_count == 1
    assert n2_r_count == 1
    assert n3_r_count == 1

    assert z1_r_count + z2_r_count == n_replicas</code></pre>
</details>
<div class="desc"><p>Scenario: replica auto-balance to nodes with duplicated replicas in the
zone.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set volume spec <code>replicaAutoBalance</code> to <code>least-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-2.
And disable scheduling for node-3.
And create a volume with 3 replicas.
And attach the volume to self-node.
And zone-1 and zone-2 should contain 3 replica in total.</p>
<p>When set node-3 to the zone with duplicated replicas.
And enable scheduling for node-3.
Then count replicas running on each node.
And 1 replica running on node-1
1 replica running on node-2
1 replica running on node-3.
And count replicas running in each zone.
And total of 3 replicas running in zone-1 and zone-2.</p></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_should_respect_node_selector"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_should_respect_node_selector</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_replica_auto_balance_should_respect_node_selector(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Background:

    Given Setting (replica-soft-anti-affinity) is (true).
    And Setting (replica-zone-soft-anti-affinity) is (true).
    And Node (node-1, node-2) has tag (tag-0).
    And Node (node-1) is in zone (lh-zone-1).
        Node (node-2) is in zone (lh-zone-2).
        Node (node-3) is in zone (should-not-schedule).

    Scenario Outline: replica auto-balance should respect node-selector.

    Issue: https://github.com/longhorn/longhorn/issues/5971

    Given Volume created.
    And Volume replica number is (3).
    And Volume has node selector (tag-0).
    And Volume attached (node-1).
    And Replica is in zone (lh-zone-1, lh-zone-2).

    When Setting (replica-auto-balance) is (least-effort).

    Then Replica is in zone (lh-zone-1, lh-zone-2) (loop 10 sec).
    &#34;&#34;&#34;
    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)

    n1, n2, n3 = client.list_node()

    selected_nodes = [n1, n2]

    node_tag = &#34;tag0&#34;
    for node in selected_nodes:
        set_node_tags(client, node, tags=[node_tag])
        wait_for_node_tag_update(client, node.name, [node_tag])

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {
            n1.name: ZONE1,
            n2.name: ZONE2,
            n3.name: &#34;should-not-schedule&#34;
        }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    n_replicas = 3
    client.create_volume(name=volume_name,
                         numberOfReplicas=n_replicas,
                         nodeSelector=[node_tag],
                         dataEngine=DATA_ENGINE)
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=selected_nodes[0].name)

    z1_r_count = get_zone_replica_count(client, volume_name, ZONE1)
    z2_r_count = get_zone_replica_count(client, volume_name, ZONE2)
    assert z1_r_count + z2_r_count == n_replicas

    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;least-effort&#34;)

    # Check over 10 seconds to check for unexpected re-scheduling.
    for _ in range(10):
        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(1)

        check_z1_r_count = get_zone_replica_count(client, volume_name, ZONE1)
        check_z2_r_count = get_zone_replica_count(client, volume_name, ZONE2)

        assert check_z1_r_count == z1_r_count
        assert check_z2_r_count == z2_r_count</code></pre>
</details>
<div class="desc"><p>Background:</p>
<p>Given Setting (replica-soft-anti-affinity) is (true).
And Setting (replica-zone-soft-anti-affinity) is (true).
And Node (node-1, node-2) has tag (tag-0).
And Node (node-1) is in zone (lh-zone-1).
Node (node-2) is in zone (lh-zone-2).
Node (node-3) is in zone (should-not-schedule).</p>
<p>Scenario Outline: replica auto-balance should respect node-selector.</p>
<p>Issue: <a href="https://github.com/longhorn/longhorn/issues/5971">https://github.com/longhorn/longhorn/issues/5971</a></p>
<p>Given Volume created.
And Volume replica number is (3).
And Volume has node selector (tag-0).
And Volume attached (node-1).
And Replica is in zone (lh-zone-1, lh-zone-2).</p>
<p>When Setting (replica-auto-balance) is (least-effort).</p>
<p>Then Replica is in zone (lh-zone-1, lh-zone-2) (loop 10 sec).</p></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_when_disabled_disk_scheduling_in_zone"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_when_disabled_disk_scheduling_in_zone</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_replica_auto_balance_when_disabled_disk_scheduling_in_zone(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance when disk scheduling is disabled on nodes
              in a zone.

    Issue: https://github.com/longhorn/longhorn/issues/6508

    Given `replica-soft-anti-affinity` setting is `true`.
    And node-1 is in zone-1.
        node-2 is in zone-2.
        node-3 is in zone-3.
    And disk scheduling is disabled on node-3.
    And create a volume with 3 replicas.
    And attach the volume to test pod node.
    And 3 replicas running in zone-1 and zone-2.
        0 replicas running in zone-3.

    When set `replica-auto-balance` to `best-effort`.

    Then 3 replicas running in zone-1 and zone-2.
         0 replicas running in zone-3.
    And replica count remains stable across zones and nodes.
    &#34;&#34;&#34;
    # Set `replica-soft-anti-affinity` to `true`.
    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)

    # Assign nodes to respective zones
    node1, node2, node3 = client.list_node()

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {
            node1.name: ZONE1,
            node2.name: ZONE2,
            node3.name: ZONE3
        }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    # Disable disk scheduling on node 3
    cleanup_node_disks(client, node3.name)

    # Create a volume with 3 replicas
    num_of_replicas = 3
    volume = client.create_volume(name=volume_name,
                                  numberOfReplicas=num_of_replicas,
                                  dataEngine=DATA_ENGINE)

    # Wait for the volume to detach and attach it to the test pod node
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())

    # Define a function to assert replica count
    def assert_replica_count(is_stable=False):
        assert_tolerated = 0
        for _ in range(RETRY_COUNTS):
            if is_k8s_node_gke_cos(core_api):
                _set_and_wait_k8s_node_zone_label()

            time.sleep(RETRY_INTERVAL)

            zone3_replica_count = get_zone_replica_count(
                client, volume_name, ZONE3, chk_running=True)
            assert zone3_replica_count == 0

            total_replica_count = \
                get_zone_replica_count(
                    client, volume_name, ZONE1, chk_running=True) + \
                get_zone_replica_count(
                    client, volume_name, ZONE2, chk_running=True)

            if is_stable:
                try:
                    assert total_replica_count == num_of_replicas
                except AssertionError as e:
                    # The GKE zone label undergoes periodic updates to reflect
                    # the current zone. Consequently, we cannot guarantee the
                    # exact zone of the replica node. Therefore, we&#39;ll allow
                    # for one assertion error to accommodate GKE&#39;s update
                    # process.
                    if is_k8s_node_gke_cos(core_api) and assert_tolerated &lt; 1:
                        assert_tolerated += 1
                    else:
                        raise AssertionError(e)
            elif total_replica_count == num_of_replicas:
                break

        assert total_replica_count == 3

    # Perform the initial assertion to ensure the replica count is as expected
    assert_replica_count()

    # Update the replica-auto-balance setting to `best-effort`
    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    # Perform the final assertion to ensure the replica count is as expected,
    # and stable after the setting update
    assert_replica_count(is_stable=True)</code></pre>
</details>
<div class="desc"><p>Scenario: replica auto-balance when disk scheduling is disabled on nodes
in a zone.</p>
<p>Issue: <a href="https://github.com/longhorn/longhorn/issues/6508">https://github.com/longhorn/longhorn/issues/6508</a></p>
<p>Given <code>replica-soft-anti-affinity</code> setting is <code>true</code>.
And node-1 is in zone-1.
node-2 is in zone-2.
node-3 is in zone-3.
And disk scheduling is disabled on node-3.
And create a volume with 3 replicas.
And attach the volume to test pod node.
And 3 replicas running in zone-1 and zone-2.
0 replicas running in zone-3.</p>
<p>When set <code>replica-auto-balance</code> to <code>best-effort</code>.</p>
<p>Then 3 replicas running in zone-1 and zone-2.
0 replicas running in zone-3.
And replica count remains stable across zones and nodes.</p></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_when_no_storage_available_in_zone"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_when_no_storage_available_in_zone</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_replica_auto_balance_when_no_storage_available_in_zone(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance when there is no storage available on nodes
              in a zone.

    Issue: https://github.com/longhorn/longhorn/issues/6671

    Given `replica-soft-anti-affinity` setting is `true`.
    And node-1 is in zone-1.
        node-2 is in zone-2.
        node-3 is in zone-3.
    And fill up the storage on node-3.
    And create a volume with 3 replicas.
    And attach the volume to test pod node.
    And 3 replicas running in zone-1 and zone-2.
        0 replicas running in zone-3.

    When set `replica-auto-balance` to `best-effort`.

    Then 3 replicas running in zone-1 and zone-2.
         0 replicas running in zone-3.
    And replica count remains stable across zones and nodes.
    &#34;&#34;&#34;
    # Set `replica-soft-anti-affinity` to `true`.
    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)

    # Assign nodes to respective zones
    node1, node2, node3 = client.list_node()

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {
            node1.name: ZONE1,
            node2.name: ZONE2,
            node3.name: ZONE3
        }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    # Fill up the storage on node 3
    for _, disk in node3.disks.items():
        disk.storageReserved = disk.storageMaximum

    update_disks = get_update_disks(node3.disks)
    update_node_disks(client, node3.name, disks=update_disks, retry=True)

    # Create a volume with 3 replicas
    num_of_replicas = 3
    volume = client.create_volume(name=volume_name,
                                  numberOfReplicas=num_of_replicas,
                                  dataEngine=DATA_ENGINE)

    # Wait for the volume to detach and attach it to the test pod node
    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())

    # Define a function to assert replica count
    def assert_replica_count(is_stable=False):
        assert_tolerated = 0
        for _ in range(RETRY_COUNTS):
            if is_k8s_node_gke_cos(core_api):
                _set_and_wait_k8s_node_zone_label()

            time.sleep(RETRY_INTERVAL)

            zone3_replica_count = get_zone_replica_count(
                client, volume_name, ZONE3, chk_running=True)
            assert zone3_replica_count == 0

            total_replica_count = \
                get_zone_replica_count(
                    client, volume_name, ZONE1, chk_running=True) + \
                get_zone_replica_count(
                    client, volume_name, ZONE2, chk_running=True)

            if is_stable:
                try:
                    assert total_replica_count == num_of_replicas
                except AssertionError as e:
                    # The GKE zone label undergoes periodic updates to reflect
                    # the current zone. Consequently, we cannot guarantee the
                    # exact zone of the replica node. Therefore, we&#39;ll allow
                    # for one assertion error to accommodate GKE&#39;s update
                    # process.
                    if is_k8s_node_gke_cos(core_api) and assert_tolerated &lt; 1:
                        assert_tolerated += 1
                    else:
                        raise AssertionError(e)
            elif total_replica_count == num_of_replicas:
                break

        assert total_replica_count == 3

    # Perform the initial assertion to ensure the replica count is as expected
    assert_replica_count()

    # Update the replica-auto-balance setting to `best-effort`
    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    # Perform the final assertion to ensure the replica count is as expected,
    # and stable after the setting update
    assert_replica_count(is_stable=True)</code></pre>
</details>
<div class="desc"><p>Scenario: replica auto-balance when there is no storage available on nodes
in a zone.</p>
<p>Issue: <a href="https://github.com/longhorn/longhorn/issues/6671">https://github.com/longhorn/longhorn/issues/6671</a></p>
<p>Given <code>replica-soft-anti-affinity</code> setting is <code>true</code>.
And node-1 is in zone-1.
node-2 is in zone-2.
node-3 is in zone-3.
And fill up the storage on node-3.
And create a volume with 3 replicas.
And attach the volume to test pod node.
And 3 replicas running in zone-1 and zone-2.
0 replicas running in zone-3.</p>
<p>When set <code>replica-auto-balance</code> to <code>best-effort</code>.</p>
<p>Then 3 replicas running in zone-1 and zone-2.
0 replicas running in zone-3.
And replica count remains stable across zones and nodes.</p></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_when_replica_on_unschedulable_node"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_when_replica_on_unschedulable_node</span></span>(<span>client, core_api, volume_name, request)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_replica_auto_balance_when_replica_on_unschedulable_node(client, core_api, volume_name, request):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance when replica already running on
              an unschedulable node.

    Issue: https://github.com/longhorn/longhorn/issues/4502

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `least-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
        set node-3 to zone-3.
    And node-2 tagged `AVAIL`.
        node-3 tagged `AVAIL`.
    And create a volume with 2 replicas and nodeSelector `AVAIL`.
    And attach the volume to self-node.
    And 0 replicas running in zone-1.
        1 replicas running in zone-2.
        1 replicas running in zone-3.

    When cordone node-2.
    Then replicas should remain balanced with,
         0 replicas running in zone-1.
         1 replicas running in zone-2.
         1 replicas running in zone-3.
    &#34;&#34;&#34;
    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;least-effort&#34;)

    n1, n2, n3 = client.list_node()

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {
            n1.name: ZONE1,
            n2.name: ZONE2,
            n3.name: ZONE3
        }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    client.update(n2, allowScheduling=True, tags=[&#34;AVAIL&#34;])
    client.update(n3, allowScheduling=True, tags=[&#34;AVAIL&#34;])

    n_replicas = 2
    volume = client.create_volume(name=volume_name,
                                  numberOfReplicas=n_replicas,
                                  nodeSelector=[&#34;AVAIL&#34;],
                                  dataLocality=&#34;best-effort&#34;,
                                  dataEngine=DATA_ENGINE)

    volume = wait_for_volume_detached(client, volume_name)
    volume.attach(hostId=get_self_host_id())

    for _ in range(RETRY_COUNTS):
        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == 0 and (z2_r_count and z3_r_count == 1):
            break
        time.sleep(RETRY_INTERVAL)

    assert z1_r_count == 0 and (z2_r_count and z3_r_count == 1)

    # Set cordon on node
    def finalizer():
        set_node_cordon(core_api, n2.name, False)
    request.addfinalizer(finalizer)

    set_node_cordon(core_api, n2.name, True)

    assert_tolerated = 0
    for _ in range(RETRY_COUNTS):
        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)
        try:
            assert z1_r_count == 0 and (z2_r_count and z3_r_count == 1)
        except AssertionError as e:
            # The GKE zone label undergoes periodic updates to reflect
            # the current zone. Consequently, we cannot guarantee the
            # exact zone of the replica node. Therefore, we&#39;ll allow
            # for one assertion error to accommodate GKE&#39;s update process.
            if is_k8s_node_gke_cos(core_api) and assert_tolerated &lt; 1:
                assert_tolerated += 1
            else:
                raise AssertionError(e)

        volume = client.by_id_volume(volume_name)
        for status in volume.rebuildStatus:
            assert not status.isRebuilding

        time.sleep(RETRY_INTERVAL)</code></pre>
</details>
<div class="desc"><p>Scenario: replica auto-balance when replica already running on
an unschedulable node.</p>
<p>Issue: <a href="https://github.com/longhorn/longhorn/issues/4502">https://github.com/longhorn/longhorn/issues/4502</a></p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set volume spec <code>replicaAutoBalance</code> to <code>least-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-2.
set node-3 to zone-3.
And node-2 tagged <code>AVAIL</code>.
node-3 tagged <code>AVAIL</code>.
And create a volume with 2 replicas and nodeSelector <code>AVAIL</code>.
And attach the volume to self-node.
And 0 replicas running in zone-1.
1 replicas running in zone-2.
1 replicas running in zone-3.</p>
<p>When cordone node-2.
Then replicas should remain balanced with,
0 replicas running in zone-1.
1 replicas running in zone-2.
1 replicas running in zone-3.</p></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_zone_best_effort"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_zone_best_effort</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_replica_auto_balance_zone_best_effort(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance zones with best-effort.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
        set node-3 to zone-3.
    And disable scheduling for node-2.
        disable scheduling for node-3.
    And create a volume with 6 replicas.
    And attach the volume to self-node.
    And 6 replicas running in zone-1.
        0 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-2.
    Then count replicas running on each node.
    And 3 replicas running in zone-1.
        3 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-3.
    Then count replicas running on each node.
    And 2 replicas running in zone-1.
        2 replicas running in zone-2.
        2 replicas running in zone-3.
    &#34;&#34;&#34;

    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3 = client.list_node()

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {
            n1.name: ZONE1,
            n2.name: ZONE2,
            n3.name: ZONE3
        }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)

    n_replicas = 6
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == 6 and z2_r_count == z3_r_count == 0:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count == 6
    assert z2_r_count == 0
    assert z3_r_count == 0

    client.update(n2, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == z2_r_count == 3 and z3_r_count == 0:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL_LONG)
    assert z1_r_count == 3
    assert z2_r_count == 3
    assert z3_r_count == 0

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == z2_r_count == z3_r_count == 2:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL_LONG)
    assert z1_r_count == 2
    assert z2_r_count == 2
    assert z3_r_count == 2</code></pre>
</details>
<div class="desc"><p>Scenario: replica auto-balance zones with best-effort.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set volume spec <code>replicaAutoBalance</code> to <code>best-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-2.
set node-3 to zone-3.
And disable scheduling for node-2.
disable scheduling for node-3.
And create a volume with 6 replicas.
And attach the volume to self-node.
And 6 replicas running in zone-1.
0 replicas running in zone-2.
0 replicas running in zone-3.</p>
<p>When enable scheduling for node-2.
Then count replicas running on each node.
And 3 replicas running in zone-1.
3 replicas running in zone-2.
0 replicas running in zone-3.</p>
<p>When enable scheduling for node-3.
Then count replicas running on each node.
And 2 replicas running in zone-1.
2 replicas running in zone-2.
2 replicas running in zone-3.</p></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_zone_best_effort_with_data_locality"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_zone_best_effort_with_data_locality</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_replica_auto_balance_zone_best_effort_with_data_locality(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Background:
    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set `default-data-locality` to `best-effort`.
    And set `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-1.
        set node-3 to zone-2.
    And create volume with 2 replicas.
    And create pv for volume.
    And create pvc for volume.

    Scenario Outline: replica auto-balance zones with best-effort should
                      not remove pod local replicas when data locality is
                      enabled (best-effort).

    Given create and wait pod on &lt;pod-node&gt;.
    And disable scheduling and evict node-3.
    And count replicas on each nodes.
    And 1 replica running on &lt;pod-node&gt;.
        1 replica running on &lt;duplicate-node&gt;.
        0 replica running on node-3.

    When enable scheduling for node-3.
    Then count replicas on each nodes.
    And 1 replica running on &lt;pod-node&gt;.
        0 replica running on &lt;duplicate-node&gt;.
        1 replica running on node-3.
    And count replicas in each zones.
    And 1 replica running in zone-1.
        1 replica running in zone-2.
    And loop 3 times with each wait 5 seconds and count replicas on each nodes.
        To ensure no addition scheduling is happening.
        1 replica running on &lt;pod-node&gt;.
        0 replica running on &lt;duplicate-node&gt;.
        1 replica running on node-3.

    And delete pod.

    Examples:
        | pod-node | duplicate-node |
        | node-1   | node-2         |
        | node-2   | node-1         |
        | node-1   | node-2         |
    &#34;&#34;&#34;

    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_DEFAULT_DATA_LOCALITY, &#34;best-effort&#34;)
    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3 = client.list_node()

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {
            n1.name: ZONE1,
            n2.name: ZONE1,
            n3.name: ZONE2
        }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    n_replicas = 2
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    create_pv_for_volume(client, core_api, volume, volume_name)
    create_pvc_for_volume(client, core_api, volume, volume_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [{
        &#34;name&#34;: &#34;pod-data&#34;,
        &#34;persistentVolumeClaim&#34;: {
            &#34;claimName&#34;: volume_name
        }
    }]

    for i in range(1, 4):
        pod_node_name = n2.name if i % 2 == 0 else n1.name
        pod[&#39;spec&#39;][&#39;nodeSelector&#39;] = {
            &#34;kubernetes.io/hostname&#34;: pod_node_name
        }
        create_and_wait_pod(core_api, pod)

        client.update(n3, allowScheduling=False, evictionRequested=True)

        duplicate_node = [n1.name, n2.name]
        duplicate_node.remove(pod_node_name)
        for _ in range(RETRY_COUNTS):
            if is_k8s_node_gke_cos(core_api):
                _set_and_wait_k8s_node_zone_label()

            pod_node_r_count = get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            duplicate_node_r_count = get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=True)
            balance_node_r_count = get_host_replica_count(
                client, volume_name, n3.name, chk_running=False)

            if pod_node_r_count == duplicate_node_r_count == 1 and \
                    balance_node_r_count == 0:
                break

            time.sleep(RETRY_INTERVAL)
        assert pod_node_r_count == 1
        assert duplicate_node_r_count == 1
        assert balance_node_r_count == 0

        client.update(n3, allowScheduling=True)

        for _ in range(RETRY_COUNTS):
            if is_k8s_node_gke_cos(core_api):
                _set_and_wait_k8s_node_zone_label()

            pod_node_r_count = get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            duplicate_node_r_count = get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=False)
            balance_node_r_count = get_host_replica_count(
                client, volume_name, n3.name, chk_running=True)

            if pod_node_r_count == balance_node_r_count == 1 and \
                    duplicate_node_r_count == 0:
                break

            time.sleep(RETRY_INTERVAL)
        assert pod_node_r_count == 1
        assert duplicate_node_r_count == 0
        assert balance_node_r_count == 1

        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        assert z1_r_count == z2_r_count == 1

        # loop 3 times and each to wait 5 seconds to ensure there is no
        # re-scheduling happening.
        for _ in range(3):
            if is_k8s_node_gke_cos(core_api):
                _set_and_wait_k8s_node_zone_label()

            time.sleep(5)
            assert pod_node_r_count == get_host_replica_count(
                client, volume_name, pod_node_name, chk_running=True)
            assert duplicate_node_r_count == get_host_replica_count(
                client, volume_name, duplicate_node[0], chk_running=False)
            assert balance_node_r_count == get_host_replica_count(
                client, volume_name, n3.name, chk_running=True)

        delete_and_wait_pod(core_api, pod[&#39;metadata&#39;][&#39;name&#39;])</code></pre>
</details>
<div class="desc"><p>Background:
Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set <code>default-data-locality</code> to <code>best-effort</code>.
And set <code>replicaAutoBalance</code> to <code>best-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-1.
set node-3 to zone-2.
And create volume with 2 replicas.
And create pv for volume.
And create pvc for volume.</p>
<p>Scenario Outline: replica auto-balance zones with best-effort should
not remove pod local replicas when data locality is
enabled (best-effort).</p>
<p>Given create and wait pod on <pod-node>.
And disable scheduling and evict node-3.
And count replicas on each nodes.
And 1 replica running on <pod-node>.
1 replica running on <duplicate-node>.
0 replica running on node-3.</p>
<p>When enable scheduling for node-3.
Then count replicas on each nodes.
And 1 replica running on <pod-node>.
0 replica running on <duplicate-node>.
1 replica running on node-3.
And count replicas in each zones.
And 1 replica running in zone-1.
1 replica running in zone-2.
And loop 3 times with each wait 5 seconds and count replicas on each nodes.
To ensure no addition scheduling is happening.
1 replica running on <pod-node>.
0 replica running on <duplicate-node>.
1 replica running on node-3.</p>
<p>And delete pod.</p>
<h2 id="examples">Examples</h2>
<p>| pod-node | duplicate-node |
| node-1
| node-2
|
| node-2
| node-1
|
| node-1
| node-2
|</p></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones</span></span>(<span>client, core_api, volume_name, pod)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.skip(reason=&#34;REQUIRE_5_NODES&#34;)
def test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones(client, core_api, volume_name, pod):  # NOQA
    &#34;&#34;&#34;
    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set `replicaAutoBalance` to `best-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-1.
        set node-3 to zone-1.
        set node-4 to zone-2.
        set node-5 to zone-2.
    And disable scheduling for node-2.
        disable scheduling for node-3.
        disable scheduling for node-4.
        disable scheduling for node-5.
    And create volume with 4 replicas.
    And attach the volume to node-1.

    Scenario: replica auto-balance zones with best-effort should balance
              replicas in zone.

    Given 4 replica running on node-1.
          0 replica running on node-2.
          0 replica running on node-3.
          0 replica running on node-4.
          0 replica running on node-5.

    When enable scheduling for node-4.
    Then count replicas on each zones.
    And 2 replica running on zode-1.
        2 replica running on zode-2.

    When enable scheduling for node-2.
         enable scheduling for node-3.
    Then count replicas on each nodes.
    And 1 replica running on node-1.
        1 replica running on node-2.
        1 replica running on node-3.
        1 replica running on node-4.
        0 replica running on node-5.

    When enable scheduling for node-5.
    Then count replicas on each zones.
    And 2 replica running on zode-1.
        2 replica running on zode-2.
    &#34;&#34;&#34;

    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_DEFAULT_DATA_LOCALITY, &#34;best-effort&#34;)
    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;best-effort&#34;)

    n1, n2, n3, n4, n5 = client.list_node()

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {
            n1.name: ZONE1,
            n2.name: ZONE1,
            n3.name: ZONE1,
            n4.name: ZONE2,
            n5.name: ZONE2
        }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)
    client.update(n4, allowScheduling=False)
    client.update(n5, allowScheduling=False)

    n_replicas = 4
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=n1.name)

    for _ in range(RETRY_COUNTS):
        n1_r_count = get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = get_host_replica_count(
            client, volume_name, n2.name, chk_running=False)
        n3_r_count = get_host_replica_count(
            client, volume_name, n3.name, chk_running=False)
        n4_r_count = get_host_replica_count(
            client, volume_name, n4.name, chk_running=False)
        n5_r_count = get_host_replica_count(
            client, volume_name, n5.name, chk_running=False)

        if n1_r_count == 4 and \
                n2_r_count == n3_r_count == n4_r_count == n5_r_count == 0:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL)
    assert n1_r_count == 4
    assert n2_r_count == 0
    assert n3_r_count == 0
    assert n4_r_count == 0
    assert n5_r_count == 0

    client.update(n4, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)

        if z1_r_count == z2_r_count == 2:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL)

    assert z1_r_count == 2
    assert z2_r_count == 2

    client.update(n2, allowScheduling=True)
    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        n1_r_count = get_host_replica_count(
            client, volume_name, n1.name, chk_running=True)
        n2_r_count = get_host_replica_count(
            client, volume_name, n2.name, chk_running=True)
        n3_r_count = get_host_replica_count(
            client, volume_name, n3.name, chk_running=True)
        n4_r_count = get_host_replica_count(
            client, volume_name, n4.name, chk_running=True)
        n5_r_count = get_host_replica_count(
            client, volume_name, n5.name, chk_running=False)

        if n1_r_count == n2_r_count == n3_r_count == n4_r_count == 1 and \
                n5_r_count == 0:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL)
    assert n1_r_count == 1
    assert n2_r_count == 1
    assert n3_r_count == 1
    assert n4_r_count == 1
    assert n5_r_count == 0

    client.update(n5, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)

        if z1_r_count == z2_r_count == 2:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL)

    assert z1_r_count == 2
    assert z2_r_count == 2</code></pre>
</details>
<div class="desc"><p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set <code>replicaAutoBalance</code> to <code>best-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-1.
set node-3 to zone-1.
set node-4 to zone-2.
set node-5 to zone-2.
And disable scheduling for node-2.
disable scheduling for node-3.
disable scheduling for node-4.
disable scheduling for node-5.
And create volume with 4 replicas.
And attach the volume to node-1.</p>
<p>Scenario: replica auto-balance zones with best-effort should balance
replicas in zone.</p>
<p>Given 4 replica running on node-1.
0 replica running on node-2.
0 replica running on node-3.
0 replica running on node-4.
0 replica running on node-5.</p>
<p>When enable scheduling for node-4.
Then count replicas on each zones.
And 2 replica running on zode-1.
2 replica running on zode-2.</p>
<p>When enable scheduling for node-2.
enable scheduling for node-3.
Then count replicas on each nodes.
And 1 replica running on node-1.
1 replica running on node-2.
1 replica running on node-3.
1 replica running on node-4.
0 replica running on node-5.</p>
<p>When enable scheduling for node-5.
Then count replicas on each zones.
And 2 replica running on zode-1.
2 replica running on zode-2.</p></div>
</dd>
<dt id="tests.test_zone.test_replica_auto_balance_zone_least_effort"><code class="name flex">
<span>def <span class="ident">test_replica_auto_balance_zone_least_effort</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_replica_auto_balance_zone_least_effort(client, core_api, volume_name):  # NOQA
    &#34;&#34;&#34;
    Scenario: replica auto-balance zones with least-effort.

    Given set `replica-soft-anti-affinity` to `true`.
    And set `replica-zone-soft-anti-affinity` to `true`.
    And set volume spec `replicaAutoBalance` to `least-effort`.
    And set node-1 to zone-1.
        set node-2 to zone-2.
        set node-3 to zone-3.
    And disable scheduling for node-2.
        disable scheduling for node-3.
    And create a volume with 6 replicas.
    And attach the volume to self-node.
    And 6 replicas running in zone-1.
        0 replicas running in zone-2.
        0 replicas running in zone-3.

    When enable scheduling for node-2.
    Then count replicas running on each node.
    And zone-1 replica count != zone-2 replica count.
        zone-2 replica count != 0.
        zone-3 replica count == 0.

    When enable scheduling for node-3.
    Then count replicas running on each node.
    And zone-1 replica count != zone-3 replica count.
        zone-2 replica count != 0.
        zone-3 replica count != 0.
    &#34;&#34;&#34;
    update_setting(client, SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY, &#34;true&#34;)
    update_setting(client, SETTING_REPLICA_AUTO_BALANCE, &#34;least-effort&#34;)

    n1, n2, n3 = client.list_node()

    # The GKE zone label is periodically updated with the actual zone.
    # Invoke _set_k8s_node_zone_label to refresh the zone label with each
    # retry iteration to maintain the expected zone label.
    def _set_and_wait_k8s_node_zone_label():
        node_zone_map = {
            n1.name: ZONE1,
            n2.name: ZONE2,
            n3.name: ZONE3
        }
        set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    _set_and_wait_k8s_node_zone_label()

    client.update(n2, allowScheduling=False)
    client.update(n3, allowScheduling=False)

    n_replicas = 6
    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=n_replicas)
    volume.attach(hostId=get_self_host_id())

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        if z1_r_count == 6 and z2_r_count == z3_r_count == 0:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count == 6
    assert z2_r_count == 0
    assert z3_r_count == 0

    client.update(n2, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        all_r_count = z1_r_count + z2_r_count + z3_r_count
        if z2_r_count != 0 and all_r_count == n_replicas:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count != z2_r_count
    assert z2_r_count != 0
    assert z3_r_count == 0

    client.update(n3, allowScheduling=True)

    for _ in range(RETRY_COUNTS):
        z1_r_count = get_zone_replica_count(
            client, volume_name, ZONE1, chk_running=True)
        z2_r_count = get_zone_replica_count(
            client, volume_name, ZONE2, chk_running=True)
        z3_r_count = get_zone_replica_count(
            client, volume_name, ZONE3, chk_running=True)

        all_r_count = z1_r_count + z2_r_count + z3_r_count
        if z3_r_count != 0 and all_r_count == n_replicas:
            break

        if is_k8s_node_gke_cos(core_api):
            _set_and_wait_k8s_node_zone_label()

        time.sleep(RETRY_INTERVAL)
    assert z1_r_count != z3_r_count
    assert z2_r_count != 0
    assert z3_r_count != 0</code></pre>
</details>
<div class="desc"><p>Scenario: replica auto-balance zones with least-effort.</p>
<p>Given set <code>replica-soft-anti-affinity</code> to <code>true</code>.
And set <code>replica-zone-soft-anti-affinity</code> to <code>true</code>.
And set volume spec <code>replicaAutoBalance</code> to <code>least-effort</code>.
And set node-1 to zone-1.
set node-2 to zone-2.
set node-3 to zone-3.
And disable scheduling for node-2.
disable scheduling for node-3.
And create a volume with 6 replicas.
And attach the volume to self-node.
And 6 replicas running in zone-1.
0 replicas running in zone-2.
0 replicas running in zone-3.</p>
<p>When enable scheduling for node-2.
Then count replicas running on each node.
And zone-1 replica count != zone-2 replica count.
zone-2 replica count != 0.
zone-3 replica count == 0.</p>
<p>When enable scheduling for node-3.
Then count replicas running on each node.
And zone-1 replica count != zone-3 replica count.
zone-2 replica count != 0.
zone-3 replica count != 0.</p></div>
</dd>
<dt id="tests.test_zone.test_replica_zone_anti_affinity"><code class="name flex">
<span>def <span class="ident">test_replica_zone_anti_affinity</span></span>(<span>client, core_api, volume_name, k8s_node_zone_tags)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
@pytest.mark.node  # NOQA
def test_replica_zone_anti_affinity(client, core_api, volume_name, k8s_node_zone_tags):  # NOQA
    &#34;&#34;&#34;
    Test replica scheduler with zone anti-affinity

    1. Set zone anti-affinity to hard.
    2. Label nodes 1 &amp; 2 with same zone label &#34;zone1&#34;.
    Label node 3 with zone label &#34;zone2&#34;.
    3. Create a volume with 3 replicas.
    4. Wait for volume condition `scheduled` to be false.
    5. Label node 2 with zone label &#34;zone3&#34;.
    6. Wait for volume condition `scheduled` to be success.
    7. Clear the volume.
    8. Set zone anti-affinity to soft.
    9. Change the zone labels on node 1 &amp; 2 &amp; 3 to &#34;zone1&#34;.
    10. Create a volume.
    11. Wait for volume condition `scheduled` to be success.
    12. Clean up the replica count, the zone labels and the volume.
    &#34;&#34;&#34;

    wait_longhorn_nodes_zone_not_empty(client)

    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    client.update(replica_node_soft_anti_affinity_setting, value=&#34;false&#34;)

    replica_zone_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY)
    client.update(replica_zone_soft_anti_affinity_setting, value=&#34;false&#34;)

    volume = create_and_check_volume(client, volume_name)

    lh_nodes = client.list_node()

    count = 0
    node_zone_map = {}
    for node in lh_nodes:
        count += 1
        node_zone_map[node.name] = &#34;lh-zone&#34; + str(count)

    set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    wait_for_volume_condition_scheduled(client, volume_name,
                                        &#34;status&#34;,
                                        CONDITION_STATUS_TRUE)

    replica_zone_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY)
    client.update(replica_zone_soft_anti_affinity_setting, value=&#34;true&#34;)

    volume = client.by_id_volume(volume_name)
    client.delete(volume)
    wait_for_volume_delete(client, volume_name)

    node_zone_map = {}
    for node in lh_nodes:
        node_zone_map[node.name] = &#34;lh-zone1&#34;

    set_and_wait_k8s_nodes_zone_label(core_api, node_zone_map)

    volume = create_and_check_volume(client, volume_name)
    wait_for_volume_condition_scheduled(client, volume_name,
                                        &#34;status&#34;,
                                        CONDITION_STATUS_TRUE)</code></pre>
</details>
<div class="desc"><p>Test replica scheduler with zone anti-affinity</p>
<ol>
<li>Set zone anti-affinity to hard.</li>
<li>Label nodes 1 &amp; 2 with same zone label "zone1".
Label node 3 with zone label "zone2".</li>
<li>Create a volume with 3 replicas.</li>
<li>Wait for volume condition <code>scheduled</code> to be false.</li>
<li>Label node 2 with zone label "zone3".</li>
<li>Wait for volume condition <code>scheduled</code> to be success.</li>
<li>Clear the volume.</li>
<li>Set zone anti-affinity to soft.</li>
<li>Change the zone labels on node 1 &amp; 2 &amp; 3 to "zone1".</li>
<li>Create a volume.</li>
<li>Wait for volume condition <code>scheduled</code> to be success.</li>
<li>Clean up the replica count, the zone labels and the volume.</li>
</ol></div>
</dd>
<dt id="tests.test_zone.test_zone_tags"><code class="name flex">
<span>def <span class="ident">test_zone_tags</span></span>(<span>client, core_api, volume_name, k8s_node_zone_tags)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.v2_volume_test  # NOQA
def test_zone_tags(client, core_api, volume_name, k8s_node_zone_tags):  # NOQA
    &#34;&#34;&#34;
    Test anti affinity zone feature

    1. Add Kubernetes zone labels to the nodes
        1. Only two zones now: zone1 and zone2
    2. Create a volume with two replicas
    3. Verify zone1 and zone2 either has one replica.
    4. Remove a random replica and wait for volume to finish rebuild
    5. Verify zone1 and zone2 either has one replica.
    6. Repeat step 4-5 a few times.
    7. Update volume to 3 replicas, make sure they&#39;re scheduled on 3 nodes
    8. Remove a random replica and wait for volume to finish rebuild
    9. Make sure replicas are on different nodes
    10. Repeat step 8-9 a few times
    &#34;&#34;&#34;

    wait_longhorn_nodes_zone_not_empty(client)

    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=2)

    host_id = get_self_host_id()

    volume.attach(hostId=host_id)

    volume = wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)

    zone1_replica_count = get_zone_replica_count(client, volume_name, ZONE1)
    zone2_replica_count = get_zone_replica_count(client, volume_name, ZONE2)

    assert zone1_replica_count == zone2_replica_count

    for i in range(randrange(3, 5)):
        volume = client.by_id_volume(volume_name)

        replica_count = len(volume.replicas)
        assert replica_count == 2

        replica_id = randrange(0, replica_count)

        replica_name = volume.replicas[replica_id].name

        volume.replicaRemove(name=replica_name)

        wait_for_volume_degraded(client, volume_name)

        wait_for_volume_healthy(client, volume_name)

        wait_for_volume_replica_count(client, volume_name, replica_count)

        volume = client.by_id_volume(volume_name)

        replica_names = map(lambda replica: replica.name, volume[&#34;replicas&#34;])

        wait_new_replica_ready(client, volume_name, replica_names)

        zone1_replica_count = \
            get_zone_replica_count(client, volume_name, ZONE1)
        zone2_replica_count = \
            get_zone_replica_count(client, volume_name, ZONE2)

        assert zone1_replica_count == zone2_replica_count

    volume.updateReplicaCount(replicaCount=3)

    wait_for_volume_degraded(client, volume_name)

    wait_for_volume_replica_count(client, volume_name, 3)

    wait_for_volume_healthy(client, volume_name)

    volume = client.by_id_volume(volume_name)

    lh_node_names = list(map(lambda node: node.name, client.list_node()))

    for replica in volume.replicas:
        lh_node_names.remove(replica.hostId)

    assert lh_node_names == []

    for i in range(randrange(3, 5)):
        volume = client.by_id_volume(volume_name)

        replica_count = len(volume.replicas)
        assert replica_count == 3

        replica_id = randrange(0, replica_count)

        replica_name = volume.replicas[replica_id].name

        volume.replicaRemove(name=replica_name)

        wait_for_volume_degraded(client, volume_name)

        wait_for_volume_healthy(client, volume_name)

        wait_for_volume_replica_count(client, volume_name, replica_count)

        volume = client.by_id_volume(volume_name)

        lh_node_names = list(map(lambda node: node.name, client.list_node()))

        for replica in volume.replicas:
            lh_node_names.remove(replica.hostId)

        assert lh_node_names == []</code></pre>
</details>
<div class="desc"><p>Test anti affinity zone feature</p>
<ol>
<li>Add Kubernetes zone labels to the nodes<ol>
<li>Only two zones now: zone1 and zone2</li>
</ol>
</li>
<li>Create a volume with two replicas</li>
<li>Verify zone1 and zone2 either has one replica.</li>
<li>Remove a random replica and wait for volume to finish rebuild</li>
<li>Verify zone1 and zone2 either has one replica.</li>
<li>Repeat step 4-5 a few times.</li>
<li>Update volume to 3 replicas, make sure they're scheduled on 3 nodes</li>
<li>Remove a random replica and wait for volume to finish rebuild</li>
<li>Make sure replicas are on different nodes</li>
<li>Repeat step 8-9 a few times</li>
</ol></div>
</dd>
<dt id="tests.test_zone.wait_longhorn_nodes_zone_not_empty"><code class="name flex">
<span>def <span class="ident">wait_longhorn_nodes_zone_not_empty</span></span>(<span>client)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_longhorn_nodes_zone_not_empty(client): # NOQA

    lh_nodes = client.list_node()
    node_names = map(lambda node: node.name, lh_nodes)

    for node_name in node_names:
        for j in range(RETRY_COUNTS):
            lh_node = client.by_id_node(node_name)
            if lh_node.zone != &#39;&#39;:
                break
            time.sleep(RETRY_INTERVAL)

        assert lh_node.zone != &#39;&#39;</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_zone.get_zone_replica_count" href="#tests.test_zone.get_zone_replica_count">get_zone_replica_count</a></code></li>
<li><code><a title="tests.test_zone.k8s_node_zone_tags" href="#tests.test_zone.k8s_node_zone_tags">k8s_node_zone_tags</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_node_duplicates_in_multiple_zones" href="#tests.test_zone.test_replica_auto_balance_node_duplicates_in_multiple_zones">test_replica_auto_balance_node_duplicates_in_multiple_zones</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_should_respect_node_selector" href="#tests.test_zone.test_replica_auto_balance_should_respect_node_selector">test_replica_auto_balance_should_respect_node_selector</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_when_disabled_disk_scheduling_in_zone" href="#tests.test_zone.test_replica_auto_balance_when_disabled_disk_scheduling_in_zone">test_replica_auto_balance_when_disabled_disk_scheduling_in_zone</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_when_no_storage_available_in_zone" href="#tests.test_zone.test_replica_auto_balance_when_no_storage_available_in_zone">test_replica_auto_balance_when_no_storage_available_in_zone</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_when_replica_on_unschedulable_node" href="#tests.test_zone.test_replica_auto_balance_when_replica_on_unschedulable_node">test_replica_auto_balance_when_replica_on_unschedulable_node</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_zone_best_effort" href="#tests.test_zone.test_replica_auto_balance_zone_best_effort">test_replica_auto_balance_zone_best_effort</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_zone_best_effort_with_data_locality" href="#tests.test_zone.test_replica_auto_balance_zone_best_effort_with_data_locality">test_replica_auto_balance_zone_best_effort_with_data_locality</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones" href="#tests.test_zone.test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones">test_replica_auto_balance_zone_best_effort_with_uneven_node_in_zones</a></code></li>
<li><code><a title="tests.test_zone.test_replica_auto_balance_zone_least_effort" href="#tests.test_zone.test_replica_auto_balance_zone_least_effort">test_replica_auto_balance_zone_least_effort</a></code></li>
<li><code><a title="tests.test_zone.test_replica_zone_anti_affinity" href="#tests.test_zone.test_replica_zone_anti_affinity">test_replica_zone_anti_affinity</a></code></li>
<li><code><a title="tests.test_zone.test_zone_tags" href="#tests.test_zone.test_zone_tags">test_zone_tags</a></code></li>
<li><code><a title="tests.test_zone.wait_longhorn_nodes_zone_not_empty" href="#tests.test_zone.wait_longhorn_nodes_zone_not_empty">wait_longhorn_nodes_zone_not_empty</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
