<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>tests.common API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.common</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import fcntl
import struct
import time
import os
import stat
import random
import string
import subprocess
import json
import hashlib
import signal

import socket
import pytest

import longhorn

from kubernetes import client as k8sclient, config as k8sconfig
from kubernetes.client import Configuration
from kubernetes.stream import stream

from kubernetes.client.rest import ApiException

from urllib.parse import urlparse

Mi = (1024 * 1024)
Gi = (1024 * Mi)

SIZE = str(16 * Mi)
EXPAND_SIZE = str(32 * Mi)
VOLUME_NAME = &#34;longhorn-testvol&#34;
DEV_PATH = &#34;/dev/longhorn/&#34;
VOLUME_RWTEST_SIZE = 512
VOLUME_INVALID_POS = -1

BASE_IMAGE_EXT4 = &#34;rancher/longhorn-test:baseimage-ext4&#34;
BASE_IMAGE_EXT4_SIZE = 32 * Mi

PORT = &#34;:9500&#34;

RETRY_COMMAND_COUNT = 3
RETRY_COUNTS = 300
RETRY_INTERVAL = 0.5
RETRY_INTERVAL_LONG = 2
RETRY_BACKUP_COUNTS = 600
RETRY_BACKUP_INTERVAL = 0.5
RETRY_EXEC_COUNTS = 30
RETRY_EXEC_INTERVAL = 5

LONGHORN_NAMESPACE = &#34;longhorn-system&#34;

COMPATIBILTY_TEST_IMAGE_PREFIX = &#34;longhornio/longhorn-test:version-test&#34;
UPGRADE_TEST_IMAGE_PREFIX = &#34;longhornio/longhorn-test:upgrade-test&#34;

ISCSI_DEV_PATH = &#34;/dev/disk/by-path&#34;

VOLUME_FIELD_STATE = &#34;state&#34;
VOLUME_STATE_ATTACHED = &#34;attached&#34;
VOLUME_STATE_DETACHED = &#34;detached&#34;

VOLUME_FIELD_ROBUSTNESS = &#34;robustness&#34;
VOLUME_ROBUSTNESS_HEALTHY = &#34;healthy&#34;
VOLUME_ROBUSTNESS_DEGRADED = &#34;degraded&#34;
VOLUME_ROBUSTNESS_FAULTED = &#34;faulted&#34;
VOLUME_ROBUSTNESS_UNKNOWN = &#34;unknown&#34;

VOLUME_FIELD_INITIALRESTORATIONREQUIRED = &#34;initialRestorationRequired&#34;

DEFAULT_STORAGECLASS_NAME = &#39;longhorn-test&#39;

DEFAULT_LONGHORN_PARAMS = {
    &#39;numberOfReplicas&#39;: &#39;3&#39;,
    &#39;staleReplicaTimeout&#39;: &#39;30&#39;
}

DEFAULT_BACKUP_TIMEOUT = 100

DEFAULT_POD_INTERVAL = 1
DEFAULT_POD_TIMEOUT = 180

DEFAULT_STATEFULSET_INTERVAL = 5
DEFAULT_STATEFULSET_TIMEOUT = 180

DEFAULT_DEPLOYMENT_INTERVAL = 1
DEFAULT_DEPLOYMENT_TIMEOUT = 120


DEFAULT_VOLUME_SIZE = 3  # In Gi
EXPANDED_VOLUME_SIZE = 4  # In Gi

DIRECTORY_PATH = &#39;/tmp/longhorn-test/&#39;

VOLUME_CONDITION_SCHEDULED = &#34;scheduled&#34;
VOLUME_CONDITION_RESTORE = &#34;restore&#34;
VOLUME_CONDITION_STATUS = &#34;status&#34;

CONDITION_STATUS_TRUE = &#34;True&#34;
CONDITION_STATUS_FALSE = &#34;False&#34;
CONDITION_STATUS_UNKNOWN = &#34;Unknown&#34;

CONDITION_REASON_SCHEDULING_FAILURE = &#34;ReplicaSchedulingFailure&#34;

VOLUME_FRONTEND_BLOCKDEV = &#34;blockdev&#34;
VOLUME_FRONTEND_ISCSI = &#34;iscsi&#34;

SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE = \
    &#34;storage-over-provisioning-percentage&#34;
SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE = \
    &#34;storage-minimal-available-percentage&#34;
SETTING_CREATE_DEFAULT_DISK_LABELED_NODES = &#34;create-default-disk-labeled-nodes&#34;
SETTING_DISABLE_SCHEDULING_ON_CORDONED_NODE = \
    &#34;disable-scheduling-on-cordoned-node&#34;
SETTING_DEFAULT_DATA_PATH = &#34;default-data-path&#34;
DEFAULT_DISK_PATH = &#34;/var/lib/longhorn/&#34;
DEFAULT_STORAGE_OVER_PROVISIONING_PERCENTAGE = &#34;500&#34;
DEFAULT_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE = &#34;10&#34;
DEFAULT_LONGHORN_STATIC_STORAGECLASS_NAME = &#34;longhorn-static&#34;

DEFAULT_REPLICA_DIRECTORY = os.path.join(DEFAULT_DISK_PATH, &#34;replicas/&#34;)

NODE_CONDITION_MOUNTPROPAGATION = &#34;MountPropagation&#34;
NODE_CONDITION_SCHEDULABLE = &#34;Schedulable&#34;
DISK_CONDITION_SCHEDULABLE = &#34;Schedulable&#34;
DISK_CONDITION_READY = &#34;Ready&#34;

STREAM_EXEC_TIMEOUT = 60

SETTING_BACKUP_TARGET = &#34;backup-target&#34;
SETTING_BACKUP_TARGET_CREDENTIAL_SECRET = &#34;backup-target-credential-secret&#34;
SETTING_DEFAULT_REPLICA_COUNT = &#34;default-replica-count&#34;
SETTING_DEFAULT_LONGHORN_STATIC_SC = &#34;default-longhorn-static-storage-class&#34;
SETTING_TAINT_TOLERATION = &#34;taint-toleration&#34;

SETTING_AUTO_SALVAGE = &#34;auto-salvage&#34;

SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY = &#34;replica-soft-anti-affinity&#34;
SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY = &#34;replica-zone-soft-anti-affinity&#34;

SETTING_GUARANTEED_ENGINE_CPU = &#34;guaranteed-engine-cpu&#34;
SETTING_PRIORITY_CLASS = &#34;priority-class&#34;

SETTING_MKFS_EXT4_PARAMS = &#34;mkfs-ext4-parameters&#34;

CSI_UNKNOWN = 0
CSI_TRUE = 1
CSI_FALSE = 2

BASE_IMAGE_LABEL = &#34;ranchervm-base-image&#34;
KUBERNETES_STATUS_LABEL = &#34;KubernetesStatus&#34;

# https://github.com/kubernetes/kubernetes/blob/a9f0db16614ae62563ead2018f1692407bd93d8f/pkg/apis/scheduling/types.go#L29  # NOQA
PRIORITY_CLASS_MAX = 1000000000
PRIORITY_CLASS_MIN = 1
PRIORITY_CLASS_NAME = &#34;priority-class&#34;

# Default Tag test case set up to fulfill as many test inputs as
# possible.
DEFAULT_TAGS = [
    {
        &#34;disk&#34;: [&#34;nvme&#34;, &#34;ssd&#34;],
        &#34;node&#34;: [&#34;main&#34;, &#34;storage&#34;]
    },
    {
        &#34;disk&#34;: [&#34;nvme&#34;, &#34;ssd&#34;],
        &#34;node&#34;: [&#34;fallback&#34;, &#34;storage&#34;]
    },
    {
        &#34;disk&#34;: [&#34;m2&#34;, &#34;nvme&#34;],
        &#34;node&#34;: [&#34;main&#34;, &#34;storage&#34;]
    }
]

INSTANCE_MANAGER_HOST_PATH_PREFIX = &#34;/host&#34;
EXPANSION_SNAP_TMP_META_NAME_PATTERN = &#34;volume-snap-expand-%s.img.meta.tmp&#34;

DATA_SIZE_IN_MB_1 = 100
DATA_SIZE_IN_MB_2 = 300
DATA_SIZE_IN_MB_3 = 800

MESSAGE_TYPE_ERROR = &#34;error&#34;

BACKUP_BLOCK_SIZE = 2 * Mi


def load_k8s_config():
    c = Configuration()
    c.assert_hostname = False
    Configuration.set_default(c)
    k8sconfig.load_incluster_config()


def get_apps_api_client():
    load_k8s_config()
    return k8sclient.AppsV1Api()


def get_core_api_client():
    load_k8s_config()
    return k8sclient.CoreV1Api()


def get_scheduling_api_client():
    load_k8s_config()
    return k8sclient.SchedulingV1Api()


def get_storage_api_client():
    load_k8s_config()
    return k8sclient.StorageV1Api()


def get_version_api_client():
    load_k8s_config()
    return k8sclient.VersionApi()


def get_longhorn_api_client():
    for i in range(RETRY_COUNTS):
        try:
            k8sconfig.load_incluster_config()
            ips = get_mgr_ips()

            # check if longhorn manager port is open before calling get_client
            for ip in ips:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                mgr_port_open = sock.connect_ex((ip, 9500))

                if mgr_port_open == 0:
                    client = get_client(ip + PORT)
                    break
            return client
        except Exception:
            time.sleep(RETRY_INTERVAL)


def cleanup_volume(client, volume):
    &#34;&#34;&#34;
    Clean up the volume after the test.
    :param client: The Longhorn client to use in the request.
    :param volume: The volume to clean up.
    &#34;&#34;&#34;
    volume.detach()
    volume = wait_for_volume_detached(client, volume.name)
    client.delete(volume)
    wait_for_volume_delete(client, volume.name)
    volumes = client.list_volume()
    assert len(volumes) == 0


def create_backup(client, volname, data={}, labels={}):
    volume = client.by_id_volume(volname)
    create_snapshot(client, volname)
    if not data:
        data = write_volume_random_data(volume)
    else:
        data = write_volume_data(volume, data)
    snap = create_snapshot(client, volname)
    create_snapshot(client, volname)
    volume.snapshotBackup(name=snap.name, labels=labels)

    verified = False
    for i in range(RETRY_COMMAND_COUNT):
        bv, b = find_backup(client, volname, snap.name)
        new_b = bv.backupGet(name=b.name)
        if new_b.name == b.name and \
           new_b.url == b.url and \
           new_b.snapshotName == b.snapshotName and \
           new_b.snapshotCreated == b.snapshotCreated and \
           new_b.created == b.created and \
           new_b.volumeName == b.volumeName and \
           new_b.volumeSize == b.volumeSize and \
           new_b.volumeCreated == b.volumeCreated:
            verified = True
            break
        time.sleep(RETRY_INTERVAL)
    assert verified

    # Don&#39;t directly compare the Label dictionaries, since the server could
    # have added extra Labels (for things like BaseImage).
    for key, val in iter(labels.items()):
        assert new_b.labels.get(key) == val

    volume = wait_for_backup_completion(client, volname, snap.name)
    volume = wait_for_volume_status(client, volname,
                                    &#34;lastBackup&#34;,
                                    b.name)
    assert volume.lastBackupAt != &#34;&#34;

    return bv, b, snap, data


def delete_backup(client, volume_name, backup_name):
    backup_volume = client.by_id_backupVolume(volume_name)
    backup_volume.backupDelete(name=backup_name)
    wait_for_backup_delete(client, volume_name, backup_name)


def delete_backup_volume(client, volume_name):
    bv = client.by_id_backupVolume(volume_name)
    client.delete(bv)
    wait_for_backup_volume_delete(client, volume_name)


def create_and_check_volume(client, volume_name, num_of_replicas=3, size=SIZE,
                            base_image=&#34;&#34;, frontend=VOLUME_FRONTEND_BLOCKDEV):
    &#34;&#34;&#34;
    Create a new volume with the specified parameters. Assert that the new
    volume is detached and that all of the requested parameters match.

    :param client: The Longhorn client to use in the request.
    :param volume_name: The name of the volume.
    :param num_of_replicas: The number of replicas the volume should have.
    :param size: The size of the volume, as a string representing the number
    of bytes.
    :param base_image: The base image to use for the volume.
    :param frontend: The frontend to use for the volume.
    :return: The volume instance created.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=size,
                         numberOfReplicas=num_of_replicas,
                         baseImage=base_image, frontend=frontend)
    volume = wait_for_volume_detached(client, volume_name)
    assert volume.name == volume_name
    assert volume.size == size
    assert volume.numberOfReplicas == num_of_replicas
    assert volume.state == &#34;detached&#34;
    assert volume.baseImage == base_image
    assert volume.frontend == frontend
    assert volume.created != &#34;&#34;
    return volume


def wait_pod(pod_name):
    api = get_core_api_client()

    for i in range(DEFAULT_POD_TIMEOUT):
        pod = api.read_namespaced_pod(
            name=pod_name,
            namespace=&#39;default&#39;)
        if pod.status.phase != &#39;Pending&#39;:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert pod.status.phase == &#39;Running&#39;


def create_and_wait_pod(api, pod_manifest):
    &#34;&#34;&#34;
    Creates a new Pod attached to a PersistentVolumeClaim for testing.

    The function will block until the Pod is online or until it times out,
    whichever occurs first. The volume created by the manifest passed in will
    be mounted to &#39;/data&#39;.

    Args:
        api: An instance of CoreV1API.
        pod_name: The name of the Pod.
        volume: The volume manifest.
    &#34;&#34;&#34;
    api.create_namespaced_pod(
        body=pod_manifest,
        namespace=&#39;default&#39;)

    pod_name = pod_manifest[&#39;metadata&#39;][&#39;name&#39;]

    wait_pod(pod_name)


def create_pvc_spec(name):
    # type: (str) -&gt; dict
    &#34;&#34;&#34;
    Generate a volume manifest using the given name for the PVC.

    This spec is used to test dynamically provisioned PersistentVolumes (those
    created using a storage class).
    &#34;&#34;&#34;
    return {
        &#39;name&#39;: &#39;pod-data&#39;,
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: name,
            &#39;readOnly&#39;: False
        }
    }


def delete_and_wait_pod(api, pod_name):
    &#34;&#34;&#34;
    Delete a specified Pod from the &#34;default&#34; namespace.

    This function does not check if the Pod does exist and will throw an error
    if a nonexistent Pod is specified.

    Args:
        api: An instance of CoreV1API.
        pod_name: The name of the Pod.
    &#34;&#34;&#34;
    try:
        api.delete_namespaced_pod(
            name=pod_name, namespace=&#39;default&#39;,
            body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404

    wait_delete_pod(api, pod_name)


def delete_and_wait_statefulset(api, client, statefulset):
    apps_api = get_apps_api_client()
    if not check_statefulset_existence(apps_api,
                                       statefulset[&#39;metadata&#39;][&#39;name&#39;]):
        return

    # We need to generate the names for the PVCs on our own so we can
    # delete them.
    pod_data = get_statefulset_pod_info(api, statefulset)

    try:
        apps_api.delete_namespaced_stateful_set(
            name=statefulset[&#39;metadata&#39;][&#39;name&#39;],
            namespace=&#39;default&#39;, body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404

    for i in range(DEFAULT_POD_TIMEOUT):
        ret = apps_api.list_namespaced_stateful_set(namespace=&#39;default&#39;)
        found = False
        for item in ret.items:
            if item.metadata.name == statefulset[&#39;metadata&#39;][&#39;name&#39;]:
                found = True
                break
        if not found:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert not found
    client = get_longhorn_api_client()
    for pod in pod_data:
        # Wait on Pods too, we apparently had timeout issues with them.
        wait_delete_pod(api, pod[&#39;pod_name&#39;])
        delete_and_wait_pvc(api, pod[&#39;pvc_name&#39;])
        # The StatefulSet tests involve both StorageClass provisioned volumes
        # and our manually created PVs. This checks the status of our PV once
        # the PVC is deleted. If it is Failed, we know it is a PV and we must
        # delete it manually. If it is removed from the system, we can just
        # wait for deletion.
        for i in range(DEFAULT_POD_TIMEOUT):
            ret = api.list_persistent_volume()
            found = False
            for item in ret.items:
                if item.metadata.name == pod[&#39;pv_name&#39;]:
                    if item.status.phase in (&#39;Failed&#39;, &#39;Released&#39;):
                        delete_and_wait_pv(api, pod[&#39;pv_name&#39;])
                        delete_and_wait_longhorn(client, pod[&#39;pv_name&#39;])
                    else:
                        found = True
                        break
            if not found:
                break
            time.sleep(DEFAULT_POD_INTERVAL)
        assert not found
        wait_for_volume_delete(client, pod[&#39;pv_name&#39;])


def get_volume_name(api, pvc_name):
    # type: (dict) -&gt; str
    &#34;&#34;&#34;
    Given a PersistentVolumeClaim, return the name of the associated PV.
    &#34;&#34;&#34;
    claim = api.read_namespaced_persistent_volume_claim(
        name=pvc_name, namespace=&#39;default&#39;)
    return claim.spec.volume_name


def get_statefulset_pod_info(api, s_set):
    pod_info = []
    for i in range(s_set[&#39;spec&#39;][&#39;replicas&#39;]):
        pod_name = s_set[&#39;metadata&#39;][&#39;name&#39;] + &#39;-&#39; + str(i)
        pod = api.read_namespaced_pod(name=pod_name, namespace=&#39;default&#39;)
        pvc_name = pod.spec.volumes[0].persistent_volume_claim.claim_name
        pv_name = get_volume_name(api, pvc_name)
        pod_info.append({
            &#39;pod_name&#39;: pod_name,
            &#39;pv_name&#39;: pv_name,
            &#39;pvc_name&#39;: pvc_name,
        })
    return pod_info


def delete_and_wait_longhorn(client, name):
    &#34;&#34;&#34;
    Delete a volume from Longhorn.
    &#34;&#34;&#34;
    try:
        v = client.by_id_volume(name)
        client.delete(v)
    except ApiException as ex:
        assert ex.status == 404
    except longhorn.ApiError as err:
        # for deleting a non-existing volume,
        # the status_code is 500 Server Error.
        assert err.error.code == 500

    wait_for_volume_delete(client, name)


def read_volume_data(api, pod_name, filename=&#39;test&#39;):
    &#34;&#34;&#34;
    Retrieve data from a Pod&#39;s volume.

    Args:
        api: An instance of CoreV1API.
        pod_name: The name of the Pod.

    Returns:
        The data contained within the volume.
    &#34;&#34;&#34;
    read_command = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;cat /data/&#39; + filename
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream read&#39;):
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=read_command, stderr=True, stdin=False, stdout=True,
            tty=False)


def write_pod_volume_data(api, pod_name, test_data, filename=&#39;test&#39;):
    &#34;&#34;&#34;
    Write data into a Pod&#39;s volume.

    Args:
        api: An instance of CoreV1API.
        pod_name: The name of the Pod.
        test_data: The data to be written.
    &#34;&#34;&#34;
    write_command = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;echo -ne &#39; + test_data + &#39; &gt; /data/&#39; + filename
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream write&#39;):
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=write_command, stderr=True, stdin=False, stdout=True,
            tty=False)


def write_pod_block_volume_data(api, pod_name, test_data, offset, device_path):
    tmp_file = &#39;/var/test_data&#39;
    pre_write_cmd = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;echo -ne &#39; + test_data + &#39; &gt; &#39; + tmp_file
    ]
    write_cmd = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;dd if=&#39; + tmp_file + &#39; of=&#39; + device_path +
        &#39; bs=&#39; + str(len(test_data)) + &#39; count=1 seek=&#39; + str(offset)
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream write&#39;):
        stream(api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
               command=pre_write_cmd, stderr=True, stdin=False, stdout=True,
               tty=False)
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=write_cmd, stderr=True, stdin=False, stdout=True,
            tty=False)


def read_pod_block_volume_data(api, pod_name, data_size, offset, device_path):
    read_command = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;dd if=&#39; + device_path +
        &#39; status=none bs=&#39; + str(data_size) + &#39; count=1 skip=&#39; + str(offset)
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream read&#39;):
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=read_command, stderr=True, stdin=False, stdout=True,
            tty=False)


def get_pod_data_md5sum(api, pod_name, path):
    md5sum_command = [
        &#39;/bin/sh&#39;, &#39;-c&#39;, &#39;md5sum &#39; + path + &#34; | awk &#39;{print $1}&#39;&#34;
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT * 3,
                 error_message=&#39;Timeout on executing stream md5sum&#39;):
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=md5sum_command, stderr=True, stdin=False, stdout=True,
            tty=False)


def write_pod_volume_random_data(api, pod_name, path, size_in_mb):
    write_cmd = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;dd if=/dev/urandom of=&#39; + path +
        &#39; bs=1M&#39; + &#39; count=&#39; + str(size_in_mb)
    ]
    return stream(
        api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
        command=write_cmd, stderr=True, stdin=False, stdout=True,
        tty=False)


def copy_pod_volume_data(api, pod_name, src_path, dest_path):
    write_cmd = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;dd if=&#39; + src_path + &#39; of=&#39; + dest_path
    ]
    return stream(
        api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
        command=write_cmd, stderr=True, stdin=False, stdout=True,
        tty=False)


def size_to_string(volume_size):
    # type: (int) -&gt; str
    &#34;&#34;&#34;
    Convert a volume size to string format to pass into Kubernetes.
    Args:
        volume_size: The size of the volume in bytes.
    Returns:
        The size of the volume in gigabytes as a passable string to Kubernetes.
    &#34;&#34;&#34;
    if volume_size &gt;= Gi:
        return str(volume_size &gt;&gt; 30) + &#39;Gi&#39;
    elif volume_size &gt;= Mi:
        return str(volume_size &gt;&gt; 20) + &#39;Mi&#39;
    else:
        return str(volume_size &gt;&gt; 10) + &#39;Ki&#39;


def wait_delete_pod(api, pod_name):
    for i in range(DEFAULT_POD_TIMEOUT):
        ret = api.list_namespaced_pod(namespace=&#39;default&#39;)
        found = False
        for item in ret.items:
            if item.metadata.name == pod_name:
                found = True
                break
        if not found:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert not found


def check_volume_replicas(volume, spec, tag_mapping):
    &#34;&#34;&#34;
    Check the replicas on the volume to ensure that they were scheduled
    properly.
    :param volume: The Volume to check.
    :param spec: The spec to validate the Tag against.
    :param tag_mapping: The mapping of Nodes to the Tags they have.
    :raise AssertionError: If the Volume doesn&#39;t match all the conditions.
    &#34;&#34;&#34;
    found_hosts = {}
    # Make sure that all the Tags the Volume requested were fulfilled.
    for replica in volume.replicas:
        found_hosts[replica.hostId] = {}
        assert not len(set(spec[&#34;disk&#34;]) -
                       set(tag_mapping[replica.hostId][&#34;disk&#34;]))
        assert not len(set(spec[&#34;node&#34;]) -
                       set(tag_mapping[replica.hostId][&#34;node&#34;]))

    # The Volume should have replicas on as many nodes as matched
    # the requirements (specified by &#34;expected&#34; in the spec variable).
    assert len(found_hosts) == spec[&#34;expected&#34;]


# Default argument is mutable on this function, but it&#39;s fine since we&#39;re only
# using it as an empty tag list to pass to the server and will never actually
# modify it.
def set_node_tags(client, node, tags=[]):  # NOQA
    &#34;&#34;&#34;
    Set the tags on a node without modifying its scheduling status.
    :param client: The Longhorn client to use in the request.
    :param node: The Node to update.
    :param tags: The tags to set on the node.
    :return: The updated Node.
    &#34;&#34;&#34;
    return client.update(node, allowScheduling=node.allowScheduling,
                         tags=tags)


@pytest.fixture
def pod_make(request):
    def make_pod(name=&#39;test-pod&#39;):
        pod_manifest = {
            &#39;apiVersion&#39;: &#39;v1&#39;,
            &#39;kind&#39;: &#39;Pod&#39;,
            &#39;metadata&#39;: {
                &#39;name&#39;: name
            },
            &#39;spec&#39;: {
                &#39;containers&#39;: [{
                    &#39;image&#39;: &#39;busybox&#39;,
                    &#39;imagePullPolicy&#39;: &#39;IfNotPresent&#39;,
                    &#39;name&#39;: &#39;sleep&#39;,
                    &#34;args&#34;: [
                        &#34;/bin/sh&#34;,
                        &#34;-c&#34;,
                        &#34;while true; do date; sleep 5; done&#34;
                    ],
                    &#34;volumeMounts&#34;: [{
                        &#39;name&#39;: &#39;pod-data&#39;,
                        &#39;mountPath&#39;: &#39;/data&#39;
                    }],
                }],
                &#39;volumes&#39;: []
            }
        }

        def finalizer():
            api = get_core_api_client()
            try:
                delete_and_wait_pod(api, pod_manifest[&#39;metadata&#39;][&#39;name&#39;])
            except Exception as e:
                print(&#34;Exception when waiting for pod deletion&#34;, e)
                return
            try:
                volume_details = pod_manifest[&#39;spec&#39;][&#39;volumes&#39;][0]
                pvc_name = volume_details[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
                delete_and_wait_pvc(api, pvc_name)
            except Exception as e:
                print(&#34;Exception when waiting for PVC deletion&#34;, e)
            try:
                pv = wait_and_get_pv_for_pvc(api, pvc_name)
                pv_name = pv.metadata.name
                delete_and_wait_pv(api, pv_name)
            except Exception as e:
                print(&#34;Exception when waiting for PV deletion&#34;, e)

        request.addfinalizer(finalizer)
        return pod_manifest

    return make_pod


@pytest.fixture
def pod(request):
    pod_manifest = {
        &#39;apiVersion&#39;: &#39;v1&#39;,
        &#39;kind&#39;: &#39;Pod&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: &#39;test-pod&#39;
        },
        &#39;spec&#39;: {
            &#39;containers&#39;: [{
                &#39;image&#39;: &#39;busybox&#39;,
                &#39;imagePullPolicy&#39;: &#39;IfNotPresent&#39;,
                &#39;name&#39;: &#39;sleep&#39;,
                &#34;args&#34;: [
                    &#34;/bin/sh&#34;,
                    &#34;-c&#34;,
                    &#34;while true;do date;sleep 5; done&#34;
                ],
                &#34;volumeMounts&#34;: [{
                    &#39;name&#39;: &#39;pod-data&#39;,
                    &#39;mountPath&#39;: &#39;/data&#39;
                }],
            }],
            &#39;volumes&#39;: []
        }
    }

    def finalizer():
        api = get_core_api_client()
        delete_and_wait_pod(api, pod_manifest[&#39;metadata&#39;][&#39;name&#39;])

    request.addfinalizer(finalizer)

    return pod_manifest


@pytest.fixture
def scheduling_api(request):
    &#34;&#34;&#34;
    Create a new SchedulingV1API instance.
    Returns:
        A new CoreV1API Instance.
    &#34;&#34;&#34;
    c = Configuration()
    c.assert_hostname = False
    Configuration.set_default(c)
    k8sconfig.load_incluster_config()
    scheduling_api = k8sclient.SchedulingV1Api()

    return scheduling_api


@pytest.fixture
def core_api(request):
    &#34;&#34;&#34;
    Create a new CoreV1API instance.
    Returns:
        A new CoreV1API Instance.
    &#34;&#34;&#34;
    c = Configuration()
    c.assert_hostname = False
    Configuration.set_default(c)
    k8sconfig.load_incluster_config()
    core_api = k8sclient.CoreV1Api()

    return core_api


@pytest.fixture
def apps_api(request):
    &#34;&#34;&#34;
    Create a new AppsV1API instance.
    Returns:
        A new AppsV1API Instance.
    &#34;&#34;&#34;
    c = Configuration()
    c.assert_hostname = False
    Configuration.set_default(c)
    k8sconfig.load_incluster_config()
    apps_api = k8sclient.AppsV1Api()

    return apps_api


@pytest.fixture
def csi_pv(request):
    volume_name = generate_volume_name()
    pv_manifest = {
        &#39;apiVersion&#39;: &#39;v1&#39;,
        &#39;kind&#39;: &#39;PersistentVolume&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: volume_name
        },
        &#39;spec&#39;: {
            &#39;capacity&#39;: {
                &#39;storage&#39;: size_to_string(DEFAULT_VOLUME_SIZE * Gi)
            },
            &#39;volumeMode&#39;: &#39;Filesystem&#39;,
            &#39;accessModes&#39;: [&#39;ReadWriteOnce&#39;],
            &#39;persistentVolumeReclaimPolicy&#39;: &#39;Delete&#39;,
            &#39;csi&#39;: {
                &#39;driver&#39;: &#39;driver.longhorn.io&#39;,
                &#39;fsType&#39;: &#39;ext4&#39;,
                &#39;volumeAttributes&#39;: {
                    &#39;numberOfReplicas&#39;:
                        DEFAULT_LONGHORN_PARAMS[&#39;numberOfReplicas&#39;],
                    &#39;staleReplicaTimeout&#39;:
                        DEFAULT_LONGHORN_PARAMS[&#39;staleReplicaTimeout&#39;]
                },
                &#39;volumeHandle&#39;: volume_name
            }
        }
    }

    def finalizer():
        api = get_core_api_client()
        delete_and_wait_pv(api, pv_manifest[&#39;metadata&#39;][&#39;name&#39;])

        client = get_longhorn_api_client()
        delete_and_wait_longhorn(client, pv_manifest[&#39;metadata&#39;][&#39;name&#39;])

    request.addfinalizer(finalizer)

    return pv_manifest


@pytest.fixture
def csi_pv_baseimage(request):
    pv_manifest = csi_pv(request)
    pv_manifest[&#39;spec&#39;][&#39;capacity&#39;][&#39;storage&#39;] = \
        size_to_string(BASE_IMAGE_EXT4_SIZE)
    pv_manifest[&#39;spec&#39;][&#39;csi&#39;][&#39;volumeAttributes&#39;][&#39;baseImage&#39;] = \
        BASE_IMAGE_EXT4
    return pv_manifest


@pytest.fixture
def pvc(request):
    pvc_manifest = {
        &#39;apiVersion&#39;: &#39;v1&#39;,
        &#39;kind&#39;: &#39;PersistentVolumeClaim&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: generate_volume_name()
        },
        &#39;spec&#39;: {
            &#39;accessModes&#39;: [
                &#39;ReadWriteOnce&#39;
            ],
            &#39;resources&#39;: {
                &#39;requests&#39;: {
                    &#39;storage&#39;: size_to_string(DEFAULT_VOLUME_SIZE * Gi)
                }
            }
        }
    }

    def finalizer():
        api = k8sclient.CoreV1Api()

        if not check_pvc_existence(api, pvc_manifest[&#39;metadata&#39;][&#39;name&#39;]):
            return

        claim = api.read_namespaced_persistent_volume_claim(
            name=pvc_manifest[&#39;metadata&#39;][&#39;name&#39;], namespace=&#39;default&#39;)
        volume_name = claim.spec.volume_name

        api = get_core_api_client()
        delete_and_wait_pvc(api, pvc_manifest[&#39;metadata&#39;][&#39;name&#39;])

        # Working around line break issue.
        key = &#39;volume.beta.kubernetes.io/storage-provisioner&#39;
        # If not using StorageClass (such as in CSI test), the Longhorn volume
        # will not be automatically deleted, causing this to throw an error.
        if (key in claim.metadata.annotations):
            client = get_longhorn_api_client()
            wait_for_volume_delete(client, volume_name)

    request.addfinalizer(finalizer)

    return pvc_manifest


@pytest.fixture
def pvc_baseimage(request):
    pvc_manifest = pvc(request)
    pvc_manifest[&#39;spec&#39;][&#39;resources&#39;][&#39;requests&#39;][&#39;storage&#39;] = \
        size_to_string(BASE_IMAGE_EXT4_SIZE)
    return pvc_manifest


@pytest.fixture
def statefulset(request):
    statefulset_manifest = {
        &#39;apiVersion&#39;: &#39;apps/v1&#39;,
        &#39;kind&#39;: &#39;StatefulSet&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: &#39;test-statefulset&#39;
        },
        &#39;spec&#39;: {
            &#39;selector&#39;: {
                &#39;matchLabels&#39;: {
                    &#39;app&#39;: &#39;test-statefulset&#39;
                }
            },
            &#39;serviceName&#39;: &#39;test-statefulset&#39;,
            &#39;replicas&#39;: 2,
            &#39;template&#39;: {
                &#39;metadata&#39;: {
                    &#39;labels&#39;: {
                        &#39;app&#39;: &#39;test-statefulset&#39;
                    }
                },
                &#39;spec&#39;: {
                    &#39;terminationGracePeriodSeconds&#39;: 10,
                    &#39;containers&#39;: [{
                        &#39;image&#39;: &#39;busybox&#39;,
                        &#39;imagePullPolicy&#39;: &#39;IfNotPresent&#39;,
                        &#39;name&#39;: &#39;sleep&#39;,
                        &#39;args&#39;: [
                            &#39;/bin/sh&#39;,
                            &#39;-c&#39;,
                            &#39;while true;do date;sleep 5; done&#39;
                        ],
                        &#39;volumeMounts&#39;: [{
                            &#39;name&#39;: &#39;pod-data&#39;,
                            &#39;mountPath&#39;: &#39;/data&#39;
                        }]
                    }]
                }
            },
            &#39;volumeClaimTemplates&#39;: [{
                &#39;metadata&#39;: {
                    &#39;name&#39;: &#39;pod-data&#39;
                },
                &#39;spec&#39;: {
                    &#39;accessModes&#39;: [
                        &#39;ReadWriteOnce&#39;
                    ],
                    &#39;storageClassName&#39;: DEFAULT_STORAGECLASS_NAME,
                    &#39;resources&#39;: {
                        &#39;requests&#39;: {
                            &#39;storage&#39;: size_to_string(
                                           DEFAULT_VOLUME_SIZE * Gi)
                        }
                    }
                }
            }]
        }
    }

    def finalizer():
        api = get_core_api_client()
        client = get_longhorn_api_client()
        delete_and_wait_statefulset(api, client, statefulset_manifest)

    request.addfinalizer(finalizer)

    return statefulset_manifest


@pytest.fixture
def storage_class(request):
    sc_manifest = {
        &#39;apiVersion&#39;: &#39;storage.k8s.io/v1&#39;,
        &#39;kind&#39;: &#39;StorageClass&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: DEFAULT_STORAGECLASS_NAME
        },
        &#39;provisioner&#39;: &#39;driver.longhorn.io&#39;,
        &#39;allowVolumeExpansion&#39;: True,
        &#39;parameters&#39;: {
            &#39;numberOfReplicas&#39;: DEFAULT_LONGHORN_PARAMS[&#39;numberOfReplicas&#39;],
            &#39;staleReplicaTimeout&#39;:
                DEFAULT_LONGHORN_PARAMS[&#39;staleReplicaTimeout&#39;]
        },
        &#39;reclaimPolicy&#39;: &#39;Delete&#39;
    }

    def finalizer():
        api = get_storage_api_client()
        try:
            api.delete_storage_class(name=sc_manifest[&#39;metadata&#39;][&#39;name&#39;],
                                     body=k8sclient.V1DeleteOptions())
        except ApiException as e:
            assert e.status == 404

    request.addfinalizer(finalizer)

    return sc_manifest


@pytest.fixture
def priority_class(request):
    priority_class = {
        &#39;apiVersion&#39;: &#39;scheduling.k8s.io/v1&#39;,
        &#39;kind&#39;: &#39;PriorityClass&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: PRIORITY_CLASS_NAME + &#34;-&#34; + &#39;&#39;.join(
                random.choice(string.ascii_lowercase +
                              string.digits)
                for _ in range(6))
        },
        &#39;value&#39;: random.randrange(PRIORITY_CLASS_MIN, PRIORITY_CLASS_MAX)
    }

    def finalizer():
        api = get_scheduling_api_client()
        try:
            api.delete_priority_class(name=priority_class[&#39;metadata&#39;][&#39;name&#39;],
                                      body=k8sclient.V1DeleteOptions())
        except ApiException as e:
            assert e.status == 404

    request.addfinalizer(finalizer)

    return priority_class


@pytest.yield_fixture
def node_default_tags():
    &#34;&#34;&#34;
    Assign the Tags under DEFAULT_TAGS to the Longhorn client&#39;s Nodes to
    provide a base set of Tags to work with in the tests.
    :return: A dictionary mapping a Node&#39;s ID to the Tags it has.
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    nodes = client.list_node()
    assert len(nodes) == 3

    tag_mappings = {}
    for tags, node in zip(DEFAULT_TAGS, nodes):
        assert len(node.disks) == 1

        update_disks = get_update_disks(node.disks)
        update_disks[list(update_disks)[0]].tags = tags[&#34;disk&#34;]
        new_node = node.diskUpdate(disks=update_disks)
        disks = get_update_disks(new_node.disks)
        assert disks[list(new_node.disks)[0]].tags == tags[&#34;disk&#34;]

        new_node = set_node_tags(client, node, tags[&#34;node&#34;])
        assert new_node.tags == tags[&#34;node&#34;]

        tag_mappings[node.id] = tags
    yield tag_mappings

    client = get_longhorn_api_client()  # NOQA
    nodes = client.list_node()
    for node in nodes:
        update_disks = get_update_disks(node.disks)
        update_disks[list(update_disks)[0]].tags = []
        new_node = node.diskUpdate(disks=update_disks)
        disks = get_update_disks(new_node.disks)
        assert disks[list(new_node.disks)[0]].tags is None

        new_node = set_node_tags(client, node)
        assert new_node.tags is None


@pytest.fixture
def random_labels():
    labels = {}
    i = 0
    while i &lt; 3:
        key = &#34;label/&#34; + &#34;&#34;.join(random.choice(string.ascii_lowercase +
                                               string.digits)
                                 for _ in range(6))
        if not labels.get(key):
            labels[&#34;key&#34;] = generate_random_data(VOLUME_RWTEST_SIZE)
            i += 1
    return labels


@pytest.fixture
def client(request):
    &#34;&#34;&#34;
    Return an individual Longhorn API client for testing.
    &#34;&#34;&#34;
    k8sconfig.load_incluster_config()
    # Make sure nodes and managers are all online.
    ips = get_mgr_ips()

    # check if longhorn manager port is open before calling get_client
    for ip in ips:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        mgr_port_open = sock.connect_ex((ip, 9500))

        if mgr_port_open == 0:
            client = get_client(ip + PORT)
            break

    hosts = client.list_node()
    assert len(hosts) == len(ips)

    request.addfinalizer(lambda: cleanup_client())

    cleanup_client()

    return client


@pytest.fixture
def clients(request):
    k8sconfig.load_incluster_config()
    ips = get_mgr_ips()
    client = get_client(ips[0] + PORT)
    hosts = client.list_node()
    assert len(hosts) == len(ips)
    clis = get_clients(hosts)

    def finalizer():
        cleanup_client()

    request.addfinalizer(finalizer)

    cleanup_client()

    return clis


def cleanup_client():
    client = get_longhorn_api_client()
    # cleanup test disks
    cleanup_test_disks(client)

    volumes = client.list_volume()
    for v in volumes:
        # ignore the error when clean up
        try:
            client.delete(v)
        except Exception as e:
            print(&#34;Exception when cleanup volume &#34;, v, e)
            pass
    images = client.list_engine_image()
    for img in images:
        if not img.default:
            # ignore the error when clean up
            try:
                client.delete(img)
            except Exception as e:
                print(&#34;Exception when cleanup image&#34;, img, e)
                pass

    # enable nodes scheduling
    reset_node(client)
    reset_settings(client)
    reset_disks_for_all_nodes(client)
    reset_engine_image(client)

    # check replica subdirectory of default disk path
    if not os.path.exists(DEFAULT_REPLICA_DIRECTORY):
        subprocess.check_call(
            [&#34;mkdir&#34;, &#34;-p&#34;, DEFAULT_REPLICA_DIRECTORY])


def get_client(address):
    url = &#39;http://&#39; + address + &#39;/v1/schemas&#39;
    c = longhorn.from_env(url=url)
    return c


def get_mgr_ips():
    ret = k8sclient.CoreV1Api().list_pod_for_all_namespaces(
            label_selector=&#34;app=longhorn-manager&#34;,
            watch=False)
    mgr_ips = []
    for i in ret.items:
        mgr_ips.append(i.status.pod_ip)
    return mgr_ips


def get_self_host_id():
    envs = os.environ
    return envs[&#34;NODE_NAME&#34;]


def get_backupstore_url():
    backupstore = os.environ[&#39;LONGHORN_BACKUPSTORES&#39;]
    backupstore = backupstore.replace(&#34; &#34;, &#34;&#34;)
    backupstores = backupstore.split(&#34;,&#34;)

    assert len(backupstores) != 0
    return backupstores


def get_clients(hosts):
    clients = {}
    for host in hosts:
        assert host.name is not None
        assert host.address is not None
        clients[host.name] = get_client(host.address + PORT)
    return clients


def wait_scheduling_failure(client, volume_name):
    &#34;&#34;&#34;
    Wait and make sure no new replicas are running on the specified
    volume. Trigger a failed assertion of one is detected.
    :param client: The Longhorn client to use in the request.
    :param volume_name: The name of the volume.
    &#34;&#34;&#34;
    scheduling_failure = False
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        if v.conditions.scheduled.status == &#34;False&#34; and \
                v.conditions.scheduled.reason == \
                &#34;ReplicaSchedulingFailure&#34;:
            scheduling_failure = True
        if scheduling_failure:
            break
        time.sleep(RETRY_INTERVAL)
    assert scheduling_failure


def wait_for_device_login(dest_path, name):
    dev = &#34;&#34;
    for i in range(RETRY_COUNTS):
        for j in range(RETRY_COMMAND_COUNT):
            files = []
            try:
                files = os.listdir(dest_path)
                break
            except Exception:
                time.sleep(1)
        assert files
        if name in files:
            dev = name
            break
        time.sleep(RETRY_INTERVAL)
    assert dev == name
    return dev


def wait_for_replica_directory():
    found = False
    for i in range(RETRY_COUNTS):
        if os.path.exists(DEFAULT_REPLICA_DIRECTORY):
            found = True
            break
        time.sleep(RETRY_INTERVAL)
    assert found


def wait_for_volume_creation(client, name):
    for i in range(RETRY_COUNTS):
        volumes = client.list_volume()
        found = False
        for volume in volumes:
            if volume.name == name:
                found = True
                break
        if found:
            break
    assert found


def wait_for_volume_endpoint(client, name):
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(name)
        engine = get_volume_engine(v)
        if engine.endpoint != &#34;&#34;:
            break
        time.sleep(RETRY_INTERVAL)
    check_volume_endpoint(v)
    return v


def wait_for_volume_detached(client, name):
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_STATE,
                                  VOLUME_STATE_DETACHED)


def wait_for_volume_detached_unknown(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_ROBUSTNESS,
                           VOLUME_ROBUSTNESS_UNKNOWN)
    return wait_for_volume_detached(client, name)


def wait_for_volume_healthy(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_STATE,
                           VOLUME_STATE_ATTACHED)
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_ROBUSTNESS,
                           VOLUME_ROBUSTNESS_HEALTHY)
    return wait_for_volume_endpoint(client, name)


def wait_for_volume_healthy_no_frontend(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_STATE,
                           VOLUME_STATE_ATTACHED)
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_ROBUSTNESS,
                                  VOLUME_ROBUSTNESS_HEALTHY)


def wait_for_volume_degraded(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_STATE,
                           VOLUME_STATE_ATTACHED)
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_ROBUSTNESS,
                                  VOLUME_ROBUSTNESS_DEGRADED)


def wait_for_volume_faulted(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_STATE,
                           VOLUME_STATE_DETACHED)
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_ROBUSTNESS,
                                  VOLUME_ROBUSTNESS_FAULTED)


def wait_for_volume_status(client, name, key, value):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        if volume[key] == value:
            break
        time.sleep(RETRY_INTERVAL)
    assert volume[key] == value
    return volume


def wait_for_volume_delete(client, name):
    for i in range(RETRY_COUNTS):
        volumes = client.list_volume()
        found = False
        for volume in volumes:
            if volume.name == name:
                found = True
                break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found


def wait_for_backup_volume_delete(client, name):
    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()
        found = False
        for bv in bvs:
            if bv.name == name:
                found = True
                break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found


def wait_for_volume_current_image(client, name, image):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        if volume.currentImage == image:
            break
        time.sleep(RETRY_INTERVAL)
    assert volume.currentImage == image
    return volume


def wait_for_volume_replica_count(client, name, count):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        if len(volume.replicas) == count:
            break
        time.sleep(RETRY_INTERVAL)
    assert len(volume.replicas) == count
    return volume


def wait_for_volume_replicas_mode(client, volname, mode, replicas_name=None):
    verified = False
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volname)
        count = 0
        replicas = []
        if replicas_name is None:
            replicas = volume.replicas
        else:
            for r_name in replicas_name:
                found = False
                for r in volume.replicas:
                    if r.name == r_name:
                        replicas.append(r)
                        found = True
                assert found
        for r in replicas:
            if r.mode == mode:
                count += 1
        if count == len(replicas):
            verified = True
            break
        time.sleep(RETRY_INTERVAL)

    assert verified
    return volume


def wait_for_snapshot_purge(client, volume_name, *snaps):
    completed = 0
    last_purge_progress = {}
    purge_status = {}
    for i in range(RETRY_COUNTS):
        completed = 0
        v = client.by_id_volume(volume_name)
        purge_status = v.purgeStatus
        for status in purge_status:
            assert status.error == &#34;&#34;

            progress = status.progress
            assert progress &lt;= 100
            replica = status.replica
            last = last_purge_progress.get(replica)
            assert last is None or last &lt;= status.progress
            last_purge_progress[&#34;replica&#34;] = progress

            if status.state == &#34;complete&#34;:
                assert progress == 100
                completed += 1
        if completed == len(purge_status):
            break
        time.sleep(RETRY_INTERVAL)
    assert completed == len(purge_status)

    # Now that the purge has been reported to be completed, the Snapshots
    # should should be removed or &#34;marked as removed&#34; in the case of
    # the latest snapshot.
    found = False
    snapshots = v.snapshotList(volume=volume_name)

    for snap in snaps:
        for vs in snapshots.data:
            if snap == vs[&#34;name&#34;]:
                if vs[&#34;removed&#34;] is False:
                    found = True
                    break

                if &#34;volume-head&#34; not in vs[&#34;children&#34;]:
                    found = True
                    break
    assert not found
    return v


def wait_for_engine_image_creation(client, image_name):
    for i in range(RETRY_COUNTS):
        images = client.list_engine_image()
        found = False
        for img in images:
            if img.name == image_name:
                found = True
                break
        if found:
            break
    assert found


def wait_for_engine_image_state(client, image_name, state):
    wait_for_engine_image_creation(client, image_name)
    for i in range(RETRY_COUNTS):
        image = client.by_id_engine_image(image_name)
        if image.state == state:
            break
        time.sleep(RETRY_INTERVAL)
    assert image.state == state
    return image


def wait_for_engine_image_ref_count(client, image_name, count):
    wait_for_engine_image_creation(client, image_name)
    for i in range(RETRY_COUNTS):
        image = client.by_id_engine_image(image_name)
        if image.refCount == count:
            break
        time.sleep(RETRY_INTERVAL)
    assert image.refCount == count
    if count == 0:
        assert image.noRefSince != &#34;&#34;
    return image


def json_string_go_to_python(str):
    return str.replace(&#34;u\&#39;&#34;, &#34;\&#34;&#34;).replace(&#34;\&#39;&#34;, &#34;\&#34;&#34;). \
        replace(&#34;True&#34;, &#34;true&#34;).replace(&#34;False&#34;, &#34;false&#34;)


def delete_replica_processes(client, api, volname):
    replica_map = {}
    volume = client.by_id_volume(volname)
    for r in volume.replicas:
        replica_map[r.instanceManagerName] = r.name

    for rm_name, r_name in replica_map.items():
        delete_command = &#39;longhorn-instance-manager process delete &#39; + \
                         &#39;--name &#39; + r_name
        exec_instance_manager(api, rm_name, delete_command)


def crash_replica_processes(client, api, volname, replicas=None,
                            wait_to_fail=True):

    if replicas is None:
        volume = client.by_id_volume(volname)
        replicas = volume.replicas

    for r in replicas:
        assert r.instanceManagerName != &#34;&#34;
        kill_command = &#34;kill `ps aux | grep &#39;&#34; + r[&#39;dataPath&#39;] +\
                       &#34;&#39; | grep -v grep | awk &#39;{print $2}&#39;`&#34;
        exec_instance_manager(api, r.instanceManagerName, kill_command)

    if wait_to_fail is True:
        for r in replicas:
            wait_for_replica_failed(client, volname, r[&#39;name&#39;])


def exec_instance_manager(api, im_name, cmd):
    exec_cmd = [&#39;/bin/sh&#39;, &#39;-c&#39;, cmd]

    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream read&#39;):
        stream(api.connect_get_namespaced_pod_exec,
               im_name,
               LONGHORN_NAMESPACE, command=exec_cmd,
               stderr=True, stdin=False, stdout=True, tty=False)


def wait_for_replica_failed(client, volname, replica_name):
    failed = True
    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL)
        failed = True
        volume = client.by_id_volume(volname)
        for r in volume.replicas:
            if r[&#39;name&#39;] != replica_name:
                continue
            if r[&#39;running&#39;] or r[&#39;failedAt&#39;] == &#34;&#34;:
                failed = False
                break
            if r[&#39;instanceManagerName&#39;] != &#34;&#34;:
                im = client.by_id_instance_manager(
                    r[&#39;instanceManagerName&#39;])
                if r[&#39;name&#39;] in im[&#39;instances&#39;]:
                    failed = False
                    break
        if failed:
            break
    assert failed


def wait_for_replica_running(client, volname, replica_name):
    is_running = False
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volname)
        for r in volume.replicas:
            if r[&#39;name&#39;] != replica_name:
                continue
            if r[&#39;running&#39;] and r[&#39;instanceManagerName&#39;] != &#34;&#34;:
                im = client.by_id_instance_manager(
                    r[&#39;instanceManagerName&#39;])
                if r[&#39;name&#39;] in im[&#39;instances&#39;]:
                    is_running = True
                    break
        if is_running:
            break
        time.sleep(RETRY_INTERVAL)
    assert is_running


@pytest.fixture
def volume_name(request):
    return generate_volume_name()


@pytest.fixture
def pvc_name(request):
    return generate_volume_name()


@pytest.fixture
def csi_pvc_name(request):
    return generate_volume_name()


def generate_volume_name():
    return VOLUME_NAME + &#34;-&#34; + \
        &#39;&#39;.join(random.choice(string.ascii_lowercase + string.digits)
                for _ in range(6))


def get_default_engine_image(client):
    images = client.list_engine_image()
    for img in images:
        if img.default:
            return img
    assert False


def get_compatibility_test_image(cli_v, cli_minv,
                                 ctl_v, ctl_minv,
                                 data_v, data_minv):
    return &#34;%s.%d-%d.%d-%d.%d-%d&#34; % (COMPATIBILTY_TEST_IMAGE_PREFIX,
                                     cli_v, cli_minv,
                                     ctl_v, ctl_minv,
                                     data_v, data_minv)


def generate_random_data(count):
    return &#39;&#39;.join(random.choice(string.ascii_lowercase + string.digits)
                   for _ in range(count))


def check_volume_data(volume, data, check_checksum=True):
    dev = get_volume_endpoint(volume)
    check_device_data(dev, data, check_checksum)


def write_volume_random_data(volume, position={}):
    dev = get_volume_endpoint(volume)
    return write_device_random_data(dev, position={})


def check_device_data(dev, data, check_checksum=True):
    r_data = dev_read(dev, data[&#39;pos&#39;], data[&#39;len&#39;])
    assert r_data == bytes(data[&#39;content&#39;], encoding=&#39;utf8&#39;)
    if check_checksum:
        r_checksum = get_device_checksum(dev)
        assert r_checksum == data[&#39;checksum&#39;]


def write_device_random_data(dev, position={}):
    data = generate_random_data(VOLUME_RWTEST_SIZE)
    data_pos = generate_random_pos(VOLUME_RWTEST_SIZE, position)
    data_len = dev_write(dev, data_pos, data)
    checksum = get_device_checksum(dev)

    return {
        &#39;content&#39;: data,
        &#39;pos&#39;: data_pos,
        &#39;len&#39;: data_len,
        &#39;checksum&#39;: checksum
    }


def write_volume_data(volume, data):
    dev = get_volume_endpoint(volume)
    data_len = dev_write(dev, data[&#39;pos&#39;], data[&#39;content&#39;])
    checksum = get_device_checksum(dev)

    return {
        &#39;content&#39;: data[&#39;content&#39;],
        &#39;pos&#39;: data[&#39;pos&#39;],
        &#39;len&#39;: data_len,
        &#39;checksum&#39;: checksum
    }


def get_device_checksum(dev):
    hash = hashlib.sha512()

    with open(dev, &#39;rb&#39;) as fdev:
        if fdev is not None:
            for chunk in iter(lambda: fdev.read(4096), b&#34;&#34;):
                hash.update(chunk)

    return hash.hexdigest()


def volume_read(v, start, count):
    dev = get_volume_endpoint(v)
    return dev_read(dev, start, count)


def dev_read(dev, start, count):
    r_data = &#34;&#34;
    fdev = open(dev, &#39;rb&#39;)
    if fdev is not None:
        fdev.seek(start)
        r_data = fdev.read(count)
        fdev.close()
    return r_data


def volume_write(v, start, data):
    dev = get_volume_endpoint(v)
    return dev_write(dev, start, data)


def dev_write(dev, start, data):
    data = bytes(data, encoding=&#39;utf-8&#39;)
    w_length = 0
    fdev = open(dev, &#39;rb+&#39;)
    if fdev is not None:
        fdev.seek(start)
        fdev.write(data)
        fdev.close()
        w_length = len(data)
    return w_length


def volume_valid(dev):
    return stat.S_ISBLK(os.stat(dev).st_mode)


def parse_iscsi_endpoint(iscsi):
    iscsi_endpoint = iscsi[8:]
    return iscsi_endpoint.split(&#39;/&#39;)


def get_iscsi_ip(iscsi):
    iscsi_endpoint = parse_iscsi_endpoint(iscsi)
    ip = iscsi_endpoint[0].split(&#39;:&#39;)
    return ip[0]


def get_iscsi_port(iscsi):
    iscsi_endpoint = parse_iscsi_endpoint(iscsi)
    ip = iscsi_endpoint[0].split(&#39;:&#39;)
    return ip[1]


def get_iscsi_target(iscsi):
    iscsi_endpoint = parse_iscsi_endpoint(iscsi)
    return iscsi_endpoint[1]


def get_iscsi_lun(iscsi):
    iscsi_endpoint = parse_iscsi_endpoint(iscsi)
    return iscsi_endpoint[2]


def exec_nsenter(cmd):
    dockerd_pid = find_dockerd_pid() or &#34;1&#34;
    exec_cmd = [&#34;nsenter&#34;, &#34;--mount=/host/proc/{}/ns/mnt&#34;.format(dockerd_pid),
                &#34;--net=/host/proc/{}/ns/net&#34;.format(dockerd_pid),
                &#34;bash&#34;, &#34;-c&#34;, cmd]
    return subprocess.check_output(exec_cmd)


def iscsi_login(iscsi_ep):
    ip = get_iscsi_ip(iscsi_ep)
    port = get_iscsi_port(iscsi_ep)
    target = get_iscsi_target(iscsi_ep)
    lun = get_iscsi_lun(iscsi_ep)
    # discovery
    cmd_discovery = &#34;iscsiadm -m discovery -t st -p &#34; + ip
    exec_nsenter(cmd_discovery)
    # login
    cmd_login = &#34;iscsiadm -m node -T &#34; + target + &#34; -p &#34; + ip + &#34; --login&#34;
    exec_nsenter(cmd_login)
    blk_name = &#34;ip-%s:%s-iscsi-%s-lun-%s&#34; % (ip, port, target, lun)
    wait_for_device_login(ISCSI_DEV_PATH, blk_name)
    dev = os.path.realpath(ISCSI_DEV_PATH + &#34;/&#34; + blk_name)
    return dev


def iscsi_logout(iscsi_ep):
    ip = get_iscsi_ip(iscsi_ep)
    target = get_iscsi_target(iscsi_ep)
    cmd_logout = &#34;iscsiadm -m node -T &#34; + target + &#34; -p &#34; + ip + &#34; --logout&#34;
    exec_nsenter(cmd_logout)
    cmd_rm_discovery = &#34;iscsiadm -m discovery -p &#34; + ip + &#34; -o delete&#34;
    exec_nsenter(cmd_rm_discovery)


def get_process_info(p_path):
    info = {}
    with open(p_path) as file:
        for line in file.readlines():
            if &#39;Name:\t&#39; == line[0:len(&#39;Name:\t&#39;)]:
                info[&#34;Name&#34;] = line[len(&#34;Name:&#34;):].strip()
            if &#39;Pid:\t&#39; == line[0:len(&#39;Pid:\t&#39;)]:
                info[&#34;Pid&#34;] = line[len(&#34;Pid:&#34;):].strip()
            if &#39;PPid:\t&#39; == line[0:len(&#39;PPid:\t&#39;)]:
                info[&#34;PPid&#34;] = line[len(&#34;PPid:&#34;):].strip()
    if &#34;Name&#34; not in info or &#34;Pid&#34; not in info or &#34;PPid&#34; not in info:
        return
    return info


def find_self():
    return get_process_info(&#34;/host/proc/self/status&#34;)


def find_ancestor_process_by_name(ancestor_name):
    p = find_self()
    while True:
        if not p or p[&#34;Pid&#34;] == &#34;1&#34;:
            break
        if p[&#34;Name&#34;] == ancestor_name:
            return p[&#34;Pid&#34;]
        p = get_process_info(&#34;/host/proc/{}/status&#34;.format(p[&#34;PPid&#34;]))
    return


def find_dockerd_pid():
    return find_ancestor_process_by_name(&#34;dockerd&#34;)


def generate_random_pos(size, used={}):
    for i in range(RETRY_COUNTS):
        pos = 0
        if int(SIZE) != size:
            pos = random.randrange(0, int(SIZE)-size, 1)
        collided = False
        # it&#39;s [start, end) vs [pos, pos + size)
        for start, end in used.items():
            if pos + size &lt;= start or pos &gt;= end:
                continue
            collided = True
            break
        if not collided:
            break
    assert not collided
    used[pos] = pos + size
    return pos


def get_upgrade_test_image(cli_v, cli_minv,
                           ctl_v, ctl_minv,
                           data_v, data_minv):
    return &#34;%s.%d-%d.%d-%d.%d-%d&#34; % (UPGRADE_TEST_IMAGE_PREFIX,
                                     cli_v, cli_minv,
                                     ctl_v, ctl_minv,
                                     data_v, data_minv)


def prepare_host_disk(dev, vol_name, mkfs_ext4_options=&#34;&#34;):
    if mkfs_ext4_options == &#34;&#34;:
        cmd = [&#39;mkfs.ext4&#39;, dev]
    else:
        cmd = [&#39;mkfs.ext4&#39;, mkfs_ext4_options, dev]
    subprocess.check_call(cmd)

    mount_path = os.path.join(DIRECTORY_PATH, vol_name)
    # create directory before mount
    cmd = [&#39;mkdir&#39;, &#39;-p&#39;, mount_path]
    subprocess.check_call(cmd)

    mount_disk(dev, mount_path)
    return mount_path


def mount_disk(dev, mount_path):
    cmd = [&#39;mount&#39;, dev, mount_path]
    subprocess.check_call(cmd)


def umount_disk(mount_path):
    cmd = [&#39;umount&#39;, mount_path]
    subprocess.check_call(cmd)


def lazy_umount_disk(mount_path):
    cmd = [&#39;umount&#39;, &#39;-l&#39;, mount_path]
    subprocess.check_call(cmd)


def cleanup_host_disk(vol_name):
    mount_path = os.path.join(DIRECTORY_PATH, vol_name)
    umount_disk(mount_path)

    cmd = [&#39;rm&#39;, &#39;-r&#39;, mount_path]
    subprocess.check_call(cmd)


def wait_for_volume_condition_scheduled(client, name, key, value):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        conditions = volume.conditions
        if conditions is not None and \
                conditions != {} and \
                conditions[VOLUME_CONDITION_SCHEDULED] and \
                conditions[VOLUME_CONDITION_SCHEDULED][key] and \
                conditions[VOLUME_CONDITION_SCHEDULED][key] == value:
            break
        time.sleep(RETRY_INTERVAL)
    conditions = volume.conditions
    assert conditions[VOLUME_CONDITION_SCHEDULED][key] == value
    return volume


def wait_for_volume_condition_restore(client, name, key, value):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        conditions = volume.conditions
        if conditions is not None and \
                conditions != {} and \
                conditions[VOLUME_CONDITION_RESTORE] and \
                conditions[VOLUME_CONDITION_RESTORE][key] and \
                conditions[VOLUME_CONDITION_RESTORE][key] == value:
            break
        time.sleep(RETRY_INTERVAL)
    conditions = volume.conditions
    assert conditions[VOLUME_CONDITION_RESTORE][key] == value
    return volume


def get_host_disk_size(disk):
    cmd = [&#39;stat&#39;, &#39;-fc&#39;,
           &#39;{&#34;path&#34;:&#34;%n&#34;,&#34;fsid&#34;:&#34;%i&#34;,&#34;type&#34;:&#34;%T&#34;,&#34;freeBlock&#34;:%f,&#39;
           &#39;&#34;totalBlock&#34;:%b,&#34;blockSize&#34;:%S}&#39;,
           disk]
    output = subprocess.check_output(cmd)
    disk_info = json.loads(output)
    block_size = disk_info[&#34;blockSize&#34;]
    free_blk = disk_info[&#34;freeBlock&#34;]
    total_blk = disk_info[&#34;totalBlock&#34;]
    free = (free_blk * block_size)
    total = (total_blk * block_size)
    return free, total


def wait_for_disk_status(client, node_name, disk_name, key, value):
    # use wait_for_disk_storage_available to check storageAvailable
    assert key != &#34;storageAvailable&#34;
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(node_name)
        disks = node.disks
        if len(disks) &gt; 0 and \
                disk_name in disks and \
                disks[disk_name][key] == value:
            break
        time.sleep(RETRY_INTERVAL)
    assert len(disks) != 0
    assert disk_name in disks
    assert disks[disk_name][key] == value
    return node


def wait_for_disk_storage_available(client, node_name, disk_name, disk_path):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(node_name)
        disks = node.disks
        if len(disks) &gt; 0 and disk_name in disks:
            free, _ = get_host_disk_size(disk_path)
            if disks[disk_name][&#34;storageAvailable&#34;] == free:
                break
        time.sleep(RETRY_INTERVAL)
    assert len(disks) != 0
    assert disk_name in disks
    assert disks[disk_name][&#34;storageAvailable&#34;] == free
    return node


def wait_for_disk_uuid(client, node_name, uuid):
    found = False
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(node_name)
        disks = node.disks
        for name in disks:
            if disks[name][&#34;diskUUID&#34;] == uuid:
                found = True
                break
        if found:
            break
        time.sleep(RETRY_INTERVAL)
    assert found
    return node


def wait_for_disk_conditions(client, node_name, disk_name, key, value):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(node_name)
        disks = node.disks
        disk = disks[disk_name]
        conditions = disk.conditions
        if conditions[key][&#34;status&#34;] == value:
            break
        time.sleep(RETRY_INTERVAL)
    assert conditions[key][&#34;status&#34;] == value
    return node


def wait_for_node_update(client, name, key, value):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        if str(node[key]) == str(value):
            break
        time.sleep(RETRY_INTERVAL)
    assert str(node[key]) == str(value)
    return node


def wait_for_disk_update(client, name, disk_num):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        if len(node.disks) == disk_num:
            allUpdated = True
            disks = node.disks
            for d in disks:
                if disks[d][&#34;diskUUID&#34;] == &#34;&#34;:
                    allUpdated = False
                    break
            if allUpdated:
                break
        time.sleep(RETRY_INTERVAL)
    assert len(node.disks) == disk_num
    return node


def wait_for_node_tag_update(client, name, tags):
    updated = False
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        if not tags and not node.tags:
            updated = True
            break
        elif node.tags is not None and set(node.tags) == set(tags):
            updated = True
            break
        time.sleep(RETRY_INTERVAL)
    assert updated
    return node


def cleanup_node_disks(client, node_name):
    node = client.by_id_node(node_name)
    disks = node.disks
    for _, disk in iter(disks.items()):
        disk.allowScheduling = False
    update_disks = get_update_disks(disks)
    node = client.by_id_node(node_name)
    node.diskUpdate(disks=update_disks)
    node.diskUpdate(disks={})
    return wait_for_disk_update(client, node_name, 0)


def get_volume_engine(v):
    engines = v.controllers
    assert len(engines) != 0
    return engines[0]


def get_volume_endpoint(v):
    endpoint = check_volume_endpoint(v)
    return endpoint


def check_volume_endpoint(v):
    engine = get_volume_engine(v)
    endpoint = engine.endpoint
    if v.disableFrontend:
        assert endpoint == &#34;&#34;
    else:
        if v.frontend == VOLUME_FRONTEND_BLOCKDEV:
            assert endpoint == os.path.join(DEV_PATH, v.name)
        elif v.frontend == VOLUME_FRONTEND_ISCSI:
            assert endpoint.startswith(&#34;iscsi://&#34;)
        else:
            raise Exception(&#34;Unexpected volume frontend:&#34;, v.frontend)
    return endpoint


def get_volume_attached_nodes(v):
    nodes = []
    engines = v.controllers
    for e in engines:
        node = e.hostId
        if node != &#34;&#34;:
            nodes.append(node)
    return nodes


def wait_for_backup_completion(client, volume_name, snapshot_name):
    completed = False
    for i in range(RETRY_BACKUP_COUNTS):
        v = client.by_id_volume(volume_name)
        for b in v.backupStatus:
            if b.snapshot == snapshot_name and b.state == &#34;complete&#34;:
                assert b.progress == 100
                assert b.error == &#34;&#34;
                completed = True
                break
        if completed:
            break
        time.sleep(RETRY_BACKUP_INTERVAL)
    assert completed is True
    return v


def wait_for_backup_state(client, volume_name, predicate):
    completed = False
    for i in range(RETRY_BACKUP_COUNTS):
        v = client.by_id_volume(volume_name)
        for b in v.backupStatus:
            if predicate(b):
                completed = True
                break
        if completed:
            break
        time.sleep(RETRY_BACKUP_INTERVAL)
    assert completed is True
    return v


def monitor_restore_progress(client, volume_name):
    completed = 0
    rs = {}
    for i in range(RETRY_COUNTS):
        completed = 0
        v = client.by_id_volume(volume_name)
        rs = v.restoreStatus
        for r in rs:
            assert r.error == &#34;&#34;
            if r.state == &#34;complete&#34;:
                assert r.progress == 100
                completed += 1
        if completed == len(rs):
            break
        time.sleep(RETRY_INTERVAL)
    assert completed == len(rs)
    return v


def wait_for_volume_migration_ready(client, volume_name):
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        engines = v.controllers
        ready = True
        if len(engines) == 2:
            for e in v.controllers:
                if e.endpoint == &#34;&#34;:
                    ready = False
                    break
        else:
            ready = False
        if ready:
            break
        time.sleep(RETRY_INTERVAL)
    assert ready
    return v


def wait_for_volume_migration_node(client, volume_name, node_id):
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        engines = v.controllers
        replicas = v.replicas
        if len(engines) == 1 and len(replicas) == v.numberOfReplicas:
            e = engines[0]
            if e.endpoint != &#34;&#34;:
                break
        time.sleep(RETRY_INTERVAL)
    assert e.hostId == node_id
    assert e.endpoint != &#34;&#34;
    return v


def get_random_client(clients):
    for _, client in iter(clients.items()):
        break
    return client


def get_update_disks(disks):
    update_disk = {}
    for key, disk in iter(disks.items()):
        update_disk[key] = disk
    return update_disk


def reset_node(client):
    nodes = client.list_node()
    for node in nodes:
        try:
            node = client.update(node, tags=[])
            node = wait_for_node_tag_update(client, node.id, [])
            node = client.update(node, allowScheduling=True)
            wait_for_node_update(client, node.id,
                                 &#34;allowScheduling&#34;, True)
        except Exception as e:
            print(&#34;Exception when reset node schedulding and tags&#34;, node, e)


def cleanup_test_disks(client):
    del_dirs = os.listdir(DIRECTORY_PATH)
    host_id = get_self_host_id()
    node = client.by_id_node(host_id)
    disks = node.disks
    for name, disk in iter(disks.items()):
        for del_dir in del_dirs:
            dir_path = os.path.join(DIRECTORY_PATH, del_dir)
            if dir_path == disk.path:
                disk.allowScheduling = False
    update_disks = get_update_disks(disks)
    try:
        node = node.diskUpdate(disks=update_disks)
        disks = node.disks
        for name, disk in iter(disks.items()):
            for del_dir in del_dirs:
                dir_path = os.path.join(DIRECTORY_PATH, del_dir)
                if dir_path == disk.path:
                    wait_for_disk_status(client, host_id, name,
                                         &#34;allowScheduling&#34;, False)
    except Exception as e:
        print(&#34;Exception when update node disks&#34;, node, e)
        pass

    # delete test disks
    disks = node.disks
    update_disks = {}
    for name, disk in iter(disks.items()):
        if disk.allowScheduling:
            update_disks[name] = disk
    try:
        node.diskUpdate(disks=update_disks)
        wait_for_disk_update(client, host_id, len(update_disks))
    except Exception as e:
        print(&#34;Exception when delete node test disks&#34;, node, e)
        pass
    # cleanup host disks
    for del_dir in del_dirs:
        try:
            cleanup_host_disk(del_dir)
        except Exception as e:
            print(&#34;Exception when cleanup host disk&#34;, del_dir, e)
            pass


def reset_disks_for_all_nodes(client):  # NOQA
    nodes = client.list_node()
    for node in nodes:
        # Reset default disk if there are more than 1 disk
        # on the node.
        if len(node.disks) &gt; 1:
            update_disks = get_update_disks(node.disks)
            for disk_name, disk in update_disks:
                disk.allowScheduling = False
                update_disks[disk_name] = disk
                node = node.diskUpdate(disks=update_disks)
            update_disks = {}
            node = node.diskUpdate(disks=update_disks)
            node = wait_for_disk_update(client, node.name, 0)
        if len(node.disks) == 0:
            default_disk = {&#34;default-disk&#34;:
                            {&#34;path&#34;: DEFAULT_DISK_PATH,
                             &#34;allowScheduling&#34;: True}}
            node = node.diskUpdate(disks=default_disk)
            node = wait_for_disk_update(client, node.name, 1)
            assert len(node.disks) == 1
        # wait for node controller to update disk status
        disks = node.disks
        update_disks = {}
        for name, disk in iter(disks.items()):
            update_disk = disk
            update_disk.allowScheduling = True
            update_disk.storageReserved = \
                int(update_disk.storageMaximum * 30 / 100)
            update_disk.tags = []
            update_disks[name] = update_disk
        node = node.diskUpdate(disks=update_disks)
        for name, disk in iter(node.disks.items()):
            # wait for node controller update disk status
            wait_for_disk_status(client, node.name, name,
                                 &#34;allowScheduling&#34;, True)
            wait_for_disk_status(client, node.name, name,
                                 &#34;storageScheduled&#34;, 0)
            wait_for_disk_status(client, node.name, name,
                                 &#34;storageReserved&#34;,
                                 int(update_disk.storageMaximum * 30 / 100))


def reset_settings(client):
    minimal_setting = client.by_id_setting(
        SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE)
    try:
        client.update(minimal_setting,
                      value=DEFAULT_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;storage minimal available percentage settings&#34;,
              minimal_setting, e)
        pass

    over_provisioning_setting = client.by_id_setting(
        SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE)
    try:
        client.update(over_provisioning_setting,
                      value=DEFAULT_STORAGE_OVER_PROVISIONING_PERCENTAGE)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;storage over provisioning percentage settings&#34;,
              over_provisioning_setting, e)

    default_data_path_setting = client.by_id_setting(
        SETTING_DEFAULT_DATA_PATH)
    try:
        client.update(default_data_path_setting,
                      value=DEFAULT_DISK_PATH)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;default data path setting&#34;,
              default_data_path_setting, e)

    create_default_disk_labeled_nodes_setting = client.by_id_setting(
        SETTING_CREATE_DEFAULT_DISK_LABELED_NODES)
    try:
        client.update(create_default_disk_labeled_nodes_setting,
                      value=&#34;false&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;create default disk labeled nodes setting&#34;,
              create_default_disk_labeled_nodes_setting, e)

    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    try:
        client.update(replica_node_soft_anti_affinity_setting,
                      value=&#34;false&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;Replica Node Level Soft Anti-Affinity setting&#34;,
              replica_node_soft_anti_affinity_setting, e)

    replica_zone_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY)
    try:
        client.update(replica_zone_soft_anti_affinity_setting,
                      value=&#34;true&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;Replica Zone Level Soft Anti-Affinity setting&#34;,
              replica_zone_soft_anti_affinity_setting, e)

    disable_scheduling_on_cordoned_node_setting = \
        client.by_id_setting(SETTING_DISABLE_SCHEDULING_ON_CORDONED_NODE)
    try:
        client.update(disable_scheduling_on_cordoned_node_setting,
                      value=&#34;true&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;Disable Scheduling On Cordoned Node setting&#34;,
              disable_scheduling_on_cordoned_node_setting, e)
    auto_salvage_setting = client.by_id_setting(SETTING_AUTO_SALVAGE)
    try:
        client.update(auto_salvage_setting, value=&#34;true&#34;)
    except Exception as e:
        print(&#34;Exception when update Auto Salvage setting&#34;,
              auto_salvage_setting, e)

    guaranteed_engine_cpu_setting = \
        client.by_id_setting(SETTING_GUARANTEED_ENGINE_CPU)
    try:
        client.update(guaranteed_engine_cpu_setting,
                      value=&#34;0.25&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;Guaranteed Engine CPU setting&#34;,
              guaranteed_engine_cpu_setting, e)

    instance_managers = client.list_instance_manager()
    core_api = get_core_api_client()
    # Wait for the current instance manager running
    for im in instance_managers:
        wait_for_instance_manager_desire_state(client, core_api,
                                               im.name, &#34;Running&#34;, True)


def reset_engine_image(client):
    core_api = get_core_api_client()
    ready = False

    for i in range(RETRY_COUNTS):
        ready = True
        ei_list = client.list_engine_image().data
        for ei in ei_list:
            if ei.default:
                if ei.state != &#39;ready&#39;:
                    ready = False
            else:
                client.delete(ei)
                wait_for_engine_image_deletion(client, core_api, ei.name)
        if ready:
            break
        time.sleep(RETRY_INTERVAL)

    assert ready


def wait_for_node_mountpropagation_condition(client, name):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        conditions = {}
        if &#34;conditions&#34; in node.keys():
            conditions = node.conditions

        if NODE_CONDITION_MOUNTPROPAGATION in \
                conditions.keys() and \
                &#34;status&#34; in \
                conditions[NODE_CONDITION_MOUNTPROPAGATION].keys() \
                and conditions[NODE_CONDITION_MOUNTPROPAGATION][&#34;status&#34;] != \
                CONDITION_STATUS_UNKNOWN:
            break
        time.sleep(RETRY_INTERVAL)
    return node


def wait_for_node_schedulable_condition(client, name):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        conditions = {}
        if &#34;conditions&#34; in node.keys():
            conditions = node.conditions

        if NODE_CONDITION_SCHEDULABLE in \
                conditions.keys() and \
                &#34;status&#34; in \
                conditions[NODE_CONDITION_SCHEDULABLE].keys() \
                and conditions[NODE_CONDITION_SCHEDULABLE][&#34;status&#34;] != \
                CONDITION_STATUS_UNKNOWN:
            break
        time.sleep(RETRY_INTERVAL)
    return node


class timeout:

    def __init__(self, seconds=1, error_message=&#39;Timeout&#39;):
        self.seconds = seconds
        self.error_message = error_message

    def handle_timeout(self, signum, frame):
        raise Exception(self.error_message)

    def __enter__(self):
        signal.signal(signal.SIGALRM, self.handle_timeout)
        signal.alarm(self.seconds)

    def __exit__(self, type, value, traceback):
        signal.alarm(0)


def is_backupTarget_s3(s):
    return s.startswith(&#34;s3://&#34;)


def is_backupTarget_nfs(s):
    return s.startswith(&#34;nfs://&#34;)


def find_backup(client, vol_name, snap_name):
    found = False
    for i in range(100):
        bvs = client.list_backupVolume()
        for bv in bvs:
            if bv.name == vol_name:
                found = True
                break
        if found:
            break
        time.sleep(1)
    assert found

    found = False
    for i in range(20):
        backups = bv.backupList().data
        for b in backups:
            if b.snapshotName == snap_name:
                found = True
                break
        if found:
            break
        time.sleep(1)
    assert found

    return bv, b


def check_longhorn(core_api):
    ready = False
    has_engine_image = False
    has_driver_deployer = False
    has_manager = False
    has_ui = False
    has_instance_manager = False

    pod_running = True

    try:
        longhorn_pod_list = core_api.list_namespaced_pod(&#39;longhorn-system&#39;)
        for item in longhorn_pod_list.items:
            labels = item.metadata.labels

            if not labels:
                pass
            elif labels.get(&#39;longhorn.io/component&#39;, &#39;&#39;) == &#39;engine-image&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_engine_image = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;longhorn-driver-deployer&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_driver_deployer = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;longhorn-manager&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_manager = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;longhorn-ui&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_ui = True
            elif labels.get(&#39;longhorn.io/component&#39;, &#39;&#39;) == \
                    &#39;instance-manager&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_instance_manager = True

        if has_engine_image and has_driver_deployer and has_manager and \
                has_ui and has_instance_manager and pod_running:
            ready = True

    except ApiException as e:
        if (e.status == 404):
            ready = False

    assert ready


def check_csi(core_api):
    using_csi = CSI_UNKNOWN

    has_attacher = False
    has_provisioner = False
    has_csi_plugin = False

    pod_running = True

    try:
        longhorn_pod_list = core_api.list_namespaced_pod(&#39;longhorn-system&#39;)
        for item in longhorn_pod_list.items:
            if item.status.phase != &#34;Running&#34;:
                pod_running = False

            labels = item.metadata.labels
            if not labels:
                pass
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;csi-attacher&#39;:
                has_attacher = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;csi-provisioner&#39;:
                has_provisioner = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;longhorn-csi-plugin&#39;:
                has_csi_plugin = True

        if has_attacher and has_provisioner and has_csi_plugin and pod_running:
            using_csi = CSI_TRUE
        elif not has_attacher and not has_provisioner \
                and not has_csi_plugin and not pod_running:
            using_csi = CSI_FALSE

    except ApiException as e:
        if (e.status == 404):
            using_csi = CSI_FALSE

    assert using_csi != CSI_UNKNOWN

    return True if using_csi == CSI_TRUE else False


def check_csi_expansion(core_api):
    csi_expansion_enabled = False
    has_csi_resizer = False
    pod_running = True

    try:
        longhorn_pod_list = core_api.list_namespaced_pod(&#39;longhorn-system&#39;)
        for item in longhorn_pod_list.items:
            if item.status.phase != &#34;Running&#34;:
                pod_running = False

            labels = item.metadata.labels
            if not labels:
                pass
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;csi-resizer&#39;:
                has_csi_resizer = True
        if has_csi_resizer and pod_running:
            csi_expansion_enabled = True

    except ApiException:
        pass

    return csi_expansion_enabled


def create_and_wait_statefulset(statefulset_manifest):
    &#34;&#34;&#34;
    Create a new StatefulSet for testing.

    This function will block until all replicas in the StatefulSet are online
    or it times out, whichever occurs first.
    &#34;&#34;&#34;
    api = get_apps_api_client()
    api.create_namespaced_stateful_set(
        body=statefulset_manifest,
        namespace=&#39;default&#39;)
    wait_statefulset(statefulset_manifest)


def wait_statefulset(statefulset_manifest):
    api = get_apps_api_client()
    replicas = statefulset_manifest[&#39;spec&#39;][&#39;replicas&#39;]
    for i in range(DEFAULT_STATEFULSET_TIMEOUT):
        s_set = api.read_namespaced_stateful_set(
            name=statefulset_manifest[&#39;metadata&#39;][&#39;name&#39;],
            namespace=&#39;default&#39;)
        if s_set.status.ready_replicas == replicas:
            break
        time.sleep(DEFAULT_STATEFULSET_INTERVAL)
    assert s_set.status.ready_replicas == replicas


def create_storage_class(sc_manifest):
    api = get_storage_api_client()
    api.create_storage_class(
        body=sc_manifest)


def delete_storage_class(sc_name):
    api = get_storage_api_client()
    try:
        api.delete_storage_class(sc_name, body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404


def create_pvc(pvc_manifest):
    api = get_core_api_client()
    api.create_namespaced_persistent_volume_claim(
        &#39;default&#39;, pvc_manifest)


def update_statefulset_manifests(ss_manifest, sc_manifest, name):
    &#34;&#34;&#34;
    Write in a new StatefulSet name and the proper StorageClass name for tests.
    &#34;&#34;&#34;
    ss_manifest[&#39;metadata&#39;][&#39;name&#39;] = \
        ss_manifest[&#39;spec&#39;][&#39;selector&#39;][&#39;matchLabels&#39;][&#39;app&#39;] = \
        ss_manifest[&#39;spec&#39;][&#39;serviceName&#39;] = \
        ss_manifest[&#39;spec&#39;][&#39;template&#39;][&#39;metadata&#39;][&#39;labels&#39;][&#39;app&#39;] = \
        name
    ss_manifest[&#39;spec&#39;][&#39;volumeClaimTemplates&#39;][0][&#39;spec&#39;][&#39;storageClassName&#39;]\
        = DEFAULT_STORAGECLASS_NAME
    sc_manifest[&#39;metadata&#39;][&#39;name&#39;] = DEFAULT_STORAGECLASS_NAME


def check_volume_existence(client, volume_name):
    volumes = client.list_volume()
    for volume in volumes:
        if volume.name == volume_name:
            return True
    return False


def check_pod_existence(api, pod_name, namespace=&#34;default&#34;):
    pods = api.list_namespaced_pod(namespace)
    for pod in pods.items:
        if pod.metadata.name == pod_name and \
                not pod.metadata.deletion_timestamp:
            return True
    return False


def check_pvc_existence(api, pvc_name, namespace=&#34;default&#34;):
    pvcs = api.list_namespaced_persistent_volume_claim(namespace)
    for pvc in pvcs.items:
        if pvc.metadata.name == pvc_name and not \
                pvc.metadata.deletion_timestamp:
            return True
    return False


def check_pv_existence(api, pv_name):
    pvs = api.list_persistent_volume()
    for pv in pvs.items:
        if pv.metadata.name == pv_name and not pv.metadata.deletion_timestamp:
            return True
    return False


def check_statefulset_existence(api, ss_name, namespace=&#34;default&#34;):
    ss_list = api.list_namespaced_stateful_set(namespace)
    for ss in ss_list.items:
        if ss.metadata.name == ss_name and not ss.metadata.deletion_timestamp:
            return True
    return False


def delete_and_wait_pvc(api, pvc_name):
    try:
        api.delete_namespaced_persistent_volume_claim(
            name=pvc_name, namespace=&#39;default&#39;,
            body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404

    wait_delete_pvc(api, pvc_name)


def wait_delete_pvc(api, pvc_name):
    for i in range(RETRY_COUNTS):
        found = False
        ret = api.list_namespaced_persistent_volume_claim(namespace=&#39;default&#39;)
        for item in ret.items:
            if item.metadata.name == pvc_name:
                found = True
                break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found


def delete_and_wait_pv(api, pv_name):
    try:
        api.delete_persistent_volume(
            name=pv_name, body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404

    wait_delete_pv(api, pv_name)


def wait_delete_pv(api, pv_name):
    for i in range(RETRY_COUNTS):
        found = False
        pvs = api.list_persistent_volume()
        for item in pvs.items:
            if item.metadata.name == pv_name:
                if item.status.phase == &#39;Failed&#39;:
                    try:
                        api.delete_persistent_volume(
                            name=pv_name, body=k8sclient.V1DeleteOptions())
                    except ApiException as e:
                        assert e.status == 404
                else:
                    found = True
                    break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found


def wait_volume_kubernetes_status(client, volume_name, expect_ks):
    for i in range(RETRY_COUNTS):
        expected = True
        volume = client.by_id_volume(volume_name)
        ks = volume.kubernetesStatus
        ks = json.loads(json.dumps(ks, default=lambda o: o.__dict__))

        for k, v in expect_ks.items():
            if k in (&#39;lastPVCRefAt&#39;, &#39;lastPodRefAt&#39;):
                if (v != &#39;&#39; and ks[k] == &#39;&#39;) or \
                   (v == &#39;&#39; and ks[k] != &#39;&#39;):
                    expected = False
                    break
            else:
                if ks[k] != v:
                    expected = False
                    break
        if expected:
            break
        time.sleep(RETRY_INTERVAL)
    assert expected


def create_pv_for_volume(client, core_api, volume, pv_name, fs_type=&#34;ext4&#34;):
    volume.pvCreate(pvName=pv_name, fsType=fs_type)
    for i in range(RETRY_COUNTS):
        if check_pv_existence(core_api, pv_name):
            break
        time.sleep(RETRY_INTERVAL)
    assert check_pv_existence(core_api, pv_name)

    ks = {
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;lastPodRefAt&#39;: &#39;&#39;,
    }
    wait_volume_kubernetes_status(client, volume.name, ks)


def create_pvc_for_volume(client, core_api, volume, pvc_name):
    volume.pvcCreate(namespace=&#34;default&#34;, pvcName=pvc_name)
    for i in range(RETRY_COUNTS):
        if check_pvc_existence(core_api, pvc_name):
            break
        time.sleep(RETRY_INTERVAL)
    assert check_pvc_existence(core_api, pvc_name)

    ks = {
        &#39;pvStatus&#39;: &#39;Bound&#39;,
        &#39;namespace&#39;: &#39;default&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
    }
    wait_volume_kubernetes_status(client, volume.name, ks)


def activate_standby_volume(client, volume_name,
                            frontend=VOLUME_FRONTEND_BLOCKDEV):
    volume = client.by_id_volume(volume_name)
    assert volume.standby is True
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volume_name)
        engines = volume.controllers
        if len(engines) != 1 or \
                (volume.lastBackup != &#34;&#34; and
                 engines[0].lastRestoredBackup != volume.lastBackup):
            time.sleep(RETRY_INTERVAL)
            continue
        activated = False
        try:
            volume.activate(frontend=frontend)
            activated = True
            break
        except Exception as e:
            assert &#34;hasn&#39;t finished incremental restored&#34; \
                   in str(e.error.message)
            time.sleep(RETRY_INTERVAL)
        if activated:
            break
    volume = client.by_id_volume(volume_name)
    assert volume.standby is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    wait_for_volume_detached(client, volume_name)

    volume = client.by_id_volume(volume_name)
    engine = get_volume_engine(volume)
    assert engine.lastRestoredBackup == &#34;&#34;
    assert engine.requestedBackupRestore == &#34;&#34;


def check_volume_last_backup(client, volume_name, last_backup):
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volume_name)
        if volume.lastBackup == last_backup:
            break
        time.sleep(RETRY_INTERVAL)
    volume = client.by_id_volume(volume_name)
    assert volume.lastBackup == last_backup


def set_random_backupstore(client):
    setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    backupstores = get_backupstore_url()
    for backupstore in backupstores:
        if is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;
        break


def generate_pod_with_pvc_manifest(pod_name, pvc_name):
    pod_manifest = {
        &#34;apiVersion&#34;: &#34;v1&#34;,
        &#34;kind&#34;: &#34;Pod&#34;,
        &#34;metadata&#34;: {
           &#34;name&#34;: pod_name,
           &#34;namespace&#34;: &#34;default&#34;
        },
        &#34;spec&#34;: {
           &#34;containers&#34;: [
              {
                 &#34;name&#34;: &#34;volume-test&#34;,
                 &#34;image&#34;: &#34;nginx:stable-alpine&#34;,
                 &#34;imagePullPolicy&#34;: &#34;IfNotPresent&#34;,
                 &#34;volumeMounts&#34;: [
                    {
                       &#34;name&#34;: &#34;volv&#34;,
                       &#34;mountPath&#34;: &#34;/data&#34;
                    }
                 ],
                 &#34;ports&#34;: [
                    {
                       &#34;containerPort&#34;: 80
                    }
                 ]
              }
           ],
           &#34;volumes&#34;: [
              {
                 &#34;name&#34;: &#34;volv&#34;,
                 &#34;persistentVolumeClaim&#34;: {
                    &#34;claimName&#34;: pvc_name
                 }
              }
           ]
        }
    }

    return pod_manifest


def delete_and_wait_volume_attachment(storage_api, volume_attachment_name):
    try:
        storage_api.delete_volume_attachment(
            name=volume_attachment_name
        )
    except ApiException as e:
        assert e.status == 404

    wait_delete_volume_attachment(storage_api, volume_attachment_name)


def wait_delete_volume_attachment(storage_api, volume_attachment_name):
    for i in range(RETRY_COUNTS):
        found = False
        ret = storage_api.list_volume_attachment()
        for item in ret.items:
            if item.metadata.name == volume_attachment_name:
                found = True
                break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found


def wait_for_engine_image_deletion(client, core_api, engine_image_name):
    deleted = False

    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL)
        deleted = True

        ei_list = client.list_engine_image().data
        for ei in ei_list:
            if ei.name == engine_image_name:
                deleted = False
                break
        if not deleted:
            continue

        labels = &#34;longhorn.io/component=engine-image,&#34; \
                 &#34;longhorn.io/engine-image=&#34;+engine_image_name
        ei_pod_list = core_api.list_namespaced_pod(
            LONGHORN_NAMESPACE, label_selector=labels).items
        if len(ei_pod_list) != 0:
            deleted = False
            continue

    assert deleted


def create_snapshot(longhorn_api_client, volume_name):
    volume = longhorn_api_client.by_id_volume(volume_name)
    snapshots = volume.snapshotList(volume=volume_name)
    snap = volume.snapshotCreate()
    snap_name = snap.name

    snapshot_created = False
    for i in range(RETRY_COUNTS):
        snapshots = volume.snapshotList(volume=volume_name)

        for vs in snapshots.data:
            if vs.name == snap_name:
                snapshot_created = True
                break
        if snapshot_created is True:
            break
        time.sleep(RETRY_INTERVAL)

    assert snapshot_created
    return snap


def wait_and_get_pv_for_pvc(api, pvc_name):
    found = False
    for i in range(RETRY_COUNTS):
        pvs = api.list_persistent_volume()
        for item in pvs.items:
            if item.spec.claim_ref.name == pvc_name:
                found = True
                pv = item
                break
        if found:
            break
        time.sleep(RETRY_INTERVAL)

    assert found
    return pv


def wait_for_volume_expansion(longhorn_api_client, volume_name):
    complete = False
    for i in range(RETRY_COUNTS):
        volume = longhorn_api_client.by_id_volume(volume_name)
        engine = get_volume_engine(volume)
        if engine.size == volume.size and volume.state == &#34;detached&#34;:
            complete = True
            break
        time.sleep(RETRY_INTERVAL)
    assert complete


def check_block_device_size(volume, size):
    dev = get_volume_endpoint(volume)
    # BLKGETSIZE64, result is bytes as unsigned 64-bit integer (uint64)
    req = 0x80081272
    buf = &#39; &#39; * 8
    with open(dev) as dev:
        buf = fcntl.ioctl(dev.fileno(), req, buf)
    device_size = struct.unpack(&#39;L&#39;, buf)[0]
    assert device_size == size


def wait_for_dr_volume_expansion(longhorn_api_client, volume_name, size_str):
    complete = False
    for i in range(RETRY_COUNTS):
        volume = longhorn_api_client.by_id_volume(volume_name)
        if volume.size == size_str:
            engine = get_volume_engine(volume)
            if engine.size == volume.size:
                complete = True
                break
        time.sleep(RETRY_INTERVAL)
    assert complete


def expand_and_wait_for_pvc(api, pvc):
    pvc_name = pvc[&#39;metadata&#39;][&#39;name&#39;]
    api.patch_namespaced_persistent_volume_claim(
        pvc_name, &#39;default&#39;, pvc)
    complete = False
    for i in range(RETRY_COUNTS):
        claim = api.read_namespaced_persistent_volume_claim(
            name=pvc_name, namespace=&#39;default&#39;)
        if claim.spec.resources.requests[&#39;storage&#39;] ==\
                claim.status.capacity[&#39;storage&#39;]:
            complete = True
            break
        time.sleep(RETRY_INTERVAL)
    assert complete
    return claim


def fail_replica_expansion(client, api, volname, size, replicas=None):
    if replicas is None:
        volume = client.by_id_volume(volname)
        replicas = volume.replicas

    for r in replicas:
        tmp_meta_file_name = \
            EXPANSION_SNAP_TMP_META_NAME_PATTERN % size
        # os.path.join() cannot deal with the path containing &#34;/&#34;
        cmd = [
            &#39;/bin/sh&#39;, &#39;-c&#39;,
            &#39;mkdir %s &amp;&amp; sync&#39; %
            (INSTANCE_MANAGER_HOST_PATH_PREFIX + r.dataPath +
             &#34;/&#34; + tmp_meta_file_name)
        ]
        if not r.instanceManagerName:
            raise Exception(
                &#34;Should use replica objects in the running volume,&#34;
                &#34;otherwise the field r.instanceManagerName is emtpy&#34;)
        stream(api.connect_get_namespaced_pod_exec,
               r.instanceManagerName,
               LONGHORN_NAMESPACE, command=cmd,
               stderr=True, stdin=False, stdout=True, tty=False)


def wait_for_expansion_failure(client, volume_name, last_failed_at=&#34;&#34;):
    failed = False
    for i in range(30):
        volume = client.by_id_volume(volume_name)
        engine = get_volume_engine(volume)
        if engine.lastExpansionFailedAt != last_failed_at:
            failed = True
            break
        time.sleep(RETRY_INTERVAL)
    assert failed


def wait_for_rebuild_complete(client, volume_name):
    completed = 0
    rebuild_statuses = {}
    for i in range(RETRY_COUNTS):
        completed = 0
        v = client.by_id_volume(volume_name)
        rebuild_statuses = v.rebuildStatus
        for status in rebuild_statuses:
            if status.state == &#34;complete&#34;:
                assert status.progress == 100
                assert not status.error
                assert not status.isRebuilding
                completed += 1
            elif status.state == &#34;&#34;:
                assert not status.error
                assert not status.isRebuilding
                completed += 1
            elif status.state == &#34;in_progress&#34;:
                assert status.isRebuilding
            else:
                assert status.state == &#34;error&#34;
                assert status.error != &#34;&#34;
                assert not status.isRebuilding
        if completed == len(rebuild_statuses):
            break
        time.sleep(RETRY_INTERVAL)
    assert completed == len(rebuild_statuses)


def wait_for_rebuild_start(client, volume_name):
    started = False
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        rebuild_statuses = v.rebuildStatus
        for status in rebuild_statuses:
            if status.state == &#34;in_progress&#34;:
                started = True
                break
        if started:
            break
        time.sleep(RETRY_INTERVAL)
    assert started
    return status.fromReplica, status.replica


def wait_for_volume_restoration_completed(client, name):
    wait_for_volume_creation(client, name)
    monitor_restore_progress(client, name)
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_INITIALRESTORATIONREQUIRED,
                                  False)


def wait_for_volume_restoration_start(client, volume_name, backup_name):
    wait_for_volume_status(client, volume_name,
                           VOLUME_FIELD_STATE, VOLUME_STATE_ATTACHED)
    started = False
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volume_name)
        for status in volume.restoreStatus:
            if status.state == &#34;in_progress&#34; and \
                    status.progress &gt; 0:
                started = True
                break
        #  Sometime the restore time is pretty short
        #  and the test may not be able to catch the intermediate status.
        if volume.controllers[0].lastRestoredBackup == backup_name:
            started = True
        if started:
            break
        time.sleep(RETRY_INTERVAL)
    assert started
    return status.replica


@pytest.fixture
def make_deployment_with_pvc(request):
    def _generate_deployment_with_pvc_manifest(deployment_name, pvc_name, replicas=1): # NOQA
        make_deployment_with_pvc.deployment_manifest = {
            &#34;apiVersion&#34;: &#34;apps/v1&#34;,
            &#34;kind&#34;: &#34;Deployment&#34;,
            &#34;metadata&#34;: {
               &#34;name&#34;: deployment_name,
               &#34;labels&#34;: {
                  &#34;name&#34;: deployment_name
               }
            },
            &#34;spec&#34;: {
               &#34;replicas&#34;: replicas,
               &#34;selector&#34;: {
                  &#34;matchLabels&#34;: {
                     &#34;name&#34;: deployment_name
                  }
               },
               &#34;template&#34;: {
                  &#34;metadata&#34;: {
                     &#34;labels&#34;: {
                        &#34;name&#34;: deployment_name
                     }
                  },
                  &#34;spec&#34;: {
                     &#34;containers&#34;: [
                        {
                           &#34;name&#34;: deployment_name,
                           &#34;image&#34;: &#34;nginx:stable-alpine&#34;,
                           &#34;volumeMounts&#34;: [
                              {
                                 &#34;name&#34;: &#34;volv&#34;,
                                 &#34;mountPath&#34;: &#34;/data&#34;
                              }
                           ]
                        }
                     ],
                     &#34;volumes&#34;: [
                        {
                           &#34;name&#34;: &#34;volv&#34;,
                           &#34;persistentVolumeClaim&#34;: {
                              &#34;claimName&#34;: pvc_name
                           }
                        }
                     ]
                  }
               }
            }
        }

        return make_deployment_with_pvc.deployment_manifest

    def finalizer():
        apps_api = get_apps_api_client()
        deployment_name = \
            make_deployment_with_pvc.deployment_manifest[&#34;metadata&#34;][&#34;name&#34;]
        delete_and_wait_deployment(
            apps_api,
            deployment_name
        )

    request.addfinalizer(finalizer)

    return _generate_deployment_with_pvc_manifest


def wait_deployment_replica_ready(apps_api, deployment_name, desired_replica_count): # NOQA
    replicas_ready = False
    for i in range(DEFAULT_DEPLOYMENT_TIMEOUT):
        deployment = apps_api.read_namespaced_deployment(
            name=deployment_name,
            namespace=&#34;default&#34;)

        if deployment.status.ready_replicas == desired_replica_count:
            replicas_ready = True
            break

        time.sleep(DEFAULT_DEPLOYMENT_INTERVAL)

    assert replicas_ready


def create_and_wait_deployment(apps_api, deployment_manifest):
    apps_api.create_namespaced_deployment(
        body=deployment_manifest,
        namespace=&#39;default&#39;)

    deployment_name = deployment_manifest[&#34;metadata&#34;][&#34;name&#34;]
    desired_replica_count = deployment_manifest[&#34;spec&#34;][&#34;replicas&#34;]

    wait_deployment_replica_ready(
        apps_api,
        deployment_name,
        desired_replica_count
    )


def wait_delete_deployment(apps_api, deployment_name):
    for i in range(DEFAULT_DEPLOYMENT_TIMEOUT):
        ret = apps_api.list_namespaced_deployment(namespace=&#39;default&#39;)
        found = False
        for item in ret.items:
            if item.metadata.name == deployment_name:
                found = True
                break
        if not found:
            break
        time.sleep(DEFAULT_DEPLOYMENT_INTERVAL)
    assert not found


def delete_and_wait_deployment(apps_api, deployment_name):
    try:
        apps_api.delete_namespaced_deployment(
            name=deployment_name,
            namespace=&#39;default&#39;
        )
    except ApiException as e:
        assert e.status == 404

    wait_delete_deployment(apps_api, deployment_name)


@pytest.fixture
def disable_auto_salvage(client):
    auto_salvage_setting = client.by_id_setting(SETTING_AUTO_SALVAGE)
    setting = client.update(auto_salvage_setting, value=&#34;false&#34;)

    assert setting.name == SETTING_AUTO_SALVAGE
    assert setting.value == &#34;false&#34;

    yield

    auto_salvage_setting = client.by_id_setting(SETTING_AUTO_SALVAGE)
    setting = client.update(auto_salvage_setting, value=&#34;true&#34;)

    assert setting.name == SETTING_AUTO_SALVAGE
    assert setting.value == &#34;true&#34;


def get_liveness_probe_spec(initial_delay=5, period=5):
    pod_liveness_probe_spec = {
        &#34;exec&#34;: {
            &#34;command&#34;: [
                &#34;ls&#34;,
                &#34;/data/lost+found&#34;
            ]
        },
        &#34;initialDelaySeconds&#34;: initial_delay,
        &#34;periodSeconds&#34;: period
    }

    return pod_liveness_probe_spec


def wait_for_pod_remount(core_api, pod_name):
    check_command = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;ls /data/lost+found&#39;
    ]

    ready = False
    for i in range(RETRY_EXEC_COUNTS):
        try:
            output = stream(core_api.connect_get_namespaced_pod_exec,
                            pod_name,
                            &#39;default&#39;,
                            command=check_command,
                            stderr=True, stdin=False,
                            stdout=True, tty=False)
            if &#34;Input/output error&#34; not in output:
                ready = True
                break
        except Exception:
            pass
        if ready:
            break
        time.sleep(RETRY_EXEC_INTERVAL)
    assert ready


def expand_attached_volume(client, volume_name):
    volume = wait_for_volume_healthy(client, volume_name)
    engine = get_volume_engine(volume)

    volume.detach()
    volume = wait_for_volume_detached(client, volume.name)
    volume.expand(size=EXPAND_SIZE)
    wait_for_volume_expansion(client, volume.name)
    volume.attach(hostId=engine.hostId, disableFrontend=False)
    wait_for_volume_healthy(client, volume_name)


def prepare_pod_with_data_in_mb(
        client, core_api, pod_make, volume_name, volume_size=str(1*Gi),
        data_path=&#34;/data/test&#34;, data_size_in_mb=DATA_SIZE_IN_MB_1,
        add_liveness_prope=True):  # NOQA:
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)

    if add_liveness_prope is True:
        pod_liveness_probe_spec = \
            get_liveness_probe_spec(initial_delay=1,
                                    period=1)
        pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = \
            pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=3, size=volume_size)
    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    write_pod_volume_random_data(core_api, pod_name,
                                 data_path, data_size_in_mb)
    md5sum = get_pod_data_md5sum(core_api, pod_name, data_path)

    stream(core_api.connect_get_namespaced_pod_exec,
           pod_name, &#39;default&#39;, command=[&#34;sync&#34;],
           stderr=True, stdin=False, stdout=True, tty=False)

    return pod_name, pv_name, pvc_name, md5sum


@pytest.fixture
def settings_reset():
    yield

    client = get_longhorn_api_client()
    reset_settings(client)


@pytest.fixture
def set_backupstore_s3(client):
    backup_target_setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    backupstores = get_backupstore_url()
    for backupstore in backupstores:
        if is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            backup_target_setting = client.update(backup_target_setting,
                                                  value=backupsettings[0])
            assert backup_target_setting.value == backupsettings[0]

            backup_target_credential_setting = client.by_id_setting(
                SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            backup_target_credential_setting = \
                client.update(backup_target_credential_setting,
                              value=backupsettings[1])
            assert backup_target_credential_setting.value == backupsettings[1]
            break

    yield
    backup_target_setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    client.update(backup_target_setting, value=&#34;&#34;)
    backup_target_credential_setting = client.by_id_setting(
        SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
    client.update(backup_target_credential_setting, value=&#34;&#34;)


@pytest.fixture
def set_backupstore_nfs(client):
    backup_target_setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    backupstores = get_backupstore_url()
    for backupstore in backupstores:
        if is_backupTarget_nfs(backupstore):
            backup_target_setting = client.update(backup_target_setting,
                                                  value=backupstore)
            assert backup_target_setting.value == backupstore
            backup_target_credential_setting = client.by_id_setting(
                SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            backup_target_credential_setting = \
                client.update(backup_target_credential_setting, value=&#34;&#34;)
            assert backup_target_credential_setting.value == &#34;&#34;
            break

    yield
    backup_target_setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    client.update(backup_target_setting, value=&#34;&#34;)
    backup_target_credential_setting = client.by_id_setting(
        SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
    client.update(backup_target_credential_setting, value=&#34;&#34;)


@pytest.fixture
def mount_nfs_backupstore(client, set_backupstore_nfs):
    cmd = [&#34;mkdir&#34;, &#34;-p&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    nfs_backuptarget = client.by_id_setting(SETTING_BACKUP_TARGET).value
    nfs_url = urlparse(nfs_backuptarget).netloc + \
        urlparse(nfs_backuptarget).path
    cmd = [&#34;mount&#34;, &#34;-t&#34;, &#34;nfs4&#34;, nfs_url, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)

    yield &#34;/mnt/nfs&#34;
    cmd = [&#34;umount&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;rmdir&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)


def crash_engine_process_with_sigkill(client, core_api, volume_name):
    volume = client.by_id_volume(volume_name)
    ins_mgr_name = volume.controllers[0].instanceManagerName

    kill_command = [
            &#39;/bin/sh&#39;, &#39;-c&#39;,
            &#34;kill -9 `ps aux | grep -i \&#34;controller &#34; +
            volume_name + &#34;\&#34; | grep -v grep | awk &#39;{print $2}&#39;`&#34;]

    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream read&#39;):
        stream(core_api.connect_get_namespaced_pod_exec,
               ins_mgr_name,
               LONGHORN_NAMESPACE, command=kill_command,
               stderr=True, stdin=False, stdout=True, tty=False)


def wait_for_pod_restart(core_api, pod_name, namespace=&#34;default&#34;):
    pod = core_api.read_namespaced_pod(name=pod_name,
                                       namespace=namespace)
    restart_count = pod.status.container_statuses[0].restart_count

    pod_restarted = False
    for i in range(RETRY_COUNTS):
        pod = core_api.read_namespaced_pod(name=pod_name,
                                           namespace=namespace)
        count = pod.status.container_statuses[0].restart_count
        if count &gt; restart_count:
            pod_restarted = True
            break

        time.sleep(RETRY_INTERVAL)
    assert pod_restarted


def wait_for_instance_manager_desire_state(client, core_api, im_name,
                                           state, desire=True):
    for i in range(RETRY_COUNTS):
        im = client.by_id_instance_manager(im_name)
        try:
            pod = core_api.read_namespaced_pod(name=im_name,
                                               namespace=LONGHORN_NAMESPACE)
        except Exception as e:
            # Continue with pod restarted case
            if e.reason == &#34;Not Found&#34;:
                time.sleep(RETRY_INTERVAL)
                continue
            # Report any other error
            else:
                assert(not e)
        if desire:
            if im.currentState == state.lower() and pod.status.phase == state:
                break
        else:
            if im.currentState != state.lower() and pod.status.phase != state:
                break
        time.sleep(RETRY_INTERVAL)
    if desire:
        assert im.currentState == state.lower()
        assert pod.status.phase == state
    else:
        assert im.currentState != state.lower()
        assert pod.status.phase != state
    return im


def wait_for_backup_delete(client, volume_name, backup_name):
    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()
        bv_found = False

        for bv in bvs:
            if bv.name == volume_name:
                bv_found = True
                break

        assert bv_found

        backups = bv.backupList()

        backup_found = False
        for b in backups:
            if b.name == backup_name:
                backup_found = True
                break

        if backup_found is False:
            break

        time.sleep(RETRY_INTERVAL)

    assert not backup_found


def assert_backup_state(b_actual, b_expected):
    assert b_expected.name == b_actual.name
    assert b_expected.url == b_actual.url
    assert b_expected.snapshotName == b_actual.snapshotName
    assert b_expected.snapshotCreated == b_actual.snapshotCreated
    assert b_expected.created == b_actual.created
    assert b_expected.volumeName == b_actual.volumeName
    assert b_expected.volumeSize == b_actual.volumeSize
    assert b_expected.volumeCreated == b_actual.volumeCreated
    assert b_expected.messages == b_actual.messages is None</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.common.activate_standby_volume"><code class="name flex">
<span>def <span class="ident">activate_standby_volume</span></span>(<span>client, volume_name, frontend='blockdev')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def activate_standby_volume(client, volume_name,
                            frontend=VOLUME_FRONTEND_BLOCKDEV):
    volume = client.by_id_volume(volume_name)
    assert volume.standby is True
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volume_name)
        engines = volume.controllers
        if len(engines) != 1 or \
                (volume.lastBackup != &#34;&#34; and
                 engines[0].lastRestoredBackup != volume.lastBackup):
            time.sleep(RETRY_INTERVAL)
            continue
        activated = False
        try:
            volume.activate(frontend=frontend)
            activated = True
            break
        except Exception as e:
            assert &#34;hasn&#39;t finished incremental restored&#34; \
                   in str(e.error.message)
            time.sleep(RETRY_INTERVAL)
        if activated:
            break
    volume = client.by_id_volume(volume_name)
    assert volume.standby is False
    assert volume.frontend == VOLUME_FRONTEND_BLOCKDEV

    wait_for_volume_detached(client, volume_name)

    volume = client.by_id_volume(volume_name)
    engine = get_volume_engine(volume)
    assert engine.lastRestoredBackup == &#34;&#34;
    assert engine.requestedBackupRestore == &#34;&#34;</code></pre>
</details>
</dd>
<dt id="tests.common.apps_api"><code class="name flex">
<span>def <span class="ident">apps_api</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a new AppsV1API instance.</p>
<h2 id="returns">Returns</h2>
<p>A new AppsV1API Instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def apps_api(request):
    &#34;&#34;&#34;
    Create a new AppsV1API instance.
    Returns:
        A new AppsV1API Instance.
    &#34;&#34;&#34;
    c = Configuration()
    c.assert_hostname = False
    Configuration.set_default(c)
    k8sconfig.load_incluster_config()
    apps_api = k8sclient.AppsV1Api()

    return apps_api</code></pre>
</details>
</dd>
<dt id="tests.common.assert_backup_state"><code class="name flex">
<span>def <span class="ident">assert_backup_state</span></span>(<span>b_actual, b_expected)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_backup_state(b_actual, b_expected):
    assert b_expected.name == b_actual.name
    assert b_expected.url == b_actual.url
    assert b_expected.snapshotName == b_actual.snapshotName
    assert b_expected.snapshotCreated == b_actual.snapshotCreated
    assert b_expected.created == b_actual.created
    assert b_expected.volumeName == b_actual.volumeName
    assert b_expected.volumeSize == b_actual.volumeSize
    assert b_expected.volumeCreated == b_actual.volumeCreated
    assert b_expected.messages == b_actual.messages is None</code></pre>
</details>
</dd>
<dt id="tests.common.check_block_device_size"><code class="name flex">
<span>def <span class="ident">check_block_device_size</span></span>(<span>volume, size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_block_device_size(volume, size):
    dev = get_volume_endpoint(volume)
    # BLKGETSIZE64, result is bytes as unsigned 64-bit integer (uint64)
    req = 0x80081272
    buf = &#39; &#39; * 8
    with open(dev) as dev:
        buf = fcntl.ioctl(dev.fileno(), req, buf)
    device_size = struct.unpack(&#39;L&#39;, buf)[0]
    assert device_size == size</code></pre>
</details>
</dd>
<dt id="tests.common.check_csi"><code class="name flex">
<span>def <span class="ident">check_csi</span></span>(<span>core_api)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_csi(core_api):
    using_csi = CSI_UNKNOWN

    has_attacher = False
    has_provisioner = False
    has_csi_plugin = False

    pod_running = True

    try:
        longhorn_pod_list = core_api.list_namespaced_pod(&#39;longhorn-system&#39;)
        for item in longhorn_pod_list.items:
            if item.status.phase != &#34;Running&#34;:
                pod_running = False

            labels = item.metadata.labels
            if not labels:
                pass
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;csi-attacher&#39;:
                has_attacher = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;csi-provisioner&#39;:
                has_provisioner = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;longhorn-csi-plugin&#39;:
                has_csi_plugin = True

        if has_attacher and has_provisioner and has_csi_plugin and pod_running:
            using_csi = CSI_TRUE
        elif not has_attacher and not has_provisioner \
                and not has_csi_plugin and not pod_running:
            using_csi = CSI_FALSE

    except ApiException as e:
        if (e.status == 404):
            using_csi = CSI_FALSE

    assert using_csi != CSI_UNKNOWN

    return True if using_csi == CSI_TRUE else False</code></pre>
</details>
</dd>
<dt id="tests.common.check_csi_expansion"><code class="name flex">
<span>def <span class="ident">check_csi_expansion</span></span>(<span>core_api)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_csi_expansion(core_api):
    csi_expansion_enabled = False
    has_csi_resizer = False
    pod_running = True

    try:
        longhorn_pod_list = core_api.list_namespaced_pod(&#39;longhorn-system&#39;)
        for item in longhorn_pod_list.items:
            if item.status.phase != &#34;Running&#34;:
                pod_running = False

            labels = item.metadata.labels
            if not labels:
                pass
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;csi-resizer&#39;:
                has_csi_resizer = True
        if has_csi_resizer and pod_running:
            csi_expansion_enabled = True

    except ApiException:
        pass

    return csi_expansion_enabled</code></pre>
</details>
</dd>
<dt id="tests.common.check_device_data"><code class="name flex">
<span>def <span class="ident">check_device_data</span></span>(<span>dev, data, check_checksum=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_device_data(dev, data, check_checksum=True):
    r_data = dev_read(dev, data[&#39;pos&#39;], data[&#39;len&#39;])
    assert r_data == bytes(data[&#39;content&#39;], encoding=&#39;utf8&#39;)
    if check_checksum:
        r_checksum = get_device_checksum(dev)
        assert r_checksum == data[&#39;checksum&#39;]</code></pre>
</details>
</dd>
<dt id="tests.common.check_longhorn"><code class="name flex">
<span>def <span class="ident">check_longhorn</span></span>(<span>core_api)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_longhorn(core_api):
    ready = False
    has_engine_image = False
    has_driver_deployer = False
    has_manager = False
    has_ui = False
    has_instance_manager = False

    pod_running = True

    try:
        longhorn_pod_list = core_api.list_namespaced_pod(&#39;longhorn-system&#39;)
        for item in longhorn_pod_list.items:
            labels = item.metadata.labels

            if not labels:
                pass
            elif labels.get(&#39;longhorn.io/component&#39;, &#39;&#39;) == &#39;engine-image&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_engine_image = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;longhorn-driver-deployer&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_driver_deployer = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;longhorn-manager&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_manager = True
            elif labels.get(&#39;app&#39;, &#39;&#39;) == &#39;longhorn-ui&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_ui = True
            elif labels.get(&#39;longhorn.io/component&#39;, &#39;&#39;) == \
                    &#39;instance-manager&#39; \
                    and item.status.phase == &#34;Running&#34;:
                has_instance_manager = True

        if has_engine_image and has_driver_deployer and has_manager and \
                has_ui and has_instance_manager and pod_running:
            ready = True

    except ApiException as e:
        if (e.status == 404):
            ready = False

    assert ready</code></pre>
</details>
</dd>
<dt id="tests.common.check_pod_existence"><code class="name flex">
<span>def <span class="ident">check_pod_existence</span></span>(<span>api, pod_name, namespace='default')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_pod_existence(api, pod_name, namespace=&#34;default&#34;):
    pods = api.list_namespaced_pod(namespace)
    for pod in pods.items:
        if pod.metadata.name == pod_name and \
                not pod.metadata.deletion_timestamp:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="tests.common.check_pv_existence"><code class="name flex">
<span>def <span class="ident">check_pv_existence</span></span>(<span>api, pv_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_pv_existence(api, pv_name):
    pvs = api.list_persistent_volume()
    for pv in pvs.items:
        if pv.metadata.name == pv_name and not pv.metadata.deletion_timestamp:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="tests.common.check_pvc_existence"><code class="name flex">
<span>def <span class="ident">check_pvc_existence</span></span>(<span>api, pvc_name, namespace='default')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_pvc_existence(api, pvc_name, namespace=&#34;default&#34;):
    pvcs = api.list_namespaced_persistent_volume_claim(namespace)
    for pvc in pvcs.items:
        if pvc.metadata.name == pvc_name and not \
                pvc.metadata.deletion_timestamp:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="tests.common.check_statefulset_existence"><code class="name flex">
<span>def <span class="ident">check_statefulset_existence</span></span>(<span>api, ss_name, namespace='default')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_statefulset_existence(api, ss_name, namespace=&#34;default&#34;):
    ss_list = api.list_namespaced_stateful_set(namespace)
    for ss in ss_list.items:
        if ss.metadata.name == ss_name and not ss.metadata.deletion_timestamp:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="tests.common.check_volume_data"><code class="name flex">
<span>def <span class="ident">check_volume_data</span></span>(<span>volume, data, check_checksum=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_volume_data(volume, data, check_checksum=True):
    dev = get_volume_endpoint(volume)
    check_device_data(dev, data, check_checksum)</code></pre>
</details>
</dd>
<dt id="tests.common.check_volume_endpoint"><code class="name flex">
<span>def <span class="ident">check_volume_endpoint</span></span>(<span>v)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_volume_endpoint(v):
    engine = get_volume_engine(v)
    endpoint = engine.endpoint
    if v.disableFrontend:
        assert endpoint == &#34;&#34;
    else:
        if v.frontend == VOLUME_FRONTEND_BLOCKDEV:
            assert endpoint == os.path.join(DEV_PATH, v.name)
        elif v.frontend == VOLUME_FRONTEND_ISCSI:
            assert endpoint.startswith(&#34;iscsi://&#34;)
        else:
            raise Exception(&#34;Unexpected volume frontend:&#34;, v.frontend)
    return endpoint</code></pre>
</details>
</dd>
<dt id="tests.common.check_volume_existence"><code class="name flex">
<span>def <span class="ident">check_volume_existence</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_volume_existence(client, volume_name):
    volumes = client.list_volume()
    for volume in volumes:
        if volume.name == volume_name:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="tests.common.check_volume_last_backup"><code class="name flex">
<span>def <span class="ident">check_volume_last_backup</span></span>(<span>client, volume_name, last_backup)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_volume_last_backup(client, volume_name, last_backup):
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volume_name)
        if volume.lastBackup == last_backup:
            break
        time.sleep(RETRY_INTERVAL)
    volume = client.by_id_volume(volume_name)
    assert volume.lastBackup == last_backup</code></pre>
</details>
</dd>
<dt id="tests.common.check_volume_replicas"><code class="name flex">
<span>def <span class="ident">check_volume_replicas</span></span>(<span>volume, spec, tag_mapping)</span>
</code></dt>
<dd>
<div class="desc"><p>Check the replicas on the volume to ensure that they were scheduled
properly.
:param volume: The Volume to check.
:param spec: The spec to validate the Tag against.
:param tag_mapping: The mapping of Nodes to the Tags they have.
:raise AssertionError: If the Volume doesn't match all the conditions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_volume_replicas(volume, spec, tag_mapping):
    &#34;&#34;&#34;
    Check the replicas on the volume to ensure that they were scheduled
    properly.
    :param volume: The Volume to check.
    :param spec: The spec to validate the Tag against.
    :param tag_mapping: The mapping of Nodes to the Tags they have.
    :raise AssertionError: If the Volume doesn&#39;t match all the conditions.
    &#34;&#34;&#34;
    found_hosts = {}
    # Make sure that all the Tags the Volume requested were fulfilled.
    for replica in volume.replicas:
        found_hosts[replica.hostId] = {}
        assert not len(set(spec[&#34;disk&#34;]) -
                       set(tag_mapping[replica.hostId][&#34;disk&#34;]))
        assert not len(set(spec[&#34;node&#34;]) -
                       set(tag_mapping[replica.hostId][&#34;node&#34;]))

    # The Volume should have replicas on as many nodes as matched
    # the requirements (specified by &#34;expected&#34; in the spec variable).
    assert len(found_hosts) == spec[&#34;expected&#34;]</code></pre>
</details>
</dd>
<dt id="tests.common.cleanup_client"><code class="name flex">
<span>def <span class="ident">cleanup_client</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup_client():
    client = get_longhorn_api_client()
    # cleanup test disks
    cleanup_test_disks(client)

    volumes = client.list_volume()
    for v in volumes:
        # ignore the error when clean up
        try:
            client.delete(v)
        except Exception as e:
            print(&#34;Exception when cleanup volume &#34;, v, e)
            pass
    images = client.list_engine_image()
    for img in images:
        if not img.default:
            # ignore the error when clean up
            try:
                client.delete(img)
            except Exception as e:
                print(&#34;Exception when cleanup image&#34;, img, e)
                pass

    # enable nodes scheduling
    reset_node(client)
    reset_settings(client)
    reset_disks_for_all_nodes(client)
    reset_engine_image(client)

    # check replica subdirectory of default disk path
    if not os.path.exists(DEFAULT_REPLICA_DIRECTORY):
        subprocess.check_call(
            [&#34;mkdir&#34;, &#34;-p&#34;, DEFAULT_REPLICA_DIRECTORY])</code></pre>
</details>
</dd>
<dt id="tests.common.cleanup_host_disk"><code class="name flex">
<span>def <span class="ident">cleanup_host_disk</span></span>(<span>vol_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup_host_disk(vol_name):
    mount_path = os.path.join(DIRECTORY_PATH, vol_name)
    umount_disk(mount_path)

    cmd = [&#39;rm&#39;, &#39;-r&#39;, mount_path]
    subprocess.check_call(cmd)</code></pre>
</details>
</dd>
<dt id="tests.common.cleanup_node_disks"><code class="name flex">
<span>def <span class="ident">cleanup_node_disks</span></span>(<span>client, node_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup_node_disks(client, node_name):
    node = client.by_id_node(node_name)
    disks = node.disks
    for _, disk in iter(disks.items()):
        disk.allowScheduling = False
    update_disks = get_update_disks(disks)
    node = client.by_id_node(node_name)
    node.diskUpdate(disks=update_disks)
    node.diskUpdate(disks={})
    return wait_for_disk_update(client, node_name, 0)</code></pre>
</details>
</dd>
<dt id="tests.common.cleanup_test_disks"><code class="name flex">
<span>def <span class="ident">cleanup_test_disks</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup_test_disks(client):
    del_dirs = os.listdir(DIRECTORY_PATH)
    host_id = get_self_host_id()
    node = client.by_id_node(host_id)
    disks = node.disks
    for name, disk in iter(disks.items()):
        for del_dir in del_dirs:
            dir_path = os.path.join(DIRECTORY_PATH, del_dir)
            if dir_path == disk.path:
                disk.allowScheduling = False
    update_disks = get_update_disks(disks)
    try:
        node = node.diskUpdate(disks=update_disks)
        disks = node.disks
        for name, disk in iter(disks.items()):
            for del_dir in del_dirs:
                dir_path = os.path.join(DIRECTORY_PATH, del_dir)
                if dir_path == disk.path:
                    wait_for_disk_status(client, host_id, name,
                                         &#34;allowScheduling&#34;, False)
    except Exception as e:
        print(&#34;Exception when update node disks&#34;, node, e)
        pass

    # delete test disks
    disks = node.disks
    update_disks = {}
    for name, disk in iter(disks.items()):
        if disk.allowScheduling:
            update_disks[name] = disk
    try:
        node.diskUpdate(disks=update_disks)
        wait_for_disk_update(client, host_id, len(update_disks))
    except Exception as e:
        print(&#34;Exception when delete node test disks&#34;, node, e)
        pass
    # cleanup host disks
    for del_dir in del_dirs:
        try:
            cleanup_host_disk(del_dir)
        except Exception as e:
            print(&#34;Exception when cleanup host disk&#34;, del_dir, e)
            pass</code></pre>
</details>
</dd>
<dt id="tests.common.cleanup_volume"><code class="name flex">
<span>def <span class="ident">cleanup_volume</span></span>(<span>client, volume)</span>
</code></dt>
<dd>
<div class="desc"><p>Clean up the volume after the test.
:param client: The Longhorn client to use in the request.
:param volume: The volume to clean up.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup_volume(client, volume):
    &#34;&#34;&#34;
    Clean up the volume after the test.
    :param client: The Longhorn client to use in the request.
    :param volume: The volume to clean up.
    &#34;&#34;&#34;
    volume.detach()
    volume = wait_for_volume_detached(client, volume.name)
    client.delete(volume)
    wait_for_volume_delete(client, volume.name)
    volumes = client.list_volume()
    assert len(volumes) == 0</code></pre>
</details>
</dd>
<dt id="tests.common.client"><code class="name flex">
<span>def <span class="ident">client</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"><p>Return an individual Longhorn API client for testing.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def client(request):
    &#34;&#34;&#34;
    Return an individual Longhorn API client for testing.
    &#34;&#34;&#34;
    k8sconfig.load_incluster_config()
    # Make sure nodes and managers are all online.
    ips = get_mgr_ips()

    # check if longhorn manager port is open before calling get_client
    for ip in ips:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        mgr_port_open = sock.connect_ex((ip, 9500))

        if mgr_port_open == 0:
            client = get_client(ip + PORT)
            break

    hosts = client.list_node()
    assert len(hosts) == len(ips)

    request.addfinalizer(lambda: cleanup_client())

    cleanup_client()

    return client</code></pre>
</details>
</dd>
<dt id="tests.common.clients"><code class="name flex">
<span>def <span class="ident">clients</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def clients(request):
    k8sconfig.load_incluster_config()
    ips = get_mgr_ips()
    client = get_client(ips[0] + PORT)
    hosts = client.list_node()
    assert len(hosts) == len(ips)
    clis = get_clients(hosts)

    def finalizer():
        cleanup_client()

    request.addfinalizer(finalizer)

    cleanup_client()

    return clis</code></pre>
</details>
</dd>
<dt id="tests.common.copy_pod_volume_data"><code class="name flex">
<span>def <span class="ident">copy_pod_volume_data</span></span>(<span>api, pod_name, src_path, dest_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_pod_volume_data(api, pod_name, src_path, dest_path):
    write_cmd = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;dd if=&#39; + src_path + &#39; of=&#39; + dest_path
    ]
    return stream(
        api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
        command=write_cmd, stderr=True, stdin=False, stdout=True,
        tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.core_api"><code class="name flex">
<span>def <span class="ident">core_api</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a new CoreV1API instance.</p>
<h2 id="returns">Returns</h2>
<p>A new CoreV1API Instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def core_api(request):
    &#34;&#34;&#34;
    Create a new CoreV1API instance.
    Returns:
        A new CoreV1API Instance.
    &#34;&#34;&#34;
    c = Configuration()
    c.assert_hostname = False
    Configuration.set_default(c)
    k8sconfig.load_incluster_config()
    core_api = k8sclient.CoreV1Api()

    return core_api</code></pre>
</details>
</dd>
<dt id="tests.common.crash_engine_process_with_sigkill"><code class="name flex">
<span>def <span class="ident">crash_engine_process_with_sigkill</span></span>(<span>client, core_api, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crash_engine_process_with_sigkill(client, core_api, volume_name):
    volume = client.by_id_volume(volume_name)
    ins_mgr_name = volume.controllers[0].instanceManagerName

    kill_command = [
            &#39;/bin/sh&#39;, &#39;-c&#39;,
            &#34;kill -9 `ps aux | grep -i \&#34;controller &#34; +
            volume_name + &#34;\&#34; | grep -v grep | awk &#39;{print $2}&#39;`&#34;]

    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream read&#39;):
        stream(core_api.connect_get_namespaced_pod_exec,
               ins_mgr_name,
               LONGHORN_NAMESPACE, command=kill_command,
               stderr=True, stdin=False, stdout=True, tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.crash_replica_processes"><code class="name flex">
<span>def <span class="ident">crash_replica_processes</span></span>(<span>client, api, volname, replicas=None, wait_to_fail=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crash_replica_processes(client, api, volname, replicas=None,
                            wait_to_fail=True):

    if replicas is None:
        volume = client.by_id_volume(volname)
        replicas = volume.replicas

    for r in replicas:
        assert r.instanceManagerName != &#34;&#34;
        kill_command = &#34;kill `ps aux | grep &#39;&#34; + r[&#39;dataPath&#39;] +\
                       &#34;&#39; | grep -v grep | awk &#39;{print $2}&#39;`&#34;
        exec_instance_manager(api, r.instanceManagerName, kill_command)

    if wait_to_fail is True:
        for r in replicas:
            wait_for_replica_failed(client, volname, r[&#39;name&#39;])</code></pre>
</details>
</dd>
<dt id="tests.common.create_and_check_volume"><code class="name flex">
<span>def <span class="ident">create_and_check_volume</span></span>(<span>client, volume_name, num_of_replicas=3, size='16777216', base_image='', frontend='blockdev')</span>
</code></dt>
<dd>
<div class="desc"><p>Create a new volume with the specified parameters. Assert that the new
volume is detached and that all of the requested parameters match.</p>
<p>:param client: The Longhorn client to use in the request.
:param volume_name: The name of the volume.
:param num_of_replicas: The number of replicas the volume should have.
:param size: The size of the volume, as a string representing the number
of bytes.
:param base_image: The base image to use for the volume.
:param frontend: The frontend to use for the volume.
:return: The volume instance created.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_and_check_volume(client, volume_name, num_of_replicas=3, size=SIZE,
                            base_image=&#34;&#34;, frontend=VOLUME_FRONTEND_BLOCKDEV):
    &#34;&#34;&#34;
    Create a new volume with the specified parameters. Assert that the new
    volume is detached and that all of the requested parameters match.

    :param client: The Longhorn client to use in the request.
    :param volume_name: The name of the volume.
    :param num_of_replicas: The number of replicas the volume should have.
    :param size: The size of the volume, as a string representing the number
    of bytes.
    :param base_image: The base image to use for the volume.
    :param frontend: The frontend to use for the volume.
    :return: The volume instance created.
    &#34;&#34;&#34;
    client.create_volume(name=volume_name, size=size,
                         numberOfReplicas=num_of_replicas,
                         baseImage=base_image, frontend=frontend)
    volume = wait_for_volume_detached(client, volume_name)
    assert volume.name == volume_name
    assert volume.size == size
    assert volume.numberOfReplicas == num_of_replicas
    assert volume.state == &#34;detached&#34;
    assert volume.baseImage == base_image
    assert volume.frontend == frontend
    assert volume.created != &#34;&#34;
    return volume</code></pre>
</details>
</dd>
<dt id="tests.common.create_and_wait_deployment"><code class="name flex">
<span>def <span class="ident">create_and_wait_deployment</span></span>(<span>apps_api, deployment_manifest)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_and_wait_deployment(apps_api, deployment_manifest):
    apps_api.create_namespaced_deployment(
        body=deployment_manifest,
        namespace=&#39;default&#39;)

    deployment_name = deployment_manifest[&#34;metadata&#34;][&#34;name&#34;]
    desired_replica_count = deployment_manifest[&#34;spec&#34;][&#34;replicas&#34;]

    wait_deployment_replica_ready(
        apps_api,
        deployment_name,
        desired_replica_count
    )</code></pre>
</details>
</dd>
<dt id="tests.common.create_and_wait_pod"><code class="name flex">
<span>def <span class="ident">create_and_wait_pod</span></span>(<span>api, pod_manifest)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new Pod attached to a PersistentVolumeClaim for testing.</p>
<p>The function will block until the Pod is online or until it times out,
whichever occurs first. The volume created by the manifest passed in will
be mounted to '/data'.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>api</code></strong></dt>
<dd>An instance of CoreV1API.</dd>
<dt><strong><code>pod_name</code></strong></dt>
<dd>The name of the Pod.</dd>
<dt><strong><code>volume</code></strong></dt>
<dd>The volume manifest.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_and_wait_pod(api, pod_manifest):
    &#34;&#34;&#34;
    Creates a new Pod attached to a PersistentVolumeClaim for testing.

    The function will block until the Pod is online or until it times out,
    whichever occurs first. The volume created by the manifest passed in will
    be mounted to &#39;/data&#39;.

    Args:
        api: An instance of CoreV1API.
        pod_name: The name of the Pod.
        volume: The volume manifest.
    &#34;&#34;&#34;
    api.create_namespaced_pod(
        body=pod_manifest,
        namespace=&#39;default&#39;)

    pod_name = pod_manifest[&#39;metadata&#39;][&#39;name&#39;]

    wait_pod(pod_name)</code></pre>
</details>
</dd>
<dt id="tests.common.create_and_wait_statefulset"><code class="name flex">
<span>def <span class="ident">create_and_wait_statefulset</span></span>(<span>statefulset_manifest)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a new StatefulSet for testing.</p>
<p>This function will block until all replicas in the StatefulSet are online
or it times out, whichever occurs first.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_and_wait_statefulset(statefulset_manifest):
    &#34;&#34;&#34;
    Create a new StatefulSet for testing.

    This function will block until all replicas in the StatefulSet are online
    or it times out, whichever occurs first.
    &#34;&#34;&#34;
    api = get_apps_api_client()
    api.create_namespaced_stateful_set(
        body=statefulset_manifest,
        namespace=&#39;default&#39;)
    wait_statefulset(statefulset_manifest)</code></pre>
</details>
</dd>
<dt id="tests.common.create_backup"><code class="name flex">
<span>def <span class="ident">create_backup</span></span>(<span>client, volname, data={}, labels={})</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_backup(client, volname, data={}, labels={}):
    volume = client.by_id_volume(volname)
    create_snapshot(client, volname)
    if not data:
        data = write_volume_random_data(volume)
    else:
        data = write_volume_data(volume, data)
    snap = create_snapshot(client, volname)
    create_snapshot(client, volname)
    volume.snapshotBackup(name=snap.name, labels=labels)

    verified = False
    for i in range(RETRY_COMMAND_COUNT):
        bv, b = find_backup(client, volname, snap.name)
        new_b = bv.backupGet(name=b.name)
        if new_b.name == b.name and \
           new_b.url == b.url and \
           new_b.snapshotName == b.snapshotName and \
           new_b.snapshotCreated == b.snapshotCreated and \
           new_b.created == b.created and \
           new_b.volumeName == b.volumeName and \
           new_b.volumeSize == b.volumeSize and \
           new_b.volumeCreated == b.volumeCreated:
            verified = True
            break
        time.sleep(RETRY_INTERVAL)
    assert verified

    # Don&#39;t directly compare the Label dictionaries, since the server could
    # have added extra Labels (for things like BaseImage).
    for key, val in iter(labels.items()):
        assert new_b.labels.get(key) == val

    volume = wait_for_backup_completion(client, volname, snap.name)
    volume = wait_for_volume_status(client, volname,
                                    &#34;lastBackup&#34;,
                                    b.name)
    assert volume.lastBackupAt != &#34;&#34;

    return bv, b, snap, data</code></pre>
</details>
</dd>
<dt id="tests.common.create_pv_for_volume"><code class="name flex">
<span>def <span class="ident">create_pv_for_volume</span></span>(<span>client, core_api, volume, pv_name, fs_type='ext4')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_pv_for_volume(client, core_api, volume, pv_name, fs_type=&#34;ext4&#34;):
    volume.pvCreate(pvName=pv_name, fsType=fs_type)
    for i in range(RETRY_COUNTS):
        if check_pv_existence(core_api, pv_name):
            break
        time.sleep(RETRY_INTERVAL)
    assert check_pv_existence(core_api, pv_name)

    ks = {
        &#39;pvName&#39;: pv_name,
        &#39;pvStatus&#39;: &#39;Available&#39;,
        &#39;namespace&#39;: &#39;&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
        &#39;lastPodRefAt&#39;: &#39;&#39;,
    }
    wait_volume_kubernetes_status(client, volume.name, ks)</code></pre>
</details>
</dd>
<dt id="tests.common.create_pvc"><code class="name flex">
<span>def <span class="ident">create_pvc</span></span>(<span>pvc_manifest)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_pvc(pvc_manifest):
    api = get_core_api_client()
    api.create_namespaced_persistent_volume_claim(
        &#39;default&#39;, pvc_manifest)</code></pre>
</details>
</dd>
<dt id="tests.common.create_pvc_for_volume"><code class="name flex">
<span>def <span class="ident">create_pvc_for_volume</span></span>(<span>client, core_api, volume, pvc_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_pvc_for_volume(client, core_api, volume, pvc_name):
    volume.pvcCreate(namespace=&#34;default&#34;, pvcName=pvc_name)
    for i in range(RETRY_COUNTS):
        if check_pvc_existence(core_api, pvc_name):
            break
        time.sleep(RETRY_INTERVAL)
    assert check_pvc_existence(core_api, pvc_name)

    ks = {
        &#39;pvStatus&#39;: &#39;Bound&#39;,
        &#39;namespace&#39;: &#39;default&#39;,
        &#39;lastPVCRefAt&#39;: &#39;&#39;,
    }
    wait_volume_kubernetes_status(client, volume.name, ks)</code></pre>
</details>
</dd>
<dt id="tests.common.create_pvc_spec"><code class="name flex">
<span>def <span class="ident">create_pvc_spec</span></span>(<span>name)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a volume manifest using the given name for the PVC.</p>
<p>This spec is used to test dynamically provisioned PersistentVolumes (those
created using a storage class).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_pvc_spec(name):
    # type: (str) -&gt; dict
    &#34;&#34;&#34;
    Generate a volume manifest using the given name for the PVC.

    This spec is used to test dynamically provisioned PersistentVolumes (those
    created using a storage class).
    &#34;&#34;&#34;
    return {
        &#39;name&#39;: &#39;pod-data&#39;,
        &#39;persistentVolumeClaim&#39;: {
            &#39;claimName&#39;: name,
            &#39;readOnly&#39;: False
        }
    }</code></pre>
</details>
</dd>
<dt id="tests.common.create_snapshot"><code class="name flex">
<span>def <span class="ident">create_snapshot</span></span>(<span>longhorn_api_client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_snapshot(longhorn_api_client, volume_name):
    volume = longhorn_api_client.by_id_volume(volume_name)
    snapshots = volume.snapshotList(volume=volume_name)
    snap = volume.snapshotCreate()
    snap_name = snap.name

    snapshot_created = False
    for i in range(RETRY_COUNTS):
        snapshots = volume.snapshotList(volume=volume_name)

        for vs in snapshots.data:
            if vs.name == snap_name:
                snapshot_created = True
                break
        if snapshot_created is True:
            break
        time.sleep(RETRY_INTERVAL)

    assert snapshot_created
    return snap</code></pre>
</details>
</dd>
<dt id="tests.common.create_storage_class"><code class="name flex">
<span>def <span class="ident">create_storage_class</span></span>(<span>sc_manifest)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_storage_class(sc_manifest):
    api = get_storage_api_client()
    api.create_storage_class(
        body=sc_manifest)</code></pre>
</details>
</dd>
<dt id="tests.common.csi_pv"><code class="name flex">
<span>def <span class="ident">csi_pv</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def csi_pv(request):
    volume_name = generate_volume_name()
    pv_manifest = {
        &#39;apiVersion&#39;: &#39;v1&#39;,
        &#39;kind&#39;: &#39;PersistentVolume&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: volume_name
        },
        &#39;spec&#39;: {
            &#39;capacity&#39;: {
                &#39;storage&#39;: size_to_string(DEFAULT_VOLUME_SIZE * Gi)
            },
            &#39;volumeMode&#39;: &#39;Filesystem&#39;,
            &#39;accessModes&#39;: [&#39;ReadWriteOnce&#39;],
            &#39;persistentVolumeReclaimPolicy&#39;: &#39;Delete&#39;,
            &#39;csi&#39;: {
                &#39;driver&#39;: &#39;driver.longhorn.io&#39;,
                &#39;fsType&#39;: &#39;ext4&#39;,
                &#39;volumeAttributes&#39;: {
                    &#39;numberOfReplicas&#39;:
                        DEFAULT_LONGHORN_PARAMS[&#39;numberOfReplicas&#39;],
                    &#39;staleReplicaTimeout&#39;:
                        DEFAULT_LONGHORN_PARAMS[&#39;staleReplicaTimeout&#39;]
                },
                &#39;volumeHandle&#39;: volume_name
            }
        }
    }

    def finalizer():
        api = get_core_api_client()
        delete_and_wait_pv(api, pv_manifest[&#39;metadata&#39;][&#39;name&#39;])

        client = get_longhorn_api_client()
        delete_and_wait_longhorn(client, pv_manifest[&#39;metadata&#39;][&#39;name&#39;])

    request.addfinalizer(finalizer)

    return pv_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.csi_pv_baseimage"><code class="name flex">
<span>def <span class="ident">csi_pv_baseimage</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def csi_pv_baseimage(request):
    pv_manifest = csi_pv(request)
    pv_manifest[&#39;spec&#39;][&#39;capacity&#39;][&#39;storage&#39;] = \
        size_to_string(BASE_IMAGE_EXT4_SIZE)
    pv_manifest[&#39;spec&#39;][&#39;csi&#39;][&#39;volumeAttributes&#39;][&#39;baseImage&#39;] = \
        BASE_IMAGE_EXT4
    return pv_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.csi_pvc_name"><code class="name flex">
<span>def <span class="ident">csi_pvc_name</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def csi_pvc_name(request):
    return generate_volume_name()</code></pre>
</details>
</dd>
<dt id="tests.common.delete_and_wait_deployment"><code class="name flex">
<span>def <span class="ident">delete_and_wait_deployment</span></span>(<span>apps_api, deployment_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_and_wait_deployment(apps_api, deployment_name):
    try:
        apps_api.delete_namespaced_deployment(
            name=deployment_name,
            namespace=&#39;default&#39;
        )
    except ApiException as e:
        assert e.status == 404

    wait_delete_deployment(apps_api, deployment_name)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_and_wait_longhorn"><code class="name flex">
<span>def <span class="ident">delete_and_wait_longhorn</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"><p>Delete a volume from Longhorn.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_and_wait_longhorn(client, name):
    &#34;&#34;&#34;
    Delete a volume from Longhorn.
    &#34;&#34;&#34;
    try:
        v = client.by_id_volume(name)
        client.delete(v)
    except ApiException as ex:
        assert ex.status == 404
    except longhorn.ApiError as err:
        # for deleting a non-existing volume,
        # the status_code is 500 Server Error.
        assert err.error.code == 500

    wait_for_volume_delete(client, name)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_and_wait_pod"><code class="name flex">
<span>def <span class="ident">delete_and_wait_pod</span></span>(<span>api, pod_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Delete a specified Pod from the "default" namespace.</p>
<p>This function does not check if the Pod does exist and will throw an error
if a nonexistent Pod is specified.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>api</code></strong></dt>
<dd>An instance of CoreV1API.</dd>
<dt><strong><code>pod_name</code></strong></dt>
<dd>The name of the Pod.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_and_wait_pod(api, pod_name):
    &#34;&#34;&#34;
    Delete a specified Pod from the &#34;default&#34; namespace.

    This function does not check if the Pod does exist and will throw an error
    if a nonexistent Pod is specified.

    Args:
        api: An instance of CoreV1API.
        pod_name: The name of the Pod.
    &#34;&#34;&#34;
    try:
        api.delete_namespaced_pod(
            name=pod_name, namespace=&#39;default&#39;,
            body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404

    wait_delete_pod(api, pod_name)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_and_wait_pv"><code class="name flex">
<span>def <span class="ident">delete_and_wait_pv</span></span>(<span>api, pv_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_and_wait_pv(api, pv_name):
    try:
        api.delete_persistent_volume(
            name=pv_name, body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404

    wait_delete_pv(api, pv_name)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_and_wait_pvc"><code class="name flex">
<span>def <span class="ident">delete_and_wait_pvc</span></span>(<span>api, pvc_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_and_wait_pvc(api, pvc_name):
    try:
        api.delete_namespaced_persistent_volume_claim(
            name=pvc_name, namespace=&#39;default&#39;,
            body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404

    wait_delete_pvc(api, pvc_name)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_and_wait_statefulset"><code class="name flex">
<span>def <span class="ident">delete_and_wait_statefulset</span></span>(<span>api, client, statefulset)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_and_wait_statefulset(api, client, statefulset):
    apps_api = get_apps_api_client()
    if not check_statefulset_existence(apps_api,
                                       statefulset[&#39;metadata&#39;][&#39;name&#39;]):
        return

    # We need to generate the names for the PVCs on our own so we can
    # delete them.
    pod_data = get_statefulset_pod_info(api, statefulset)

    try:
        apps_api.delete_namespaced_stateful_set(
            name=statefulset[&#39;metadata&#39;][&#39;name&#39;],
            namespace=&#39;default&#39;, body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404

    for i in range(DEFAULT_POD_TIMEOUT):
        ret = apps_api.list_namespaced_stateful_set(namespace=&#39;default&#39;)
        found = False
        for item in ret.items:
            if item.metadata.name == statefulset[&#39;metadata&#39;][&#39;name&#39;]:
                found = True
                break
        if not found:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert not found
    client = get_longhorn_api_client()
    for pod in pod_data:
        # Wait on Pods too, we apparently had timeout issues with them.
        wait_delete_pod(api, pod[&#39;pod_name&#39;])
        delete_and_wait_pvc(api, pod[&#39;pvc_name&#39;])
        # The StatefulSet tests involve both StorageClass provisioned volumes
        # and our manually created PVs. This checks the status of our PV once
        # the PVC is deleted. If it is Failed, we know it is a PV and we must
        # delete it manually. If it is removed from the system, we can just
        # wait for deletion.
        for i in range(DEFAULT_POD_TIMEOUT):
            ret = api.list_persistent_volume()
            found = False
            for item in ret.items:
                if item.metadata.name == pod[&#39;pv_name&#39;]:
                    if item.status.phase in (&#39;Failed&#39;, &#39;Released&#39;):
                        delete_and_wait_pv(api, pod[&#39;pv_name&#39;])
                        delete_and_wait_longhorn(client, pod[&#39;pv_name&#39;])
                    else:
                        found = True
                        break
            if not found:
                break
            time.sleep(DEFAULT_POD_INTERVAL)
        assert not found
        wait_for_volume_delete(client, pod[&#39;pv_name&#39;])</code></pre>
</details>
</dd>
<dt id="tests.common.delete_and_wait_volume_attachment"><code class="name flex">
<span>def <span class="ident">delete_and_wait_volume_attachment</span></span>(<span>storage_api, volume_attachment_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_and_wait_volume_attachment(storage_api, volume_attachment_name):
    try:
        storage_api.delete_volume_attachment(
            name=volume_attachment_name
        )
    except ApiException as e:
        assert e.status == 404

    wait_delete_volume_attachment(storage_api, volume_attachment_name)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_backup"><code class="name flex">
<span>def <span class="ident">delete_backup</span></span>(<span>client, volume_name, backup_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_backup(client, volume_name, backup_name):
    backup_volume = client.by_id_backupVolume(volume_name)
    backup_volume.backupDelete(name=backup_name)
    wait_for_backup_delete(client, volume_name, backup_name)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_backup_volume"><code class="name flex">
<span>def <span class="ident">delete_backup_volume</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_backup_volume(client, volume_name):
    bv = client.by_id_backupVolume(volume_name)
    client.delete(bv)
    wait_for_backup_volume_delete(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_replica_processes"><code class="name flex">
<span>def <span class="ident">delete_replica_processes</span></span>(<span>client, api, volname)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_replica_processes(client, api, volname):
    replica_map = {}
    volume = client.by_id_volume(volname)
    for r in volume.replicas:
        replica_map[r.instanceManagerName] = r.name

    for rm_name, r_name in replica_map.items():
        delete_command = &#39;longhorn-instance-manager process delete &#39; + \
                         &#39;--name &#39; + r_name
        exec_instance_manager(api, rm_name, delete_command)</code></pre>
</details>
</dd>
<dt id="tests.common.delete_storage_class"><code class="name flex">
<span>def <span class="ident">delete_storage_class</span></span>(<span>sc_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_storage_class(sc_name):
    api = get_storage_api_client()
    try:
        api.delete_storage_class(sc_name, body=k8sclient.V1DeleteOptions())
    except ApiException as e:
        assert e.status == 404</code></pre>
</details>
</dd>
<dt id="tests.common.dev_read"><code class="name flex">
<span>def <span class="ident">dev_read</span></span>(<span>dev, start, count)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dev_read(dev, start, count):
    r_data = &#34;&#34;
    fdev = open(dev, &#39;rb&#39;)
    if fdev is not None:
        fdev.seek(start)
        r_data = fdev.read(count)
        fdev.close()
    return r_data</code></pre>
</details>
</dd>
<dt id="tests.common.dev_write"><code class="name flex">
<span>def <span class="ident">dev_write</span></span>(<span>dev, start, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dev_write(dev, start, data):
    data = bytes(data, encoding=&#39;utf-8&#39;)
    w_length = 0
    fdev = open(dev, &#39;rb+&#39;)
    if fdev is not None:
        fdev.seek(start)
        fdev.write(data)
        fdev.close()
        w_length = len(data)
    return w_length</code></pre>
</details>
</dd>
<dt id="tests.common.disable_auto_salvage"><code class="name flex">
<span>def <span class="ident">disable_auto_salvage</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def disable_auto_salvage(client):
    auto_salvage_setting = client.by_id_setting(SETTING_AUTO_SALVAGE)
    setting = client.update(auto_salvage_setting, value=&#34;false&#34;)

    assert setting.name == SETTING_AUTO_SALVAGE
    assert setting.value == &#34;false&#34;

    yield

    auto_salvage_setting = client.by_id_setting(SETTING_AUTO_SALVAGE)
    setting = client.update(auto_salvage_setting, value=&#34;true&#34;)

    assert setting.name == SETTING_AUTO_SALVAGE
    assert setting.value == &#34;true&#34;</code></pre>
</details>
</dd>
<dt id="tests.common.exec_instance_manager"><code class="name flex">
<span>def <span class="ident">exec_instance_manager</span></span>(<span>api, im_name, cmd)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exec_instance_manager(api, im_name, cmd):
    exec_cmd = [&#39;/bin/sh&#39;, &#39;-c&#39;, cmd]

    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream read&#39;):
        stream(api.connect_get_namespaced_pod_exec,
               im_name,
               LONGHORN_NAMESPACE, command=exec_cmd,
               stderr=True, stdin=False, stdout=True, tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.exec_nsenter"><code class="name flex">
<span>def <span class="ident">exec_nsenter</span></span>(<span>cmd)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exec_nsenter(cmd):
    dockerd_pid = find_dockerd_pid() or &#34;1&#34;
    exec_cmd = [&#34;nsenter&#34;, &#34;--mount=/host/proc/{}/ns/mnt&#34;.format(dockerd_pid),
                &#34;--net=/host/proc/{}/ns/net&#34;.format(dockerd_pid),
                &#34;bash&#34;, &#34;-c&#34;, cmd]
    return subprocess.check_output(exec_cmd)</code></pre>
</details>
</dd>
<dt id="tests.common.expand_and_wait_for_pvc"><code class="name flex">
<span>def <span class="ident">expand_and_wait_for_pvc</span></span>(<span>api, pvc)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_and_wait_for_pvc(api, pvc):
    pvc_name = pvc[&#39;metadata&#39;][&#39;name&#39;]
    api.patch_namespaced_persistent_volume_claim(
        pvc_name, &#39;default&#39;, pvc)
    complete = False
    for i in range(RETRY_COUNTS):
        claim = api.read_namespaced_persistent_volume_claim(
            name=pvc_name, namespace=&#39;default&#39;)
        if claim.spec.resources.requests[&#39;storage&#39;] ==\
                claim.status.capacity[&#39;storage&#39;]:
            complete = True
            break
        time.sleep(RETRY_INTERVAL)
    assert complete
    return claim</code></pre>
</details>
</dd>
<dt id="tests.common.expand_attached_volume"><code class="name flex">
<span>def <span class="ident">expand_attached_volume</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_attached_volume(client, volume_name):
    volume = wait_for_volume_healthy(client, volume_name)
    engine = get_volume_engine(volume)

    volume.detach()
    volume = wait_for_volume_detached(client, volume.name)
    volume.expand(size=EXPAND_SIZE)
    wait_for_volume_expansion(client, volume.name)
    volume.attach(hostId=engine.hostId, disableFrontend=False)
    wait_for_volume_healthy(client, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.common.fail_replica_expansion"><code class="name flex">
<span>def <span class="ident">fail_replica_expansion</span></span>(<span>client, api, volname, size, replicas=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fail_replica_expansion(client, api, volname, size, replicas=None):
    if replicas is None:
        volume = client.by_id_volume(volname)
        replicas = volume.replicas

    for r in replicas:
        tmp_meta_file_name = \
            EXPANSION_SNAP_TMP_META_NAME_PATTERN % size
        # os.path.join() cannot deal with the path containing &#34;/&#34;
        cmd = [
            &#39;/bin/sh&#39;, &#39;-c&#39;,
            &#39;mkdir %s &amp;&amp; sync&#39; %
            (INSTANCE_MANAGER_HOST_PATH_PREFIX + r.dataPath +
             &#34;/&#34; + tmp_meta_file_name)
        ]
        if not r.instanceManagerName:
            raise Exception(
                &#34;Should use replica objects in the running volume,&#34;
                &#34;otherwise the field r.instanceManagerName is emtpy&#34;)
        stream(api.connect_get_namespaced_pod_exec,
               r.instanceManagerName,
               LONGHORN_NAMESPACE, command=cmd,
               stderr=True, stdin=False, stdout=True, tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.find_ancestor_process_by_name"><code class="name flex">
<span>def <span class="ident">find_ancestor_process_by_name</span></span>(<span>ancestor_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_ancestor_process_by_name(ancestor_name):
    p = find_self()
    while True:
        if not p or p[&#34;Pid&#34;] == &#34;1&#34;:
            break
        if p[&#34;Name&#34;] == ancestor_name:
            return p[&#34;Pid&#34;]
        p = get_process_info(&#34;/host/proc/{}/status&#34;.format(p[&#34;PPid&#34;]))
    return</code></pre>
</details>
</dd>
<dt id="tests.common.find_backup"><code class="name flex">
<span>def <span class="ident">find_backup</span></span>(<span>client, vol_name, snap_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_backup(client, vol_name, snap_name):
    found = False
    for i in range(100):
        bvs = client.list_backupVolume()
        for bv in bvs:
            if bv.name == vol_name:
                found = True
                break
        if found:
            break
        time.sleep(1)
    assert found

    found = False
    for i in range(20):
        backups = bv.backupList().data
        for b in backups:
            if b.snapshotName == snap_name:
                found = True
                break
        if found:
            break
        time.sleep(1)
    assert found

    return bv, b</code></pre>
</details>
</dd>
<dt id="tests.common.find_dockerd_pid"><code class="name flex">
<span>def <span class="ident">find_dockerd_pid</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_dockerd_pid():
    return find_ancestor_process_by_name(&#34;dockerd&#34;)</code></pre>
</details>
</dd>
<dt id="tests.common.find_self"><code class="name flex">
<span>def <span class="ident">find_self</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_self():
    return get_process_info(&#34;/host/proc/self/status&#34;)</code></pre>
</details>
</dd>
<dt id="tests.common.generate_pod_with_pvc_manifest"><code class="name flex">
<span>def <span class="ident">generate_pod_with_pvc_manifest</span></span>(<span>pod_name, pvc_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_pod_with_pvc_manifest(pod_name, pvc_name):
    pod_manifest = {
        &#34;apiVersion&#34;: &#34;v1&#34;,
        &#34;kind&#34;: &#34;Pod&#34;,
        &#34;metadata&#34;: {
           &#34;name&#34;: pod_name,
           &#34;namespace&#34;: &#34;default&#34;
        },
        &#34;spec&#34;: {
           &#34;containers&#34;: [
              {
                 &#34;name&#34;: &#34;volume-test&#34;,
                 &#34;image&#34;: &#34;nginx:stable-alpine&#34;,
                 &#34;imagePullPolicy&#34;: &#34;IfNotPresent&#34;,
                 &#34;volumeMounts&#34;: [
                    {
                       &#34;name&#34;: &#34;volv&#34;,
                       &#34;mountPath&#34;: &#34;/data&#34;
                    }
                 ],
                 &#34;ports&#34;: [
                    {
                       &#34;containerPort&#34;: 80
                    }
                 ]
              }
           ],
           &#34;volumes&#34;: [
              {
                 &#34;name&#34;: &#34;volv&#34;,
                 &#34;persistentVolumeClaim&#34;: {
                    &#34;claimName&#34;: pvc_name
                 }
              }
           ]
        }
    }

    return pod_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.generate_random_data"><code class="name flex">
<span>def <span class="ident">generate_random_data</span></span>(<span>count)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_random_data(count):
    return &#39;&#39;.join(random.choice(string.ascii_lowercase + string.digits)
                   for _ in range(count))</code></pre>
</details>
</dd>
<dt id="tests.common.generate_random_pos"><code class="name flex">
<span>def <span class="ident">generate_random_pos</span></span>(<span>size, used={})</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_random_pos(size, used={}):
    for i in range(RETRY_COUNTS):
        pos = 0
        if int(SIZE) != size:
            pos = random.randrange(0, int(SIZE)-size, 1)
        collided = False
        # it&#39;s [start, end) vs [pos, pos + size)
        for start, end in used.items():
            if pos + size &lt;= start or pos &gt;= end:
                continue
            collided = True
            break
        if not collided:
            break
    assert not collided
    used[pos] = pos + size
    return pos</code></pre>
</details>
</dd>
<dt id="tests.common.generate_volume_name"><code class="name flex">
<span>def <span class="ident">generate_volume_name</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_volume_name():
    return VOLUME_NAME + &#34;-&#34; + \
        &#39;&#39;.join(random.choice(string.ascii_lowercase + string.digits)
                for _ in range(6))</code></pre>
</details>
</dd>
<dt id="tests.common.get_apps_api_client"><code class="name flex">
<span>def <span class="ident">get_apps_api_client</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_apps_api_client():
    load_k8s_config()
    return k8sclient.AppsV1Api()</code></pre>
</details>
</dd>
<dt id="tests.common.get_backupstore_url"><code class="name flex">
<span>def <span class="ident">get_backupstore_url</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_backupstore_url():
    backupstore = os.environ[&#39;LONGHORN_BACKUPSTORES&#39;]
    backupstore = backupstore.replace(&#34; &#34;, &#34;&#34;)
    backupstores = backupstore.split(&#34;,&#34;)

    assert len(backupstores) != 0
    return backupstores</code></pre>
</details>
</dd>
<dt id="tests.common.get_client"><code class="name flex">
<span>def <span class="ident">get_client</span></span>(<span>address)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_client(address):
    url = &#39;http://&#39; + address + &#39;/v1/schemas&#39;
    c = longhorn.from_env(url=url)
    return c</code></pre>
</details>
</dd>
<dt id="tests.common.get_clients"><code class="name flex">
<span>def <span class="ident">get_clients</span></span>(<span>hosts)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_clients(hosts):
    clients = {}
    for host in hosts:
        assert host.name is not None
        assert host.address is not None
        clients[host.name] = get_client(host.address + PORT)
    return clients</code></pre>
</details>
</dd>
<dt id="tests.common.get_compatibility_test_image"><code class="name flex">
<span>def <span class="ident">get_compatibility_test_image</span></span>(<span>cli_v, cli_minv, ctl_v, ctl_minv, data_v, data_minv)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_compatibility_test_image(cli_v, cli_minv,
                                 ctl_v, ctl_minv,
                                 data_v, data_minv):
    return &#34;%s.%d-%d.%d-%d.%d-%d&#34; % (COMPATIBILTY_TEST_IMAGE_PREFIX,
                                     cli_v, cli_minv,
                                     ctl_v, ctl_minv,
                                     data_v, data_minv)</code></pre>
</details>
</dd>
<dt id="tests.common.get_core_api_client"><code class="name flex">
<span>def <span class="ident">get_core_api_client</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_core_api_client():
    load_k8s_config()
    return k8sclient.CoreV1Api()</code></pre>
</details>
</dd>
<dt id="tests.common.get_default_engine_image"><code class="name flex">
<span>def <span class="ident">get_default_engine_image</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_default_engine_image(client):
    images = client.list_engine_image()
    for img in images:
        if img.default:
            return img
    assert False</code></pre>
</details>
</dd>
<dt id="tests.common.get_device_checksum"><code class="name flex">
<span>def <span class="ident">get_device_checksum</span></span>(<span>dev)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_device_checksum(dev):
    hash = hashlib.sha512()

    with open(dev, &#39;rb&#39;) as fdev:
        if fdev is not None:
            for chunk in iter(lambda: fdev.read(4096), b&#34;&#34;):
                hash.update(chunk)

    return hash.hexdigest()</code></pre>
</details>
</dd>
<dt id="tests.common.get_host_disk_size"><code class="name flex">
<span>def <span class="ident">get_host_disk_size</span></span>(<span>disk)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_host_disk_size(disk):
    cmd = [&#39;stat&#39;, &#39;-fc&#39;,
           &#39;{&#34;path&#34;:&#34;%n&#34;,&#34;fsid&#34;:&#34;%i&#34;,&#34;type&#34;:&#34;%T&#34;,&#34;freeBlock&#34;:%f,&#39;
           &#39;&#34;totalBlock&#34;:%b,&#34;blockSize&#34;:%S}&#39;,
           disk]
    output = subprocess.check_output(cmd)
    disk_info = json.loads(output)
    block_size = disk_info[&#34;blockSize&#34;]
    free_blk = disk_info[&#34;freeBlock&#34;]
    total_blk = disk_info[&#34;totalBlock&#34;]
    free = (free_blk * block_size)
    total = (total_blk * block_size)
    return free, total</code></pre>
</details>
</dd>
<dt id="tests.common.get_iscsi_ip"><code class="name flex">
<span>def <span class="ident">get_iscsi_ip</span></span>(<span>iscsi)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_iscsi_ip(iscsi):
    iscsi_endpoint = parse_iscsi_endpoint(iscsi)
    ip = iscsi_endpoint[0].split(&#39;:&#39;)
    return ip[0]</code></pre>
</details>
</dd>
<dt id="tests.common.get_iscsi_lun"><code class="name flex">
<span>def <span class="ident">get_iscsi_lun</span></span>(<span>iscsi)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_iscsi_lun(iscsi):
    iscsi_endpoint = parse_iscsi_endpoint(iscsi)
    return iscsi_endpoint[2]</code></pre>
</details>
</dd>
<dt id="tests.common.get_iscsi_port"><code class="name flex">
<span>def <span class="ident">get_iscsi_port</span></span>(<span>iscsi)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_iscsi_port(iscsi):
    iscsi_endpoint = parse_iscsi_endpoint(iscsi)
    ip = iscsi_endpoint[0].split(&#39;:&#39;)
    return ip[1]</code></pre>
</details>
</dd>
<dt id="tests.common.get_iscsi_target"><code class="name flex">
<span>def <span class="ident">get_iscsi_target</span></span>(<span>iscsi)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_iscsi_target(iscsi):
    iscsi_endpoint = parse_iscsi_endpoint(iscsi)
    return iscsi_endpoint[1]</code></pre>
</details>
</dd>
<dt id="tests.common.get_liveness_probe_spec"><code class="name flex">
<span>def <span class="ident">get_liveness_probe_spec</span></span>(<span>initial_delay=5, period=5)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_liveness_probe_spec(initial_delay=5, period=5):
    pod_liveness_probe_spec = {
        &#34;exec&#34;: {
            &#34;command&#34;: [
                &#34;ls&#34;,
                &#34;/data/lost+found&#34;
            ]
        },
        &#34;initialDelaySeconds&#34;: initial_delay,
        &#34;periodSeconds&#34;: period
    }

    return pod_liveness_probe_spec</code></pre>
</details>
</dd>
<dt id="tests.common.get_longhorn_api_client"><code class="name flex">
<span>def <span class="ident">get_longhorn_api_client</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_longhorn_api_client():
    for i in range(RETRY_COUNTS):
        try:
            k8sconfig.load_incluster_config()
            ips = get_mgr_ips()

            # check if longhorn manager port is open before calling get_client
            for ip in ips:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                mgr_port_open = sock.connect_ex((ip, 9500))

                if mgr_port_open == 0:
                    client = get_client(ip + PORT)
                    break
            return client
        except Exception:
            time.sleep(RETRY_INTERVAL)</code></pre>
</details>
</dd>
<dt id="tests.common.get_mgr_ips"><code class="name flex">
<span>def <span class="ident">get_mgr_ips</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mgr_ips():
    ret = k8sclient.CoreV1Api().list_pod_for_all_namespaces(
            label_selector=&#34;app=longhorn-manager&#34;,
            watch=False)
    mgr_ips = []
    for i in ret.items:
        mgr_ips.append(i.status.pod_ip)
    return mgr_ips</code></pre>
</details>
</dd>
<dt id="tests.common.get_pod_data_md5sum"><code class="name flex">
<span>def <span class="ident">get_pod_data_md5sum</span></span>(<span>api, pod_name, path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pod_data_md5sum(api, pod_name, path):
    md5sum_command = [
        &#39;/bin/sh&#39;, &#39;-c&#39;, &#39;md5sum &#39; + path + &#34; | awk &#39;{print $1}&#39;&#34;
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT * 3,
                 error_message=&#39;Timeout on executing stream md5sum&#39;):
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=md5sum_command, stderr=True, stdin=False, stdout=True,
            tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.get_process_info"><code class="name flex">
<span>def <span class="ident">get_process_info</span></span>(<span>p_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_process_info(p_path):
    info = {}
    with open(p_path) as file:
        for line in file.readlines():
            if &#39;Name:\t&#39; == line[0:len(&#39;Name:\t&#39;)]:
                info[&#34;Name&#34;] = line[len(&#34;Name:&#34;):].strip()
            if &#39;Pid:\t&#39; == line[0:len(&#39;Pid:\t&#39;)]:
                info[&#34;Pid&#34;] = line[len(&#34;Pid:&#34;):].strip()
            if &#39;PPid:\t&#39; == line[0:len(&#39;PPid:\t&#39;)]:
                info[&#34;PPid&#34;] = line[len(&#34;PPid:&#34;):].strip()
    if &#34;Name&#34; not in info or &#34;Pid&#34; not in info or &#34;PPid&#34; not in info:
        return
    return info</code></pre>
</details>
</dd>
<dt id="tests.common.get_random_client"><code class="name flex">
<span>def <span class="ident">get_random_client</span></span>(<span>clients)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_random_client(clients):
    for _, client in iter(clients.items()):
        break
    return client</code></pre>
</details>
</dd>
<dt id="tests.common.get_scheduling_api_client"><code class="name flex">
<span>def <span class="ident">get_scheduling_api_client</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_scheduling_api_client():
    load_k8s_config()
    return k8sclient.SchedulingV1Api()</code></pre>
</details>
</dd>
<dt id="tests.common.get_self_host_id"><code class="name flex">
<span>def <span class="ident">get_self_host_id</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_self_host_id():
    envs = os.environ
    return envs[&#34;NODE_NAME&#34;]</code></pre>
</details>
</dd>
<dt id="tests.common.get_statefulset_pod_info"><code class="name flex">
<span>def <span class="ident">get_statefulset_pod_info</span></span>(<span>api, s_set)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_statefulset_pod_info(api, s_set):
    pod_info = []
    for i in range(s_set[&#39;spec&#39;][&#39;replicas&#39;]):
        pod_name = s_set[&#39;metadata&#39;][&#39;name&#39;] + &#39;-&#39; + str(i)
        pod = api.read_namespaced_pod(name=pod_name, namespace=&#39;default&#39;)
        pvc_name = pod.spec.volumes[0].persistent_volume_claim.claim_name
        pv_name = get_volume_name(api, pvc_name)
        pod_info.append({
            &#39;pod_name&#39;: pod_name,
            &#39;pv_name&#39;: pv_name,
            &#39;pvc_name&#39;: pvc_name,
        })
    return pod_info</code></pre>
</details>
</dd>
<dt id="tests.common.get_storage_api_client"><code class="name flex">
<span>def <span class="ident">get_storage_api_client</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_storage_api_client():
    load_k8s_config()
    return k8sclient.StorageV1Api()</code></pre>
</details>
</dd>
<dt id="tests.common.get_update_disks"><code class="name flex">
<span>def <span class="ident">get_update_disks</span></span>(<span>disks)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_update_disks(disks):
    update_disk = {}
    for key, disk in iter(disks.items()):
        update_disk[key] = disk
    return update_disk</code></pre>
</details>
</dd>
<dt id="tests.common.get_upgrade_test_image"><code class="name flex">
<span>def <span class="ident">get_upgrade_test_image</span></span>(<span>cli_v, cli_minv, ctl_v, ctl_minv, data_v, data_minv)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_upgrade_test_image(cli_v, cli_minv,
                           ctl_v, ctl_minv,
                           data_v, data_minv):
    return &#34;%s.%d-%d.%d-%d.%d-%d&#34; % (UPGRADE_TEST_IMAGE_PREFIX,
                                     cli_v, cli_minv,
                                     ctl_v, ctl_minv,
                                     data_v, data_minv)</code></pre>
</details>
</dd>
<dt id="tests.common.get_version_api_client"><code class="name flex">
<span>def <span class="ident">get_version_api_client</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_version_api_client():
    load_k8s_config()
    return k8sclient.VersionApi()</code></pre>
</details>
</dd>
<dt id="tests.common.get_volume_attached_nodes"><code class="name flex">
<span>def <span class="ident">get_volume_attached_nodes</span></span>(<span>v)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_volume_attached_nodes(v):
    nodes = []
    engines = v.controllers
    for e in engines:
        node = e.hostId
        if node != &#34;&#34;:
            nodes.append(node)
    return nodes</code></pre>
</details>
</dd>
<dt id="tests.common.get_volume_endpoint"><code class="name flex">
<span>def <span class="ident">get_volume_endpoint</span></span>(<span>v)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_volume_endpoint(v):
    endpoint = check_volume_endpoint(v)
    return endpoint</code></pre>
</details>
</dd>
<dt id="tests.common.get_volume_engine"><code class="name flex">
<span>def <span class="ident">get_volume_engine</span></span>(<span>v)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_volume_engine(v):
    engines = v.controllers
    assert len(engines) != 0
    return engines[0]</code></pre>
</details>
</dd>
<dt id="tests.common.get_volume_name"><code class="name flex">
<span>def <span class="ident">get_volume_name</span></span>(<span>api, pvc_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Given a PersistentVolumeClaim, return the name of the associated PV.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_volume_name(api, pvc_name):
    # type: (dict) -&gt; str
    &#34;&#34;&#34;
    Given a PersistentVolumeClaim, return the name of the associated PV.
    &#34;&#34;&#34;
    claim = api.read_namespaced_persistent_volume_claim(
        name=pvc_name, namespace=&#39;default&#39;)
    return claim.spec.volume_name</code></pre>
</details>
</dd>
<dt id="tests.common.is_backupTarget_nfs"><code class="name flex">
<span>def <span class="ident">is_backupTarget_nfs</span></span>(<span>s)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_backupTarget_nfs(s):
    return s.startswith(&#34;nfs://&#34;)</code></pre>
</details>
</dd>
<dt id="tests.common.is_backupTarget_s3"><code class="name flex">
<span>def <span class="ident">is_backupTarget_s3</span></span>(<span>s)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_backupTarget_s3(s):
    return s.startswith(&#34;s3://&#34;)</code></pre>
</details>
</dd>
<dt id="tests.common.iscsi_login"><code class="name flex">
<span>def <span class="ident">iscsi_login</span></span>(<span>iscsi_ep)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def iscsi_login(iscsi_ep):
    ip = get_iscsi_ip(iscsi_ep)
    port = get_iscsi_port(iscsi_ep)
    target = get_iscsi_target(iscsi_ep)
    lun = get_iscsi_lun(iscsi_ep)
    # discovery
    cmd_discovery = &#34;iscsiadm -m discovery -t st -p &#34; + ip
    exec_nsenter(cmd_discovery)
    # login
    cmd_login = &#34;iscsiadm -m node -T &#34; + target + &#34; -p &#34; + ip + &#34; --login&#34;
    exec_nsenter(cmd_login)
    blk_name = &#34;ip-%s:%s-iscsi-%s-lun-%s&#34; % (ip, port, target, lun)
    wait_for_device_login(ISCSI_DEV_PATH, blk_name)
    dev = os.path.realpath(ISCSI_DEV_PATH + &#34;/&#34; + blk_name)
    return dev</code></pre>
</details>
</dd>
<dt id="tests.common.iscsi_logout"><code class="name flex">
<span>def <span class="ident">iscsi_logout</span></span>(<span>iscsi_ep)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def iscsi_logout(iscsi_ep):
    ip = get_iscsi_ip(iscsi_ep)
    target = get_iscsi_target(iscsi_ep)
    cmd_logout = &#34;iscsiadm -m node -T &#34; + target + &#34; -p &#34; + ip + &#34; --logout&#34;
    exec_nsenter(cmd_logout)
    cmd_rm_discovery = &#34;iscsiadm -m discovery -p &#34; + ip + &#34; -o delete&#34;
    exec_nsenter(cmd_rm_discovery)</code></pre>
</details>
</dd>
<dt id="tests.common.json_string_go_to_python"><code class="name flex">
<span>def <span class="ident">json_string_go_to_python</span></span>(<span>str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def json_string_go_to_python(str):
    return str.replace(&#34;u\&#39;&#34;, &#34;\&#34;&#34;).replace(&#34;\&#39;&#34;, &#34;\&#34;&#34;). \
        replace(&#34;True&#34;, &#34;true&#34;).replace(&#34;False&#34;, &#34;false&#34;)</code></pre>
</details>
</dd>
<dt id="tests.common.lazy_umount_disk"><code class="name flex">
<span>def <span class="ident">lazy_umount_disk</span></span>(<span>mount_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lazy_umount_disk(mount_path):
    cmd = [&#39;umount&#39;, &#39;-l&#39;, mount_path]
    subprocess.check_call(cmd)</code></pre>
</details>
</dd>
<dt id="tests.common.load_k8s_config"><code class="name flex">
<span>def <span class="ident">load_k8s_config</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_k8s_config():
    c = Configuration()
    c.assert_hostname = False
    Configuration.set_default(c)
    k8sconfig.load_incluster_config()</code></pre>
</details>
</dd>
<dt id="tests.common.make_deployment_with_pvc"><code class="name flex">
<span>def <span class="ident">make_deployment_with_pvc</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def make_deployment_with_pvc(request):
    def _generate_deployment_with_pvc_manifest(deployment_name, pvc_name, replicas=1): # NOQA
        make_deployment_with_pvc.deployment_manifest = {
            &#34;apiVersion&#34;: &#34;apps/v1&#34;,
            &#34;kind&#34;: &#34;Deployment&#34;,
            &#34;metadata&#34;: {
               &#34;name&#34;: deployment_name,
               &#34;labels&#34;: {
                  &#34;name&#34;: deployment_name
               }
            },
            &#34;spec&#34;: {
               &#34;replicas&#34;: replicas,
               &#34;selector&#34;: {
                  &#34;matchLabels&#34;: {
                     &#34;name&#34;: deployment_name
                  }
               },
               &#34;template&#34;: {
                  &#34;metadata&#34;: {
                     &#34;labels&#34;: {
                        &#34;name&#34;: deployment_name
                     }
                  },
                  &#34;spec&#34;: {
                     &#34;containers&#34;: [
                        {
                           &#34;name&#34;: deployment_name,
                           &#34;image&#34;: &#34;nginx:stable-alpine&#34;,
                           &#34;volumeMounts&#34;: [
                              {
                                 &#34;name&#34;: &#34;volv&#34;,
                                 &#34;mountPath&#34;: &#34;/data&#34;
                              }
                           ]
                        }
                     ],
                     &#34;volumes&#34;: [
                        {
                           &#34;name&#34;: &#34;volv&#34;,
                           &#34;persistentVolumeClaim&#34;: {
                              &#34;claimName&#34;: pvc_name
                           }
                        }
                     ]
                  }
               }
            }
        }

        return make_deployment_with_pvc.deployment_manifest

    def finalizer():
        apps_api = get_apps_api_client()
        deployment_name = \
            make_deployment_with_pvc.deployment_manifest[&#34;metadata&#34;][&#34;name&#34;]
        delete_and_wait_deployment(
            apps_api,
            deployment_name
        )

    request.addfinalizer(finalizer)

    return _generate_deployment_with_pvc_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.monitor_restore_progress"><code class="name flex">
<span>def <span class="ident">monitor_restore_progress</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def monitor_restore_progress(client, volume_name):
    completed = 0
    rs = {}
    for i in range(RETRY_COUNTS):
        completed = 0
        v = client.by_id_volume(volume_name)
        rs = v.restoreStatus
        for r in rs:
            assert r.error == &#34;&#34;
            if r.state == &#34;complete&#34;:
                assert r.progress == 100
                completed += 1
        if completed == len(rs):
            break
        time.sleep(RETRY_INTERVAL)
    assert completed == len(rs)
    return v</code></pre>
</details>
</dd>
<dt id="tests.common.mount_disk"><code class="name flex">
<span>def <span class="ident">mount_disk</span></span>(<span>dev, mount_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mount_disk(dev, mount_path):
    cmd = [&#39;mount&#39;, dev, mount_path]
    subprocess.check_call(cmd)</code></pre>
</details>
</dd>
<dt id="tests.common.mount_nfs_backupstore"><code class="name flex">
<span>def <span class="ident">mount_nfs_backupstore</span></span>(<span>client, set_backupstore_nfs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def mount_nfs_backupstore(client, set_backupstore_nfs):
    cmd = [&#34;mkdir&#34;, &#34;-p&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    nfs_backuptarget = client.by_id_setting(SETTING_BACKUP_TARGET).value
    nfs_url = urlparse(nfs_backuptarget).netloc + \
        urlparse(nfs_backuptarget).path
    cmd = [&#34;mount&#34;, &#34;-t&#34;, &#34;nfs4&#34;, nfs_url, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)

    yield &#34;/mnt/nfs&#34;
    cmd = [&#34;umount&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)
    cmd = [&#34;rmdir&#34;, &#34;/mnt/nfs&#34;]
    subprocess.check_output(cmd)</code></pre>
</details>
</dd>
<dt id="tests.common.node_default_tags"><code class="name flex">
<span>def <span class="ident">node_default_tags</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Assign the Tags under DEFAULT_TAGS to the Longhorn client's Nodes to
provide a base set of Tags to work with in the tests.
:return: A dictionary mapping a Node's ID to the Tags it has.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.yield_fixture
def node_default_tags():
    &#34;&#34;&#34;
    Assign the Tags under DEFAULT_TAGS to the Longhorn client&#39;s Nodes to
    provide a base set of Tags to work with in the tests.
    :return: A dictionary mapping a Node&#39;s ID to the Tags it has.
    &#34;&#34;&#34;
    client = get_longhorn_api_client()  # NOQA
    nodes = client.list_node()
    assert len(nodes) == 3

    tag_mappings = {}
    for tags, node in zip(DEFAULT_TAGS, nodes):
        assert len(node.disks) == 1

        update_disks = get_update_disks(node.disks)
        update_disks[list(update_disks)[0]].tags = tags[&#34;disk&#34;]
        new_node = node.diskUpdate(disks=update_disks)
        disks = get_update_disks(new_node.disks)
        assert disks[list(new_node.disks)[0]].tags == tags[&#34;disk&#34;]

        new_node = set_node_tags(client, node, tags[&#34;node&#34;])
        assert new_node.tags == tags[&#34;node&#34;]

        tag_mappings[node.id] = tags
    yield tag_mappings

    client = get_longhorn_api_client()  # NOQA
    nodes = client.list_node()
    for node in nodes:
        update_disks = get_update_disks(node.disks)
        update_disks[list(update_disks)[0]].tags = []
        new_node = node.diskUpdate(disks=update_disks)
        disks = get_update_disks(new_node.disks)
        assert disks[list(new_node.disks)[0]].tags is None

        new_node = set_node_tags(client, node)
        assert new_node.tags is None</code></pre>
</details>
</dd>
<dt id="tests.common.parse_iscsi_endpoint"><code class="name flex">
<span>def <span class="ident">parse_iscsi_endpoint</span></span>(<span>iscsi)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_iscsi_endpoint(iscsi):
    iscsi_endpoint = iscsi[8:]
    return iscsi_endpoint.split(&#39;/&#39;)</code></pre>
</details>
</dd>
<dt id="tests.common.pod"><code class="name flex">
<span>def <span class="ident">pod</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def pod(request):
    pod_manifest = {
        &#39;apiVersion&#39;: &#39;v1&#39;,
        &#39;kind&#39;: &#39;Pod&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: &#39;test-pod&#39;
        },
        &#39;spec&#39;: {
            &#39;containers&#39;: [{
                &#39;image&#39;: &#39;busybox&#39;,
                &#39;imagePullPolicy&#39;: &#39;IfNotPresent&#39;,
                &#39;name&#39;: &#39;sleep&#39;,
                &#34;args&#34;: [
                    &#34;/bin/sh&#34;,
                    &#34;-c&#34;,
                    &#34;while true;do date;sleep 5; done&#34;
                ],
                &#34;volumeMounts&#34;: [{
                    &#39;name&#39;: &#39;pod-data&#39;,
                    &#39;mountPath&#39;: &#39;/data&#39;
                }],
            }],
            &#39;volumes&#39;: []
        }
    }

    def finalizer():
        api = get_core_api_client()
        delete_and_wait_pod(api, pod_manifest[&#39;metadata&#39;][&#39;name&#39;])

    request.addfinalizer(finalizer)

    return pod_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.pod_make"><code class="name flex">
<span>def <span class="ident">pod_make</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def pod_make(request):
    def make_pod(name=&#39;test-pod&#39;):
        pod_manifest = {
            &#39;apiVersion&#39;: &#39;v1&#39;,
            &#39;kind&#39;: &#39;Pod&#39;,
            &#39;metadata&#39;: {
                &#39;name&#39;: name
            },
            &#39;spec&#39;: {
                &#39;containers&#39;: [{
                    &#39;image&#39;: &#39;busybox&#39;,
                    &#39;imagePullPolicy&#39;: &#39;IfNotPresent&#39;,
                    &#39;name&#39;: &#39;sleep&#39;,
                    &#34;args&#34;: [
                        &#34;/bin/sh&#34;,
                        &#34;-c&#34;,
                        &#34;while true; do date; sleep 5; done&#34;
                    ],
                    &#34;volumeMounts&#34;: [{
                        &#39;name&#39;: &#39;pod-data&#39;,
                        &#39;mountPath&#39;: &#39;/data&#39;
                    }],
                }],
                &#39;volumes&#39;: []
            }
        }

        def finalizer():
            api = get_core_api_client()
            try:
                delete_and_wait_pod(api, pod_manifest[&#39;metadata&#39;][&#39;name&#39;])
            except Exception as e:
                print(&#34;Exception when waiting for pod deletion&#34;, e)
                return
            try:
                volume_details = pod_manifest[&#39;spec&#39;][&#39;volumes&#39;][0]
                pvc_name = volume_details[&#39;persistentVolumeClaim&#39;][&#39;claimName&#39;]
                delete_and_wait_pvc(api, pvc_name)
            except Exception as e:
                print(&#34;Exception when waiting for PVC deletion&#34;, e)
            try:
                pv = wait_and_get_pv_for_pvc(api, pvc_name)
                pv_name = pv.metadata.name
                delete_and_wait_pv(api, pv_name)
            except Exception as e:
                print(&#34;Exception when waiting for PV deletion&#34;, e)

        request.addfinalizer(finalizer)
        return pod_manifest

    return make_pod</code></pre>
</details>
</dd>
<dt id="tests.common.prepare_host_disk"><code class="name flex">
<span>def <span class="ident">prepare_host_disk</span></span>(<span>dev, vol_name, mkfs_ext4_options='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_host_disk(dev, vol_name, mkfs_ext4_options=&#34;&#34;):
    if mkfs_ext4_options == &#34;&#34;:
        cmd = [&#39;mkfs.ext4&#39;, dev]
    else:
        cmd = [&#39;mkfs.ext4&#39;, mkfs_ext4_options, dev]
    subprocess.check_call(cmd)

    mount_path = os.path.join(DIRECTORY_PATH, vol_name)
    # create directory before mount
    cmd = [&#39;mkdir&#39;, &#39;-p&#39;, mount_path]
    subprocess.check_call(cmd)

    mount_disk(dev, mount_path)
    return mount_path</code></pre>
</details>
</dd>
<dt id="tests.common.prepare_pod_with_data_in_mb"><code class="name flex">
<span>def <span class="ident">prepare_pod_with_data_in_mb</span></span>(<span>client, core_api, pod_make, volume_name, volume_size='1073741824', data_path='/data/test', data_size_in_mb=100, add_liveness_prope=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_pod_with_data_in_mb(
        client, core_api, pod_make, volume_name, volume_size=str(1*Gi),
        data_path=&#34;/data/test&#34;, data_size_in_mb=DATA_SIZE_IN_MB_1,
        add_liveness_prope=True):  # NOQA:
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)

    if add_liveness_prope is True:
        pod_liveness_probe_spec = \
            get_liveness_probe_spec(initial_delay=1,
                                    period=1)
        pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = \
            pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=3, size=volume_size)
    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    write_pod_volume_random_data(core_api, pod_name,
                                 data_path, data_size_in_mb)
    md5sum = get_pod_data_md5sum(core_api, pod_name, data_path)

    stream(core_api.connect_get_namespaced_pod_exec,
           pod_name, &#39;default&#39;, command=[&#34;sync&#34;],
           stderr=True, stdin=False, stdout=True, tty=False)

    return pod_name, pv_name, pvc_name, md5sum</code></pre>
</details>
</dd>
<dt id="tests.common.priority_class"><code class="name flex">
<span>def <span class="ident">priority_class</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def priority_class(request):
    priority_class = {
        &#39;apiVersion&#39;: &#39;scheduling.k8s.io/v1&#39;,
        &#39;kind&#39;: &#39;PriorityClass&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: PRIORITY_CLASS_NAME + &#34;-&#34; + &#39;&#39;.join(
                random.choice(string.ascii_lowercase +
                              string.digits)
                for _ in range(6))
        },
        &#39;value&#39;: random.randrange(PRIORITY_CLASS_MIN, PRIORITY_CLASS_MAX)
    }

    def finalizer():
        api = get_scheduling_api_client()
        try:
            api.delete_priority_class(name=priority_class[&#39;metadata&#39;][&#39;name&#39;],
                                      body=k8sclient.V1DeleteOptions())
        except ApiException as e:
            assert e.status == 404

    request.addfinalizer(finalizer)

    return priority_class</code></pre>
</details>
</dd>
<dt id="tests.common.pvc"><code class="name flex">
<span>def <span class="ident">pvc</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def pvc(request):
    pvc_manifest = {
        &#39;apiVersion&#39;: &#39;v1&#39;,
        &#39;kind&#39;: &#39;PersistentVolumeClaim&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: generate_volume_name()
        },
        &#39;spec&#39;: {
            &#39;accessModes&#39;: [
                &#39;ReadWriteOnce&#39;
            ],
            &#39;resources&#39;: {
                &#39;requests&#39;: {
                    &#39;storage&#39;: size_to_string(DEFAULT_VOLUME_SIZE * Gi)
                }
            }
        }
    }

    def finalizer():
        api = k8sclient.CoreV1Api()

        if not check_pvc_existence(api, pvc_manifest[&#39;metadata&#39;][&#39;name&#39;]):
            return

        claim = api.read_namespaced_persistent_volume_claim(
            name=pvc_manifest[&#39;metadata&#39;][&#39;name&#39;], namespace=&#39;default&#39;)
        volume_name = claim.spec.volume_name

        api = get_core_api_client()
        delete_and_wait_pvc(api, pvc_manifest[&#39;metadata&#39;][&#39;name&#39;])

        # Working around line break issue.
        key = &#39;volume.beta.kubernetes.io/storage-provisioner&#39;
        # If not using StorageClass (such as in CSI test), the Longhorn volume
        # will not be automatically deleted, causing this to throw an error.
        if (key in claim.metadata.annotations):
            client = get_longhorn_api_client()
            wait_for_volume_delete(client, volume_name)

    request.addfinalizer(finalizer)

    return pvc_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.pvc_baseimage"><code class="name flex">
<span>def <span class="ident">pvc_baseimage</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def pvc_baseimage(request):
    pvc_manifest = pvc(request)
    pvc_manifest[&#39;spec&#39;][&#39;resources&#39;][&#39;requests&#39;][&#39;storage&#39;] = \
        size_to_string(BASE_IMAGE_EXT4_SIZE)
    return pvc_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.pvc_name"><code class="name flex">
<span>def <span class="ident">pvc_name</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def pvc_name(request):
    return generate_volume_name()</code></pre>
</details>
</dd>
<dt id="tests.common.random_labels"><code class="name flex">
<span>def <span class="ident">random_labels</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def random_labels():
    labels = {}
    i = 0
    while i &lt; 3:
        key = &#34;label/&#34; + &#34;&#34;.join(random.choice(string.ascii_lowercase +
                                               string.digits)
                                 for _ in range(6))
        if not labels.get(key):
            labels[&#34;key&#34;] = generate_random_data(VOLUME_RWTEST_SIZE)
            i += 1
    return labels</code></pre>
</details>
</dd>
<dt id="tests.common.read_pod_block_volume_data"><code class="name flex">
<span>def <span class="ident">read_pod_block_volume_data</span></span>(<span>api, pod_name, data_size, offset, device_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_pod_block_volume_data(api, pod_name, data_size, offset, device_path):
    read_command = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;dd if=&#39; + device_path +
        &#39; status=none bs=&#39; + str(data_size) + &#39; count=1 skip=&#39; + str(offset)
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream read&#39;):
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=read_command, stderr=True, stdin=False, stdout=True,
            tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.read_volume_data"><code class="name flex">
<span>def <span class="ident">read_volume_data</span></span>(<span>api, pod_name, filename='test')</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieve data from a Pod's volume.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>api</code></strong></dt>
<dd>An instance of CoreV1API.</dd>
<dt><strong><code>pod_name</code></strong></dt>
<dd>The name of the Pod.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The data contained within the volume.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_volume_data(api, pod_name, filename=&#39;test&#39;):
    &#34;&#34;&#34;
    Retrieve data from a Pod&#39;s volume.

    Args:
        api: An instance of CoreV1API.
        pod_name: The name of the Pod.

    Returns:
        The data contained within the volume.
    &#34;&#34;&#34;
    read_command = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;cat /data/&#39; + filename
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream read&#39;):
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=read_command, stderr=True, stdin=False, stdout=True,
            tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.reset_disks_for_all_nodes"><code class="name flex">
<span>def <span class="ident">reset_disks_for_all_nodes</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_disks_for_all_nodes(client):  # NOQA
    nodes = client.list_node()
    for node in nodes:
        # Reset default disk if there are more than 1 disk
        # on the node.
        if len(node.disks) &gt; 1:
            update_disks = get_update_disks(node.disks)
            for disk_name, disk in update_disks:
                disk.allowScheduling = False
                update_disks[disk_name] = disk
                node = node.diskUpdate(disks=update_disks)
            update_disks = {}
            node = node.diskUpdate(disks=update_disks)
            node = wait_for_disk_update(client, node.name, 0)
        if len(node.disks) == 0:
            default_disk = {&#34;default-disk&#34;:
                            {&#34;path&#34;: DEFAULT_DISK_PATH,
                             &#34;allowScheduling&#34;: True}}
            node = node.diskUpdate(disks=default_disk)
            node = wait_for_disk_update(client, node.name, 1)
            assert len(node.disks) == 1
        # wait for node controller to update disk status
        disks = node.disks
        update_disks = {}
        for name, disk in iter(disks.items()):
            update_disk = disk
            update_disk.allowScheduling = True
            update_disk.storageReserved = \
                int(update_disk.storageMaximum * 30 / 100)
            update_disk.tags = []
            update_disks[name] = update_disk
        node = node.diskUpdate(disks=update_disks)
        for name, disk in iter(node.disks.items()):
            # wait for node controller update disk status
            wait_for_disk_status(client, node.name, name,
                                 &#34;allowScheduling&#34;, True)
            wait_for_disk_status(client, node.name, name,
                                 &#34;storageScheduled&#34;, 0)
            wait_for_disk_status(client, node.name, name,
                                 &#34;storageReserved&#34;,
                                 int(update_disk.storageMaximum * 30 / 100))</code></pre>
</details>
</dd>
<dt id="tests.common.reset_engine_image"><code class="name flex">
<span>def <span class="ident">reset_engine_image</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_engine_image(client):
    core_api = get_core_api_client()
    ready = False

    for i in range(RETRY_COUNTS):
        ready = True
        ei_list = client.list_engine_image().data
        for ei in ei_list:
            if ei.default:
                if ei.state != &#39;ready&#39;:
                    ready = False
            else:
                client.delete(ei)
                wait_for_engine_image_deletion(client, core_api, ei.name)
        if ready:
            break
        time.sleep(RETRY_INTERVAL)

    assert ready</code></pre>
</details>
</dd>
<dt id="tests.common.reset_node"><code class="name flex">
<span>def <span class="ident">reset_node</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_node(client):
    nodes = client.list_node()
    for node in nodes:
        try:
            node = client.update(node, tags=[])
            node = wait_for_node_tag_update(client, node.id, [])
            node = client.update(node, allowScheduling=True)
            wait_for_node_update(client, node.id,
                                 &#34;allowScheduling&#34;, True)
        except Exception as e:
            print(&#34;Exception when reset node schedulding and tags&#34;, node, e)</code></pre>
</details>
</dd>
<dt id="tests.common.reset_settings"><code class="name flex">
<span>def <span class="ident">reset_settings</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_settings(client):
    minimal_setting = client.by_id_setting(
        SETTING_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE)
    try:
        client.update(minimal_setting,
                      value=DEFAULT_STORAGE_MINIMAL_AVAILABLE_PERCENTAGE)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;storage minimal available percentage settings&#34;,
              minimal_setting, e)
        pass

    over_provisioning_setting = client.by_id_setting(
        SETTING_STORAGE_OVER_PROVISIONING_PERCENTAGE)
    try:
        client.update(over_provisioning_setting,
                      value=DEFAULT_STORAGE_OVER_PROVISIONING_PERCENTAGE)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;storage over provisioning percentage settings&#34;,
              over_provisioning_setting, e)

    default_data_path_setting = client.by_id_setting(
        SETTING_DEFAULT_DATA_PATH)
    try:
        client.update(default_data_path_setting,
                      value=DEFAULT_DISK_PATH)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;default data path setting&#34;,
              default_data_path_setting, e)

    create_default_disk_labeled_nodes_setting = client.by_id_setting(
        SETTING_CREATE_DEFAULT_DISK_LABELED_NODES)
    try:
        client.update(create_default_disk_labeled_nodes_setting,
                      value=&#34;false&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;create default disk labeled nodes setting&#34;,
              create_default_disk_labeled_nodes_setting, e)

    replica_node_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_NODE_SOFT_ANTI_AFFINITY)
    try:
        client.update(replica_node_soft_anti_affinity_setting,
                      value=&#34;false&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;Replica Node Level Soft Anti-Affinity setting&#34;,
              replica_node_soft_anti_affinity_setting, e)

    replica_zone_soft_anti_affinity_setting = \
        client.by_id_setting(SETTING_REPLICA_ZONE_SOFT_ANTI_AFFINITY)
    try:
        client.update(replica_zone_soft_anti_affinity_setting,
                      value=&#34;true&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;Replica Zone Level Soft Anti-Affinity setting&#34;,
              replica_zone_soft_anti_affinity_setting, e)

    disable_scheduling_on_cordoned_node_setting = \
        client.by_id_setting(SETTING_DISABLE_SCHEDULING_ON_CORDONED_NODE)
    try:
        client.update(disable_scheduling_on_cordoned_node_setting,
                      value=&#34;true&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;Disable Scheduling On Cordoned Node setting&#34;,
              disable_scheduling_on_cordoned_node_setting, e)
    auto_salvage_setting = client.by_id_setting(SETTING_AUTO_SALVAGE)
    try:
        client.update(auto_salvage_setting, value=&#34;true&#34;)
    except Exception as e:
        print(&#34;Exception when update Auto Salvage setting&#34;,
              auto_salvage_setting, e)

    guaranteed_engine_cpu_setting = \
        client.by_id_setting(SETTING_GUARANTEED_ENGINE_CPU)
    try:
        client.update(guaranteed_engine_cpu_setting,
                      value=&#34;0.25&#34;)
    except Exception as e:
        print(&#34;Exception when update &#34;
              &#34;Guaranteed Engine CPU setting&#34;,
              guaranteed_engine_cpu_setting, e)

    instance_managers = client.list_instance_manager()
    core_api = get_core_api_client()
    # Wait for the current instance manager running
    for im in instance_managers:
        wait_for_instance_manager_desire_state(client, core_api,
                                               im.name, &#34;Running&#34;, True)</code></pre>
</details>
</dd>
<dt id="tests.common.scheduling_api"><code class="name flex">
<span>def <span class="ident">scheduling_api</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a new SchedulingV1API instance.</p>
<h2 id="returns">Returns</h2>
<p>A new CoreV1API Instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def scheduling_api(request):
    &#34;&#34;&#34;
    Create a new SchedulingV1API instance.
    Returns:
        A new CoreV1API Instance.
    &#34;&#34;&#34;
    c = Configuration()
    c.assert_hostname = False
    Configuration.set_default(c)
    k8sconfig.load_incluster_config()
    scheduling_api = k8sclient.SchedulingV1Api()

    return scheduling_api</code></pre>
</details>
</dd>
<dt id="tests.common.set_backupstore_nfs"><code class="name flex">
<span>def <span class="ident">set_backupstore_nfs</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def set_backupstore_nfs(client):
    backup_target_setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    backupstores = get_backupstore_url()
    for backupstore in backupstores:
        if is_backupTarget_nfs(backupstore):
            backup_target_setting = client.update(backup_target_setting,
                                                  value=backupstore)
            assert backup_target_setting.value == backupstore
            backup_target_credential_setting = client.by_id_setting(
                SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            backup_target_credential_setting = \
                client.update(backup_target_credential_setting, value=&#34;&#34;)
            assert backup_target_credential_setting.value == &#34;&#34;
            break

    yield
    backup_target_setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    client.update(backup_target_setting, value=&#34;&#34;)
    backup_target_credential_setting = client.by_id_setting(
        SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
    client.update(backup_target_credential_setting, value=&#34;&#34;)</code></pre>
</details>
</dd>
<dt id="tests.common.set_backupstore_s3"><code class="name flex">
<span>def <span class="ident">set_backupstore_s3</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def set_backupstore_s3(client):
    backup_target_setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    backupstores = get_backupstore_url()
    for backupstore in backupstores:
        if is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            backup_target_setting = client.update(backup_target_setting,
                                                  value=backupsettings[0])
            assert backup_target_setting.value == backupsettings[0]

            backup_target_credential_setting = client.by_id_setting(
                SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            backup_target_credential_setting = \
                client.update(backup_target_credential_setting,
                              value=backupsettings[1])
            assert backup_target_credential_setting.value == backupsettings[1]
            break

    yield
    backup_target_setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    client.update(backup_target_setting, value=&#34;&#34;)
    backup_target_credential_setting = client.by_id_setting(
        SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
    client.update(backup_target_credential_setting, value=&#34;&#34;)</code></pre>
</details>
</dd>
<dt id="tests.common.set_node_tags"><code class="name flex">
<span>def <span class="ident">set_node_tags</span></span>(<span>client, node, tags=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Set the tags on a node without modifying its scheduling status.
:param client: The Longhorn client to use in the request.
:param node: The Node to update.
:param tags: The tags to set on the node.
:return: The updated Node.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_node_tags(client, node, tags=[]):  # NOQA
    &#34;&#34;&#34;
    Set the tags on a node without modifying its scheduling status.
    :param client: The Longhorn client to use in the request.
    :param node: The Node to update.
    :param tags: The tags to set on the node.
    :return: The updated Node.
    &#34;&#34;&#34;
    return client.update(node, allowScheduling=node.allowScheduling,
                         tags=tags)</code></pre>
</details>
</dd>
<dt id="tests.common.set_random_backupstore"><code class="name flex">
<span>def <span class="ident">set_random_backupstore</span></span>(<span>client)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_random_backupstore(client):
    setting = client.by_id_setting(SETTING_BACKUP_TARGET)
    backupstores = get_backupstore_url()
    for backupstore in backupstores:
        if is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;
        break</code></pre>
</details>
</dd>
<dt id="tests.common.settings_reset"><code class="name flex">
<span>def <span class="ident">settings_reset</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def settings_reset():
    yield

    client = get_longhorn_api_client()
    reset_settings(client)</code></pre>
</details>
</dd>
<dt id="tests.common.size_to_string"><code class="name flex">
<span>def <span class="ident">size_to_string</span></span>(<span>volume_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a volume size to string format to pass into Kubernetes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>volume_size</code></strong></dt>
<dd>The size of the volume in bytes.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The size of the volume in gigabytes as a passable string to Kubernetes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def size_to_string(volume_size):
    # type: (int) -&gt; str
    &#34;&#34;&#34;
    Convert a volume size to string format to pass into Kubernetes.
    Args:
        volume_size: The size of the volume in bytes.
    Returns:
        The size of the volume in gigabytes as a passable string to Kubernetes.
    &#34;&#34;&#34;
    if volume_size &gt;= Gi:
        return str(volume_size &gt;&gt; 30) + &#39;Gi&#39;
    elif volume_size &gt;= Mi:
        return str(volume_size &gt;&gt; 20) + &#39;Mi&#39;
    else:
        return str(volume_size &gt;&gt; 10) + &#39;Ki&#39;</code></pre>
</details>
</dd>
<dt id="tests.common.statefulset"><code class="name flex">
<span>def <span class="ident">statefulset</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def statefulset(request):
    statefulset_manifest = {
        &#39;apiVersion&#39;: &#39;apps/v1&#39;,
        &#39;kind&#39;: &#39;StatefulSet&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: &#39;test-statefulset&#39;
        },
        &#39;spec&#39;: {
            &#39;selector&#39;: {
                &#39;matchLabels&#39;: {
                    &#39;app&#39;: &#39;test-statefulset&#39;
                }
            },
            &#39;serviceName&#39;: &#39;test-statefulset&#39;,
            &#39;replicas&#39;: 2,
            &#39;template&#39;: {
                &#39;metadata&#39;: {
                    &#39;labels&#39;: {
                        &#39;app&#39;: &#39;test-statefulset&#39;
                    }
                },
                &#39;spec&#39;: {
                    &#39;terminationGracePeriodSeconds&#39;: 10,
                    &#39;containers&#39;: [{
                        &#39;image&#39;: &#39;busybox&#39;,
                        &#39;imagePullPolicy&#39;: &#39;IfNotPresent&#39;,
                        &#39;name&#39;: &#39;sleep&#39;,
                        &#39;args&#39;: [
                            &#39;/bin/sh&#39;,
                            &#39;-c&#39;,
                            &#39;while true;do date;sleep 5; done&#39;
                        ],
                        &#39;volumeMounts&#39;: [{
                            &#39;name&#39;: &#39;pod-data&#39;,
                            &#39;mountPath&#39;: &#39;/data&#39;
                        }]
                    }]
                }
            },
            &#39;volumeClaimTemplates&#39;: [{
                &#39;metadata&#39;: {
                    &#39;name&#39;: &#39;pod-data&#39;
                },
                &#39;spec&#39;: {
                    &#39;accessModes&#39;: [
                        &#39;ReadWriteOnce&#39;
                    ],
                    &#39;storageClassName&#39;: DEFAULT_STORAGECLASS_NAME,
                    &#39;resources&#39;: {
                        &#39;requests&#39;: {
                            &#39;storage&#39;: size_to_string(
                                           DEFAULT_VOLUME_SIZE * Gi)
                        }
                    }
                }
            }]
        }
    }

    def finalizer():
        api = get_core_api_client()
        client = get_longhorn_api_client()
        delete_and_wait_statefulset(api, client, statefulset_manifest)

    request.addfinalizer(finalizer)

    return statefulset_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.storage_class"><code class="name flex">
<span>def <span class="ident">storage_class</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def storage_class(request):
    sc_manifest = {
        &#39;apiVersion&#39;: &#39;storage.k8s.io/v1&#39;,
        &#39;kind&#39;: &#39;StorageClass&#39;,
        &#39;metadata&#39;: {
            &#39;name&#39;: DEFAULT_STORAGECLASS_NAME
        },
        &#39;provisioner&#39;: &#39;driver.longhorn.io&#39;,
        &#39;allowVolumeExpansion&#39;: True,
        &#39;parameters&#39;: {
            &#39;numberOfReplicas&#39;: DEFAULT_LONGHORN_PARAMS[&#39;numberOfReplicas&#39;],
            &#39;staleReplicaTimeout&#39;:
                DEFAULT_LONGHORN_PARAMS[&#39;staleReplicaTimeout&#39;]
        },
        &#39;reclaimPolicy&#39;: &#39;Delete&#39;
    }

    def finalizer():
        api = get_storage_api_client()
        try:
            api.delete_storage_class(name=sc_manifest[&#39;metadata&#39;][&#39;name&#39;],
                                     body=k8sclient.V1DeleteOptions())
        except ApiException as e:
            assert e.status == 404

    request.addfinalizer(finalizer)

    return sc_manifest</code></pre>
</details>
</dd>
<dt id="tests.common.umount_disk"><code class="name flex">
<span>def <span class="ident">umount_disk</span></span>(<span>mount_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def umount_disk(mount_path):
    cmd = [&#39;umount&#39;, mount_path]
    subprocess.check_call(cmd)</code></pre>
</details>
</dd>
<dt id="tests.common.update_statefulset_manifests"><code class="name flex">
<span>def <span class="ident">update_statefulset_manifests</span></span>(<span>ss_manifest, sc_manifest, name)</span>
</code></dt>
<dd>
<div class="desc"><p>Write in a new StatefulSet name and the proper StorageClass name for tests.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_statefulset_manifests(ss_manifest, sc_manifest, name):
    &#34;&#34;&#34;
    Write in a new StatefulSet name and the proper StorageClass name for tests.
    &#34;&#34;&#34;
    ss_manifest[&#39;metadata&#39;][&#39;name&#39;] = \
        ss_manifest[&#39;spec&#39;][&#39;selector&#39;][&#39;matchLabels&#39;][&#39;app&#39;] = \
        ss_manifest[&#39;spec&#39;][&#39;serviceName&#39;] = \
        ss_manifest[&#39;spec&#39;][&#39;template&#39;][&#39;metadata&#39;][&#39;labels&#39;][&#39;app&#39;] = \
        name
    ss_manifest[&#39;spec&#39;][&#39;volumeClaimTemplates&#39;][0][&#39;spec&#39;][&#39;storageClassName&#39;]\
        = DEFAULT_STORAGECLASS_NAME
    sc_manifest[&#39;metadata&#39;][&#39;name&#39;] = DEFAULT_STORAGECLASS_NAME</code></pre>
</details>
</dd>
<dt id="tests.common.volume_name"><code class="name flex">
<span>def <span class="ident">volume_name</span></span>(<span>request)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.fixture
def volume_name(request):
    return generate_volume_name()</code></pre>
</details>
</dd>
<dt id="tests.common.volume_read"><code class="name flex">
<span>def <span class="ident">volume_read</span></span>(<span>v, start, count)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_read(v, start, count):
    dev = get_volume_endpoint(v)
    return dev_read(dev, start, count)</code></pre>
</details>
</dd>
<dt id="tests.common.volume_valid"><code class="name flex">
<span>def <span class="ident">volume_valid</span></span>(<span>dev)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_valid(dev):
    return stat.S_ISBLK(os.stat(dev).st_mode)</code></pre>
</details>
</dd>
<dt id="tests.common.volume_write"><code class="name flex">
<span>def <span class="ident">volume_write</span></span>(<span>v, start, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def volume_write(v, start, data):
    dev = get_volume_endpoint(v)
    return dev_write(dev, start, data)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_and_get_pv_for_pvc"><code class="name flex">
<span>def <span class="ident">wait_and_get_pv_for_pvc</span></span>(<span>api, pvc_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_and_get_pv_for_pvc(api, pvc_name):
    found = False
    for i in range(RETRY_COUNTS):
        pvs = api.list_persistent_volume()
        for item in pvs.items:
            if item.spec.claim_ref.name == pvc_name:
                found = True
                pv = item
                break
        if found:
            break
        time.sleep(RETRY_INTERVAL)

    assert found
    return pv</code></pre>
</details>
</dd>
<dt id="tests.common.wait_delete_deployment"><code class="name flex">
<span>def <span class="ident">wait_delete_deployment</span></span>(<span>apps_api, deployment_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_delete_deployment(apps_api, deployment_name):
    for i in range(DEFAULT_DEPLOYMENT_TIMEOUT):
        ret = apps_api.list_namespaced_deployment(namespace=&#39;default&#39;)
        found = False
        for item in ret.items:
            if item.metadata.name == deployment_name:
                found = True
                break
        if not found:
            break
        time.sleep(DEFAULT_DEPLOYMENT_INTERVAL)
    assert not found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_delete_pod"><code class="name flex">
<span>def <span class="ident">wait_delete_pod</span></span>(<span>api, pod_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_delete_pod(api, pod_name):
    for i in range(DEFAULT_POD_TIMEOUT):
        ret = api.list_namespaced_pod(namespace=&#39;default&#39;)
        found = False
        for item in ret.items:
            if item.metadata.name == pod_name:
                found = True
                break
        if not found:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert not found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_delete_pv"><code class="name flex">
<span>def <span class="ident">wait_delete_pv</span></span>(<span>api, pv_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_delete_pv(api, pv_name):
    for i in range(RETRY_COUNTS):
        found = False
        pvs = api.list_persistent_volume()
        for item in pvs.items:
            if item.metadata.name == pv_name:
                if item.status.phase == &#39;Failed&#39;:
                    try:
                        api.delete_persistent_volume(
                            name=pv_name, body=k8sclient.V1DeleteOptions())
                    except ApiException as e:
                        assert e.status == 404
                else:
                    found = True
                    break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_delete_pvc"><code class="name flex">
<span>def <span class="ident">wait_delete_pvc</span></span>(<span>api, pvc_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_delete_pvc(api, pvc_name):
    for i in range(RETRY_COUNTS):
        found = False
        ret = api.list_namespaced_persistent_volume_claim(namespace=&#39;default&#39;)
        for item in ret.items:
            if item.metadata.name == pvc_name:
                found = True
                break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_delete_volume_attachment"><code class="name flex">
<span>def <span class="ident">wait_delete_volume_attachment</span></span>(<span>storage_api, volume_attachment_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_delete_volume_attachment(storage_api, volume_attachment_name):
    for i in range(RETRY_COUNTS):
        found = False
        ret = storage_api.list_volume_attachment()
        for item in ret.items:
            if item.metadata.name == volume_attachment_name:
                found = True
                break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_deployment_replica_ready"><code class="name flex">
<span>def <span class="ident">wait_deployment_replica_ready</span></span>(<span>apps_api, deployment_name, desired_replica_count)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_deployment_replica_ready(apps_api, deployment_name, desired_replica_count): # NOQA
    replicas_ready = False
    for i in range(DEFAULT_DEPLOYMENT_TIMEOUT):
        deployment = apps_api.read_namespaced_deployment(
            name=deployment_name,
            namespace=&#34;default&#34;)

        if deployment.status.ready_replicas == desired_replica_count:
            replicas_ready = True
            break

        time.sleep(DEFAULT_DEPLOYMENT_INTERVAL)

    assert replicas_ready</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_backup_completion"><code class="name flex">
<span>def <span class="ident">wait_for_backup_completion</span></span>(<span>client, volume_name, snapshot_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_backup_completion(client, volume_name, snapshot_name):
    completed = False
    for i in range(RETRY_BACKUP_COUNTS):
        v = client.by_id_volume(volume_name)
        for b in v.backupStatus:
            if b.snapshot == snapshot_name and b.state == &#34;complete&#34;:
                assert b.progress == 100
                assert b.error == &#34;&#34;
                completed = True
                break
        if completed:
            break
        time.sleep(RETRY_BACKUP_INTERVAL)
    assert completed is True
    return v</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_backup_delete"><code class="name flex">
<span>def <span class="ident">wait_for_backup_delete</span></span>(<span>client, volume_name, backup_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_backup_delete(client, volume_name, backup_name):
    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()
        bv_found = False

        for bv in bvs:
            if bv.name == volume_name:
                bv_found = True
                break

        assert bv_found

        backups = bv.backupList()

        backup_found = False
        for b in backups:
            if b.name == backup_name:
                backup_found = True
                break

        if backup_found is False:
            break

        time.sleep(RETRY_INTERVAL)

    assert not backup_found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_backup_state"><code class="name flex">
<span>def <span class="ident">wait_for_backup_state</span></span>(<span>client, volume_name, predicate)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_backup_state(client, volume_name, predicate):
    completed = False
    for i in range(RETRY_BACKUP_COUNTS):
        v = client.by_id_volume(volume_name)
        for b in v.backupStatus:
            if predicate(b):
                completed = True
                break
        if completed:
            break
        time.sleep(RETRY_BACKUP_INTERVAL)
    assert completed is True
    return v</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_backup_volume_delete"><code class="name flex">
<span>def <span class="ident">wait_for_backup_volume_delete</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_backup_volume_delete(client, name):
    for i in range(RETRY_COUNTS):
        bvs = client.list_backupVolume()
        found = False
        for bv in bvs:
            if bv.name == name:
                found = True
                break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_device_login"><code class="name flex">
<span>def <span class="ident">wait_for_device_login</span></span>(<span>dest_path, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_device_login(dest_path, name):
    dev = &#34;&#34;
    for i in range(RETRY_COUNTS):
        for j in range(RETRY_COMMAND_COUNT):
            files = []
            try:
                files = os.listdir(dest_path)
                break
            except Exception:
                time.sleep(1)
        assert files
        if name in files:
            dev = name
            break
        time.sleep(RETRY_INTERVAL)
    assert dev == name
    return dev</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_disk_conditions"><code class="name flex">
<span>def <span class="ident">wait_for_disk_conditions</span></span>(<span>client, node_name, disk_name, key, value)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_disk_conditions(client, node_name, disk_name, key, value):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(node_name)
        disks = node.disks
        disk = disks[disk_name]
        conditions = disk.conditions
        if conditions[key][&#34;status&#34;] == value:
            break
        time.sleep(RETRY_INTERVAL)
    assert conditions[key][&#34;status&#34;] == value
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_disk_status"><code class="name flex">
<span>def <span class="ident">wait_for_disk_status</span></span>(<span>client, node_name, disk_name, key, value)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_disk_status(client, node_name, disk_name, key, value):
    # use wait_for_disk_storage_available to check storageAvailable
    assert key != &#34;storageAvailable&#34;
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(node_name)
        disks = node.disks
        if len(disks) &gt; 0 and \
                disk_name in disks and \
                disks[disk_name][key] == value:
            break
        time.sleep(RETRY_INTERVAL)
    assert len(disks) != 0
    assert disk_name in disks
    assert disks[disk_name][key] == value
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_disk_storage_available"><code class="name flex">
<span>def <span class="ident">wait_for_disk_storage_available</span></span>(<span>client, node_name, disk_name, disk_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_disk_storage_available(client, node_name, disk_name, disk_path):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(node_name)
        disks = node.disks
        if len(disks) &gt; 0 and disk_name in disks:
            free, _ = get_host_disk_size(disk_path)
            if disks[disk_name][&#34;storageAvailable&#34;] == free:
                break
        time.sleep(RETRY_INTERVAL)
    assert len(disks) != 0
    assert disk_name in disks
    assert disks[disk_name][&#34;storageAvailable&#34;] == free
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_disk_update"><code class="name flex">
<span>def <span class="ident">wait_for_disk_update</span></span>(<span>client, name, disk_num)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_disk_update(client, name, disk_num):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        if len(node.disks) == disk_num:
            allUpdated = True
            disks = node.disks
            for d in disks:
                if disks[d][&#34;diskUUID&#34;] == &#34;&#34;:
                    allUpdated = False
                    break
            if allUpdated:
                break
        time.sleep(RETRY_INTERVAL)
    assert len(node.disks) == disk_num
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_disk_uuid"><code class="name flex">
<span>def <span class="ident">wait_for_disk_uuid</span></span>(<span>client, node_name, uuid)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_disk_uuid(client, node_name, uuid):
    found = False
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(node_name)
        disks = node.disks
        for name in disks:
            if disks[name][&#34;diskUUID&#34;] == uuid:
                found = True
                break
        if found:
            break
        time.sleep(RETRY_INTERVAL)
    assert found
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_dr_volume_expansion"><code class="name flex">
<span>def <span class="ident">wait_for_dr_volume_expansion</span></span>(<span>longhorn_api_client, volume_name, size_str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_dr_volume_expansion(longhorn_api_client, volume_name, size_str):
    complete = False
    for i in range(RETRY_COUNTS):
        volume = longhorn_api_client.by_id_volume(volume_name)
        if volume.size == size_str:
            engine = get_volume_engine(volume)
            if engine.size == volume.size:
                complete = True
                break
        time.sleep(RETRY_INTERVAL)
    assert complete</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_engine_image_creation"><code class="name flex">
<span>def <span class="ident">wait_for_engine_image_creation</span></span>(<span>client, image_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_engine_image_creation(client, image_name):
    for i in range(RETRY_COUNTS):
        images = client.list_engine_image()
        found = False
        for img in images:
            if img.name == image_name:
                found = True
                break
        if found:
            break
    assert found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_engine_image_deletion"><code class="name flex">
<span>def <span class="ident">wait_for_engine_image_deletion</span></span>(<span>client, core_api, engine_image_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_engine_image_deletion(client, core_api, engine_image_name):
    deleted = False

    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL)
        deleted = True

        ei_list = client.list_engine_image().data
        for ei in ei_list:
            if ei.name == engine_image_name:
                deleted = False
                break
        if not deleted:
            continue

        labels = &#34;longhorn.io/component=engine-image,&#34; \
                 &#34;longhorn.io/engine-image=&#34;+engine_image_name
        ei_pod_list = core_api.list_namespaced_pod(
            LONGHORN_NAMESPACE, label_selector=labels).items
        if len(ei_pod_list) != 0:
            deleted = False
            continue

    assert deleted</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_engine_image_ref_count"><code class="name flex">
<span>def <span class="ident">wait_for_engine_image_ref_count</span></span>(<span>client, image_name, count)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_engine_image_ref_count(client, image_name, count):
    wait_for_engine_image_creation(client, image_name)
    for i in range(RETRY_COUNTS):
        image = client.by_id_engine_image(image_name)
        if image.refCount == count:
            break
        time.sleep(RETRY_INTERVAL)
    assert image.refCount == count
    if count == 0:
        assert image.noRefSince != &#34;&#34;
    return image</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_engine_image_state"><code class="name flex">
<span>def <span class="ident">wait_for_engine_image_state</span></span>(<span>client, image_name, state)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_engine_image_state(client, image_name, state):
    wait_for_engine_image_creation(client, image_name)
    for i in range(RETRY_COUNTS):
        image = client.by_id_engine_image(image_name)
        if image.state == state:
            break
        time.sleep(RETRY_INTERVAL)
    assert image.state == state
    return image</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_expansion_failure"><code class="name flex">
<span>def <span class="ident">wait_for_expansion_failure</span></span>(<span>client, volume_name, last_failed_at='')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_expansion_failure(client, volume_name, last_failed_at=&#34;&#34;):
    failed = False
    for i in range(30):
        volume = client.by_id_volume(volume_name)
        engine = get_volume_engine(volume)
        if engine.lastExpansionFailedAt != last_failed_at:
            failed = True
            break
        time.sleep(RETRY_INTERVAL)
    assert failed</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_instance_manager_desire_state"><code class="name flex">
<span>def <span class="ident">wait_for_instance_manager_desire_state</span></span>(<span>client, core_api, im_name, state, desire=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_instance_manager_desire_state(client, core_api, im_name,
                                           state, desire=True):
    for i in range(RETRY_COUNTS):
        im = client.by_id_instance_manager(im_name)
        try:
            pod = core_api.read_namespaced_pod(name=im_name,
                                               namespace=LONGHORN_NAMESPACE)
        except Exception as e:
            # Continue with pod restarted case
            if e.reason == &#34;Not Found&#34;:
                time.sleep(RETRY_INTERVAL)
                continue
            # Report any other error
            else:
                assert(not e)
        if desire:
            if im.currentState == state.lower() and pod.status.phase == state:
                break
        else:
            if im.currentState != state.lower() and pod.status.phase != state:
                break
        time.sleep(RETRY_INTERVAL)
    if desire:
        assert im.currentState == state.lower()
        assert pod.status.phase == state
    else:
        assert im.currentState != state.lower()
        assert pod.status.phase != state
    return im</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_node_mountpropagation_condition"><code class="name flex">
<span>def <span class="ident">wait_for_node_mountpropagation_condition</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_node_mountpropagation_condition(client, name):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        conditions = {}
        if &#34;conditions&#34; in node.keys():
            conditions = node.conditions

        if NODE_CONDITION_MOUNTPROPAGATION in \
                conditions.keys() and \
                &#34;status&#34; in \
                conditions[NODE_CONDITION_MOUNTPROPAGATION].keys() \
                and conditions[NODE_CONDITION_MOUNTPROPAGATION][&#34;status&#34;] != \
                CONDITION_STATUS_UNKNOWN:
            break
        time.sleep(RETRY_INTERVAL)
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_node_schedulable_condition"><code class="name flex">
<span>def <span class="ident">wait_for_node_schedulable_condition</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_node_schedulable_condition(client, name):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        conditions = {}
        if &#34;conditions&#34; in node.keys():
            conditions = node.conditions

        if NODE_CONDITION_SCHEDULABLE in \
                conditions.keys() and \
                &#34;status&#34; in \
                conditions[NODE_CONDITION_SCHEDULABLE].keys() \
                and conditions[NODE_CONDITION_SCHEDULABLE][&#34;status&#34;] != \
                CONDITION_STATUS_UNKNOWN:
            break
        time.sleep(RETRY_INTERVAL)
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_node_tag_update"><code class="name flex">
<span>def <span class="ident">wait_for_node_tag_update</span></span>(<span>client, name, tags)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_node_tag_update(client, name, tags):
    updated = False
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        if not tags and not node.tags:
            updated = True
            break
        elif node.tags is not None and set(node.tags) == set(tags):
            updated = True
            break
        time.sleep(RETRY_INTERVAL)
    assert updated
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_node_update"><code class="name flex">
<span>def <span class="ident">wait_for_node_update</span></span>(<span>client, name, key, value)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_node_update(client, name, key, value):
    for i in range(RETRY_COUNTS):
        node = client.by_id_node(name)
        if str(node[key]) == str(value):
            break
        time.sleep(RETRY_INTERVAL)
    assert str(node[key]) == str(value)
    return node</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_pod_remount"><code class="name flex">
<span>def <span class="ident">wait_for_pod_remount</span></span>(<span>core_api, pod_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_pod_remount(core_api, pod_name):
    check_command = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;ls /data/lost+found&#39;
    ]

    ready = False
    for i in range(RETRY_EXEC_COUNTS):
        try:
            output = stream(core_api.connect_get_namespaced_pod_exec,
                            pod_name,
                            &#39;default&#39;,
                            command=check_command,
                            stderr=True, stdin=False,
                            stdout=True, tty=False)
            if &#34;Input/output error&#34; not in output:
                ready = True
                break
        except Exception:
            pass
        if ready:
            break
        time.sleep(RETRY_EXEC_INTERVAL)
    assert ready</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_pod_restart"><code class="name flex">
<span>def <span class="ident">wait_for_pod_restart</span></span>(<span>core_api, pod_name, namespace='default')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_pod_restart(core_api, pod_name, namespace=&#34;default&#34;):
    pod = core_api.read_namespaced_pod(name=pod_name,
                                       namespace=namespace)
    restart_count = pod.status.container_statuses[0].restart_count

    pod_restarted = False
    for i in range(RETRY_COUNTS):
        pod = core_api.read_namespaced_pod(name=pod_name,
                                           namespace=namespace)
        count = pod.status.container_statuses[0].restart_count
        if count &gt; restart_count:
            pod_restarted = True
            break

        time.sleep(RETRY_INTERVAL)
    assert pod_restarted</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_rebuild_complete"><code class="name flex">
<span>def <span class="ident">wait_for_rebuild_complete</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_rebuild_complete(client, volume_name):
    completed = 0
    rebuild_statuses = {}
    for i in range(RETRY_COUNTS):
        completed = 0
        v = client.by_id_volume(volume_name)
        rebuild_statuses = v.rebuildStatus
        for status in rebuild_statuses:
            if status.state == &#34;complete&#34;:
                assert status.progress == 100
                assert not status.error
                assert not status.isRebuilding
                completed += 1
            elif status.state == &#34;&#34;:
                assert not status.error
                assert not status.isRebuilding
                completed += 1
            elif status.state == &#34;in_progress&#34;:
                assert status.isRebuilding
            else:
                assert status.state == &#34;error&#34;
                assert status.error != &#34;&#34;
                assert not status.isRebuilding
        if completed == len(rebuild_statuses):
            break
        time.sleep(RETRY_INTERVAL)
    assert completed == len(rebuild_statuses)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_rebuild_start"><code class="name flex">
<span>def <span class="ident">wait_for_rebuild_start</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_rebuild_start(client, volume_name):
    started = False
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        rebuild_statuses = v.rebuildStatus
        for status in rebuild_statuses:
            if status.state == &#34;in_progress&#34;:
                started = True
                break
        if started:
            break
        time.sleep(RETRY_INTERVAL)
    assert started
    return status.fromReplica, status.replica</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_replica_directory"><code class="name flex">
<span>def <span class="ident">wait_for_replica_directory</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_replica_directory():
    found = False
    for i in range(RETRY_COUNTS):
        if os.path.exists(DEFAULT_REPLICA_DIRECTORY):
            found = True
            break
        time.sleep(RETRY_INTERVAL)
    assert found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_replica_failed"><code class="name flex">
<span>def <span class="ident">wait_for_replica_failed</span></span>(<span>client, volname, replica_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_replica_failed(client, volname, replica_name):
    failed = True
    for i in range(RETRY_COUNTS):
        time.sleep(RETRY_INTERVAL)
        failed = True
        volume = client.by_id_volume(volname)
        for r in volume.replicas:
            if r[&#39;name&#39;] != replica_name:
                continue
            if r[&#39;running&#39;] or r[&#39;failedAt&#39;] == &#34;&#34;:
                failed = False
                break
            if r[&#39;instanceManagerName&#39;] != &#34;&#34;:
                im = client.by_id_instance_manager(
                    r[&#39;instanceManagerName&#39;])
                if r[&#39;name&#39;] in im[&#39;instances&#39;]:
                    failed = False
                    break
        if failed:
            break
    assert failed</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_replica_running"><code class="name flex">
<span>def <span class="ident">wait_for_replica_running</span></span>(<span>client, volname, replica_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_replica_running(client, volname, replica_name):
    is_running = False
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volname)
        for r in volume.replicas:
            if r[&#39;name&#39;] != replica_name:
                continue
            if r[&#39;running&#39;] and r[&#39;instanceManagerName&#39;] != &#34;&#34;:
                im = client.by_id_instance_manager(
                    r[&#39;instanceManagerName&#39;])
                if r[&#39;name&#39;] in im[&#39;instances&#39;]:
                    is_running = True
                    break
        if is_running:
            break
        time.sleep(RETRY_INTERVAL)
    assert is_running</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_snapshot_purge"><code class="name flex">
<span>def <span class="ident">wait_for_snapshot_purge</span></span>(<span>client, volume_name, *snaps)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_snapshot_purge(client, volume_name, *snaps):
    completed = 0
    last_purge_progress = {}
    purge_status = {}
    for i in range(RETRY_COUNTS):
        completed = 0
        v = client.by_id_volume(volume_name)
        purge_status = v.purgeStatus
        for status in purge_status:
            assert status.error == &#34;&#34;

            progress = status.progress
            assert progress &lt;= 100
            replica = status.replica
            last = last_purge_progress.get(replica)
            assert last is None or last &lt;= status.progress
            last_purge_progress[&#34;replica&#34;] = progress

            if status.state == &#34;complete&#34;:
                assert progress == 100
                completed += 1
        if completed == len(purge_status):
            break
        time.sleep(RETRY_INTERVAL)
    assert completed == len(purge_status)

    # Now that the purge has been reported to be completed, the Snapshots
    # should should be removed or &#34;marked as removed&#34; in the case of
    # the latest snapshot.
    found = False
    snapshots = v.snapshotList(volume=volume_name)

    for snap in snaps:
        for vs in snapshots.data:
            if snap == vs[&#34;name&#34;]:
                if vs[&#34;removed&#34;] is False:
                    found = True
                    break

                if &#34;volume-head&#34; not in vs[&#34;children&#34;]:
                    found = True
                    break
    assert not found
    return v</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_condition_restore"><code class="name flex">
<span>def <span class="ident">wait_for_volume_condition_restore</span></span>(<span>client, name, key, value)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_condition_restore(client, name, key, value):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        conditions = volume.conditions
        if conditions is not None and \
                conditions != {} and \
                conditions[VOLUME_CONDITION_RESTORE] and \
                conditions[VOLUME_CONDITION_RESTORE][key] and \
                conditions[VOLUME_CONDITION_RESTORE][key] == value:
            break
        time.sleep(RETRY_INTERVAL)
    conditions = volume.conditions
    assert conditions[VOLUME_CONDITION_RESTORE][key] == value
    return volume</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_condition_scheduled"><code class="name flex">
<span>def <span class="ident">wait_for_volume_condition_scheduled</span></span>(<span>client, name, key, value)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_condition_scheduled(client, name, key, value):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        conditions = volume.conditions
        if conditions is not None and \
                conditions != {} and \
                conditions[VOLUME_CONDITION_SCHEDULED] and \
                conditions[VOLUME_CONDITION_SCHEDULED][key] and \
                conditions[VOLUME_CONDITION_SCHEDULED][key] == value:
            break
        time.sleep(RETRY_INTERVAL)
    conditions = volume.conditions
    assert conditions[VOLUME_CONDITION_SCHEDULED][key] == value
    return volume</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_creation"><code class="name flex">
<span>def <span class="ident">wait_for_volume_creation</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_creation(client, name):
    for i in range(RETRY_COUNTS):
        volumes = client.list_volume()
        found = False
        for volume in volumes:
            if volume.name == name:
                found = True
                break
        if found:
            break
    assert found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_current_image"><code class="name flex">
<span>def <span class="ident">wait_for_volume_current_image</span></span>(<span>client, name, image)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_current_image(client, name, image):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        if volume.currentImage == image:
            break
        time.sleep(RETRY_INTERVAL)
    assert volume.currentImage == image
    return volume</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_degraded"><code class="name flex">
<span>def <span class="ident">wait_for_volume_degraded</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_degraded(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_STATE,
                           VOLUME_STATE_ATTACHED)
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_ROBUSTNESS,
                                  VOLUME_ROBUSTNESS_DEGRADED)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_delete"><code class="name flex">
<span>def <span class="ident">wait_for_volume_delete</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_delete(client, name):
    for i in range(RETRY_COUNTS):
        volumes = client.list_volume()
        found = False
        for volume in volumes:
            if volume.name == name:
                found = True
                break
        if not found:
            break
        time.sleep(RETRY_INTERVAL)
    assert not found</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_detached"><code class="name flex">
<span>def <span class="ident">wait_for_volume_detached</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_detached(client, name):
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_STATE,
                                  VOLUME_STATE_DETACHED)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_detached_unknown"><code class="name flex">
<span>def <span class="ident">wait_for_volume_detached_unknown</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_detached_unknown(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_ROBUSTNESS,
                           VOLUME_ROBUSTNESS_UNKNOWN)
    return wait_for_volume_detached(client, name)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_endpoint"><code class="name flex">
<span>def <span class="ident">wait_for_volume_endpoint</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_endpoint(client, name):
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(name)
        engine = get_volume_engine(v)
        if engine.endpoint != &#34;&#34;:
            break
        time.sleep(RETRY_INTERVAL)
    check_volume_endpoint(v)
    return v</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_expansion"><code class="name flex">
<span>def <span class="ident">wait_for_volume_expansion</span></span>(<span>longhorn_api_client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_expansion(longhorn_api_client, volume_name):
    complete = False
    for i in range(RETRY_COUNTS):
        volume = longhorn_api_client.by_id_volume(volume_name)
        engine = get_volume_engine(volume)
        if engine.size == volume.size and volume.state == &#34;detached&#34;:
            complete = True
            break
        time.sleep(RETRY_INTERVAL)
    assert complete</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_faulted"><code class="name flex">
<span>def <span class="ident">wait_for_volume_faulted</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_faulted(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_STATE,
                           VOLUME_STATE_DETACHED)
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_ROBUSTNESS,
                                  VOLUME_ROBUSTNESS_FAULTED)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_healthy"><code class="name flex">
<span>def <span class="ident">wait_for_volume_healthy</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_healthy(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_STATE,
                           VOLUME_STATE_ATTACHED)
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_ROBUSTNESS,
                           VOLUME_ROBUSTNESS_HEALTHY)
    return wait_for_volume_endpoint(client, name)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_healthy_no_frontend"><code class="name flex">
<span>def <span class="ident">wait_for_volume_healthy_no_frontend</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_healthy_no_frontend(client, name):
    wait_for_volume_status(client, name,
                           VOLUME_FIELD_STATE,
                           VOLUME_STATE_ATTACHED)
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_ROBUSTNESS,
                                  VOLUME_ROBUSTNESS_HEALTHY)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_migration_node"><code class="name flex">
<span>def <span class="ident">wait_for_volume_migration_node</span></span>(<span>client, volume_name, node_id)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_migration_node(client, volume_name, node_id):
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        engines = v.controllers
        replicas = v.replicas
        if len(engines) == 1 and len(replicas) == v.numberOfReplicas:
            e = engines[0]
            if e.endpoint != &#34;&#34;:
                break
        time.sleep(RETRY_INTERVAL)
    assert e.hostId == node_id
    assert e.endpoint != &#34;&#34;
    return v</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_migration_ready"><code class="name flex">
<span>def <span class="ident">wait_for_volume_migration_ready</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_migration_ready(client, volume_name):
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        engines = v.controllers
        ready = True
        if len(engines) == 2:
            for e in v.controllers:
                if e.endpoint == &#34;&#34;:
                    ready = False
                    break
        else:
            ready = False
        if ready:
            break
        time.sleep(RETRY_INTERVAL)
    assert ready
    return v</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_replica_count"><code class="name flex">
<span>def <span class="ident">wait_for_volume_replica_count</span></span>(<span>client, name, count)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_replica_count(client, name, count):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        if len(volume.replicas) == count:
            break
        time.sleep(RETRY_INTERVAL)
    assert len(volume.replicas) == count
    return volume</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_replicas_mode"><code class="name flex">
<span>def <span class="ident">wait_for_volume_replicas_mode</span></span>(<span>client, volname, mode, replicas_name=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_replicas_mode(client, volname, mode, replicas_name=None):
    verified = False
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volname)
        count = 0
        replicas = []
        if replicas_name is None:
            replicas = volume.replicas
        else:
            for r_name in replicas_name:
                found = False
                for r in volume.replicas:
                    if r.name == r_name:
                        replicas.append(r)
                        found = True
                assert found
        for r in replicas:
            if r.mode == mode:
                count += 1
        if count == len(replicas):
            verified = True
            break
        time.sleep(RETRY_INTERVAL)

    assert verified
    return volume</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_restoration_completed"><code class="name flex">
<span>def <span class="ident">wait_for_volume_restoration_completed</span></span>(<span>client, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_restoration_completed(client, name):
    wait_for_volume_creation(client, name)
    monitor_restore_progress(client, name)
    return wait_for_volume_status(client, name,
                                  VOLUME_FIELD_INITIALRESTORATIONREQUIRED,
                                  False)</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_restoration_start"><code class="name flex">
<span>def <span class="ident">wait_for_volume_restoration_start</span></span>(<span>client, volume_name, backup_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_restoration_start(client, volume_name, backup_name):
    wait_for_volume_status(client, volume_name,
                           VOLUME_FIELD_STATE, VOLUME_STATE_ATTACHED)
    started = False
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(volume_name)
        for status in volume.restoreStatus:
            if status.state == &#34;in_progress&#34; and \
                    status.progress &gt; 0:
                started = True
                break
        #  Sometime the restore time is pretty short
        #  and the test may not be able to catch the intermediate status.
        if volume.controllers[0].lastRestoredBackup == backup_name:
            started = True
        if started:
            break
        time.sleep(RETRY_INTERVAL)
    assert started
    return status.replica</code></pre>
</details>
</dd>
<dt id="tests.common.wait_for_volume_status"><code class="name flex">
<span>def <span class="ident">wait_for_volume_status</span></span>(<span>client, name, key, value)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_for_volume_status(client, name, key, value):
    wait_for_volume_creation(client, name)
    for i in range(RETRY_COUNTS):
        volume = client.by_id_volume(name)
        if volume[key] == value:
            break
        time.sleep(RETRY_INTERVAL)
    assert volume[key] == value
    return volume</code></pre>
</details>
</dd>
<dt id="tests.common.wait_pod"><code class="name flex">
<span>def <span class="ident">wait_pod</span></span>(<span>pod_name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_pod(pod_name):
    api = get_core_api_client()

    for i in range(DEFAULT_POD_TIMEOUT):
        pod = api.read_namespaced_pod(
            name=pod_name,
            namespace=&#39;default&#39;)
        if pod.status.phase != &#39;Pending&#39;:
            break
        time.sleep(DEFAULT_POD_INTERVAL)
    assert pod.status.phase == &#39;Running&#39;</code></pre>
</details>
</dd>
<dt id="tests.common.wait_scheduling_failure"><code class="name flex">
<span>def <span class="ident">wait_scheduling_failure</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Wait and make sure no new replicas are running on the specified
volume. Trigger a failed assertion of one is detected.
:param client: The Longhorn client to use in the request.
:param volume_name: The name of the volume.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_scheduling_failure(client, volume_name):
    &#34;&#34;&#34;
    Wait and make sure no new replicas are running on the specified
    volume. Trigger a failed assertion of one is detected.
    :param client: The Longhorn client to use in the request.
    :param volume_name: The name of the volume.
    &#34;&#34;&#34;
    scheduling_failure = False
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        if v.conditions.scheduled.status == &#34;False&#34; and \
                v.conditions.scheduled.reason == \
                &#34;ReplicaSchedulingFailure&#34;:
            scheduling_failure = True
        if scheduling_failure:
            break
        time.sleep(RETRY_INTERVAL)
    assert scheduling_failure</code></pre>
</details>
</dd>
<dt id="tests.common.wait_statefulset"><code class="name flex">
<span>def <span class="ident">wait_statefulset</span></span>(<span>statefulset_manifest)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_statefulset(statefulset_manifest):
    api = get_apps_api_client()
    replicas = statefulset_manifest[&#39;spec&#39;][&#39;replicas&#39;]
    for i in range(DEFAULT_STATEFULSET_TIMEOUT):
        s_set = api.read_namespaced_stateful_set(
            name=statefulset_manifest[&#39;metadata&#39;][&#39;name&#39;],
            namespace=&#39;default&#39;)
        if s_set.status.ready_replicas == replicas:
            break
        time.sleep(DEFAULT_STATEFULSET_INTERVAL)
    assert s_set.status.ready_replicas == replicas</code></pre>
</details>
</dd>
<dt id="tests.common.wait_volume_kubernetes_status"><code class="name flex">
<span>def <span class="ident">wait_volume_kubernetes_status</span></span>(<span>client, volume_name, expect_ks)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_volume_kubernetes_status(client, volume_name, expect_ks):
    for i in range(RETRY_COUNTS):
        expected = True
        volume = client.by_id_volume(volume_name)
        ks = volume.kubernetesStatus
        ks = json.loads(json.dumps(ks, default=lambda o: o.__dict__))

        for k, v in expect_ks.items():
            if k in (&#39;lastPVCRefAt&#39;, &#39;lastPodRefAt&#39;):
                if (v != &#39;&#39; and ks[k] == &#39;&#39;) or \
                   (v == &#39;&#39; and ks[k] != &#39;&#39;):
                    expected = False
                    break
            else:
                if ks[k] != v:
                    expected = False
                    break
        if expected:
            break
        time.sleep(RETRY_INTERVAL)
    assert expected</code></pre>
</details>
</dd>
<dt id="tests.common.write_device_random_data"><code class="name flex">
<span>def <span class="ident">write_device_random_data</span></span>(<span>dev, position={})</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_device_random_data(dev, position={}):
    data = generate_random_data(VOLUME_RWTEST_SIZE)
    data_pos = generate_random_pos(VOLUME_RWTEST_SIZE, position)
    data_len = dev_write(dev, data_pos, data)
    checksum = get_device_checksum(dev)

    return {
        &#39;content&#39;: data,
        &#39;pos&#39;: data_pos,
        &#39;len&#39;: data_len,
        &#39;checksum&#39;: checksum
    }</code></pre>
</details>
</dd>
<dt id="tests.common.write_pod_block_volume_data"><code class="name flex">
<span>def <span class="ident">write_pod_block_volume_data</span></span>(<span>api, pod_name, test_data, offset, device_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_pod_block_volume_data(api, pod_name, test_data, offset, device_path):
    tmp_file = &#39;/var/test_data&#39;
    pre_write_cmd = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;echo -ne &#39; + test_data + &#39; &gt; &#39; + tmp_file
    ]
    write_cmd = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;dd if=&#39; + tmp_file + &#39; of=&#39; + device_path +
        &#39; bs=&#39; + str(len(test_data)) + &#39; count=1 seek=&#39; + str(offset)
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream write&#39;):
        stream(api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
               command=pre_write_cmd, stderr=True, stdin=False, stdout=True,
               tty=False)
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=write_cmd, stderr=True, stdin=False, stdout=True,
            tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.write_pod_volume_data"><code class="name flex">
<span>def <span class="ident">write_pod_volume_data</span></span>(<span>api, pod_name, test_data, filename='test')</span>
</code></dt>
<dd>
<div class="desc"><p>Write data into a Pod's volume.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>api</code></strong></dt>
<dd>An instance of CoreV1API.</dd>
<dt><strong><code>pod_name</code></strong></dt>
<dd>The name of the Pod.</dd>
<dt><strong><code>test_data</code></strong></dt>
<dd>The data to be written.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_pod_volume_data(api, pod_name, test_data, filename=&#39;test&#39;):
    &#34;&#34;&#34;
    Write data into a Pod&#39;s volume.

    Args:
        api: An instance of CoreV1API.
        pod_name: The name of the Pod.
        test_data: The data to be written.
    &#34;&#34;&#34;
    write_command = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;echo -ne &#39; + test_data + &#39; &gt; /data/&#39; + filename
    ]
    with timeout(seconds=STREAM_EXEC_TIMEOUT,
                 error_message=&#39;Timeout on executing stream write&#39;):
        return stream(
            api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
            command=write_command, stderr=True, stdin=False, stdout=True,
            tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.write_pod_volume_random_data"><code class="name flex">
<span>def <span class="ident">write_pod_volume_random_data</span></span>(<span>api, pod_name, path, size_in_mb)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_pod_volume_random_data(api, pod_name, path, size_in_mb):
    write_cmd = [
        &#39;/bin/sh&#39;,
        &#39;-c&#39;,
        &#39;dd if=/dev/urandom of=&#39; + path +
        &#39; bs=1M&#39; + &#39; count=&#39; + str(size_in_mb)
    ]
    return stream(
        api.connect_get_namespaced_pod_exec, pod_name, &#39;default&#39;,
        command=write_cmd, stderr=True, stdin=False, stdout=True,
        tty=False)</code></pre>
</details>
</dd>
<dt id="tests.common.write_volume_data"><code class="name flex">
<span>def <span class="ident">write_volume_data</span></span>(<span>volume, data)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_volume_data(volume, data):
    dev = get_volume_endpoint(volume)
    data_len = dev_write(dev, data[&#39;pos&#39;], data[&#39;content&#39;])
    checksum = get_device_checksum(dev)

    return {
        &#39;content&#39;: data[&#39;content&#39;],
        &#39;pos&#39;: data[&#39;pos&#39;],
        &#39;len&#39;: data_len,
        &#39;checksum&#39;: checksum
    }</code></pre>
</details>
</dd>
<dt id="tests.common.write_volume_random_data"><code class="name flex">
<span>def <span class="ident">write_volume_random_data</span></span>(<span>volume, position={})</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_volume_random_data(volume, position={}):
    dev = get_volume_endpoint(volume)
    return write_device_random_data(dev, position={})</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tests.common.timeout"><code class="flex name class">
<span>class <span class="ident">timeout</span></span>
<span>(</span><span>seconds=1, error_message='Timeout')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class timeout:

    def __init__(self, seconds=1, error_message=&#39;Timeout&#39;):
        self.seconds = seconds
        self.error_message = error_message

    def handle_timeout(self, signum, frame):
        raise Exception(self.error_message)

    def __enter__(self):
        signal.signal(signal.SIGALRM, self.handle_timeout)
        signal.alarm(self.seconds)

    def __exit__(self, type, value, traceback):
        signal.alarm(0)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tests.common.timeout.handle_timeout"><code class="name flex">
<span>def <span class="ident">handle_timeout</span></span>(<span>self, signum, frame)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_timeout(self, signum, frame):
    raise Exception(self.error_message)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.common.activate_standby_volume" href="#tests.common.activate_standby_volume">activate_standby_volume</a></code></li>
<li><code><a title="tests.common.apps_api" href="#tests.common.apps_api">apps_api</a></code></li>
<li><code><a title="tests.common.assert_backup_state" href="#tests.common.assert_backup_state">assert_backup_state</a></code></li>
<li><code><a title="tests.common.check_block_device_size" href="#tests.common.check_block_device_size">check_block_device_size</a></code></li>
<li><code><a title="tests.common.check_csi" href="#tests.common.check_csi">check_csi</a></code></li>
<li><code><a title="tests.common.check_csi_expansion" href="#tests.common.check_csi_expansion">check_csi_expansion</a></code></li>
<li><code><a title="tests.common.check_device_data" href="#tests.common.check_device_data">check_device_data</a></code></li>
<li><code><a title="tests.common.check_longhorn" href="#tests.common.check_longhorn">check_longhorn</a></code></li>
<li><code><a title="tests.common.check_pod_existence" href="#tests.common.check_pod_existence">check_pod_existence</a></code></li>
<li><code><a title="tests.common.check_pv_existence" href="#tests.common.check_pv_existence">check_pv_existence</a></code></li>
<li><code><a title="tests.common.check_pvc_existence" href="#tests.common.check_pvc_existence">check_pvc_existence</a></code></li>
<li><code><a title="tests.common.check_statefulset_existence" href="#tests.common.check_statefulset_existence">check_statefulset_existence</a></code></li>
<li><code><a title="tests.common.check_volume_data" href="#tests.common.check_volume_data">check_volume_data</a></code></li>
<li><code><a title="tests.common.check_volume_endpoint" href="#tests.common.check_volume_endpoint">check_volume_endpoint</a></code></li>
<li><code><a title="tests.common.check_volume_existence" href="#tests.common.check_volume_existence">check_volume_existence</a></code></li>
<li><code><a title="tests.common.check_volume_last_backup" href="#tests.common.check_volume_last_backup">check_volume_last_backup</a></code></li>
<li><code><a title="tests.common.check_volume_replicas" href="#tests.common.check_volume_replicas">check_volume_replicas</a></code></li>
<li><code><a title="tests.common.cleanup_client" href="#tests.common.cleanup_client">cleanup_client</a></code></li>
<li><code><a title="tests.common.cleanup_host_disk" href="#tests.common.cleanup_host_disk">cleanup_host_disk</a></code></li>
<li><code><a title="tests.common.cleanup_node_disks" href="#tests.common.cleanup_node_disks">cleanup_node_disks</a></code></li>
<li><code><a title="tests.common.cleanup_test_disks" href="#tests.common.cleanup_test_disks">cleanup_test_disks</a></code></li>
<li><code><a title="tests.common.cleanup_volume" href="#tests.common.cleanup_volume">cleanup_volume</a></code></li>
<li><code><a title="tests.common.client" href="#tests.common.client">client</a></code></li>
<li><code><a title="tests.common.clients" href="#tests.common.clients">clients</a></code></li>
<li><code><a title="tests.common.copy_pod_volume_data" href="#tests.common.copy_pod_volume_data">copy_pod_volume_data</a></code></li>
<li><code><a title="tests.common.core_api" href="#tests.common.core_api">core_api</a></code></li>
<li><code><a title="tests.common.crash_engine_process_with_sigkill" href="#tests.common.crash_engine_process_with_sigkill">crash_engine_process_with_sigkill</a></code></li>
<li><code><a title="tests.common.crash_replica_processes" href="#tests.common.crash_replica_processes">crash_replica_processes</a></code></li>
<li><code><a title="tests.common.create_and_check_volume" href="#tests.common.create_and_check_volume">create_and_check_volume</a></code></li>
<li><code><a title="tests.common.create_and_wait_deployment" href="#tests.common.create_and_wait_deployment">create_and_wait_deployment</a></code></li>
<li><code><a title="tests.common.create_and_wait_pod" href="#tests.common.create_and_wait_pod">create_and_wait_pod</a></code></li>
<li><code><a title="tests.common.create_and_wait_statefulset" href="#tests.common.create_and_wait_statefulset">create_and_wait_statefulset</a></code></li>
<li><code><a title="tests.common.create_backup" href="#tests.common.create_backup">create_backup</a></code></li>
<li><code><a title="tests.common.create_pv_for_volume" href="#tests.common.create_pv_for_volume">create_pv_for_volume</a></code></li>
<li><code><a title="tests.common.create_pvc" href="#tests.common.create_pvc">create_pvc</a></code></li>
<li><code><a title="tests.common.create_pvc_for_volume" href="#tests.common.create_pvc_for_volume">create_pvc_for_volume</a></code></li>
<li><code><a title="tests.common.create_pvc_spec" href="#tests.common.create_pvc_spec">create_pvc_spec</a></code></li>
<li><code><a title="tests.common.create_snapshot" href="#tests.common.create_snapshot">create_snapshot</a></code></li>
<li><code><a title="tests.common.create_storage_class" href="#tests.common.create_storage_class">create_storage_class</a></code></li>
<li><code><a title="tests.common.csi_pv" href="#tests.common.csi_pv">csi_pv</a></code></li>
<li><code><a title="tests.common.csi_pv_baseimage" href="#tests.common.csi_pv_baseimage">csi_pv_baseimage</a></code></li>
<li><code><a title="tests.common.csi_pvc_name" href="#tests.common.csi_pvc_name">csi_pvc_name</a></code></li>
<li><code><a title="tests.common.delete_and_wait_deployment" href="#tests.common.delete_and_wait_deployment">delete_and_wait_deployment</a></code></li>
<li><code><a title="tests.common.delete_and_wait_longhorn" href="#tests.common.delete_and_wait_longhorn">delete_and_wait_longhorn</a></code></li>
<li><code><a title="tests.common.delete_and_wait_pod" href="#tests.common.delete_and_wait_pod">delete_and_wait_pod</a></code></li>
<li><code><a title="tests.common.delete_and_wait_pv" href="#tests.common.delete_and_wait_pv">delete_and_wait_pv</a></code></li>
<li><code><a title="tests.common.delete_and_wait_pvc" href="#tests.common.delete_and_wait_pvc">delete_and_wait_pvc</a></code></li>
<li><code><a title="tests.common.delete_and_wait_statefulset" href="#tests.common.delete_and_wait_statefulset">delete_and_wait_statefulset</a></code></li>
<li><code><a title="tests.common.delete_and_wait_volume_attachment" href="#tests.common.delete_and_wait_volume_attachment">delete_and_wait_volume_attachment</a></code></li>
<li><code><a title="tests.common.delete_backup" href="#tests.common.delete_backup">delete_backup</a></code></li>
<li><code><a title="tests.common.delete_backup_volume" href="#tests.common.delete_backup_volume">delete_backup_volume</a></code></li>
<li><code><a title="tests.common.delete_replica_processes" href="#tests.common.delete_replica_processes">delete_replica_processes</a></code></li>
<li><code><a title="tests.common.delete_storage_class" href="#tests.common.delete_storage_class">delete_storage_class</a></code></li>
<li><code><a title="tests.common.dev_read" href="#tests.common.dev_read">dev_read</a></code></li>
<li><code><a title="tests.common.dev_write" href="#tests.common.dev_write">dev_write</a></code></li>
<li><code><a title="tests.common.disable_auto_salvage" href="#tests.common.disable_auto_salvage">disable_auto_salvage</a></code></li>
<li><code><a title="tests.common.exec_instance_manager" href="#tests.common.exec_instance_manager">exec_instance_manager</a></code></li>
<li><code><a title="tests.common.exec_nsenter" href="#tests.common.exec_nsenter">exec_nsenter</a></code></li>
<li><code><a title="tests.common.expand_and_wait_for_pvc" href="#tests.common.expand_and_wait_for_pvc">expand_and_wait_for_pvc</a></code></li>
<li><code><a title="tests.common.expand_attached_volume" href="#tests.common.expand_attached_volume">expand_attached_volume</a></code></li>
<li><code><a title="tests.common.fail_replica_expansion" href="#tests.common.fail_replica_expansion">fail_replica_expansion</a></code></li>
<li><code><a title="tests.common.find_ancestor_process_by_name" href="#tests.common.find_ancestor_process_by_name">find_ancestor_process_by_name</a></code></li>
<li><code><a title="tests.common.find_backup" href="#tests.common.find_backup">find_backup</a></code></li>
<li><code><a title="tests.common.find_dockerd_pid" href="#tests.common.find_dockerd_pid">find_dockerd_pid</a></code></li>
<li><code><a title="tests.common.find_self" href="#tests.common.find_self">find_self</a></code></li>
<li><code><a title="tests.common.generate_pod_with_pvc_manifest" href="#tests.common.generate_pod_with_pvc_manifest">generate_pod_with_pvc_manifest</a></code></li>
<li><code><a title="tests.common.generate_random_data" href="#tests.common.generate_random_data">generate_random_data</a></code></li>
<li><code><a title="tests.common.generate_random_pos" href="#tests.common.generate_random_pos">generate_random_pos</a></code></li>
<li><code><a title="tests.common.generate_volume_name" href="#tests.common.generate_volume_name">generate_volume_name</a></code></li>
<li><code><a title="tests.common.get_apps_api_client" href="#tests.common.get_apps_api_client">get_apps_api_client</a></code></li>
<li><code><a title="tests.common.get_backupstore_url" href="#tests.common.get_backupstore_url">get_backupstore_url</a></code></li>
<li><code><a title="tests.common.get_client" href="#tests.common.get_client">get_client</a></code></li>
<li><code><a title="tests.common.get_clients" href="#tests.common.get_clients">get_clients</a></code></li>
<li><code><a title="tests.common.get_compatibility_test_image" href="#tests.common.get_compatibility_test_image">get_compatibility_test_image</a></code></li>
<li><code><a title="tests.common.get_core_api_client" href="#tests.common.get_core_api_client">get_core_api_client</a></code></li>
<li><code><a title="tests.common.get_default_engine_image" href="#tests.common.get_default_engine_image">get_default_engine_image</a></code></li>
<li><code><a title="tests.common.get_device_checksum" href="#tests.common.get_device_checksum">get_device_checksum</a></code></li>
<li><code><a title="tests.common.get_host_disk_size" href="#tests.common.get_host_disk_size">get_host_disk_size</a></code></li>
<li><code><a title="tests.common.get_iscsi_ip" href="#tests.common.get_iscsi_ip">get_iscsi_ip</a></code></li>
<li><code><a title="tests.common.get_iscsi_lun" href="#tests.common.get_iscsi_lun">get_iscsi_lun</a></code></li>
<li><code><a title="tests.common.get_iscsi_port" href="#tests.common.get_iscsi_port">get_iscsi_port</a></code></li>
<li><code><a title="tests.common.get_iscsi_target" href="#tests.common.get_iscsi_target">get_iscsi_target</a></code></li>
<li><code><a title="tests.common.get_liveness_probe_spec" href="#tests.common.get_liveness_probe_spec">get_liveness_probe_spec</a></code></li>
<li><code><a title="tests.common.get_longhorn_api_client" href="#tests.common.get_longhorn_api_client">get_longhorn_api_client</a></code></li>
<li><code><a title="tests.common.get_mgr_ips" href="#tests.common.get_mgr_ips">get_mgr_ips</a></code></li>
<li><code><a title="tests.common.get_pod_data_md5sum" href="#tests.common.get_pod_data_md5sum">get_pod_data_md5sum</a></code></li>
<li><code><a title="tests.common.get_process_info" href="#tests.common.get_process_info">get_process_info</a></code></li>
<li><code><a title="tests.common.get_random_client" href="#tests.common.get_random_client">get_random_client</a></code></li>
<li><code><a title="tests.common.get_scheduling_api_client" href="#tests.common.get_scheduling_api_client">get_scheduling_api_client</a></code></li>
<li><code><a title="tests.common.get_self_host_id" href="#tests.common.get_self_host_id">get_self_host_id</a></code></li>
<li><code><a title="tests.common.get_statefulset_pod_info" href="#tests.common.get_statefulset_pod_info">get_statefulset_pod_info</a></code></li>
<li><code><a title="tests.common.get_storage_api_client" href="#tests.common.get_storage_api_client">get_storage_api_client</a></code></li>
<li><code><a title="tests.common.get_update_disks" href="#tests.common.get_update_disks">get_update_disks</a></code></li>
<li><code><a title="tests.common.get_upgrade_test_image" href="#tests.common.get_upgrade_test_image">get_upgrade_test_image</a></code></li>
<li><code><a title="tests.common.get_version_api_client" href="#tests.common.get_version_api_client">get_version_api_client</a></code></li>
<li><code><a title="tests.common.get_volume_attached_nodes" href="#tests.common.get_volume_attached_nodes">get_volume_attached_nodes</a></code></li>
<li><code><a title="tests.common.get_volume_endpoint" href="#tests.common.get_volume_endpoint">get_volume_endpoint</a></code></li>
<li><code><a title="tests.common.get_volume_engine" href="#tests.common.get_volume_engine">get_volume_engine</a></code></li>
<li><code><a title="tests.common.get_volume_name" href="#tests.common.get_volume_name">get_volume_name</a></code></li>
<li><code><a title="tests.common.is_backupTarget_nfs" href="#tests.common.is_backupTarget_nfs">is_backupTarget_nfs</a></code></li>
<li><code><a title="tests.common.is_backupTarget_s3" href="#tests.common.is_backupTarget_s3">is_backupTarget_s3</a></code></li>
<li><code><a title="tests.common.iscsi_login" href="#tests.common.iscsi_login">iscsi_login</a></code></li>
<li><code><a title="tests.common.iscsi_logout" href="#tests.common.iscsi_logout">iscsi_logout</a></code></li>
<li><code><a title="tests.common.json_string_go_to_python" href="#tests.common.json_string_go_to_python">json_string_go_to_python</a></code></li>
<li><code><a title="tests.common.lazy_umount_disk" href="#tests.common.lazy_umount_disk">lazy_umount_disk</a></code></li>
<li><code><a title="tests.common.load_k8s_config" href="#tests.common.load_k8s_config">load_k8s_config</a></code></li>
<li><code><a title="tests.common.make_deployment_with_pvc" href="#tests.common.make_deployment_with_pvc">make_deployment_with_pvc</a></code></li>
<li><code><a title="tests.common.monitor_restore_progress" href="#tests.common.monitor_restore_progress">monitor_restore_progress</a></code></li>
<li><code><a title="tests.common.mount_disk" href="#tests.common.mount_disk">mount_disk</a></code></li>
<li><code><a title="tests.common.mount_nfs_backupstore" href="#tests.common.mount_nfs_backupstore">mount_nfs_backupstore</a></code></li>
<li><code><a title="tests.common.node_default_tags" href="#tests.common.node_default_tags">node_default_tags</a></code></li>
<li><code><a title="tests.common.parse_iscsi_endpoint" href="#tests.common.parse_iscsi_endpoint">parse_iscsi_endpoint</a></code></li>
<li><code><a title="tests.common.pod" href="#tests.common.pod">pod</a></code></li>
<li><code><a title="tests.common.pod_make" href="#tests.common.pod_make">pod_make</a></code></li>
<li><code><a title="tests.common.prepare_host_disk" href="#tests.common.prepare_host_disk">prepare_host_disk</a></code></li>
<li><code><a title="tests.common.prepare_pod_with_data_in_mb" href="#tests.common.prepare_pod_with_data_in_mb">prepare_pod_with_data_in_mb</a></code></li>
<li><code><a title="tests.common.priority_class" href="#tests.common.priority_class">priority_class</a></code></li>
<li><code><a title="tests.common.pvc" href="#tests.common.pvc">pvc</a></code></li>
<li><code><a title="tests.common.pvc_baseimage" href="#tests.common.pvc_baseimage">pvc_baseimage</a></code></li>
<li><code><a title="tests.common.pvc_name" href="#tests.common.pvc_name">pvc_name</a></code></li>
<li><code><a title="tests.common.random_labels" href="#tests.common.random_labels">random_labels</a></code></li>
<li><code><a title="tests.common.read_pod_block_volume_data" href="#tests.common.read_pod_block_volume_data">read_pod_block_volume_data</a></code></li>
<li><code><a title="tests.common.read_volume_data" href="#tests.common.read_volume_data">read_volume_data</a></code></li>
<li><code><a title="tests.common.reset_disks_for_all_nodes" href="#tests.common.reset_disks_for_all_nodes">reset_disks_for_all_nodes</a></code></li>
<li><code><a title="tests.common.reset_engine_image" href="#tests.common.reset_engine_image">reset_engine_image</a></code></li>
<li><code><a title="tests.common.reset_node" href="#tests.common.reset_node">reset_node</a></code></li>
<li><code><a title="tests.common.reset_settings" href="#tests.common.reset_settings">reset_settings</a></code></li>
<li><code><a title="tests.common.scheduling_api" href="#tests.common.scheduling_api">scheduling_api</a></code></li>
<li><code><a title="tests.common.set_backupstore_nfs" href="#tests.common.set_backupstore_nfs">set_backupstore_nfs</a></code></li>
<li><code><a title="tests.common.set_backupstore_s3" href="#tests.common.set_backupstore_s3">set_backupstore_s3</a></code></li>
<li><code><a title="tests.common.set_node_tags" href="#tests.common.set_node_tags">set_node_tags</a></code></li>
<li><code><a title="tests.common.set_random_backupstore" href="#tests.common.set_random_backupstore">set_random_backupstore</a></code></li>
<li><code><a title="tests.common.settings_reset" href="#tests.common.settings_reset">settings_reset</a></code></li>
<li><code><a title="tests.common.size_to_string" href="#tests.common.size_to_string">size_to_string</a></code></li>
<li><code><a title="tests.common.statefulset" href="#tests.common.statefulset">statefulset</a></code></li>
<li><code><a title="tests.common.storage_class" href="#tests.common.storage_class">storage_class</a></code></li>
<li><code><a title="tests.common.umount_disk" href="#tests.common.umount_disk">umount_disk</a></code></li>
<li><code><a title="tests.common.update_statefulset_manifests" href="#tests.common.update_statefulset_manifests">update_statefulset_manifests</a></code></li>
<li><code><a title="tests.common.volume_name" href="#tests.common.volume_name">volume_name</a></code></li>
<li><code><a title="tests.common.volume_read" href="#tests.common.volume_read">volume_read</a></code></li>
<li><code><a title="tests.common.volume_valid" href="#tests.common.volume_valid">volume_valid</a></code></li>
<li><code><a title="tests.common.volume_write" href="#tests.common.volume_write">volume_write</a></code></li>
<li><code><a title="tests.common.wait_and_get_pv_for_pvc" href="#tests.common.wait_and_get_pv_for_pvc">wait_and_get_pv_for_pvc</a></code></li>
<li><code><a title="tests.common.wait_delete_deployment" href="#tests.common.wait_delete_deployment">wait_delete_deployment</a></code></li>
<li><code><a title="tests.common.wait_delete_pod" href="#tests.common.wait_delete_pod">wait_delete_pod</a></code></li>
<li><code><a title="tests.common.wait_delete_pv" href="#tests.common.wait_delete_pv">wait_delete_pv</a></code></li>
<li><code><a title="tests.common.wait_delete_pvc" href="#tests.common.wait_delete_pvc">wait_delete_pvc</a></code></li>
<li><code><a title="tests.common.wait_delete_volume_attachment" href="#tests.common.wait_delete_volume_attachment">wait_delete_volume_attachment</a></code></li>
<li><code><a title="tests.common.wait_deployment_replica_ready" href="#tests.common.wait_deployment_replica_ready">wait_deployment_replica_ready</a></code></li>
<li><code><a title="tests.common.wait_for_backup_completion" href="#tests.common.wait_for_backup_completion">wait_for_backup_completion</a></code></li>
<li><code><a title="tests.common.wait_for_backup_delete" href="#tests.common.wait_for_backup_delete">wait_for_backup_delete</a></code></li>
<li><code><a title="tests.common.wait_for_backup_state" href="#tests.common.wait_for_backup_state">wait_for_backup_state</a></code></li>
<li><code><a title="tests.common.wait_for_backup_volume_delete" href="#tests.common.wait_for_backup_volume_delete">wait_for_backup_volume_delete</a></code></li>
<li><code><a title="tests.common.wait_for_device_login" href="#tests.common.wait_for_device_login">wait_for_device_login</a></code></li>
<li><code><a title="tests.common.wait_for_disk_conditions" href="#tests.common.wait_for_disk_conditions">wait_for_disk_conditions</a></code></li>
<li><code><a title="tests.common.wait_for_disk_status" href="#tests.common.wait_for_disk_status">wait_for_disk_status</a></code></li>
<li><code><a title="tests.common.wait_for_disk_storage_available" href="#tests.common.wait_for_disk_storage_available">wait_for_disk_storage_available</a></code></li>
<li><code><a title="tests.common.wait_for_disk_update" href="#tests.common.wait_for_disk_update">wait_for_disk_update</a></code></li>
<li><code><a title="tests.common.wait_for_disk_uuid" href="#tests.common.wait_for_disk_uuid">wait_for_disk_uuid</a></code></li>
<li><code><a title="tests.common.wait_for_dr_volume_expansion" href="#tests.common.wait_for_dr_volume_expansion">wait_for_dr_volume_expansion</a></code></li>
<li><code><a title="tests.common.wait_for_engine_image_creation" href="#tests.common.wait_for_engine_image_creation">wait_for_engine_image_creation</a></code></li>
<li><code><a title="tests.common.wait_for_engine_image_deletion" href="#tests.common.wait_for_engine_image_deletion">wait_for_engine_image_deletion</a></code></li>
<li><code><a title="tests.common.wait_for_engine_image_ref_count" href="#tests.common.wait_for_engine_image_ref_count">wait_for_engine_image_ref_count</a></code></li>
<li><code><a title="tests.common.wait_for_engine_image_state" href="#tests.common.wait_for_engine_image_state">wait_for_engine_image_state</a></code></li>
<li><code><a title="tests.common.wait_for_expansion_failure" href="#tests.common.wait_for_expansion_failure">wait_for_expansion_failure</a></code></li>
<li><code><a title="tests.common.wait_for_instance_manager_desire_state" href="#tests.common.wait_for_instance_manager_desire_state">wait_for_instance_manager_desire_state</a></code></li>
<li><code><a title="tests.common.wait_for_node_mountpropagation_condition" href="#tests.common.wait_for_node_mountpropagation_condition">wait_for_node_mountpropagation_condition</a></code></li>
<li><code><a title="tests.common.wait_for_node_schedulable_condition" href="#tests.common.wait_for_node_schedulable_condition">wait_for_node_schedulable_condition</a></code></li>
<li><code><a title="tests.common.wait_for_node_tag_update" href="#tests.common.wait_for_node_tag_update">wait_for_node_tag_update</a></code></li>
<li><code><a title="tests.common.wait_for_node_update" href="#tests.common.wait_for_node_update">wait_for_node_update</a></code></li>
<li><code><a title="tests.common.wait_for_pod_remount" href="#tests.common.wait_for_pod_remount">wait_for_pod_remount</a></code></li>
<li><code><a title="tests.common.wait_for_pod_restart" href="#tests.common.wait_for_pod_restart">wait_for_pod_restart</a></code></li>
<li><code><a title="tests.common.wait_for_rebuild_complete" href="#tests.common.wait_for_rebuild_complete">wait_for_rebuild_complete</a></code></li>
<li><code><a title="tests.common.wait_for_rebuild_start" href="#tests.common.wait_for_rebuild_start">wait_for_rebuild_start</a></code></li>
<li><code><a title="tests.common.wait_for_replica_directory" href="#tests.common.wait_for_replica_directory">wait_for_replica_directory</a></code></li>
<li><code><a title="tests.common.wait_for_replica_failed" href="#tests.common.wait_for_replica_failed">wait_for_replica_failed</a></code></li>
<li><code><a title="tests.common.wait_for_replica_running" href="#tests.common.wait_for_replica_running">wait_for_replica_running</a></code></li>
<li><code><a title="tests.common.wait_for_snapshot_purge" href="#tests.common.wait_for_snapshot_purge">wait_for_snapshot_purge</a></code></li>
<li><code><a title="tests.common.wait_for_volume_condition_restore" href="#tests.common.wait_for_volume_condition_restore">wait_for_volume_condition_restore</a></code></li>
<li><code><a title="tests.common.wait_for_volume_condition_scheduled" href="#tests.common.wait_for_volume_condition_scheduled">wait_for_volume_condition_scheduled</a></code></li>
<li><code><a title="tests.common.wait_for_volume_creation" href="#tests.common.wait_for_volume_creation">wait_for_volume_creation</a></code></li>
<li><code><a title="tests.common.wait_for_volume_current_image" href="#tests.common.wait_for_volume_current_image">wait_for_volume_current_image</a></code></li>
<li><code><a title="tests.common.wait_for_volume_degraded" href="#tests.common.wait_for_volume_degraded">wait_for_volume_degraded</a></code></li>
<li><code><a title="tests.common.wait_for_volume_delete" href="#tests.common.wait_for_volume_delete">wait_for_volume_delete</a></code></li>
<li><code><a title="tests.common.wait_for_volume_detached" href="#tests.common.wait_for_volume_detached">wait_for_volume_detached</a></code></li>
<li><code><a title="tests.common.wait_for_volume_detached_unknown" href="#tests.common.wait_for_volume_detached_unknown">wait_for_volume_detached_unknown</a></code></li>
<li><code><a title="tests.common.wait_for_volume_endpoint" href="#tests.common.wait_for_volume_endpoint">wait_for_volume_endpoint</a></code></li>
<li><code><a title="tests.common.wait_for_volume_expansion" href="#tests.common.wait_for_volume_expansion">wait_for_volume_expansion</a></code></li>
<li><code><a title="tests.common.wait_for_volume_faulted" href="#tests.common.wait_for_volume_faulted">wait_for_volume_faulted</a></code></li>
<li><code><a title="tests.common.wait_for_volume_healthy" href="#tests.common.wait_for_volume_healthy">wait_for_volume_healthy</a></code></li>
<li><code><a title="tests.common.wait_for_volume_healthy_no_frontend" href="#tests.common.wait_for_volume_healthy_no_frontend">wait_for_volume_healthy_no_frontend</a></code></li>
<li><code><a title="tests.common.wait_for_volume_migration_node" href="#tests.common.wait_for_volume_migration_node">wait_for_volume_migration_node</a></code></li>
<li><code><a title="tests.common.wait_for_volume_migration_ready" href="#tests.common.wait_for_volume_migration_ready">wait_for_volume_migration_ready</a></code></li>
<li><code><a title="tests.common.wait_for_volume_replica_count" href="#tests.common.wait_for_volume_replica_count">wait_for_volume_replica_count</a></code></li>
<li><code><a title="tests.common.wait_for_volume_replicas_mode" href="#tests.common.wait_for_volume_replicas_mode">wait_for_volume_replicas_mode</a></code></li>
<li><code><a title="tests.common.wait_for_volume_restoration_completed" href="#tests.common.wait_for_volume_restoration_completed">wait_for_volume_restoration_completed</a></code></li>
<li><code><a title="tests.common.wait_for_volume_restoration_start" href="#tests.common.wait_for_volume_restoration_start">wait_for_volume_restoration_start</a></code></li>
<li><code><a title="tests.common.wait_for_volume_status" href="#tests.common.wait_for_volume_status">wait_for_volume_status</a></code></li>
<li><code><a title="tests.common.wait_pod" href="#tests.common.wait_pod">wait_pod</a></code></li>
<li><code><a title="tests.common.wait_scheduling_failure" href="#tests.common.wait_scheduling_failure">wait_scheduling_failure</a></code></li>
<li><code><a title="tests.common.wait_statefulset" href="#tests.common.wait_statefulset">wait_statefulset</a></code></li>
<li><code><a title="tests.common.wait_volume_kubernetes_status" href="#tests.common.wait_volume_kubernetes_status">wait_volume_kubernetes_status</a></code></li>
<li><code><a title="tests.common.write_device_random_data" href="#tests.common.write_device_random_data">write_device_random_data</a></code></li>
<li><code><a title="tests.common.write_pod_block_volume_data" href="#tests.common.write_pod_block_volume_data">write_pod_block_volume_data</a></code></li>
<li><code><a title="tests.common.write_pod_volume_data" href="#tests.common.write_pod_volume_data">write_pod_volume_data</a></code></li>
<li><code><a title="tests.common.write_pod_volume_random_data" href="#tests.common.write_pod_volume_random_data">write_pod_volume_random_data</a></code></li>
<li><code><a title="tests.common.write_volume_data" href="#tests.common.write_volume_data">write_volume_data</a></code></li>
<li><code><a title="tests.common.write_volume_random_data" href="#tests.common.write_volume_random_data">write_volume_random_data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tests.common.timeout" href="#tests.common.timeout">timeout</a></code></h4>
<ul class="">
<li><code><a title="tests.common.timeout.handle_timeout" href="#tests.common.timeout.handle_timeout">handle_timeout</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>