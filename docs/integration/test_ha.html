<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>tests.test_ha API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tests.test_ha</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pytest
import common
import time
import random

from common import client, core_api, volume_name  # NOQA
from common import SIZE, DEV_PATH, VOLUME_RWTEST_SIZE, EXPAND_SIZE, Gi
from common import check_volume_data, cleanup_volume, create_and_check_volume
from common import delete_replica_processes, crash_replica_processes
from common import get_self_host_id, get_volume_endpoint
from common import wait_for_snapshot_purge, write_volume_random_data
from common import RETRY_COUNTS, RETRY_INTERVAL
from common import create_snapshot
from common import expand_attached_volume, check_block_device_size
from common import write_volume_data, generate_random_data
from common import wait_for_rebuild_complete
from common import disable_auto_salvage # NOQA
from common import pod_make  # NOQA
from common import create_pv_for_volume
from common import create_pvc_for_volume
from common import create_pvc_spec
from common import create_and_wait_pod
from common import write_pod_volume_data, write_pod_volume_random_data
from common import wait_for_volume_healthy, wait_for_volume_degraded
from common import read_volume_data
from common import get_pod_data_md5sum
from common import wait_for_pod_remount
from common import get_liveness_probe_spec
from common import delete_and_wait_pod
from common import delete_and_wait_pvc, delete_and_wait_pv
from common import wait_for_rebuild_start
from kubernetes.stream import stream

RANDOM_DATA_SIZE = 300
REBUILD_RETRY_COUNT = 3


@pytest.mark.coretest   # NOQA
def test_ha_simple_recovery(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    [HA] Test recovering from one replica failure

    1. Create volume and attach to the current node
    2. Write `data` to the volume.
    3. Remove one of the replica using Longhorn API
    4. Wait for a new replica to be rebuilt.
    5. Check the volume data
    &#34;&#34;&#34;
    ha_simple_recovery_test(client, volume_name, SIZE)


def ha_simple_recovery_test(client, volume_name, size, base_image=&#34;&#34;):  # NOQA
    volume = create_and_check_volume(client, volume_name, 2, size, base_image)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    ha_rebuild_replica_test(client, volume_name)

    cleanup_volume(client, volume)


def ha_rebuild_replica_test(client, volname):   # NOQA
    volume = client.by_id_volume(volname)
    assert get_volume_endpoint(volume) == DEV_PATH + volname

    assert len(volume.replicas) == 2
    replica0 = volume.replicas[0]
    assert replica0.name != &#34;&#34;

    replica1 = volume.replicas[1]
    assert replica1.name != &#34;&#34;

    data = write_volume_random_data(volume)

    volume = volume.replicaRemove(name=replica0.name)

    # wait until we saw a replica starts rebuilding
    new_replica_found = False
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volname)
        for r in v.replicas:
            if r.name != replica0.name and \
                    r.name != replica1.name:
                new_replica_found = True
                break
        if new_replica_found:
            break
        time.sleep(RETRY_INTERVAL)
    wait_for_rebuild_complete(client, volname)
    assert new_replica_found

    volume = common.wait_for_volume_healthy(client, volname)

    volume = client.by_id_volume(volname)
    assert volume.state == common.VOLUME_STATE_ATTACHED
    assert volume.robustness == common.VOLUME_ROBUSTNESS_HEALTHY
    assert len(volume.replicas) &gt;= 2

    found = False
    for replica in volume.replicas:
        if replica.name == replica1.name:
            found = True
            break
    assert found

    check_volume_data(volume, data)


@pytest.mark.coretest   # NOQA
def test_ha_salvage(client, core_api, volume_name, disable_auto_salvage):  # NOQA
    &#34;&#34;&#34;
    [HA] Test salvage when volume faulted

    Setting: Disable auto salvage

    Case 1: Delete all replica processes using instance manager

    1. Create volume and attach to the current node
    2. Write `data` to the volume.
    3. Crash all the replicas using Instance Manager API
        1. Cannot do it using Longhorn API since a. it will delete data, b. the
    last replica is not allowed to be deleted
    4. Make sure volume detached automatically and changed into `faulted` state
    5. Make sure both replicas reports `failedAt` timestamp.
    6. Salvage the volume
    7. Verify that volume is in `detached` `unknown` state. No longer `faulted`
    8. Verify that all the replicas&#39; `failedAt` timestamp cleaned.
    9. Attach the volume and check `data`

    Case 2: Crash all replica processes

    Same steps as Case 1 except on step 3, use SIGTERM to crash the processes
    &#34;&#34;&#34;
    ha_salvage_test(client, core_api, volume_name)


def ha_salvage_test(client, core_api, # NOQA
                    volume_name, base_image=&#34;&#34;):  # NOQA
    # case: replica processes are wrongly removed
    volume = create_and_check_volume(client, volume_name, 2,
                                     base_image=base_image)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    assert len(volume.replicas) == 2
    replica0_name = volume.replicas[0].name
    replica1_name = volume.replicas[1].name

    data = write_volume_random_data(volume)

    delete_replica_processes(client, core_api, volume_name)

    volume = common.wait_for_volume_faulted(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt != &#34;&#34;
    assert volume.replicas[1].failedAt != &#34;&#34;

    volume.salvage(names=[replica0_name, replica1_name])

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt == &#34;&#34;
    assert volume.replicas[1].failedAt == &#34;&#34;

    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    check_volume_data(volume, data)

    cleanup_volume(client, volume)

    # case: replica processes get crashed
    volume = create_and_check_volume(client, volume_name, 2,
                                     base_image=base_image)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    assert len(volume.replicas) == 2
    replica0_name = volume.replicas[0].name
    replica1_name = volume.replicas[1].name

    data = write_volume_random_data(volume)

    crash_replica_processes(client, core_api, volume_name)

    volume = common.wait_for_volume_faulted(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt != &#34;&#34;
    assert volume.replicas[1].failedAt != &#34;&#34;

    volume.salvage(names=[replica0_name, replica1_name])

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt == &#34;&#34;
    assert volume.replicas[1].failedAt == &#34;&#34;

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    check_volume_data(volume, data)

    cleanup_volume(client, volume)


# https://github.com/rancher/longhorn/issues/253
def test_ha_backup_deletion_recovery(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    [HA] Test deleting the restored snapshot and rebuild

    Backupstore: all

    1. Create volume and attach it to the current node.
    2. Write `data` to the volume and create snapshot `snap2`
    3. Backup `snap2` to create a backup.
    4. Create volume `res_volume` from the backup. Check volume `data`.
    5. Check snapshot chain, make sure `backup_snapshot` exists.
    6. Delete the `backup_snapshot` and purge snapshots.
    7. After purge complete, delete one replica to verify rebuild works.

    FIXME: Needs improvement, e.g. rebuild when no snapshot is deleted for
    restored backup, delete a replica when restoring in progress.
    &#34;&#34;&#34;
    ha_backup_deletion_recovery_test(client, volume_name, SIZE)


def ha_backup_deletion_recovery_test(client, volume_name, size, base_image=&#34;&#34;):  # NOQA
    volume = client.create_volume(name=volume_name, size=size,
                                  numberOfReplicas=2, baseImage=base_image)
    volume = common.wait_for_volume_detached(client, volume_name)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    # test backupTarget for multiple settings
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                    common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                    common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;

        data = write_volume_random_data(volume)
        snap2 = create_snapshot(client, volume_name)
        create_snapshot(client, volume_name)

        volume.snapshotBackup(name=snap2.name)

        _, b = common.find_backup(client, volume_name, snap2.name)

        res_name = common.generate_volume_name()
        res_volume = client.create_volume(name=res_name, size=size,
                                          numberOfReplicas=2,
                                          fromBackup=b.url)
        res_volume = common.wait_for_volume_restoration_completed(
            client, res_name)
        res_volume = common.wait_for_volume_detached(client, res_name)
        res_volume = res_volume.attach(hostId=host_id)
        res_volume = common.wait_for_volume_healthy(client, res_name)
        check_volume_data(res_volume, data)

        snapshots = res_volume.snapshotList()
        # only the backup snapshot + volume-head
        assert len(snapshots) == 2
        backup_snapshot = &#34;&#34;
        for snap in snapshots:
            if snap.name != &#34;volume-head&#34;:
                backup_snapshot = snap.name
        assert backup_snapshot != &#34;&#34;

        create_snapshot(client, res_name)
        snapshots = res_volume.snapshotList()
        assert len(snapshots) == 3

        res_volume.snapshotDelete(name=backup_snapshot)
        res_volume.snapshotPurge()
        res_volume = wait_for_snapshot_purge(client, res_name,
                                             backup_snapshot)

        snapshots = res_volume.snapshotList()
        assert len(snapshots) == 2

        ha_rebuild_replica_test(client, res_name)

        res_volume = res_volume.detach()
        res_volume = common.wait_for_volume_detached(client, res_name)

        client.delete(res_volume)
        common.wait_for_volume_delete(client, res_name)

    cleanup_volume(client, volume)


# https://github.com/rancher/longhorn/issues/415
def test_ha_prohibit_deleting_last_replica(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test prohibiting deleting the last replica

    1. Create volume with one replica and attach to the current node.
    2. Try to delete the replica. It should error out

    FIXME: Move out of test_ha.py
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name, 1)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    assert len(volume.replicas) == 1
    replica0 = volume.replicas[0]

    with pytest.raises(Exception) as e:
        volume.replicaRemove(name=replica0.name)
    assert &#34;no other healthy replica available&#34; in str(e.value)

    cleanup_volume(client, volume)


def test_ha_recovery_with_expansion(client, volume_name):   # NOQA
    &#34;&#34;&#34;
    [HA] Test recovery with volume expansion

    1. Create a volume length `SIZE` and attach to the current node.
    2. Write `data1` to the volume
    3. Expand the volume to `EXPAND_SIZE`, and check volume has been expanded
    4. Write `data2` starting from `SIZE`.
    5. Remove replica0 from volume
    6. Wait volume to start rebuilding and complete
    7. Check the `data1` and `data2`

    FIXME: why on step 6, checked volume.replicas &gt;= 2?
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name, 2, SIZE)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == 2
    replica0 = volume.replicas[0]
    assert replica0.name != &#34;&#34;
    replica1 = volume.replicas[1]
    assert replica1.name != &#34;&#34;

    data1 = write_volume_random_data(volume)

    expand_attached_volume(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))

    data2 = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    data2 = write_volume_data(volume, data2)

    volume.replicaRemove(name=replica0.name)
    # wait until we saw a replica starts rebuilding
    new_replica_found = False
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        for r in v.replicas:
            if r.name != replica0.name and \
                    r.name != replica1.name:
                new_replica_found = True
                break
        if new_replica_found:
            break
        time.sleep(RETRY_INTERVAL)
    wait_for_rebuild_complete(client, volume_name)
    assert new_replica_found

    volume = common.wait_for_volume_healthy(client, volume_name)
    assert volume.state == common.VOLUME_STATE_ATTACHED
    assert volume.robustness == common.VOLUME_ROBUSTNESS_HEALTHY
    assert len(volume.replicas) &gt;= 2

    found = False
    for replica in volume.replicas:
        if replica.name == replica1.name:
            found = True
            break
    assert found

    check_volume_data(volume, data1, False)
    check_volume_data(volume, data2)

    cleanup_volume(client, volume)


def test_salvage_auto_crash_all_replicas(client, core_api, volume_name, pod_make):  # NOQA
    &#34;&#34;&#34;
    [HA] Test automatic salvage feature by crashing all the replicas

    1. Create PV/PVC/POD. Make sure POD has liveness check. Start the pod
    2. Generate `test_data` and write to the pod.
    3. Run `sync` command inside the pod to make sure data flush to the volume.
    4. Crash all replica processes using SIGTERM
    5. Wait for volume to `faulted`, then `detached` `unknown`, then `healthy`
    6. Check replica `failedAt` has been cleared.
    7. Wait for pod to be restarted.
    8. Check pod `test_data`.

    FIXME: Step 5 is only a intermediate state, maybe no way to get it for sure
    &#34;&#34;&#34;
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)

    pod_liveness_probe_spec = get_liveness_probe_spec(initial_delay=1,
                                                      period=1)

    pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name, num_of_replicas=2)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)

    write_pod_volume_data(core_api, pod_name, test_data)

    stream(core_api.connect_get_namespaced_pod_exec,
           pod_name,
           &#39;default&#39;,
           command=&#34;sync&#34;,
           stderr=True, stdin=True,
           stdout=True, tty=True,
           _preload_content=False)

    crash_replica_processes(client, core_api, volume_name)

    volume = common.wait_for_volume_faulted(client, volume_name)

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt == &#34;&#34;
    assert volume.replicas[1].failedAt == &#34;&#34;

    volume = wait_for_volume_healthy(client, volume_name)

    wait_for_pod_remount(core_api, pod_name)

    resp = read_volume_data(core_api, pod_name)

    assert test_data == resp

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, pvc_name)
    delete_and_wait_pv(core_api, pv_name)


# Test case #2: delete one replica process, wait for 5 seconds
# then delete all replica processes.
def test_salvage_auto_crash_replicas_short_wait(client, core_api, volume_name, pod_make):  # NOQA
    &#34;&#34;&#34;
    [HA] Test automatic salvage feature, with replica building pending

    1. Create a PV/PVC/Pod with liveness check.
    2. Create volume and start the pod.
    3. Generate `test_data` and write to the pod.
    4. Run `sync` command inside the pod to make sure data flush to the volume.
    5. Crash one of the replica. Wait for 5 seconds.
    6. Crash all the replicas.
    7. Make sure volume and Pod recovers.
    8. Check `test_data` in the Pod.

    FIXME: step 5 should wait for the replica to start rebuilding.
    &#34;&#34;&#34;
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)

    pod_liveness_probe_spec = get_liveness_probe_spec(initial_delay=1,
                                                      period=1)

    pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name, num_of_replicas=2)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)

    write_pod_volume_data(core_api, pod_name, test_data)

    stream(core_api.connect_get_namespaced_pod_exec,
           pod_name,
           &#39;default&#39;,
           command=&#34;sync&#34;,
           stderr=True, stdin=True,
           stdout=True, tty=True,
           _preload_content=False)

    volume = client.by_id_volume(volume_name)
    replica0 = volume.replicas[0]

    crash_replica_processes(client, core_api, volume_name, [replica0])

    time.sleep(5)

    volume = client.by_id_volume(volume_name)

    replicas = []
    for r in volume.replicas:
        if r.running is True:
            replicas.append(r)

    crash_replica_processes(client, core_api, volume_name, replicas)

    volume = common.wait_for_volume_faulted(client, volume_name)

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt == &#34;&#34;
    assert volume.replicas[1].failedAt == &#34;&#34;

    volume = wait_for_volume_healthy(client, volume_name)

    wait_for_pod_remount(core_api, pod_name)

    resp = read_volume_data(core_api, pod_name)

    assert test_data == resp

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, pvc_name)
    delete_and_wait_pv(core_api, pv_name)


# Test case #3: delete one replica process, wait for 60 seconds
# then delete all replica processes.
def test_salvage_auto_crash_replicas_long_wait(client, core_api, volume_name, pod_make):  # NOQA
    &#34;&#34;&#34;
    [HA] Test automatic salvage feature, with replica building pending

    1. Create a PV/PVC/Pod with liveness check.
    2. Create volume and start the pod.
    3. Generate `test_data` and write to the pod.
    4. Run `sync` command inside the pod to make sure data flush to the volume.
    5. Crash one of the replica. Wait for 60 seconds.
    6. Crash all the replicas.
    7. Make sure volume and Pod recovers.
    8. Check `test_data` in the Pod.

    FIXME: step 5 should wait for the replica to finish rebuilding.
    FIXME: should create common function with the previous couple test cases
    &#34;&#34;&#34;
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)

    pod_liveness_probe_spec = get_liveness_probe_spec(initial_delay=1,
                                                      period=1)

    pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name, num_of_replicas=2)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)

    write_pod_volume_data(core_api, pod_name, test_data)

    stream(core_api.connect_get_namespaced_pod_exec,
           pod_name,
           &#39;default&#39;,
           command=&#34;sync&#34;,
           stderr=True, stdin=True,
           stdout=True, tty=True,
           _preload_content=False)

    volume = client.by_id_volume(volume_name)
    replica0 = volume.replicas[0]

    crash_replica_processes(client, core_api, volume_name, [replica0])

    time.sleep(60)

    volume = client.by_id_volume(volume_name)

    replicas = []
    for r in volume.replicas:
        if r.running is True:
            replicas.append(r)

    crash_replica_processes(client, core_api, volume_name, replicas)

    volume = common.wait_for_volume_faulted(client, volume_name)

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 3

    volume = wait_for_volume_healthy(client, volume_name)

    wait_for_pod_remount(core_api, pod_name)

    resp = read_volume_data(core_api, pod_name)

    assert test_data == resp

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, pvc_name)
    delete_and_wait_pv(core_api, pv_name)



def test_rebuild_failure_with_intensive_data(client, core_api, volume_name, pod_make):  # NOQA
    &#34;&#34;&#34;
    [HA] Test rebuild failure with intensive data writing

    1. Create PV/PVC/POD with livenss check
    2. Create volume and wait for pod to start
    3. Write data to `/data/test1` inside the pod and get `original_checksum_1`
    4. Write data to `/data/test2` inside the pod and get `original_checksum_2`
    5. Find running replicas of the volume
    6. Crash one of the running replicas.
    7. Wait for the replica rebuild to start
    8. Crash another running replicas
    9. Wait for volume to finish two rebuilds and become healthy
    10. Check md5sum for both data location
    11. Go back step 5 and repeat for `REBUILD_RETRY_COUNT` times.
    &#34;&#34;&#34;
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)
    pod_liveness_probe_spec = get_liveness_probe_spec(initial_delay=1,
                                                      period=1)
    pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=3, size=str(1 * Gi))
    assert len(volume.replicas) == 3
    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    data_path_1 = &#34;/data/test1&#34;
    write_pod_volume_random_data(core_api, pod_name,
                                 data_path_1, RANDOM_DATA_SIZE)
    original_md5sum_1 = get_pod_data_md5sum(core_api, pod_name, data_path_1)
    create_snapshot(client, volume_name)
    data_path_2 = &#34;/data/test2&#34;
    write_pod_volume_random_data(core_api, pod_name,
                                 data_path_2, RANDOM_DATA_SIZE)
    original_md5sum_2 = get_pod_data_md5sum(core_api, pod_name, data_path_2)

    for i in range(REBUILD_RETRY_COUNT):
        volume = client.by_id_volume(volume_name)
        replicas = []
        for r in volume.replicas:
            if r.running:
                replicas.append(r)
            else:
                volume.replicaRemove(name=r.name)
        assert len(replicas) == 3
        random.shuffle(replicas)
        # Trigger rebuild
        crash_replica_processes(client, core_api, volume_name, [replicas[0]])
        wait_for_volume_degraded(client, volume_name)
        # Since replicas[1] maybe not the sender during the rebuild,
        # we can try it multiple times to trigger the rebuild failure.
        wait_for_rebuild_start(client, volume_name)
        crash_replica_processes(client, core_api, volume_name, [replicas[1]])
        wait_for_volume_healthy(client, volume_name)
        md5sum_1 = get_pod_data_md5sum(core_api, pod_name, data_path_1)
        assert original_md5sum_1 == md5sum_1
        md5sum_2 = get_pod_data_md5sum(core_api, pod_name, data_path_2)
        assert original_md5sum_2 == md5sum_2

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, pvc_name)
    delete_and_wait_pv(core_api, pv_name)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="tests.test_ha.ha_backup_deletion_recovery_test"><code class="name flex">
<span>def <span class="ident">ha_backup_deletion_recovery_test</span></span>(<span>client, volume_name, size, base_image='')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ha_backup_deletion_recovery_test(client, volume_name, size, base_image=&#34;&#34;):  # NOQA
    volume = client.create_volume(name=volume_name, size=size,
                                  numberOfReplicas=2, baseImage=base_image)
    volume = common.wait_for_volume_detached(client, volume_name)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    setting = client.by_id_setting(common.SETTING_BACKUP_TARGET)
    # test backupTarget for multiple settings
    backupstores = common.get_backupstore_url()
    for backupstore in backupstores:
        if common.is_backupTarget_s3(backupstore):
            backupsettings = backupstore.split(&#34;$&#34;)
            setting = client.update(setting, value=backupsettings[0])
            assert setting.value == backupsettings[0]

            credential = client.by_id_setting(
                    common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=backupsettings[1])
            assert credential.value == backupsettings[1]
        else:
            setting = client.update(setting, value=backupstore)
            assert setting.value == backupstore
            credential = client.by_id_setting(
                    common.SETTING_BACKUP_TARGET_CREDENTIAL_SECRET)
            credential = client.update(credential, value=&#34;&#34;)
            assert credential.value == &#34;&#34;

        data = write_volume_random_data(volume)
        snap2 = create_snapshot(client, volume_name)
        create_snapshot(client, volume_name)

        volume.snapshotBackup(name=snap2.name)

        _, b = common.find_backup(client, volume_name, snap2.name)

        res_name = common.generate_volume_name()
        res_volume = client.create_volume(name=res_name, size=size,
                                          numberOfReplicas=2,
                                          fromBackup=b.url)
        res_volume = common.wait_for_volume_restoration_completed(
            client, res_name)
        res_volume = common.wait_for_volume_detached(client, res_name)
        res_volume = res_volume.attach(hostId=host_id)
        res_volume = common.wait_for_volume_healthy(client, res_name)
        check_volume_data(res_volume, data)

        snapshots = res_volume.snapshotList()
        # only the backup snapshot + volume-head
        assert len(snapshots) == 2
        backup_snapshot = &#34;&#34;
        for snap in snapshots:
            if snap.name != &#34;volume-head&#34;:
                backup_snapshot = snap.name
        assert backup_snapshot != &#34;&#34;

        create_snapshot(client, res_name)
        snapshots = res_volume.snapshotList()
        assert len(snapshots) == 3

        res_volume.snapshotDelete(name=backup_snapshot)
        res_volume.snapshotPurge()
        res_volume = wait_for_snapshot_purge(client, res_name,
                                             backup_snapshot)

        snapshots = res_volume.snapshotList()
        assert len(snapshots) == 2

        ha_rebuild_replica_test(client, res_name)

        res_volume = res_volume.detach()
        res_volume = common.wait_for_volume_detached(client, res_name)

        client.delete(res_volume)
        common.wait_for_volume_delete(client, res_name)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.ha_rebuild_replica_test"><code class="name flex">
<span>def <span class="ident">ha_rebuild_replica_test</span></span>(<span>client, volname)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ha_rebuild_replica_test(client, volname):   # NOQA
    volume = client.by_id_volume(volname)
    assert get_volume_endpoint(volume) == DEV_PATH + volname

    assert len(volume.replicas) == 2
    replica0 = volume.replicas[0]
    assert replica0.name != &#34;&#34;

    replica1 = volume.replicas[1]
    assert replica1.name != &#34;&#34;

    data = write_volume_random_data(volume)

    volume = volume.replicaRemove(name=replica0.name)

    # wait until we saw a replica starts rebuilding
    new_replica_found = False
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volname)
        for r in v.replicas:
            if r.name != replica0.name and \
                    r.name != replica1.name:
                new_replica_found = True
                break
        if new_replica_found:
            break
        time.sleep(RETRY_INTERVAL)
    wait_for_rebuild_complete(client, volname)
    assert new_replica_found

    volume = common.wait_for_volume_healthy(client, volname)

    volume = client.by_id_volume(volname)
    assert volume.state == common.VOLUME_STATE_ATTACHED
    assert volume.robustness == common.VOLUME_ROBUSTNESS_HEALTHY
    assert len(volume.replicas) &gt;= 2

    found = False
    for replica in volume.replicas:
        if replica.name == replica1.name:
            found = True
            break
    assert found

    check_volume_data(volume, data)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.ha_salvage_test"><code class="name flex">
<span>def <span class="ident">ha_salvage_test</span></span>(<span>client, core_api, volume_name, base_image='')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ha_salvage_test(client, core_api, # NOQA
                    volume_name, base_image=&#34;&#34;):  # NOQA
    # case: replica processes are wrongly removed
    volume = create_and_check_volume(client, volume_name, 2,
                                     base_image=base_image)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    assert len(volume.replicas) == 2
    replica0_name = volume.replicas[0].name
    replica1_name = volume.replicas[1].name

    data = write_volume_random_data(volume)

    delete_replica_processes(client, core_api, volume_name)

    volume = common.wait_for_volume_faulted(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt != &#34;&#34;
    assert volume.replicas[1].failedAt != &#34;&#34;

    volume.salvage(names=[replica0_name, replica1_name])

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt == &#34;&#34;
    assert volume.replicas[1].failedAt == &#34;&#34;

    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    check_volume_data(volume, data)

    cleanup_volume(client, volume)

    # case: replica processes get crashed
    volume = create_and_check_volume(client, volume_name, 2,
                                     base_image=base_image)
    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    assert len(volume.replicas) == 2
    replica0_name = volume.replicas[0].name
    replica1_name = volume.replicas[1].name

    data = write_volume_random_data(volume)

    crash_replica_processes(client, core_api, volume_name)

    volume = common.wait_for_volume_faulted(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt != &#34;&#34;
    assert volume.replicas[1].failedAt != &#34;&#34;

    volume.salvage(names=[replica0_name, replica1_name])

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt == &#34;&#34;
    assert volume.replicas[1].failedAt == &#34;&#34;

    volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    check_volume_data(volume, data)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.ha_simple_recovery_test"><code class="name flex">
<span>def <span class="ident">ha_simple_recovery_test</span></span>(<span>client, volume_name, size, base_image='')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ha_simple_recovery_test(client, volume_name, size, base_image=&#34;&#34;):  # NOQA
    volume = create_and_check_volume(client, volume_name, 2, size, base_image)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    ha_rebuild_replica_test(client, volume_name)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_ha_backup_deletion_recovery"><code class="name flex">
<span>def <span class="ident">test_ha_backup_deletion_recovery</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>[HA] Test deleting the restored snapshot and rebuild</p>
<p>Backupstore: all</p>
<ol>
<li>Create volume and attach it to the current node.</li>
<li>Write <code>data</code> to the volume and create snapshot <code>snap2</code></li>
<li>Backup <code>snap2</code> to create a backup.</li>
<li>Create volume <code>res_volume</code> from the backup. Check volume <code>data</code>.</li>
<li>Check snapshot chain, make sure <code>backup_snapshot</code> exists.</li>
<li>Delete the <code>backup_snapshot</code> and purge snapshots.</li>
<li>After purge complete, delete one replica to verify rebuild works.</li>
</ol>
<p>FIXME: Needs improvement, e.g. rebuild when no snapshot is deleted for
restored backup, delete a replica when restoring in progress.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_ha_backup_deletion_recovery(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    [HA] Test deleting the restored snapshot and rebuild

    Backupstore: all

    1. Create volume and attach it to the current node.
    2. Write `data` to the volume and create snapshot `snap2`
    3. Backup `snap2` to create a backup.
    4. Create volume `res_volume` from the backup. Check volume `data`.
    5. Check snapshot chain, make sure `backup_snapshot` exists.
    6. Delete the `backup_snapshot` and purge snapshots.
    7. After purge complete, delete one replica to verify rebuild works.

    FIXME: Needs improvement, e.g. rebuild when no snapshot is deleted for
    restored backup, delete a replica when restoring in progress.
    &#34;&#34;&#34;
    ha_backup_deletion_recovery_test(client, volume_name, SIZE)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_ha_prohibit_deleting_last_replica"><code class="name flex">
<span>def <span class="ident">test_ha_prohibit_deleting_last_replica</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Test prohibiting deleting the last replica</p>
<ol>
<li>Create volume with one replica and attach to the current node.</li>
<li>Try to delete the replica. It should error out</li>
</ol>
<p>FIXME: Move out of test_ha.py</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_ha_prohibit_deleting_last_replica(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    Test prohibiting deleting the last replica

    1. Create volume with one replica and attach to the current node.
    2. Try to delete the replica. It should error out

    FIXME: Move out of test_ha.py
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name, 1)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)

    assert len(volume.replicas) == 1
    replica0 = volume.replicas[0]

    with pytest.raises(Exception) as e:
        volume.replicaRemove(name=replica0.name)
    assert &#34;no other healthy replica available&#34; in str(e.value)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_ha_recovery_with_expansion"><code class="name flex">
<span>def <span class="ident">test_ha_recovery_with_expansion</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>[HA] Test recovery with volume expansion</p>
<ol>
<li>Create a volume length <code>SIZE</code> and attach to the current node.</li>
<li>Write <code>data1</code> to the volume</li>
<li>Expand the volume to <code>EXPAND_SIZE</code>, and check volume has been expanded</li>
<li>Write <code>data2</code> starting from <code>SIZE</code>.</li>
<li>Remove replica0 from volume</li>
<li>Wait volume to start rebuilding and complete</li>
<li>Check the <code>data1</code> and <code>data2</code></li>
</ol>
<p>FIXME: why on step 6, checked volume.replicas &gt;= 2?</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_ha_recovery_with_expansion(client, volume_name):   # NOQA
    &#34;&#34;&#34;
    [HA] Test recovery with volume expansion

    1. Create a volume length `SIZE` and attach to the current node.
    2. Write `data1` to the volume
    3. Expand the volume to `EXPAND_SIZE`, and check volume has been expanded
    4. Write `data2` starting from `SIZE`.
    5. Remove replica0 from volume
    6. Wait volume to start rebuilding and complete
    7. Check the `data1` and `data2`

    FIXME: why on step 6, checked volume.replicas &gt;= 2?
    &#34;&#34;&#34;
    volume = create_and_check_volume(client, volume_name, 2, SIZE)

    host_id = get_self_host_id()
    volume = volume.attach(hostId=host_id)
    volume = common.wait_for_volume_healthy(client, volume_name)
    assert len(volume.replicas) == 2
    replica0 = volume.replicas[0]
    assert replica0.name != &#34;&#34;
    replica1 = volume.replicas[1]
    assert replica1.name != &#34;&#34;

    data1 = write_volume_random_data(volume)

    expand_attached_volume(client, volume_name)
    volume = client.by_id_volume(volume_name)
    check_block_device_size(volume, int(EXPAND_SIZE))

    data2 = {
        &#39;pos&#39;: int(SIZE),
        &#39;content&#39;: generate_random_data(VOLUME_RWTEST_SIZE),
    }
    data2 = write_volume_data(volume, data2)

    volume.replicaRemove(name=replica0.name)
    # wait until we saw a replica starts rebuilding
    new_replica_found = False
    for i in range(RETRY_COUNTS):
        v = client.by_id_volume(volume_name)
        for r in v.replicas:
            if r.name != replica0.name and \
                    r.name != replica1.name:
                new_replica_found = True
                break
        if new_replica_found:
            break
        time.sleep(RETRY_INTERVAL)
    wait_for_rebuild_complete(client, volume_name)
    assert new_replica_found

    volume = common.wait_for_volume_healthy(client, volume_name)
    assert volume.state == common.VOLUME_STATE_ATTACHED
    assert volume.robustness == common.VOLUME_ROBUSTNESS_HEALTHY
    assert len(volume.replicas) &gt;= 2

    found = False
    for replica in volume.replicas:
        if replica.name == replica1.name:
            found = True
            break
    assert found

    check_volume_data(volume, data1, False)
    check_volume_data(volume, data2)

    cleanup_volume(client, volume)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_ha_salvage"><code class="name flex">
<span>def <span class="ident">test_ha_salvage</span></span>(<span>client, core_api, volume_name, disable_auto_salvage)</span>
</code></dt>
<dd>
<section class="desc"><p>[HA] Test salvage when volume faulted</p>
<p>Setting: Disable auto salvage</p>
<p>Case 1: Delete all replica processes using instance manager</p>
<ol>
<li>Create volume and attach to the current node</li>
<li>Write <code>data</code> to the volume.</li>
<li>Crash all the replicas using Instance Manager API<ol>
<li>Cannot do it using Longhorn API since a. it will delete data, b. the
last replica is not allowed to be deleted</li>
</ol>
</li>
<li>Make sure volume detached automatically and changed into <code>faulted</code> state</li>
<li>Make sure both replicas reports <code>failedAt</code> timestamp.</li>
<li>Salvage the volume</li>
<li>Verify that volume is in <code>detached</code> <code>unknown</code> state. No longer <code>faulted</code></li>
<li>Verify that all the replicas' <code>failedAt</code> timestamp cleaned.</li>
<li>Attach the volume and check <code>data</code></li>
</ol>
<p>Case 2: Crash all replica processes</p>
<p>Same steps as Case 1 except on step 3, use SIGTERM to crash the processes</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_ha_salvage(client, core_api, volume_name, disable_auto_salvage):  # NOQA
    &#34;&#34;&#34;
    [HA] Test salvage when volume faulted

    Setting: Disable auto salvage

    Case 1: Delete all replica processes using instance manager

    1. Create volume and attach to the current node
    2. Write `data` to the volume.
    3. Crash all the replicas using Instance Manager API
        1. Cannot do it using Longhorn API since a. it will delete data, b. the
    last replica is not allowed to be deleted
    4. Make sure volume detached automatically and changed into `faulted` state
    5. Make sure both replicas reports `failedAt` timestamp.
    6. Salvage the volume
    7. Verify that volume is in `detached` `unknown` state. No longer `faulted`
    8. Verify that all the replicas&#39; `failedAt` timestamp cleaned.
    9. Attach the volume and check `data`

    Case 2: Crash all replica processes

    Same steps as Case 1 except on step 3, use SIGTERM to crash the processes
    &#34;&#34;&#34;
    ha_salvage_test(client, core_api, volume_name)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_ha_simple_recovery"><code class="name flex">
<span>def <span class="ident">test_ha_simple_recovery</span></span>(<span>client, volume_name)</span>
</code></dt>
<dd>
<section class="desc"><p>[HA] Test recovering from one replica failure</p>
<ol>
<li>Create volume and attach to the current node</li>
<li>Write <code>data</code> to the volume.</li>
<li>Remove one of the replica using Longhorn API</li>
<li>Wait for a new replica to be rebuilt.</li>
<li>Check the volume data</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@pytest.mark.coretest   # NOQA
def test_ha_simple_recovery(client, volume_name):  # NOQA
    &#34;&#34;&#34;
    [HA] Test recovering from one replica failure

    1. Create volume and attach to the current node
    2. Write `data` to the volume.
    3. Remove one of the replica using Longhorn API
    4. Wait for a new replica to be rebuilt.
    5. Check the volume data
    &#34;&#34;&#34;
    ha_simple_recovery_test(client, volume_name, SIZE)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_rebuild_failure_with_intensive_data"><code class="name flex">
<span>def <span class="ident">test_rebuild_failure_with_intensive_data</span></span>(<span>client, core_api, volume_name, pod_make)</span>
</code></dt>
<dd>
<section class="desc"><p>[HA] Test rebuild failure with intensive data writing</p>
<ol>
<li>Create PV/PVC/POD with livenss check</li>
<li>Create volume and wait for pod to start</li>
<li>Write data to <code>/data/test1</code> inside the pod and get <code>original_checksum_1</code></li>
<li>Write data to <code>/data/test2</code> inside the pod and get <code>original_checksum_2</code></li>
<li>Find running replicas of the volume</li>
<li>Crash one of the running replicas.</li>
<li>Wait for the replica rebuild to start</li>
<li>Crash another running replicas</li>
<li>Wait for volume to finish two rebuilds and become healthy</li>
<li>Check md5sum for both data location</li>
<li>Go back step 5 and repeat for <code>REBUILD_RETRY_COUNT</code> times.</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_rebuild_failure_with_intensive_data(client, core_api, volume_name, pod_make):  # NOQA
    &#34;&#34;&#34;
    [HA] Test rebuild failure with intensive data writing

    1. Create PV/PVC/POD with livenss check
    2. Create volume and wait for pod to start
    3. Write data to `/data/test1` inside the pod and get `original_checksum_1`
    4. Write data to `/data/test2` inside the pod and get `original_checksum_2`
    5. Find running replicas of the volume
    6. Crash one of the running replicas.
    7. Wait for the replica rebuild to start
    8. Crash another running replicas
    9. Wait for volume to finish two rebuilds and become healthy
    10. Check md5sum for both data location
    11. Go back step 5 and repeat for `REBUILD_RETRY_COUNT` times.
    &#34;&#34;&#34;
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)
    pod_liveness_probe_spec = get_liveness_probe_spec(initial_delay=1,
                                                      period=1)
    pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name,
                                     num_of_replicas=3, size=str(1 * Gi))
    assert len(volume.replicas) == 3
    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    data_path_1 = &#34;/data/test1&#34;
    write_pod_volume_random_data(core_api, pod_name,
                                 data_path_1, RANDOM_DATA_SIZE)
    original_md5sum_1 = get_pod_data_md5sum(core_api, pod_name, data_path_1)
    create_snapshot(client, volume_name)
    data_path_2 = &#34;/data/test2&#34;
    write_pod_volume_random_data(core_api, pod_name,
                                 data_path_2, RANDOM_DATA_SIZE)
    original_md5sum_2 = get_pod_data_md5sum(core_api, pod_name, data_path_2)

    for i in range(REBUILD_RETRY_COUNT):
        volume = client.by_id_volume(volume_name)
        replicas = []
        for r in volume.replicas:
            if r.running:
                replicas.append(r)
            else:
                volume.replicaRemove(name=r.name)
        assert len(replicas) == 3
        random.shuffle(replicas)
        # Trigger rebuild
        crash_replica_processes(client, core_api, volume_name, [replicas[0]])
        wait_for_volume_degraded(client, volume_name)
        # Since replicas[1] maybe not the sender during the rebuild,
        # we can try it multiple times to trigger the rebuild failure.
        wait_for_rebuild_start(client, volume_name)
        crash_replica_processes(client, core_api, volume_name, [replicas[1]])
        wait_for_volume_healthy(client, volume_name)
        md5sum_1 = get_pod_data_md5sum(core_api, pod_name, data_path_1)
        assert original_md5sum_1 == md5sum_1
        md5sum_2 = get_pod_data_md5sum(core_api, pod_name, data_path_2)
        assert original_md5sum_2 == md5sum_2

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, pvc_name)
    delete_and_wait_pv(core_api, pv_name)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_salvage_auto_crash_all_replicas"><code class="name flex">
<span>def <span class="ident">test_salvage_auto_crash_all_replicas</span></span>(<span>client, core_api, volume_name, pod_make)</span>
</code></dt>
<dd>
<section class="desc"><p>[HA] Test automatic salvage feature by crashing all the replicas</p>
<ol>
<li>Create PV/PVC/POD. Make sure POD has liveness check. Start the pod</li>
<li>Generate <code>test_data</code> and write to the pod.</li>
<li>Run <code>sync</code> command inside the pod to make sure data flush to the volume.</li>
<li>Crash all replica processes using SIGTERM</li>
<li>Wait for volume to <code>faulted</code>, then <code>detached</code> <code>unknown</code>, then <code>healthy</code></li>
<li>Check replica <code>failedAt</code> has been cleared.</li>
<li>Wait for pod to be restarted.</li>
<li>Check pod <code>test_data</code>.</li>
</ol>
<p>FIXME: Step 5 is only a intermediate state, maybe no way to get it for sure</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_salvage_auto_crash_all_replicas(client, core_api, volume_name, pod_make):  # NOQA
    &#34;&#34;&#34;
    [HA] Test automatic salvage feature by crashing all the replicas

    1. Create PV/PVC/POD. Make sure POD has liveness check. Start the pod
    2. Generate `test_data` and write to the pod.
    3. Run `sync` command inside the pod to make sure data flush to the volume.
    4. Crash all replica processes using SIGTERM
    5. Wait for volume to `faulted`, then `detached` `unknown`, then `healthy`
    6. Check replica `failedAt` has been cleared.
    7. Wait for pod to be restarted.
    8. Check pod `test_data`.

    FIXME: Step 5 is only a intermediate state, maybe no way to get it for sure
    &#34;&#34;&#34;
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)

    pod_liveness_probe_spec = get_liveness_probe_spec(initial_delay=1,
                                                      period=1)

    pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name, num_of_replicas=2)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)

    write_pod_volume_data(core_api, pod_name, test_data)

    stream(core_api.connect_get_namespaced_pod_exec,
           pod_name,
           &#39;default&#39;,
           command=&#34;sync&#34;,
           stderr=True, stdin=True,
           stdout=True, tty=True,
           _preload_content=False)

    crash_replica_processes(client, core_api, volume_name)

    volume = common.wait_for_volume_faulted(client, volume_name)

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt == &#34;&#34;
    assert volume.replicas[1].failedAt == &#34;&#34;

    volume = wait_for_volume_healthy(client, volume_name)

    wait_for_pod_remount(core_api, pod_name)

    resp = read_volume_data(core_api, pod_name)

    assert test_data == resp

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, pvc_name)
    delete_and_wait_pv(core_api, pv_name)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_salvage_auto_crash_replicas_long_wait"><code class="name flex">
<span>def <span class="ident">test_salvage_auto_crash_replicas_long_wait</span></span>(<span>client, core_api, volume_name, pod_make)</span>
</code></dt>
<dd>
<section class="desc"><p>[HA] Test automatic salvage feature, with replica building pending</p>
<ol>
<li>Create a PV/PVC/Pod with liveness check.</li>
<li>Create volume and start the pod.</li>
<li>Generate <code>test_data</code> and write to the pod.</li>
<li>Run <code>sync</code> command inside the pod to make sure data flush to the volume.</li>
<li>Crash one of the replica. Wait for 60 seconds.</li>
<li>Crash all the replicas.</li>
<li>Make sure volume and Pod recovers.</li>
<li>Check <code>test_data</code> in the Pod.</li>
</ol>
<p>FIXME: step 5 should wait for the replica to finish rebuilding.
FIXME: should create common function with the previous couple test cases</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_salvage_auto_crash_replicas_long_wait(client, core_api, volume_name, pod_make):  # NOQA
    &#34;&#34;&#34;
    [HA] Test automatic salvage feature, with replica building pending

    1. Create a PV/PVC/Pod with liveness check.
    2. Create volume and start the pod.
    3. Generate `test_data` and write to the pod.
    4. Run `sync` command inside the pod to make sure data flush to the volume.
    5. Crash one of the replica. Wait for 60 seconds.
    6. Crash all the replicas.
    7. Make sure volume and Pod recovers.
    8. Check `test_data` in the Pod.

    FIXME: step 5 should wait for the replica to finish rebuilding.
    FIXME: should create common function with the previous couple test cases
    &#34;&#34;&#34;
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)

    pod_liveness_probe_spec = get_liveness_probe_spec(initial_delay=1,
                                                      period=1)

    pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name, num_of_replicas=2)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)

    write_pod_volume_data(core_api, pod_name, test_data)

    stream(core_api.connect_get_namespaced_pod_exec,
           pod_name,
           &#39;default&#39;,
           command=&#34;sync&#34;,
           stderr=True, stdin=True,
           stdout=True, tty=True,
           _preload_content=False)

    volume = client.by_id_volume(volume_name)
    replica0 = volume.replicas[0]

    crash_replica_processes(client, core_api, volume_name, [replica0])

    time.sleep(60)

    volume = client.by_id_volume(volume_name)

    replicas = []
    for r in volume.replicas:
        if r.running is True:
            replicas.append(r)

    crash_replica_processes(client, core_api, volume_name, replicas)

    volume = common.wait_for_volume_faulted(client, volume_name)

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 3

    volume = wait_for_volume_healthy(client, volume_name)

    wait_for_pod_remount(core_api, pod_name)

    resp = read_volume_data(core_api, pod_name)

    assert test_data == resp

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, pvc_name)
    delete_and_wait_pv(core_api, pv_name)</code></pre>
</details>
</dd>
<dt id="tests.test_ha.test_salvage_auto_crash_replicas_short_wait"><code class="name flex">
<span>def <span class="ident">test_salvage_auto_crash_replicas_short_wait</span></span>(<span>client, core_api, volume_name, pod_make)</span>
</code></dt>
<dd>
<section class="desc"><p>[HA] Test automatic salvage feature, with replica building pending</p>
<ol>
<li>Create a PV/PVC/Pod with liveness check.</li>
<li>Create volume and start the pod.</li>
<li>Generate <code>test_data</code> and write to the pod.</li>
<li>Run <code>sync</code> command inside the pod to make sure data flush to the volume.</li>
<li>Crash one of the replica. Wait for 5 seconds.</li>
<li>Crash all the replicas.</li>
<li>Make sure volume and Pod recovers.</li>
<li>Check <code>test_data</code> in the Pod.</li>
</ol>
<p>FIXME: step 5 should wait for the replica to start rebuilding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_salvage_auto_crash_replicas_short_wait(client, core_api, volume_name, pod_make):  # NOQA
    &#34;&#34;&#34;
    [HA] Test automatic salvage feature, with replica building pending

    1. Create a PV/PVC/Pod with liveness check.
    2. Create volume and start the pod.
    3. Generate `test_data` and write to the pod.
    4. Run `sync` command inside the pod to make sure data flush to the volume.
    5. Crash one of the replica. Wait for 5 seconds.
    6. Crash all the replicas.
    7. Make sure volume and Pod recovers.
    8. Check `test_data` in the Pod.

    FIXME: step 5 should wait for the replica to start rebuilding.
    &#34;&#34;&#34;
    pod_name = volume_name + &#34;-pod&#34;
    pv_name = volume_name + &#34;-pv&#34;
    pvc_name = volume_name + &#34;-pvc&#34;

    pod = pod_make(name=pod_name)

    pod_liveness_probe_spec = get_liveness_probe_spec(initial_delay=1,
                                                      period=1)

    pod[&#39;spec&#39;][&#39;containers&#39;][0][&#39;livenessProbe&#39;] = pod_liveness_probe_spec

    volume = create_and_check_volume(client, volume_name, num_of_replicas=2)

    create_pv_for_volume(client, core_api, volume, pv_name)
    create_pvc_for_volume(client, core_api, volume, pvc_name)
    pod[&#39;spec&#39;][&#39;volumes&#39;] = [create_pvc_spec(pvc_name)]
    create_and_wait_pod(core_api, pod)

    test_data = generate_random_data(VOLUME_RWTEST_SIZE)

    write_pod_volume_data(core_api, pod_name, test_data)

    stream(core_api.connect_get_namespaced_pod_exec,
           pod_name,
           &#39;default&#39;,
           command=&#34;sync&#34;,
           stderr=True, stdin=True,
           stdout=True, tty=True,
           _preload_content=False)

    volume = client.by_id_volume(volume_name)
    replica0 = volume.replicas[0]

    crash_replica_processes(client, core_api, volume_name, [replica0])

    time.sleep(5)

    volume = client.by_id_volume(volume_name)

    replicas = []
    for r in volume.replicas:
        if r.running is True:
            replicas.append(r)

    crash_replica_processes(client, core_api, volume_name, replicas)

    volume = common.wait_for_volume_faulted(client, volume_name)

    volume = common.wait_for_volume_detached_unknown(client, volume_name)
    assert len(volume.replicas) == 2
    assert volume.replicas[0].failedAt == &#34;&#34;
    assert volume.replicas[1].failedAt == &#34;&#34;

    volume = wait_for_volume_healthy(client, volume_name)

    wait_for_pod_remount(core_api, pod_name)

    resp = read_volume_data(core_api, pod_name)

    assert test_data == resp

    delete_and_wait_pod(core_api, pod_name)
    delete_and_wait_pvc(core_api, pvc_name)
    delete_and_wait_pv(core_api, pv_name)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tests" href="index.html">tests</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="tests.test_ha.ha_backup_deletion_recovery_test" href="#tests.test_ha.ha_backup_deletion_recovery_test">ha_backup_deletion_recovery_test</a></code></li>
<li><code><a title="tests.test_ha.ha_rebuild_replica_test" href="#tests.test_ha.ha_rebuild_replica_test">ha_rebuild_replica_test</a></code></li>
<li><code><a title="tests.test_ha.ha_salvage_test" href="#tests.test_ha.ha_salvage_test">ha_salvage_test</a></code></li>
<li><code><a title="tests.test_ha.ha_simple_recovery_test" href="#tests.test_ha.ha_simple_recovery_test">ha_simple_recovery_test</a></code></li>
<li><code><a title="tests.test_ha.test_ha_backup_deletion_recovery" href="#tests.test_ha.test_ha_backup_deletion_recovery">test_ha_backup_deletion_recovery</a></code></li>
<li><code><a title="tests.test_ha.test_ha_prohibit_deleting_last_replica" href="#tests.test_ha.test_ha_prohibit_deleting_last_replica">test_ha_prohibit_deleting_last_replica</a></code></li>
<li><code><a title="tests.test_ha.test_ha_recovery_with_expansion" href="#tests.test_ha.test_ha_recovery_with_expansion">test_ha_recovery_with_expansion</a></code></li>
<li><code><a title="tests.test_ha.test_ha_salvage" href="#tests.test_ha.test_ha_salvage">test_ha_salvage</a></code></li>
<li><code><a title="tests.test_ha.test_ha_simple_recovery" href="#tests.test_ha.test_ha_simple_recovery">test_ha_simple_recovery</a></code></li>
<li><code><a title="tests.test_ha.test_rebuild_failure_with_intensive_data" href="#tests.test_ha.test_rebuild_failure_with_intensive_data">test_rebuild_failure_with_intensive_data</a></code></li>
<li><code><a title="tests.test_ha.test_salvage_auto_crash_all_replicas" href="#tests.test_ha.test_salvage_auto_crash_all_replicas">test_salvage_auto_crash_all_replicas</a></code></li>
<li><code><a title="tests.test_ha.test_salvage_auto_crash_replicas_long_wait" href="#tests.test_ha.test_salvage_auto_crash_replicas_long_wait">test_salvage_auto_crash_replicas_long_wait</a></code></li>
<li><code><a title="tests.test_ha.test_salvage_auto_crash_replicas_short_wait" href="#tests.test_ha.test_salvage_auto_crash_replicas_short_wait">test_salvage_auto_crash_replicas_short_wait</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>