---
title: 3. Volume
---

### Test cases for Volume
| **#**    | **Test Case** | **Test Instructions** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Check volume Details | **Prerequisite:**<br><br>*   Longhorn Nodes has node tags<br>*   Node Disks has disk tags<br>*   Backup target is set to NFS server, or S3 compatible target<br><br>1.  Create a workload using Longhorn volume<br>2.  Check volume details page<br>3.  Create volume backup | *   Volume Details<br>    *   `State` should be `Attached`<br>    *   `Health` should be healthy<br>    *   `Frontend` should be `Block Device`<br>    *   `Attached Node & Endpoint` should be node name that volume is attached to and PATH of the volume device file on that node.<br>    *   `Size` should match volume size specified in Create Volume step<br>    *   `Actual Size` should be `0Bi` (No data has been written to the volume yet)<br>    *   `Engine Image` should be `longhornio/longhorn-engine:<LONGHORN_VERSION>`<br>    *   `Created` should indicate time since volume is created.<br>    *   `Node Tags` should be empty (no node tags has been specified during creation)<br>    *   `Disk Tags` should be empty (no disk tags has been specified during creation)<br>    *   `Last Backup` should be empty (no backup has been created)<br>    *   `Last Backup At` should be empty (no backup has been created)<br>    *   `Instance Manager` should contain instance manager image name<br>    *   `Namespace` should match namespace specified in Volume Create step.<br>    *   `PVC Name` should be empty (no PV has been created for that volume yet)<br>    *   `PV Name` should be empty (no PV has been created for that volume yet)<br>    *   `PV Status` should be empty (no PV/PVC has been created for that volume yet) |
| 2   | Filter Volumes | User should be able to filter volumes using the following filters<br><br>*   Name<br>*   Node<br>*   Status (Healthy, In progress, Degraded, Faulted, detached)<br>*   Namespace<br>*   Node redundancy (Yes, Limited, No)<br>*   PV Name<br>*   PVC Name<br>*   Node tag<br>*   Disk tag<br><br>Notes:<br><br>*   Limited node redundancy: at least one healthy replica is running at the same node as another | *   Volume list should match filtering criteria applied. |
| 3   | Delete multiple volumes | *   **Prerequisite:**<br>    *   Create multiple volumes<br><br>1.  Select multiple volumes and delete | *   Volumes should be deleted |
| 4   | Attach multiple volumes | *   **Prerequisite:**<br>    *   Create multiple volumes<br><br>1.  Select multiple volumes and Attach them to a node | *   All Volumes should be attached to the same node specified in volume attach request. |
| 5   | Attach multiple volumes in maintenance mode | *   **Prerequisite:**<br>    *   Create multiple volumes<br><br>1.  Select multiple volumes and Attach them to a node in maintenance mode | *   All Volumes should be attached in maintenance mode to the same node specified in volume attach request. |
| 6   | Detach multiple volumes | *   **Prerequisite:**<br>    *   Multiple attached volumes<br>*   Select multiple volumes and detach | *   Volumes should be detached |
| 7   | Backup multiple Volumes | *   **Prerequisite:**<br>    *   Longhorn should be configured to point to a backupstore<br>    *   Multiple volumes existed and attached to node/used buy kubernetes workload<br>    *   Write some data to multiple volumes and compute it’s checksum<br>*   Select multiple volumes and Create a backup<br>*   restore volumes backups and check its data checksum | *   Volume backups should be created<br>*   Restored volumes from backup should contain the same data when backup is created |
| 8   | Create PV/PVC for multiple volumes | **Prerequisite:**<br><br>*   Create multiple volumes<br><br>1.  Select multiple volumes<br>2.  Create a PV, specify filesysem<br>3.  Check PV in Lonhgorn UI and in Kubernetes<br>4.  Create PVC<br>5.  Check PVC in Lonhgorn UI and in Kubernetes<br>6.  Delete PVC<br>7.  Check PV in Lonhgorn UI and in Kubernetes | *   For all selected volumes<br>    *   PV should created<br>    *   PV/PVC status in UI should be `Available`<br>    *   PV `spec.csi.fsType` should match filesystem specified in PV creation request<br>    *   PV `spec.storageClassName` should match the setting in `Default Longhorn Static StorageClass Name`<br>    *   PV `spec.csi.volumeHandle` should be the volume name<br>    *   PV/PVC status in UI should be `Bound` in Longhorn UI<br>    *   PVC namespace should match namespace specified in PVC creation request<br>    *   After Deleting PVC, PV/PVC status should be `Relased` in Longhorn UI. |
| 9   | Volume expansion | Check Multiple Volume expansion test cases work for multiple volumes<br><br>[Test Cases in Volume Details page](https://rancher.atlassian.net/wiki/spaces/LON/pages/354453117/Volume+detail+page) | Volume expansion should work for multiple volumes. |
| 10  | Engine Offline Upgrade For Multiple Volumes | **Prerequisite:**<br><br>*   Volume is consumed by Kubernetes deployment workload<br>*   Volume use old Longhorn Engine<br><br>1.  Write data to volume, compute it’s checksum (checksum#1)<br>2.  Scale down deployment , volume gets detached<br>3.  Upgrade Longhorn engine image to use new deployed engine image<br>4.  Scale up deployment, volume gets attached | *   Volume read/write operations should work before and after engine upgrade.<br>*   Old Engine `Reference Count` will be decreased by 1<br>*   New Engine `Reference Count` will be increased by 1 |
| 12  | Show System Hidden | **Prerequisite**:<br><br>*   Volume is created and attached to a pod.<br><br>1.  Click the volume appearing on volume list page, it takes user to volume.<br>2.  Take snapshot and upgrade the replicas.<br>3.  Under snapshot section, enable option 'Show System Hidden | Enabling this option will show system created snapshots while rebuilding of replicas. |
| 13  | Event log | **Prerequisite**:<br><br>*   Volume is created and attached to a pod.<br><br>1.  Click event log to expand | Verify details appearing in logs. |

Automation Tests
----------------

|  **#**   | **Test name** | **Description** | **Tags** |
| --- | --- | --- | --- |
| 1   | test\_attach\_without\_frontend | Test attach in maintenance mode (without frontend)<br><br>1.  Create a volume and attach to the current node with enabled frontend<br>2.  Check volume has `blockdev`<br>3.  Write `snap1_data` into volume and create snapshot `snap1`<br>4.  Write more random data into volume and create another anspshot<br>5.  Detach the volume and reattach with disabled frontend<br>6.  Check volume still has `blockdev` as frontend but no endpoint<br>7.  Revert back to `snap1`<br>8.  Detach and reattach the volume with enabled frontend<br>9.  Check volume contains data `snap1_data` | Volume |
| 2   | test\_volume\_basic | Test basic volume operations:<br><br>1.  Check volume name and parameter<br>2.  Create a volume and attach to the current node, then check volume states<br>3.  Check soft anti-affinity rule<br>4.  Write then read back to check volume data | Volume |
| 3   | test\_volume\_iscsi\_basic | Test basic volume operations with iscsi frontend<br><br>1.  Create and attach a volume with iscsi frontend<br>2.  Check the volume endpoint and connect it using the iscsi initator on the node.<br>3.  Write then read back volume data for validation | Volume |
| 4   | test\_volume\_multinode | Test the volume can be attached on multiple nodes<br><br>1.  Create one volume<br>2.  Attach it on every node once, verify the state, then detach it | Volume |
| 5   | test\_volume\_update\_replica\_count | Test updating volume's replica count<br><br>1.  Create a volume with 3 replicas<br>2.  Attach the volume<br>3.  Increase the replica to 5.<br>4.  Volume will become degraded and start rebuilding<br>5.  Wait for rebuilding to complete<br>6.  Update the replica count to 2. Volume should remain healthy<br>7.  Remove 3 replicas, so there will be 2 replicas in the volume<br>8.  Verify the volume is still healthy | Volume |
| 6   | test\_ha\_prohibit\_deleting\_last\_replica | Test prohibiting deleting the last replica<br><br>1.  Create volume with one replica and attach to the current node.<br>2.  Try to delete the replica. It should error out | Volume |
| 7   | test\_replica\_datapath\_cleanup | Test replicas data path cleanup<br><br>1.  Create host disk `extra_disk` and add it to the current node.<br>2.  Disable all the disks except for the ones on the current node.<br>3.  Create a volume with 5 replicas (soft anti-affinity on)<br>    1.  To make sure both default disk and extra disk can have one replica<br>    2.  Current we don't have anti-affinity for disks on the same node<br>4.  Verify the data path for replicas are created.<br>5.  Delete the volume.<br>6.  Verify the data path for replicas are deleted. | Volume |
| 8   | test\_expansion\_basic | Test volume expansion using Longhorn API<br><br>1.  Create volume and attach to the current node<br>2.  Generate data `snap1_data` and write it to the volume<br>3.  Create snapshot `snap1`<br>4.  Expand the volume (volume will be detached, expanded, then attached)<br>5.  Verify the volume has been expanded<br>6.  Generate data `snap2_data` and write it to the volume<br>7.  Create snapshot `snap2`<br>8.  Gerneate data `snap3_data` and write it after the original size<br>9.  Create snapshot `snap3` and verify the `snap3_data` with location<br>10.  Detach and reattach the volume.<br>11.  Verify the volume is still expanded, and `snap3_data` remain valid<br>12.  Detach the volume.<br>13.  Reattach the volume in maintence mode<br>14.  Revert to `snap2` and detach.<br>15.  Attach the volume and check data `snap2_data`<br>16.  Generate `snap4_data` and write it after the original size<br>17.  Create snapshot `snap4` and verify `snap4_data`.<br>18.  Detach the volume and revert to `snap1`<br>19.  Validate `snap1_data` | Volume: Expansion |
| 9   | test\_restore\_inc\_with\_expansion | Run test against a random backupstores<br><br>1.  Create a volume and attach to the current node<br>2.  Generate `data0`, write to the volume, make a backup `backup0`<br>3.  Create three DR(standby) volumes from the backup: `dr_volume0/1/2`<br>4.  Wait for all three DR volumes to finish the initial restoration<br>5.  Verify DR volumes's `lastBackup` is `backup0`<br>6.  Verify snapshot/pv/pvc/change backup target are not allowed as long as the DR volume exists<br>7.  Activate standby `dr_volume0` and attach it to check the volume data<br>8.  Expand the original volume. Make sure the expansion is successful.<br>9.  Generate `data1` and write to the original volume and create `backup1`<br>10.  Make sure `dr_volume1`'s `lastBackup` field has been updated to `backup1`<br>11.  Activate `dr_volume1` and check data `data0` and `data1`<br>12.  Generate `data2` and write to the original volume after original SIZE<br>13.  Create `backup2`<br>14.  Wait for `dr_volume2` to finish expansion, show `backup2` as latest<br>15.  Activate `dr_volume2` and verify `data2`<br>16.  Detach `dr_volume2`<br>17.  Create PV, PVC and Pod to use `sb_volume2`, check PV/PVC/POD are good | Volume: Expansion<br><br>Backup: Disaster Recovery |
| 10  | test\_snapshot | Test snapshot operations<br><br>1.  Create a volume and attach to the node<br>2.  Create the empty snapshot `snap1`<br>3.  Generate and write data `snap2_data`, then create `snap2`<br>4.  Generate and write data `snap3_data`, then create `snap3`<br>5.  List snapshot. Validate the snapshot chain relationship<br>6.  Mark `snap3` as removed. Make sure volume's data didn't change<br>7.  List snapshot. Make sure `snap3` is marked as removed<br>8.  Detach and reattach the volume in maintenance mode.<br>9.  Make sure the volume frontend is still `blockdev` but disabled<br>10.  Revert to `snap2`<br>11.  Detach and reattach the volume with frontend enabled<br>12.  Make sure volume's data is `snap2_data`<br>13.  List snapshot. Make sure `volume-head` is now `snap2`'s child<br>14.  Delete `snap1` and `snap2`<br>15.  Purge the snapshot.<br>16.  List the snapshot, make sure `snap1` and `snap3` are gone. `snap2` is marked as removed.<br>17.  Check volume data, make sure it's still `snap2_data`. | Volume: Snapshot |
| 11  | test\_kubernetes\_status | Test Volume feature: Kubernetes Status<br><br>1.  Create StorageClass with `reclaimPolicy = Retain`<br>2.  Create a statefulset `kubernetes-status-test` with the StorageClass<br>    1.  The statefulset has scale of 2.<br>3.  Get the volume name from the SECOND pod of the StateufulSet pod and create an `extra_pod` with the same volume on the same node<br>4.  Check the volumes that used by the StatefulSet<br>    1.  The volume used by the FIRST StatefulSet pod will have one workload<br>    2.  The volume used by the SECOND StatefulSet pod will have two workloads<br>    3.  Validate related status, e.g. pv/pod name/state, workload name/type<br>5.  Check the volumes again<br>    1.  PV/PVC should still be bound<br>    2.  The volume used by the FIRST pod should have history data<br>    3.  The volume used by the SECOND and extra pod should have current data point to the extra pod<br>6.  Delete the extra pod<br>    1.  Now all the volume's should only have history data(`lastPodRefAt` set)<br>7.  Delete the PVC<br>    1.  PVC should be updated with status `Released` and become history data<br>8.  Delete PV<br>    1.  All the Kubernetes status information should be cleaned up.<br>9.  Reuse the two Longhorn volumes to create new pods<br>    1.  Since the `reclaimPolicy == Retain`, volume won't be deleted by Longhorn<br>    2.  Check the Kubernetes status now updated, with pod info but empty workload<br>    3.  Default Longhorn Static StorageClass will remove the PV with PVC, but leave Longhorn volume | Volume: Kubernetes Status |

Replica
------

| **#**  | **Test Case** | **Test Instructions** | **Expected Results** | **Automated ? / test name** |
| --- | --- | --- | --- | --- |
| 1   | Replica list | 1.  Create a volume and change the default number of replicas<br>2.  Attach volume to a node<br>3.  Check replica list | *   Number of replicas should match number specified in volume creation request<br>*   All replicas should be `Running`, and `Healthy`<br>*   Replica info also should contain, `Node Name`, `Replica Instance Manager Name`, `Replica Path` | test\_volume\_update\_replica\_count |
| 2   | Update volume replica count (increase) | 1.  Create a volume<br>2.  Attach volume to a node<br>3.  Increase replica count +1 | *   New system hidden snapshot should be created<br>*   A new replica should be created<br>*   New replicas should be `Running` & `Rebuilding` | test\_volume\_update\_replica\_count |
| 3   | Update volume replica count (decrease) | 1.  Create a volume<br>2.  Attach volume to a node<br>3.  decrease replica count by +1<br>4.  Delete a replica | *   After decreasing replica count, nothing should happen<br>*   Deleting a replica will not trigger replica rebuild | test\_volume\_update\_replica\_count |

Snapshot
--------

|  **#**   | **Test Case** | **Test Instructions** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Create Snapshot | 1.  Create a workload using Longhorn volume<br>2.  Write data to volume, compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1) | *   Volume head should have parent as snapshot#1<br>*   Snapshot should be created |    |
| 2   | Revert Snapshot | 1.  Create a deployment workload with `nReplicas = 1` using Longhorn volume<br>2.  Write data to volume, compute it’s checksum (checksum#1)<br>3.  Write some other data, compute it’s checksum (checksum#2)<br>4.  Create a snapshot (snapshot#1)<br>5.  Scale down deployment `nReplicas = 0`<br>6.  Attach volume in `maintenance mode`<br>7.  Revert to (snapshot#1)<br>8.  Detach volume<br>9.  Scale back deployment `nReplicas = 1`<br>10.  Compute data checksum (checksum#3) | *   Volume head should have parent as snapshot#1<br>*   Volume state should be `Detached` after scaling down deployment `nReplicas = 0`<br>*   In Volume Details `Attached Node` should be Node name which volume is attached to, without an `Endpoint` (block device path)<br>*   Data checksum after revert should match data checksum when taking snapshot `checksum#3 == checksum#1` |
| 3   | Delete Snapshot | 1.  Create a workload using Longhorn volume<br>2.  Write data to volume, compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)<br>5.  Write data to volume, compute it’s checksum (checksum#4) → live data<br>6.  Delete (snapshot#2)<br>7.  Revert to (snapshot#3)<br>8.  Revert to (snapshot#1) | *   Snapshot#2 will be deleted, verify in replica /var/lib/rancher/longhorn/replicas/<br>*   After reverting to snapshot#3 verify the data.<br>*   After reverting to snapshot#1 data checksum should match (checksum#1) |
| 4   | Delete Snapshot while rebuilding replicas | 1.  Create a workload using Longhorn volume<br>2.  Write data to volume (1GB+), compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Delete a replica<br>5.  while replica is rebuilding, try to delete (snapshot#1) | *   New system snapshot should be created<br>*   Will **NOT** be able to delete snapshot#1 |
| 5   | Snapshot Purge | 1.  Create a workload using Longhorn volume<br>2.  Write data to volume (1GB+), compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Delete a replica<br>5.  After rebuild is complete, delete (snapshot#1) | *   New system snapshot should be created<br>*   Snapshot#1 will be delete<br>*   Snapshot purge process will be triggered<br>*   Only system snapshot should be present<br>*   You will not be able revert to the system snapshot |
| 6   | Create recurring snapshots | 1.  Create a deployment workload with `nReplicas = 1` using Longhorn volume<br>2.  Write data to volume , compute it’s checksum (checksum#1)<br>3.  Create a recurring snapshot `every 5 minutes`. and set retain count to `5`<br>4.  Wait for 2 recurring snapshots to triggered (snapshot#1, snapshot#2 )<br>5.  Scale down deployment `nReplicas = 0`<br>6.  Attach volume in `maintenance mode`<br>7.  Revert to (snapshot#1)<br>8.  Scale back deployment `nReplicas = 1`<br>9.  Wait for another recurring snapshots to triggered (snapshot#3)<br>10.  Delete (snapshot#1) | *   Snapshots (snapshot#1, snapshot#2) should be created<br>*   Before deleting (snapshot#1), Parent snapshot of (snapshot#3) should be (snapshot#1)<br>*   After deleting (snapshot#1), Parent snapshot of (snapshot#3) should be starting point.<br>*   Only max of `5` snapshots should be retained<br>*   Oldest snapshot will be removed when number of snapshots created by recurring job exceeds retain count<br>*   only snapshots generated by recurring job is affected by retain count, user can created manual snapshots and it will not be deleted automatically. |
| 8   | Disabling/Deleting recurring snapshots | 1.  Create a deployment workload with `nReplicas = 1` using Longhorn volume<br>2.  Write data to volume , compute it’s checksum (checksum#1)<br>3.  Create a recurring snapshot `every 5 minutes`. and set retain count to `5`<br>4.  Wait for 2 recurring snapshots to triggered (snapshot#1,snapshot#2 )<br>5.  Delete the recurring snapshots | *   Recurring snapshots will stop after deletion of it.<br>*   Existing snapshots should retain.<br>*   User should be able to take snapshot manually. |
| 9   | Operation with volume created using rancher UI | 1.  Create pv/pvc in rancher UI.<br>2.  Deploy a workload with PVC created in rancher UI.<br>3.  Write data to volume, compute it’s checksum (checksum#1)<br>4.  In longhorn UI, create a snapshot.<br>5.  Write data to volume again.<br>6.  Revert to snapshot.<br>7.  Delete the snapshot. | *   User should be able to create snapshot.<br>*   User should to revert to snapshot created verify this by checksum.<br>*   User should be able to delete the snapshot. Verify this in replicas in the nodes. |
| 10  | Delete the last snapshot | 1.  Create a workload using Longhorn volume<br>2.  Write data to volume(more than 4k), compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)<br>5.  Write data to volume, compute it’s checksum (checksum#4) → live data<br>6.  Delete (snapshot#3) | *   Data from last snapshot should not get lost. |
| 11  | Multi branch snapshot | 1.  Create a workload using Longhorn volume<br>2.  Write data to volume(more than 4k), compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)<br>5.  Write data to volume, compute it’s checksum (checksum#4) → live data<br>6.  Revert to snapshot#2<br>7.  Write data to volume, compute it’s checksum.<br>8.  Repeat steps 2,3 two more times to have (snapshot#4, snapshot#5) with different data files (checksum#5, checksum#6)<br>9.  Revert to snapshot#3 | *   Verify the data in any of the replica |
| 12  | Backup from a snapshot | 1.  Create a workload using Longhorn volume<br>2.  Write data to volume, compute it’s checksum (checksum#1)<br>3.  Create a snapshot (snapshot#1)<br>4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)<br>5.  Write data to volume, compute it’s checksum (checksum#4) → live data<br>6.  Take backup from a snapshot#2.<br>7.  Restore from the backup taken | Verify the data of the backup, it should match data from snapshot#2 |

Volume Expansion
----------------

|   **#**  | **Test Case** | **Test Instructions** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Volume **Online expansion** for attached volume<br><br>**(Not Supported for now)** | 1.  Create multiple 4 volumes, each of size 5 GB, Attach them to nodes<br>2.  Format each volume using one of the following formats (ext 2/3/4 & xfs)<br>3.  Mount volumes to directories on the nodes.<br>4.  Check volume size and used space using `df -h` command<br>5.  Write 4 GB data file to each volume<br>6.  For each volume, from Operation menu, Click `Expand Volume`, set size to `10 GB`, and click `OK`<br>7.  Check volume size and used space using `df -h` command<br>8.  Add more 4 GB data file to each volume<br>9.  Check volume size and used space using `df -h` command<br>10.  Check volume size expanded | *   Volumes should be expanded to the new size<br>*   Volume read/write operations should work after size expansion |
| 2   | Volume **Online expansion** for volume consumed by Kubernetes workload<br><br>**→ Kubernetes Version: 1.15** | 1.  Create a `2GB` volume used by a Kubernetes workload<br>2.  Expand Volume size to `10 GB`<br>3.  In Kubernetes, edit PV/PVC capacity to match new volume size.<br>4.  Check volume size using `df -h` command from Kubernetes workload<br>5.  Write 8 GB data file to volume, and compute its checksum | *   When resizing, A message indicate that `The capacity of related PV and PVC will not be updated`<br>*   Volumes should be expanded to the new size<br>*   Volume read/write operations should work after size expansion |
| 3   | Volume **Online expansion** for volume consumed by Kubernetes workload<br><br>**→ Kubernetes Version: 1.16+** | **Prerequisite:**<br><br>*   PVC is dynamically provisioned by the StorageClass.<br>*   The Kubernetes is version 1.16+ or the feature gate for volume expansion is enabled.<br>*   The StorageClass should support resize, `allowVolumeExpansion: true` is set in the StorageClass<br><br>1.  Create a `2GB` volume used by a Kubernetes workload<br>2.  Expand Volume size to `10 GB`<br>3.  Check volume size using `df -h` command from Kubernetes workload<br>4.  Write 8 GB data file to volume, and compute its checksum | *   When resizing, A message indicate that `The capacity of related PV and PVC will not be updated`<br>*   Volumes should be expanded to the new size<br>*   Volume read/write operations should work after size expansion |
| 4   | Volume Offline expansion<br><br>**Kubernetes Version: < 1.16** | 1.  Create and attach a volume<br>2.  Format and mount the volume. Fill up the volume and get the checksum (checksum#1)<br>3.  Unmount and detach the volume.<br>4.  Expand the volume and wait for the expansion complete.<br>5.  Reattach and remount the volume. Check the checksum and if the filesystem is expanded.<br>6.  Fill up the expanded parts and get the checksum. (checksum#2)<br>7.  Unmount and detach the volume.<br>8.  Launch a workload for it on a different node. Check data checksum | *   Volume size should be expanded<br>*   Volume read/write operations should work after size expansion<br>*   After expansion, data checksum should match checksum#1<br>*   Final data checksum should match checksum#2 |
| 5   | Volume expansion with revert and backup | 1.  Create and attach a volume.<br>2.  Format and mount the volume. Fill up the volume and get the checksum. (checksum#1)<br>3.  Create the 1st snapshot and backup. (snapshot#1 & backup#1)<br>4.  Expand the volume. Fill up the expanded part and get the checksum (checksum#2)<br>5.  Create the 2nd snapshot and backup. (snapshot#2 & backup#2)<br>6.  Check if the backup volume size is expanded.<br>7.  Restore backup#2 to a volume and check its data<br>8.  Clean up then refill the volume. Get the checksum. (checksum#3)<br>9.  Create the 3rd snapshot and backup. (snapshot#3 & backup#3)<br>10.  Revert to the 2nd snapshot. Check the checksum.<br>11.  Revert to the 1st snapshot. Check the checksum and if we can still use the expanded part. | *   Volume should be expanded<br>*   backup#2 size should be expanded and match volume new expanded size<br>*   Restored volume data from backup#2 should match checksum#2<br>*   After reverting to snapshot#1, data checksum should match checksum#1<br>*   After reverting to snapshot#1 expanded size should be usable.<br>*   Volume read/write operations should work after expansion and revert. |

RWX Volume native support starting with v1.1.0
-----------------

### Prerequisite:

1.  Longhorn is deployed in a cluster having 4 nodes (1 etcd/control plane and 3 worker)
2.  NFS-Common is installed on the nodes.
    
|**#**| **Test Scenario** | **Test Steps** |
| --- | --- | --- |
| 1   | Create StatefulSet/Deployment with single pod with volume attached in RWX mode | 1.  Create a StatefulSet/Deployment with 1 pod.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.<br>4.  Verify there is directory with the name of PVC exists in the ShareManager mount point i.e. `export`<br>5.  Write some data in the pod and verify the same data reflects in the ShareManager.<br>6.  Verify the longhorn volume, it should reflect the correct size. |
| 2   | Create StatefulSet/Deployment with more than 1 pod with volume attached in RWX mode. | 1.  Create a StatefulSet/Deployment with multiple pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Verify that one volume per pod in Longhorn gets created.<br>4.  Verify there is directory with the name of PVC exists in the ShareManager mount point i.e. `export`<br>5.  Verify that Longhorn UI shows all the pods name attached to the volume.<br>6.  Write some data in all the pod and verify all the data reflects in the ShareManager.<br>7.  Verify the longhorn volume, it should reflect the correct size. |
| 3   | Create StatefulSet/Deployment with the existing PVC of a RWX volume. | 1.  Create a StatefulSet/Deployment with 1 pod.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.<br>4.  Write some data in the pod and verify the same data reflects in the ShareManager.<br>5.  Create another StatefulSet/Deployment using the above created PVC.<br>6.  Write some data in the new pod, the same should be reflected in the ShareManager pod.<br>7.  Verify the longhorn volume, it should reflect the correct size. |
| 4   | Scale up StatefulSet/Deployment with one pod attached with volume in RWX mode. | 1.  Create a StatefulSet/Deployment with 1 pod.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod and verify the same data reflects in the ShareManager.<br>4.  Scale up the StatefulSet/Deployment.<br>5.  Verify a new volume gets created.<br>6.  Write some data in the new pod, the same should be reflected in the ShareManager pod.<br>7.  Verify the longhorn volume, it should reflect the correct size. |
| 5   | Scale down StatefulSet/Deployment attached with volume in RWX mode to zero. | 1.  Create a StatefulSet/Deployment with 1 pod.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod and verify the same data reflects in the ShareManager.<br>4.  Scale down the StatefulSet/Deployment to zero<br>5.  Verify the ShareManager pod gets deleted.<br>6.  Verify the volume should be in detached state.<br>7.  Create a new StatefulSet/Deployment with the existing PVC with different mount point.<br>8.  Verify the ShareManager should get created and volume should become attached.<br>9.  Verify the data.<br>10.  Delete the newly created StatefulSet/Deployment.<br>11.  Verify the ShareManager pod gets deleted again.<br>12.  Scale up the first StatefulSet/Deployment.<br>13.  Verify the ShareManager should get created and volume should become attached.<br>14.  Verify the data. |
| 6   | Delete the Workload StatefulSet/Deployment attached with RWX volume. | 1.  Create a StatefulSet/Deployment with 1 pod.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod and verify the same data reflects in the ShareManager.<br>4.  Delete the workload.<br>5.  Verify the ShareManager pod gets deleted but the CRD should not be deleted.<br>6.  Verify the volume should be in detached state.<br>7.  Create another StatefulSet with existing PVC.<br>8.  Verify the ShareManager should get created and volume should become attached.<br>9.  Verify the data. |
| 7   | Take snapshot and backup of a RWX volume in Longhorn. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Take a snapshot and a backup.<br>5.  Write some more data into the pod.<br>6.  Revert to snapshot 1 and verify the data. |
| 8   | Restore a backup taken from a RWX volume in Longhorn. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Take a backup of a RWX volume.<br>5.  Restore from the backup and attach the volume to a pod.<br>6.  Verify the data and the volume should be read write once. |
| 9   | Create DR volume of a RWX volume in Longhorn. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Take a backup of the volume.<br>5.  Create a DR volume of the backup.<br>6.  Write more data in the pods and take more backups.<br>7.  Verify the DR volume is getting synced with latest backup.<br>8.  Activate the DR volume and verify the data. |
| 10  | Expand the RWX volume. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Expand the volume.<br>5.  Verify that user is able to write data in the expanded volume. |
| 11  | Recurring Backup/Snapshot with RWX volume. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Schedule a recurring backup/Snapshot.<br>5.  Verify the recurring jobs are getting created and is taking backup/snapshot successfully. |
| 12  | Deletion of the replica of a Longhorn RWX volume. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Delete one of the replica and verify that the rebuild of replica is working fine. |
| 13  | Parallel writing | 1.  Write data in multiple pods attached to the same volume at the same time. |
| 14  | Data locality with RWX volume. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Enable `Data-locality`<br>5.  Disable `Node soft anti-affinity`.<br>6.  Disable the node where the volume is attached for some time.<br>7.  Wait for replica to be rebuilt on another node.<br>8.  Enable the node scheduling and verify a replica gets rebuilt on the attached node. |
| 15  | Node eviction with RWX volume. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Do a node eviction and verify the data. |
| 16  | Auto salvage feature on an RWX volume. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Crash all the replicas and verify the auto-salvage works fine. |
| 17  | RWX volume with `Allow Recurring Job While Volume Is Detached` enabled. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Set a recurring backup and scale down all the pods.<br>5.  Verify the volume get attached at scheduled time and backup/snapshot get created. |
| 18  | RWX volume with Toleration. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Set some Toleration.<br>5.  Verify the ShareManager pods have the toleration and annotation updated. |
| 19  | Detach/Delete operation on an RWX volume. | 1.  Detach action on the Longhorn UI should not work on RWX volume.<br>2.  On deletion of the RWX volume, the ShareManager CRDs should also get deleted. |
| 20  | Crash instance e manager of the RWX volume | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Crash the instance manager.<br>5.  On crashing the IM, the ShareManager pods should be immediately redeployed.<br>6.  Based on the setting `Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly`, the workload pods will get redeployed.<br>7.  On recreating on workload pods, the volume should get attached successfully.<br>8.  If `Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly` is disabled, user should see I/O error on the mounted point. |
| 21  | Reboot the ShareManager and workload node | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Reboot the ShareManager node.<br>5.  The ShareManager pod should move to another node.<br>6.  As the instance e manager is on the same node and based on setting `Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly`, the workload should be redeployed and volume should be available to user.<br>7.  Reboot the workload node.<br>8.  On restart on the node, pods should get attached to the volume. Verify the data. |
| 22  | Power down the ShareManager and workload node. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Power down the ShareManager node.<br>5.  The ShareManager pod should move to another node.<br>6.  As the instance manager is on the same node and based on the setting `Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly`, the workload should be redeployed and volume should be available to user.<br>7.  Power down the workload node.<br>8.  The workload pods should move to another node based on `Pod Deletion Policy When Node is Down` setting.<br>9.  Once the pods are up, they should get attached to the volume. Verify the data. |
| 23  | Kill the nfs process in the ShareManager | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Kill the NFS server in the ShareManager pod.<br>5.  The NFS server should retry to come up.<br>6.  Volume should continue to accessible. |
| 24  | Delete the ShareManager CRD. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Delete the ShareManager CRD.<br>5.  A new ShareManager CRD should be created. |
| 25  | Delete the ShareManager pod. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Delete the ShareManager pod.<br>5.  A new ShareManager pod should be immediately created. |
| 26  | Drain the ShareManager node. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Drain the ShareManager pod node.<br>5.  The volume should get detached first, then the shareManager pod should move to another node and Volume should get reattached. |
| 27  | Disk full on the ShareManager node. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod and make the disk almost full.<br>4.  Verify the RWX volume is not failed.<br>5.  Verify the creation of snapshot/backup.<br>6.  Try to write more data, and the it should error out `no space left`. |
| 28  | Scheduling failure with RWX volume. | 1.  Disable 1 node.<br>2.  Create a StatefulSet/Deployment with 2 pods.<br>3.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>4.  Verify the RWX volume gets created with degraded state.<br>5.  Write some data in the pod.<br>6.  Enable the node and the volume should become healthy. |
| 29  | Add a node in the cluster. | 1.  Add a node in the cluster.<br>2.  Create multiple statefulSet/deployment with RWX volume.<br>3.  Verify that the ShareManager pod is able to scheduled on the new node. |
| 30  | Delete a node from the cluster. | 1.  Create a StatefulSet/Deployment with 2 pods.<br>2.  Attach a volume with RWX mode using longhorn class and selecting the option `read write many`.<br>3.  Write some data in the pod.<br>4.  Delete the ShareManager node from the cluster.<br>5.  Verify the ShareManager pod move to new node and volume continues to be accessible. |
| 31  | RWX with Linux/SLES OS |     |
| 32  | RWX with K3s set up |     |
| 33  | RWX in Air gap set up. |     |
| 34  | RWX in PSP enabled set up. |     |