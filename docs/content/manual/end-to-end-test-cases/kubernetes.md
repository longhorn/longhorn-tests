---
title: 5. Kubernetes
---

### Dynamic provisioning with StorageClass

1.  Can create and use volume using StorageClass
    
2.  Can create a new StorageClass use new parameters and it will take effect on the volume created by the storage class.
    
3.  If the PV reclaim policy is delete, once PVC and PV are deleted, Longhorn volume should be deleted.
    

### Static provisioning using Longhorn created PV/PVC

1.  PVC can be used by the new workload
    
2.  Delete the PVC will not result in PV deletion
    
3.  Delete the PV will not result in Longhorn volume deletion and data loss.
    

Automation Tests
----------------


| **#**    | **Test name** | **Description** | **Tags** |
| --- | --- | --- | --- |
| 1   | test\_storage\_class\_from\_backup | Test restore backup using StorageClass<br><br>1.  Create volume and PV/PVC/POD<br>2.  Write `test_data` into pod<br>3.  Create a snapshot and back it up. Get the backup URL<br>4.  Create a new StorageClass `longhorn-from-backup` and set backup URL.<br>5.  Use `longhorn-from-backup` to create a new PVC<br>6.  Wait for the volume to be created and complete the restoration.<br>7.  Create the pod using the PVC. Verify the data | Kubernetes |
| 2   | test\_pv\_creation | Test creating PV using Longhorn API<br><br>1.  Create volume<br>2.  Create PV for the volume<br>3.  Try to create another PV for the same volume. It should fail.<br>4.  Check Kubernetes Status for the volume since PV is created. | Kubernetes |
| 3   | test\_pvc\_creation\_with\_default\_sc\_set | Test creating PVC with default StorageClass set<br><br>The target is to make sure the newly create PV/PVC won't use default StorageClass, and if there is no default StorageClass, PV/PVC can still be created.<br><br>1.  Create a StorageClass and set it to be the default StorageClass<br>2.  Update static StorageClass to `longhorn-static-test`<br>3.  Create volume then PV/PVC.<br>4.  Make sure the newly created PV/PVC using StorageClass `longhorn-static-test`<br>5.  Create pod with PVC.<br>6.  Verify volume's Kubernetes Status<br>7.  Remove PVC and Pod.<br>8.  Verify volume's Kubernetes Status only contains current PV and history<br>9.  Wait for volume to detach (since pod is deleted)<br>10.  Reuse the volume on a new pod. Wait for the pod to start<br>11.  Verify volume's Kubernetes Status reflect the new pod.<br>12.  Delete PV/PVC/Pod.<br>13.  Verify volume's Kubernetes Status only contains history<br>14.  Delete the default StorageClass.<br>15.  Create PV/PVC for the volume.<br>16.  Make sure the PV's StorageClass is static StorageClass | Kubernetes |
| 4   | test\_provisioner\_io | Test that input and output on a StorageClass provisioned PersistentVolumeClaim works as expected.<br><br>Fixtures are torn down here in reverse order that they are specified as a parameter. Take caution when reordering test fixtures.<br><br>1.  Create a StorageClass, PVC and Pod.<br>2.  Wait for pod to be up.<br>3.  Write data to the pod<br>4.  Delete the original pod and create a new one using the same PVC<br>5.  Read the data from the new pod, verify the data. | Kubernetes |
| 5   | test\_provisioner\_mount | Test that a StorageClass provisioned volume can be created, mounted, unmounted, and deleted properly on the Kubernetes cluster.<br><br>Fixtures are torn down here in reverse order that they are specified as a parameter. Take caution when reordering test fixtures.<br><br>1.  Create a StorageClass, PVC and Pod<br>2.  Verify the pod is up and volume parameters. | Kubernetes |
| 6   | test\_provisioner\_params | Test that substituting different StorageClass parameters is reflected in the resulting PersistentVolumeClaim.<br><br>Fixtures are torn down here in reverse order that they are specified as a parameter. Take caution when reordering test fixtures.<br><br>1.  Create a StorageClass with replica 2 (instead of 3) etc.<br>2.  Create PVC and Pod using it.<br>3.  Verify the volume's parameter matches the Storage Class. | Kubernetes |
| 7   | test\_provisioner\_tags | Test that a StorageClass can properly provision a volume with requested Tags.<br><br>1.  Use `node_default_tags` to add default tags to nodes.<br>2.  Create a StorageClass with disk and node tag set.<br>3.  Create PVC and Pod.<br>4.  Verify the volume has the correct parameters and tags. | Kubernetes<br><br>Scheduling: Tag |
| 8   | test\_csi\_backup | Test that backup/restore works with volumes created by CSI driver.<br><br>Run the test for all the backupstores<br><br>1.  Create PV/PVC/Pod using dynamic provisioned volume<br>2.  Write data and create snapshot using Longhorn API<br>3.  Verify the existence of backup<br>4.  Create another Pod using restored backup<br>5.  Verify the data in the new Pod | Kubernetes: CSI<br><br>Backup |
| 9   | test\_csi\_block\_volume | Test CSI feature: raw block volume<br><br>1.  Create a PVC with `volumeMode = Block`<br>2.  Create a pod using the PVC to dynamic provision a volume<br>3.  Verify the pod creation<br>4.  Generate `test_data` and write to the block volume directly in the pod<br>5.  Read the data back for validation<br>6.  Delete the pod and create `pod2` to use the same volume<br>7.  Validate the data in `pod2` is consistent with `test_data` | Kubernetes: CSI |
| 10  | test\_csi\_io | Test that input and output on a statically defined CSI volume works as expected.<br><br>Note: Fixtures are torn down here in reverse order that they are specified as a parameter. Take caution when reordering test fixtures.<br><br>1.  Create PV/PVC/Pod with dynamic positioned Longhorn volume<br>2.  Generate `test_data` and write it to volume using the equivalent of `kubectl exec`<br>3.  Delete the Pod<br>4.  Create another pod with the same PV<br>5.  Check the previous created `test_data` in the new Pod | Kubernetes: CSI |
| 11  | test\_csi\_mount | Test that a statically defined CSI volume can be created, mounted, unmounted, and deleted properly on the Kubernetes cluster.<br><br>Note: Fixtures are torn down here in reverse order that they are specified as a parameter. Take caution when reordering test fixtures.<br><br>1.  Create a PV/PVC/Pod with dynamic provisioned Longhorn volume<br>2.  Make sure the pod is running<br>3.  Verify the volume status | Kubernetes: CSI |
| 12  | test\_csi\_offline\_expansion | Test CSI feature: offline expansion<br><br>1.  Create a new `storage_class` with `allowVolumeExpansion` set<br>2.  Create PVC and Pod with dynamic provisioned volume from the StorageClass<br>3.  Generate `test_data` and write to the pod<br>4.  Delete the pod<br>5.  Update pvc.spec.resources to expand the volume<br>6.  Verify the volume expansion done using Longhorn API<br>7.  Create a new pod and validate the volume content | Kubernetes: CSI<br><br>Volume: Expansion |
| 13  | test\_xfs\_pv | Test create PV with new XFS filesystem<br><br>1.  Create a volume<br>2.  Create a PV for the existing volume, specify `xfs` as filesystem<br>3.  Create PVC and Pod<br>4.  Make sure Pod is running.<br>5.  Write data into the pod and read back for validation. | Kubernetes: CSI |
| 14  | test\_xfs\_pv\_existing\_volume | Test create PV with existing XFS filesystem<br><br>1.  Create a volume<br>2.  Create PV/PVC for the existing volume, specify `xfs` as filesystem<br>3.  Attach the volume to the current node.<br>4.  Format it to `xfs`<br>5.  Create a POD using the volume | Kubernetes: CSI |
| 15  | test\_statefulset\_backup | Test that backups on StatefulSet volumes work properly.<br><br>1.  Create a StatefulSet with VolumeClaimTemplate and Longhorn.<br>2.  Wait for pods to run.<br><br>Then create backup using following steps for each pod:<br><br>1.  Create a snapshot<br>2.  Write some data into it<br>3.  Create another snapshot `backup_snapshot`<br>4.  Create a third snapshot<br>5.  Backup the snapshot `backup_snapshot`<br>6.  Wait for backup to show up. 1 Verify the backup informations | Kubernetes: StatefulSet<br><br>Backup |
| 16  | test\_statefulset\_mount | Tests that volumes provisioned for a StatefulSet can be properly created, mounted, unmounted, and deleted on the Kubernetes cluster.<br><br>1.  Create a StatefulSet using dynamic provisioned Longhorn volume.<br>2.  Wait for pods to become running<br>3.  Check volume properites are consistent with the StorageClass | Kubernetes: StatefulSet |
| 17  | test\_statefulset\_pod\_deletion | Test that a StatefulSet can spin up a new Pod with the same data after a previous Pod has been deleted.<br><br>1.  Create a StatefulSet with VolumeClaimTemplate and Longhorn.<br>2.  Wait for pods to run.<br>3.  Write some data to one of the pod.<br>4.  Delete that pod.<br>5.  Wait for the StatefulSet to recreate the pod<br>6.  Verify the data in the pod. | Kubernetes: StatefulSet |
| 18  | test\_statefulset\_recurring\_backup | Test that recurring backups on StatefulSets work properly.<br><br>1.  Create a StatefulSet with VolumeClaimTemplate and Longhorn.<br>2.  Wait for pods to run.<br>3.  Write some data to every pod<br>4.  Schedule recurring jobs for volumes using Longhorn API<br>5.  Wait for 5 minutes<br>6.  Verify the snapshots created by the recurring jobs | Kubernetes: StatefulSet<br><br>Backup |
| 19  | test\_statefulset\_restore | Test that data can be restored into volumes usable by a StatefulSet.<br><br>1.  Create a StatefulSet with VolumeClaimTemplate and Longhorn.<br>2.  Wait for pods to run.<br>3.  Create a backup for each pod.<br>4.  Delete the StatefulSet, including the Longhorn volumes.<br>5.  Create volumes and PV/PVC using previous backups from each Pod.<br>    1.  PVs will be created using the previous names.<br>        <br>    2.  PVCs will be created using previous name + "-2" due to statefulset has a naming policy for what should be PVC name for them.<br>        <br>6.  Create a new StatefulSet using the previous name + "-2"<br>7.  Wait for pods to be up. . Verify the pods contain the previous backed up data | Kubernetes: StatefulSet<br><br>Backup |
| 20  | test\_statefulset\_scaling | Test that scaling up a StatefulSet successfully provisions new volumes.<br><br>1.  Create a StatefulSet with VolumeClaimTemplate and Longhorn.<br>2.  Wait for pods to run.<br>3.  Verify the properities of volumes.<br>4.  Scale the StatefulSet to 3 replicas<br>5.  Wait for the new pod to become ready.<br>6.  Verify the new volume properties. | Kubernetes: StatefulSet |
| 21  | test\_toleration\_setting | Test toleration setting<br><br>1.  Verify that cannot use Kubernetes tolerations for Longhorn setting<br>2.  Use "key1=value1:NoSchedule; key2:NoExecute" as toleration.<br>3.  Create a volume and attach it.<br>4.  Verify that cannot update toleration setting when any volume is attached<br>5.  Generate and write `data1` into the volume<br>6.  Detach the volume.<br>7.  Update setting `toleration` to toleration.<br>8.  Wait for all the Longhorn components to restart with new toleration<br>9.  Attach the volume again and verify the volume `data1`.<br>10.  Generate and write `data2` to the volume.<br>11.  Detach the volume.<br>12.  Clean the `toleration` setting.<br>13.  Wait for all the Longhorn components to restart with no toleration<br>14.  Attach the volume and validate `data2`.<br>15.  Generate and write `data3` to the volume | Kubernetes: Toleration |

### Test cases using kubectl

| **#**    | **Test Case** | **Test Instructions** | **Expected Results** | **Automated ? / test name** |
| --- | --- | --- | --- | --- |
| 1   | Disable Volume Expansion for volumes dynamically provisioned by a new StorageClass<br><br>  <br><br>[StorageClass example manifest](https://github.com/longhorn/longhorn/blob/master/examples/storageclass.yaml) | 1.  Create a new StorageClass with `allowVolumeExpansion: false`<br>2.  Create a PVC with using the new StorageClass | *   New Storage class should be created<br>*   Volume should be dynamically provisioned, it’s PV/PVC should be `Bound`<br>*   Volume expansion feature should be disabled for all Volumes using the new Storage Class |     |
| 2   | Number of volume replicas volumes dynamically provisioned by a new StorageClass | 1.  Create a new StorageClass, update`numberOfReplicas` parameter<br>2.  Create a PVC with using the new StorageClass | *   New Storage class should be created<br>*   Volume should be dynamically provisioned, it’s PV/PVC should be `Bound`<br>*   Volume number of replicas should match `numberOfReplicas` specified in storagecalss parameter |     |
| 3   | StorageClass `frombackup` parameter | **Prerequisite:**<br><br>*   Longhorn setting is set for Backup Target and Backup Target Credential Secret<br>*   Backup store should contain a previous volume backup<br>*   volume data checksum should be know at the time of volume backups (checksum#1)<br><br>1.  Create a StorageClass, and set `frombackup` parameter to volume backup URL in the backup store<br>2.  Create a PVC using the new StorageClass, and it’s size should be the original volume size<br>3.  Attach the new Volume to a node and check it’s data checksum | *   Volume should be dynamically provisioned, it’s PV/PVC should be `Bound`<br>*   Restore from backup process should be triggered<br>*   New volume should contain data restored from the backup, data checksum should match (checksum#1) | test\_storage\_class\_from\_backup |
| 4   | StorageClass `diskSelector` parameter | **Prerequisite:**<br><br>*   Longhorn Disks should have tags<br><br>1.  Create a new StorageClass, set `diskSelector` parameter<br>2.  Create a PVC with using the new StorageClass | *   New Storage class should be created<br>*   Volume should be dynamically provisioned, it’s PV/PVC should be `Bound`, volume replicas should only scheduled to Disks with tags that match `diskSelector` parameter tags |     |
| 5   | StorageClass `nodeSelector` parameter | **Prerequisite:**<br><br>*   Longhorn Node should have tags<br><br>1.  Create a new StorageClass, set `nodeSelector` parameter<br>2.  Create a PVC with using the new StorageClass | *   New Storage class should be created<br>*   Volume should be dynamically provisioned, it’s PV/PVC should be `Bound`, volume replicas should only scheduled to Nodes with tags that match `nodeSelector` parameter tags |     |
| 6   | StorageClass `recurringJobs` parameter | 1.  Create a new StorageClass, set `recurringJobs` parameter<br>2.  Create a PVC with using the new StorageClass<br>3.  Create a pod that consumes the created PVC<br>4.  Check Volume recurring jobs | *   New Storage class should be created<br>*   Volume should be dynamically provisioned, it’s PV/PVC should be `Bound` and attached to the pod<br>*   Volume should have recurring snapshots and backups matches ones specified in `recurringJobs` StorageClass parameter |  test\_statefulset\_recurring\_backup |
| 7   | StorageClass with `reclaimPolicy` parameter set to `Delete` | 1.  Create a new StorageClass, set `reclaimPolicy` parameter to `Delete`<br>2.  Create a PVC with using the new StorageClass<br>3.  Delete the PVC | *   Volume should be dynamically provisioned, it’s PV/PVC should be `Bound`<br>*   Deleting PVC would trigger Volume delete |     |
| 8   | StorageClass with `reclaimPolicy` parameter set to `Retain` | 1.  Create a new StorageClass, set `reclaimPolicy` parameter to `Delete`<br>2.  Create a PVC with using the new StorageClass<br>3.  Delete PVC<br>4.  Delete PV | *   Volume should be dynamically provisioned, it’s PV/PVC should be `Bound`<br>*   Deleting PVC and PV will not delete longhorn volume. | test\_kubernetes\_status |
| 9   | Static provisioning using `Default Longhorn Static StorageClass Name` Setting | 1.  Update `Default Longhorn Static StorageClass Name` setting, set a new StorageClass Name, StorageClass doesn’t have to exist or be created.<br>2.  Create a Volume<br>3.  From Longhorn, Create a PV/PVC for the volume<br>4.  Check created PV `persistentVolumeReclaimPolicy: Retain`<br>5.  Create a pod consuming created PVC<br>6.  Delete the pod<br>7.  Delete PV<br>8.  Delete PVC | *   Volume should be created<br>*   Volume PV should be created using new StorageClass Name defined in `Default Longhorn Static StorageClass Name` setting<br>*   PVC should be consumed by the pod, volume should be accessible in the pod, write/read operations should work normally.<br>*   Deleting PV/PVC will not trigger volume delete. |  test\_pvc\_creation\_with\_default\_sc\_set |


### Additional Tests to be executed from Rancher

| **#**    | **Scenario** | **Steps** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Storage Class: Create a Longhorn storage class | **Pre condition:**<br><br>*   Longhorn is deployed in the cluster<br><br>**Steps:**<br><br>1.  Go to cluster → Storage → Storage Classes<br>2.  Click on Add class<br>3.  Select Provisioner **Longhorn**<br>4.  Give in other required parameters including replica count.<br>5.  Click on **Save**.<br>6.  Verify **test-1** storage class is created<br>7.  Go to Cluster → Project (default) → Workloads<br>8.  Deploy a workload<br>9.  In the Volumes section → Add a New Volume Claim → Use a Storage Class to provision a new persistent volume → Select **test-1** from Storage class dropdown.<br>10.  Enter capacity and Name. Click on Define<br>11.  Enter Mount Point.<br>12.  Click on create workload<br>13.  Verify workload is created successfully.<br>14.  Volume gets attached to the pod in the workload<br>15.  Navigate to Longhorn UI.<br>16.  Verify user is able to view the volume attached to the workload in the UI<br>17.  Navigate to volume details page of the volume and Verify the replica count mentioned in Step 4 is available | *   Longhorn storage class should be created<br>*   Workload should be deployed with the volume mounted from the storage class<br>*   Verify volume is available on the Longhorn UI.<br>*   Verify the replica count is as mentioned during storage class creation. |
| 2   | Persistent Volume: Create a PV | **Pre condition:**<br><br>*   Longhorn is deployed in the cluster<br><br>**Steps:**<br><br>1.  Create a Volume in Longhorn UI `test-volume`<br>2.  Go to cluster → Storage → Persistent Volumes<br>3.  Click on Add PV<br>4.  Select Volume Plugin **Longhorn**<br>5.  Give in other required parameters including replica count.<br>6.  Give in Volume Plugin - `test-volume` which an existing volume in longhorn<br>7.  Click on **Save**.<br>8.  Verify **test-1** PV is created<br>9.  Go to Cluster → Project (default) → Workloads<br>10.  Deploy a workload<br>11.  In the Volumes section → Add a New Volume Claim → Use an existing persistent volume → Select **test-1** from PV dropdown.<br>12.  Click on Define<br>13.  Enter Mount Point.<br>14.  Click on create workload<br>15.  Verify workload is created successfully.<br>16.  Volume gets attached to the pod in the workload<br>17.  Navigate to Longhorn UI.<br>18.  Verify user is able to view the volume attached to the workload in the UI<br>19.  Navigate to volume details page of the volume and Verify the replica count mentioned in Step 4 is available | *   Longhorn PV should be created<br>*   Workload should be deployed with the volume mounted from the PV<br>*   Verify volume is available on the Longhorn UI.<br>*   Verify the replica count is as mentioned during storage class creation. |
| 3   | Create Storage class in Rancher; From Longhorn create volumes from this storage class. | **Pre condition:**<br><br>*   Longhorn is deployed in the cluster<br><br>**Steps:**<br><br>1.  Go to cluster → Storage → Storage Classes<br>2.  Click on Add class<br>3.  Select Provisioner **Longhorn**<br>4.  Give in other required parameters including replica count.<br>5.  Click on **Save**.<br>6.  Verify **test-1** storage class is created<br>7.  Go to Longhorn UI<br>8.  In the Settings page for “Default Longhorn Static StorageClass Name”, give in the value: “test-1”<br>9.  Go to Volumes page, click on create volume.<br>10.  Create a volume name : v1<br>11.  Verify v1 is created<br>12.  using kubectl -<br>13.  kubectl get pv <volume-name> -o yaml<br>14.  Verify “storageClassName:” ---> test-1 | *   Longhorn storage class should be created<br>*   Value of Default Longhorn Static StorageClass Name should be changed in the settings page<br>*   volume should be created in longhorn UI<br>*   “storageClassName:” value should be **test-1** |
| 4   | Create Storage Class using backup URL | 1.  Create volume and PV/PVC/POD in Longhorn<br>2.  Write `test_data` into pod<br>3.  Create a snapshot and back it up. Get the backup URL<br>4.  Create a new StorageClass `longhorn-from-backup` in rancher and set backup URL.<br>5.  Use `longhorn-from-backup` to create a new PVC<br>6.  Wait for the volume to be created and complete the restoration.<br>7.  Create the pod using the PVC. Verify the data |     |
| 5   | Create Storage class - by using different values for the input list of paramters | **Pre condition:**<br><br>*   Longhorn is deployed in the cluster<br><br>**Steps:**<br><br>1.  Go to cluster → Storage → Storage Classes<br>2.  Click on Add class<br>3.  Select Provisioner **Longhorn**<br>4.  Give in other required parameters.<br>5.  Click on **Save**.<br>6.  Use this storage class to create a PVC and deploy in a workload.<br>7.  Verify the parameters of the volume created. | Volume parameters should match the storage class paramaters. |
| 6   | StorageClass with `reclaimPolicy` parameter set to `Delete` - PVC from storage class | **Pre conditions:**<br><br>*   Create PVC from “Longhorn” storage class in rancher.<br>*   It will have a dynamic PV bound<br><br>**Steps**:<br><br>1.  'Delete PVC from Rancher<br>2.  Verify PVC is deleted<br>3.  Verify PV bound to this PVC is deleted - Rancher → Cluster → Storage → PV<br>4.  Verify the volume(Dynamic PV) in Longhorn is deleted |     |
| 7   | Volume/PV/PVC created in Longhorn | **Pre conditions:**<br><br>*   Create volume, PV, PVC in longhorn<br><br>**Steps:**<br><br>1.  'Delete PVC from Rancher<br>2.  Verify PVC is deleted<br>3.  PV will NOT. be deleted but be in “released” state in Rancher UI<br>4.  Verify Volume does not get deleted |     |
| 8   | StorageClass with `reclaimPolicy` parameter set to `Retain` - PVC from storage class | **Pre conditions:**<br><br>*   Create PVC from “Longhorn” storage class in rancher.<br>*   It will have a dynamic PV bound<br><br>**Steps**:<br><br>1.  'Delete PVC from Rancher<br>2.  Verify PVC is deleted<br>3.  Verify PV bound to this PVC is NOT deleted - Rancher → Cluster → Storage → PV<br>4.  Verify the volume(Dynamic PV) in Longhorn is NOT deleted |   |
| 9   | StorageClass with `reclaimPolicy` parameter set to `Retain` - Volume/PV/PVC created in Longhorn | **Pre conditions:**<br><br>*   Create volume, PV, PVC in longhorn<br><br>**Steps:**<br><br>1.  'Delete PVC from Rancher<br>2.  Verify PVC is deleted<br>3.  PV will NOT. be deleted but be in “released” state in Rancher UI<br>4.  Verify Volume does not get deleted |     |
| 10  | Power down node | 1.  Power down<br>2.  Replica migrates<br>3.  Power back on<br>4.  Verify if the replicas in the node have been deleted | *   When a node is powered down, the replica is rebuilt on the 4th wrker node.<br>*   When the node is powered back on, and the replica on the powered down node is not available in Longhorn UI anymore, there is no data in `/var/lib/longhorn/replicas` folder in the powered on node. |
| 11  | Power down node with. Node tag/disk tag | 1.  Add a node tag/disk tag<br>2.  Power down<br>3.  Replica cannot migrate<br>4.  Power back on<br>5.  Replica should get rebuilt on this node | *   When a node is powered down, the replica is rebuilt on the 4th wrker node.<br>*   When the node is powered back on, and the replica on the powered down node is not available in Longhorn UI anymore, there is no data in `/var/lib/longhorn/replicas` folder in the powered on node.<br>*   The new replica is rebuilt on a node which has a tag. |
| 12  | Drain a node | 1.  Drain use case — drain a worker node <br>2.  Check if the State of the node reflects in the Longhorn UI —> Node<br>3.  Verify if replica is rebuilt on another node? <br>4.  Verify if the pod migrates<br>5.  And the volume get migrated | All the components should be successfully drained. |
| 13  | kubectl - force drain | Using kubectl - force drain a node where the pod with the volume attached is available<br><br>Have snapshots before<br><br>Verify data after pod migrates | Volume attaches on the new pod<br><br>2 of the 3 replicas are in “Stopped” state - Caused replica rebuild. |
| 14  | Cordon a node | 1.  Cordon state - cordon a worker node |     |
| 15  | Delete node where the pods/workload exists<br><br>**workload type: deployment** | Verify the pods migrate to another node and verify the volume also re attaches on the pod on the other node | *   Create a workload with volume attached on n1<br>*   write data to volume<br>*   Delete node n1<br>*   The workload gets reattached to another node n2.<br>*   The volume gets attached after a minute<br>*   The volume is accessible.<br>*   data is accessible |
| 16  | Power down node where the pods/workload exists<br><br>**workload type: deployment** | Verify the pods migrate to another node and verify the volume also re attaches on the pod on the other node | host A → Pod a – Unknown – > Not able to unmount<br><br>host b → pod b - Creating – fails to attach here |
| 17  | Delete node where the pods/workload exists<br><br>**workload type: stateful set** | *   Create a workload, scale - 2 with volume attached on n1<br>*   write data to volume<br>*   Delete node n1, and n2 | *   The workload gets reattached to another node n3 and n4.<br>*   The volume gets attached after sometime<br>*   The volume is accessible.<br>*   data is accessible |
| 18  | Power down node where the pods/workload exists<br><br>**workload type: stateful set** | *   Create a workload, attach to a volume.<br>*   Write some data<br>*   Power down the node where the pod is running. | *   The pod should get recreated on another node.<br>*   The volume should get reattached.<br>*   The mount point should be accessible to read and write. |
| 19  | Delete worker node one by one |     | *   The volume should get reattach to healthy node. |     |     |
| 20  | Upgrade cluster - drain set - false | 1.  In Rancher - upgrade a cluster by changing the max-pods value<br>2.  The cluster will go into “Updating” state<br>3.  Verify upgrade completes successfully | Upgrade should finish successfully |
| 21  | Upgrade cluster - drain set - true | 1.  In Rancher - upgrade a cluster by changing the max-pods value<br>2.  The cluster will go into “Updating” state<br>3.  Verify upgrade completes successfully | Upgrade should finish successfully |