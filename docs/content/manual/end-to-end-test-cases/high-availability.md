---
title: 4. High Availability
---

Automation Tests
----------------

|   **#**  | **Test Name** | **Description** | **Tags** |
| --- | --- | --- | --- |
| 1   | test\_engine\_image\_daemonset\_restart | Test restarting engine image daemonset<br><br>1.  Get the default engine image<br>2.  Create a volume and attach to the current node<br>3.  Write random data to the volume and create a snapshot<br>4.  Delete the engine image daemonset<br>5.  Engine image daemonset should be recreated<br>6.  In the meantime, validate the volume data to prove it's still functional<br>7.  Wait for the engine image to become `ready` again<br>8.  Check the volume data again.<br>9.  Write some data and create a new snapshot.<br>    1.  Since create snapshot will use engine image binary.<br>        <br>10.  Check the volume data again | High Availablity |
| 2   | test\_ha\_recovery\_with\_expansion | \[HA\] Test recovery with volume expansion<br><br>1.  Create a volume length `SIZE` and attach to the current node.<br>2.  Write `data1` to the volume<br>3.  Expand the volume to `EXPAND_SIZE`, and check volume has been expanded<br>4.  Write `data2` starting from `SIZE`.<br>5.  Remove replica0 from volume<br>6.  Wait volume to start rebuilding and complete<br>7.  Check the `data1` and `data2` | High Availability<br><br>Volume: Expansion |
| 3   | test\_ha\_salvage | \[HA\] Test salvage when volume faulted<br><br>Setting: Disable auto salvage<br><br>**Case 1:** Delete all replica processes using instance manager<br><br>1.  Create volume and attach to the current node<br>2.  Write `data` to the volume.<br>3.  Crash all the replicas using Instance Manager API<br>    1.  Cannot do it using Longhorn API since a. it will delete data, b. the last replica is not allowed to be deleted<br>        <br>4.  Make sure volume detached automatically and changed into `faulted` state<br>5.  Make sure both replicas reports `failedAt` timestamp.<br>6.  Salvage the volume<br>7.  Verify that volume is in `detached` `unknown` state. No longer `faulted`<br>8.  Verify that all the replicas' `failedAt` timestamp cleaned.<br>9.  Attach the volume and check `data`<br><br>**Case 2:** Crash all replica processes<br><br>Same steps as Case 1 except on step 3, use SIGTERM to crash the processes | High Availability |
| 4   | test\_ha\_simple\_recovery | \[HA\] Test recovering from one replica failure<br><br>1.  Create volume and attach to the current node<br>2.  Write `data` to the volume.<br>3.  Remove one of the replica using Longhorn API<br>4.  Wait for a new replica to be rebuilt.<br>5.  Check the volume data | High Availability |
| 5   | test\_rebuild\_with\_intensive\_data | \[HA\] Test rebuild with intensive data writing<br><br>1.  Create PV/PVC/POD with livenss check<br>2.  Create volume and wait for pod to start<br>3.  Write data to `/data/test1` inside the pod and get `original_checksum_1`<br>4.  Write data to `/data/test2` inside the pod and get `original_checksum_2`<br>5.  Find running replicas of the volume<br>6.  Crash one of the running replicas.<br>7.  Wait for the replica rebuild to start<br>8.  Crash another running replicas<br>9.  Wait for volume to finish two rebuilds and become healthy<br>10.  Check md5sum for both data location<br>11.  Go back step 5 and repeat for `REBUILD_RETRY_COUNT` times. | High Availability |
| 6   | test\_salvage\_auto\_crash\_all\_replicas | \[HA\] Test automatic salvage feature by crashing all the replicas<br><br>1.  Create PV/PVC/POD. Make sure POD has liveness check. Start the pod<br>2.  Generate `test_data` and write to the pod.<br>3.  Run `sync` command inside the pod to make sure data flush to the volume.<br>4.  Crash all replica processes using SIGTERM<br>5.  Wait for volume to `faulted`, then `detached` `unknown`, then `healthy`<br>6.  Check replica `failedAt` has been cleared.<br>7.  Wait for pod to be restarted.<br>8.  Check pod `test_data`. | High Availability: Automatic salvage |
| 7   | test\_salvage\_auto\_crash\_replicas\_long\_wait | \[HA\] Test automatic salvage feature, with replica building pending<br><br>1.  Create a PV/PVC/Pod with liveness check.<br>2.  Create volume and start the pod.<br>3.  Generate `test_data` and write to the pod.<br>4.  Run `sync` command inside the pod to make sure data flush to the volume.<br>5.  Crash one of the replica. Wait for 60 seconds.<br>6.  Crash all the replicas.<br>7.  Make sure volume and Pod recovers.<br>8.  Check `test_data` in the Pod. | High Availability: Automatic salvage |
| 8   | test\_salvage\_auto\_crash\_replicas\_short\_wait | \[HA\] Test automatic salvage feature, with replica building pending<br><br>1.  Create a PV/PVC/Pod with liveness check.<br>2.  Create volume and start the pod.<br>3.  Generate `test_data` and write to the pod.<br>4.  Run `sync` command inside the pod to make sure data flush to the volume.<br>5.  Crash one of the replica. Wait for 5 seconds.<br>6.  Crash all the replicas.<br>7.  Make sure volume and Pod recovers.<br>8.  Check `test_data` in the Pod. | High Availability: Automatic salvage |
| 9   | test\_offline\_node\_with\_attached\_volume\_and\_pod | Test offline node with attached volume and pod<br><br>1.  Create PV/PVC/Deployment manifest.<br>2.  Update deployment's tolerations to 20 seconds to speed up test<br>3.  Update deployment's node affinity rule to avoid the current node<br>4.  Create volume, PV/PVC and deployment.<br>5.  Find the pod in the deployment and write `test_data` into it<br>6.  Shutdown the node pod is running on<br>7.  Wait for deployment to delete the pod<br>    1.  Deployment cannot delete the pod here because kubelet doesn't response<br>        <br>8.  Force delete the terminating pod<br>9.  Wait for the new pod to be created and the volume attached<br>10.  Check `test_data` in the new pod | High Availability |

**Test related to replica scheduling and volume recovery (salvage)**
--------------------------------------------------------------------

| **#**   | **Test Case** | **Test Instructions** | **Expected Results** | **Automated ? / test name** |
| --- | --- | --- | --- | --- |
| 1   | Rebuilding replica | 1.  Create a volume and attach it to a node<br>2.  Delete a replica | *   New system hidden snapshot should be created<br>*   A new replica should be created<br>*   New replicas should be `Running` & `Rebuilding` | test\_ha\_simple\_recovery |
| 2   | Volume Automatic Recovery Disabled (Automatic salvage setting) | *   **Prerequisite:**<br>    *   Disable Volume `Automatic salvage` Setting<br>        <br>    *   A pod is running and using Longhorn volume.<br>        <br><br>1.  Write some data to volume and compute its checksum (checksum#1)<br>2.  Stop all the replicas of the longhorn volume at the SAME time.<br>3.  Manually salvage faulted volume from one of the replicas<br>4.  Delete and recreate the pod. | *   Volume state should be faulted<br>*   All Volume replicas should be faulted<br>*   Volume **automatic recovery** will **NOT** be activated<br>*   After manual salvage is done, volume state should be detached and reattached<br>*   After pod recreated, volume should be reattached and accessible to pod, data checksum of the data should match (checksum#1) | test\_ha\_salvage |
| 3   | Volume Automatic Recovery Enabled (Automatic salvage setting) | *   **Prerequisite:**<br>    *   Enable Volume `Automatic salvage` Setting<br>        <br>    *   A pod is running and using Longhorn volume.<br>        <br><br>1.  Write some data to volume and compute its checksum (checksum#1)<br>2.  Stop all the replicas of the longhorn volume at the SAME time.<br>3.  Restart the pod to re-mount the volume directory | *   Volume state should be faulted<br>*   All Volume replicas should be faulted<br>*   Volume **automatic recovery** will be activated<br>*   Volume should be detached and re-attached after **automatic recovery** is triggered.<br>*   After pod restart, volume should be accessible, data checksum of the data should match (checksum#1). | test\_salvage\_auto\_crash\_all\_replicas |
