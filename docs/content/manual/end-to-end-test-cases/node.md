---
title: 7. Node
---

**UI specific test cases**
--------------------------
|   **#**  | **Test Case** | **Test Instructions** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Storage details | *   **Prerequisites**<br>    *   Longhorn Installed<br>        <br><br>1.  Verify the allocated/used storage show the right data in node details page.<br>2.  Create a volume of 20 GB and attach to a pod and verify the storage allocated/used is shown correctly. | Without any volume, allocated should be 0 and on creating new volume it should be updated as per volume present. |
| 2   | Filters applied to node list | *   **Prerequisites**<br>    *   Longhorn Installed<br>        <br><br>1.  In longhorn UI node tab- Change the filter based on name/status etc. Verify the nodes are appearing properly. | Nodes satisfying filter should only get displayed on page. |
| 3   | Sort the nodes view | *   **Prerequisites**<br>    *   Longhorn Installed<br>        <br><br>1.  In longhorn UI node tab- Click title to sort the nodes appearing based on status/name etc | Nodes list should get sorted ascending/descending based on status/name stc |
| 4   | Expand All | *   **Prerequisites**<br>    *   Longhorn Installed<br>        <br><br>1.  In longhorn UI node tab- Click ‘expand all’ button. | All nodes should get expanded and show disks details |

Additional Tests for UI
-----------------------
| **#** | **Scenario** | **Steps** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Readiness column | 1.  Click On a node’s readiness state say “Ready”<br>2.  Verify components window opens<br>3.  Verify Engine image and instance manager details are seen | Components window should open with details of Engine image and instance manager |
| 2   | Replicas column | 1.  Click on the count (number of replicas) for a node in the replica column<br>2.  Verify the list of replicas on the node is available<br>3.  Verify user is able to delete a replica on the node by selecting the replica and clicking on delete<br>4.  Verify the replica is deleted by navigating to the specific Volume → Volume details page → replicas | *   User should be able to view all the replicas on the node<br>*   User should be able to delete replica on the node<br>*   User should be able to see the replica deleted in the volume → Volume details page → replicas page |

Automation Tests
----------------
| **#** | **Test name** | **Description** | **Tags** |
| --- | --- | --- | --- |
| 1   | test\_hosts | Check node name and IP | Node |
| 2   | test\_offline\_node | Test offline node<br><br>1.  Bring down one of the nodes in Kuberntes cluster (avoid current node)<br>2.  Make sure the Longhorn node state become `down` | Node |
| 3   | test\_node\_controller\_sync\_disk\_state | Test node controller to sync disk state<br><br>1.  Set setting `StorageMinimalAvailablePercentage` to 100<br>2.  All the disks will become `unschedulable`.<br>3.  Restore setting `StorageMinimalAvailablePercentage` to previous<br>4.  All the disks will become `schedulable`. | Node |
| 4   | test\_node\_controller\_sync\_storage\_available | Test node controller sync storage available correctly<br><br>1.  Create a host disk `test_disk` on the current node<br>2.  Write 1MiB data to the disk, and run `sync`<br>3.  Verify the disk `storageAvailable` will update to include the file | Node |
| 5   | test\_node\_controller\_sync\_storage\_scheduled | Test node controller sync storage scheduled correctly<br><br>1.  Wait until no disk has anything scheduled<br>2.  Create a volume with "number of nodes" replicas<br>3.  Confirm that each disks now has "volume size" scheduled<br>4.  Confirm every disks are still schedulable. | Node |
| 6   | test\_node\_disk\_update | Test update node disks<br><br>The test will use Longhorn to create disks on the node.<br><br>1.  Get the current node<br>2.  Try to delete all the disks. It should fail due to scheduling is enabled<br>3.  Create two disks `disk1` and `disk2`, attach them to the current node.<br>4.  Add two disks to the current node.<br>5.  Verify two extra disks have been added to the node<br>6.  Disbale the two disks' scheduling, and set StorageReserved<br>7.  Update the two disks.<br>8.  Validate all the disks properties.<br>9.  Delete other two disks. Validate deletion works | Node |
| 7   | test\_node\_umount\_disk | \[Node\] Test umount and delete the extra disk on the node<br><br>1.  Create host disk and attach it to the current node<br>2.  Disable the existing disk's scheduling on the current node<br>3.  Add the disk to the current node<br>4.  Wait for node to recongize the disk<br>5.  Create a volume with "number of nodes" replicas<br>6.  Umount the disk from the host<br>7.  Verify the disk `READY` condition become false.<br>    1.  Maximum and available storage become zero.<br>        <br>    2.  No change to storage scheduled and storage reserved.<br>        <br>8.  Try to delete the extra disk, it should fail due to need to disable scheduling first<br>9.  Update the other disk on the node to be allow scheduling. Disable the scheduling for the extra disk<br>10.  Mount the disk back<br>11.  Verify the disk `READY` condition become true, and other states<br>12.  Umount and delete the disk. | Node |
| 8   | test\_update\_node | Test update node scheduling<br><br>1.  Get list of nodes<br>2.  Update scheduling to false for current node<br>3.  Read back to verify<br>4.  Update scheduling to true for current node<br>5.  Read back to verify | Node |
| 9   | test\_invalid\_node\_annotations | Test invalid node annotations for default disks/node configuration<br><br>Case1: The invalid disk annotation shouldn't intervene the node controller.<br><br>1.  Set invalid disk annotation<br>2.  The node tag or disks won't be updated<br>3.  Create a new disk. It will be updated by the node controller.<br><br>Case2: The existing node disks keep unchanged even if the annotation is corrected.<br><br>1.  Set valid disk annotation but set `allowScheduling` to false, etc.<br>2.  Make sure the current disk won't change<br><br>Case3: the correct annotation should be applied after cleaning up all disks<br><br>1.  Delete all the disks on the node<br>2.  Wait for the config from disk annotation applied<br><br>Case4: The invalid tag annotation shouldn't intervene the node controller.<br><br>1.  Cleanup the node annotation and remove the node disks/tags<br>2.  Set invalid tag annotation<br>3.  Disk and tags configuration will not be applied<br>4.  Disk and tags can still be updated on the node<br><br>Case5: The existing node keep unchanged even if the tag annotation is fixed up.<br><br>1.  With existing tags, change tag annotation.<br>2.  It won't change the current node's tag<br><br>Case6: Clean up all node tags then the correct annotation should be applied<br><br>1.  Clean the current tags<br>2.  New tags from node annotation should be applied | Node: Default Disks and Node Configuration |
| 10  | test\_no\_node\_annotation | Test node labeled for configuration but no annotation<br><br>1.  Set setting `create default disk labeled nodes` to true<br>2.  Set the config label on node 0 but leave annotation empty<br>3.  Verify disk update works.<br>4.  Verify tag update works<br>5.  Verify using tag annotation for configuration works.<br>6.  After remove the tag annotaion, verify unset tag node works fine.<br>7.  Set tag annotation again. Verify node updated for the tag. | Node: Default Disks and Node Configuration |
| 11  | test\_node\_config\_annotations | Test node feature: default disks/node configuration<br><br>1.  Set node 0 label and annotation.<br>2.  Set node 1 label but with invalid annotation (invalid path and tag)<br>3.  Cleanup disks on node 0 and 1.<br>    1.  The initial default disk will not be recreated.<br>        <br>4.  Enable setting `create default disk labeled nodes`<br>5.  Wait for node tag to update on node 0.<br>6.  Verify node 0 has correct disk and tags set.<br>7.  Verify node 1 has no disk or tag.<br>8.  Update node 1's label and tag to be valid<br>9.  Verify now node 1 has correct disk and tags set | Node: Default Disks and Node Configuration |
| 12  | test\_node\_default\_disk\_labeled | Test node feature: create default Disk according to the node label<br><br>Makes sure the created Disk matches the Default Data Path Setting.<br><br>1.  Add labels to node 0 and 1, don't add label to node 2.<br>2.  Remove all the disks on node 1 and 2.<br>    1.  The initial default disk will not be recreated.<br>        <br>3.  Set setting `default disk path` to a random disk path.<br>4.  Set setting `create default disk labeled node` to true.<br>5.  Check node 0. It should still use the previous default disk path.<br>    1.  Due to we didn't remove the disk from node 0.<br>        <br>6.  Check node 1. A new disk should be created at the random disk path.<br>7.  Check node 2. There is still no disks | Node: Default Disks and Node Configuration |

Test cases
----------

|     | **Test Case** | **Test Instructions** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Node scheduling | *   **Prerequisites:**<br>    *   Longhorn Deployed with 3 nodes<br>        <br><br>1.  Disable Node Scheduling on a node<br>2.  Create a volume with 3 replicas, and attach it to a node<br>3.  Re-enabled node scheduling on the node | *   Volume should be created and attached<br>*   Volume replicas should be scheduled to Schedulable nodes only<br>*   Re-enabling node scheduling will not affect existing scheduled replicas, it will only affect new replicas being created, or rebuilt. |     |
| 2   | Disk Scheduling | *   **Prerequisites:**<br>    *   Longhorn Deployed with 3 nodes<br>        <br>    *   Add additional disk (Disk#1) ,attach it and mounted to Node-01.<br>        <br><br>1.  Create a New Disk, Keep Disk Scheduling disabled<br>2.  Create a volume (vol#1), set replica count to `4` and attach it to a node<br>3.  Check (vol#1) replica paths<br>4.  Enable Scheduling on (disk#1)<br>5.  Create a volume (vol#2), set replica count to `4` and attach it to a node<br>6.  Check (vol#2) replica paths | *   (vol#1) replicas should be scheduled only to Disks withe Scheduling enabled, no replicas should be scheduled to (disk#1)<br>*   One of (vol#2) replica paths will be scheduled to (disk#1) | Pass<br><br>Case of vol#2 - Not necessarily replica will exists on disk#1 provided soft anti affinity is enabled. It might scheduled on disk#1 |
| 3   | Volume Created with Node Tags | *   **Prerequisites:**<br>    *   Longhorn Deployed with 3 nodes<br>        <br><br>1.  Create Node tags as follows:<br>    1.  Node-01: fast<br>        <br>    2.  Node-02: slow<br>        <br>    3.  Node-02: fast<br>        <br>2.  Create a volume (vol#1), set Node tags to slow<br>3.  Create a volume (vol#2), set Node tags to fast<br>4.  Check Volumes replicas paths<br>5.  Check Volume detail `Node Tags` | *   vol#1 replicas should only be scheduled to Node-02<br>*   vol#2 replicas should only be scheduled to Node-01 and Node-03<br>*   Node Tag volume detail should contain Node tag specified in volume creation request. |
| 4   | Volumes created with Disk Tags | *   **Prerequisites:**<br>    *   Longhorn Deployed with 3 nodes, with default disks (disk#01-1, disk#02-1, disk#03-1)<br>        <br>    *   `disk#0X-Y` indicate that disk is attached to `Node-0X` , and it is disk number `Y` on that node.<br>        <br>*   Create 3 additional disks (disk#01-2, disk#02-2, disk#03-2), attach each one to a different node, and mount it to a directory on that node.<br><br>1.  Create Disk tags as follows:<br>    1.  disk#01-1: fast<br>        <br>    2.  disk#01-2: fast<br>        <br>    3.  disk#02-1: slow<br>        <br>    4.  disk#02-2: slow<br>        <br>    5.  disk#03-1: fast<br>        <br>    6.  disk#01-2: fast<br>        <br>2.  Create a volume (vol#1), set Disk tags to slow<br>3.  Create a volume (vol#2), set Disk tags to fast<br>4.  Check Volumes replicas paths<br>5.  Check Volume detail `Disk Tags` | *   vol#1 replicas should only be scheduled to disks have slow tag (disk#02-1 and disk#02-2)<br>*   vol#2 replicas should can be scheduled to disks have fast Tag  <br>    (disk#01-1, disk#01-2, disk#03-1, disk#03-2)<br>*   Disk Tag volume detail should contain Disk tag specified in volume creation request. |
| 5   | Volumes created with both DIsk and Node Tags | *   Create a volume, set Disk and node tags, and attach it to a node | *   Volume replicas should be scheduled only to node that have Node tags, and only on disks that have Disk tags specified on volume creation request<br>*   If No Node match both Node and Disk tags, volume replicas will not be created. |
| 6   | Remove Disk From Node | *   **Prerequisites:**<br>    *   Longhorn Deployed with 3 nodes<br>        <br>    *   Add additional disk (Disk#1) ,attach it and mounted to Node-01.<br>        <br>    *   Some replicas should be scheduled to Disk#1<br>        <br><br>1.  Disable Scheduling on disk#1<br>2.  Delete all replicas scheduled to disk#1, replicas should start to rebuild on other disks<br>3.  Delete disk from node | *   Stopping Disk scheduling will prevent replicas to be scheduled on it<br>*   Disk can’t be deleted if at least one replicas is still scheduled to it.<br>*   Disk can be delete only after all replica have been rescheduled to other disks. |
| 7   | Power off a node | 1.  Power off a node | *   Node should report down on Node page |
| 8   | Delete Longhorn Node | 1.  Disable Scheduling on the node<br>2.  Delete all replicas on the node to be rescheduled to another nodes<br>3.  Detach all volume attached to the node, re-attach them on other nodes<br>4.  Delete Node from Kubernetes<br>5.  Delete Node From Longhorn | *   Node can’t be deleted if Node Scheduling is enabled on that node<br>*   Node can’t be deleted unless it all replicas are deleted from that node<br>*   Node can’t be deleted unless it all attached volumes get detached from that node<br>*   Node can’t be deleted unless it has been deleted from Kubernetes first<br>*   After node is deleted from Kubernetes, node should report down on Longhorn<br>*   Node should be deleted from Longhorn |
| 9   | Default Disk on Labeled Nodes | *   **Prerequisites:**<br>    *   Create 3 node k8s cluster<br>        <br>    *   Create `/home/longhorn` directory on all 3 nodes<br>        <br>    *   Add new disk to each node, format it with `ext4`, and mount it to `/mnt/disk`<br>        <br>*   Use the following label and annotations for nodes<br><br>Node-01 & Node-03<br><br>`labels:`<br><br>`node.longhorn.io/create-``default``-disk:` `"config"`<br><br>`annotations:`<br><br>`node.longhorn.io/``default``-disks-config:`<br><br>`'[{``"path"``:``"/home/longhorn"``,``"allowScheduling"``:``true``,` `"tags"``:[``"ssd"``,` `"fast"``]},`<br><br>`{``"path"``:``"/mnt/disk"``,``"allowScheduling"``:``true``,``"storageReserved"``:``1024``,``"tags"``:[``"ssd"``,``"fast"``]}]'`<br><br>`node.longhorn.io/``default``-node-tags:` `'["fast", "storage"]'`<br><br>Node-02<br><br> `labels:`<br><br>`node.longhorn.io/create-``default``-disk:` `"config"`<br><br>`annotations:`<br><br>`node.longhorn.io/``default``-disks-config:`<br><br>`'[{``"path"``:``"/home/longhorn"``,``"allowScheduling"``:``true``,` `"tags"``:[``"hdd"``,` `"slow"``]},`<br><br>`{``"path"``:``"/mnt/disk"``,``"allowScheduling"``:``true``,``"storageReserved"``:``1024``,``"tags"``:[``"hdd"``,``"slow"``]}]'`<br><br>`node.longhorn.io/``default``-node-tags:` `'["slow", "storage"]'` <br><br>1.  Set `create-default-disk-labeled-nodes: True` in `longhorn-default-setting` config map<br>2.  Deploy Longhorn | *   Longhorn Should be deployed successfully<br>*   Node-01 & Node-03<br>    *   should be tagged with `fast` and `storage` tags<br>        <br>    *   Disk scheduling should be allowed on both disks<br>        <br>    *   Disks should be tagged with `ssd` and `fast` tags<br>        <br>    *   1024 MB is reserved storage on `/mnt/disk`<br>        <br>*   Node-02<br>    *   should be tagged with `Slow` and `storage` tags<br>        <br>    *   should be tagged with `slow` and `storage` tags<br>        <br>    *   Disk scheduling should be allowed on both disks<br>        <br>    *   Disks should be tagged with `hdd` and `slow` tags<br>        <br>    *   1024 MB is reserved storage on `/mnt/disk` |
| 10  | Default Data Path | **Prerequisites:**<br><br>*   Create 3 node k8s cluster<br>*   Create `/home/longhorn` directory on all 3 nodes<br><br>  <br><br>1.  Set `defaultDataPath` to `/home/longhorn/` in `longhorn-default-setting` ConfigMap<br>2.  Deploy Longhorn<br>3.  Create a volume, attach it to a node | *   In Longhorn Setting, `Default Data Path` should be `/home/longhorn`<br>*   All volumes replicas paths should begin with `/home/longhorn` prefix |
| 11  | Update Taint Toleration Setting | **Prerequisites**<br><br>*   All Longhorn volumes should be detached then Longhorn components will be restarted to apply new tolerations.<br>*   Notice that "[kubernetes.io](http://kubernetes.io)" is used as the key of all Kubernetes default tolerations, please do not contain this substring in your toleration setting.<br>*   In Longhorn taint tolerations are column separated<br><br>1.  Using Kubernetes, taint **Some** nodes  <br>    For example, `key1=value1:NoSchedule` `key2=value2:NoExecute`<br>2.  Update Taint Toleration Setting with `key1=value1:NoSchedule;key2:NoExecute` | *   Longhorn Components will be restarted<br>*   Longhorn Components should be rescheduled to tainted nodes nodes only. |
| 12  | Default Taint Toleration Setting | **Prerequisites**<br><br>*   Create 3 node k8s cluster<br>*   Using Kubernetes, taint **Some** nodes  <br>    For example, `key1=value1:NoSchedule` `key2=value2:NoExecute`<br><br>1.  Set `taint-toleration` to `key1=value1:NoSchedule;key2:NoExecute` in `longhorn-default-setting` ConfigMap<br>2.  Deploy Longhorn | *   Longhorn components should be only deployed to tainted nodes |
| 13  | Node Readiness | *   Delete the following Longhorn components pods<br>    *   Engine image<br>        <br>    *   Instance Manager (engine)<br>        <br>    *   Instance Manager (replica) | *   Deleting any components should be reflected on node Readiness<br>*   Deleted component must be redeployed |
| 14  | Storage Minimal Available Percentage Setting | *   **Prerequisites**<br>    *   Longhorn Installed<br>        <br><br>1.  Change Storage Minimal Available Percentage to `50%`<br>2.  Fill up node disk up to 55% of it’s capacity | *   Storage Minimal Available Percentage **default value** is `25%`<br>*   Filled Disk Should be `Unschedulable`<br>*   If Node has only one disk, Node also should be `Unschedulable` |
| 15  | Storage Over Provisioning Percentage | *   **Prerequisites**<br>    *   Longhorn Installed<br>        <br>    *   Assume Nodes has disks with 100GB size<br>        <br><br>1.  Change `Storage Over Provisioning Percentage` to `300`<br>2.  Check Node Disks available size for allocation | *   Storage Over Provisioning Percentage default value is `200`<br>*   Disk storage that can be allocated relative to the hard drive's capacity should be 3x disk size == `300 GB` |

Additional Tests
----------------

| **#** | **Scenario** | **Steps** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Create Default Disk on Labeled Nodes - True | 1.  In Longhorn Setting - set Create Default Disk on Labeled Nodes - True<br>2.  Scale up the number of worker nodes in Rancher<br>3.  The node is displayed in Longhorn UI when it comes up “Active” in rancher.<br>4.  Verify the node’s status is “Disabled”<br>5.  Verify the default disk is NOT created in → Node → Edit Node and Disk<br>6.  Add label `node.longhorn.io/create-default-disk=true` on the node<br>7.  Verify the node is seen as “Schedulable” on longhorn UI. Verify Node → Edit Node and Disk, the default disk is created<br>8.  Add a node tag to this node n1<br>9.  Create a volume - volume-1, add node tag n1<br>10.  Attach it to the same node<br>11.  Verify the replica is running successfully. | 1.  Create Default Disk on Labeled Nodes should be set to True<br>2.  When a new node is added, the node should show up as disabled on Longhorn UI<br>3.  The node should NOT have any default disk<br>4.  Label should be added on the node.<br>5.  The node status changes to “Schedulable” and default disk is created on the node. |

Node/Disk Eviction Test cases
-----------------------------

| **#** | **Scenario** | **Test Steps** | **Expected Results** |
|-------|--------------|----------------|----------------------|
| 1   | Attached volume replica evict from node | **Pre-requisite:**<br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br>1.  Create a volume (3 replicas), attach it to a pod.<br>2.  Write data to it and compute md5sum.<br>3.  Enable soft anti-affinity to True.<br>4.  Evict replicas from one node<br>5.  Verify the data after the replica is evited. | 1.  A replica should be rebuilt in any other node except the evicted node.<br>2.  The replica from the evicted node should be removed.<br>3.  Data should be intact.
| 2   | Interrupt the rebuild after the eviction of replica from node | **Pre-requisite:**<br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br>1.  Create a volume (3 replicas), attach it to a pod.<br>2.  Write data to it and compute md5sum.<br>3.  Enable soft anti-affinity to True.<br>4.  Evict replicas from one node<br>5.  When the replica is rebuilding, delete it. | 1.  A replica should be rebuilt in any other node except the evicted node.<br>2.  On the deletion of the replica, the system should start to rebuild a new replica.<br>3.  The replica from the evicted node will be removed.<br>4.  Data should be intact.
| 3   | Node evicted while restore rebuilding is in progress | **Pre-requisite:**<br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br>1.  Restore a volume (3 replicas).<br>2.  While it is restoring, evict replicas from one node.<br>3.  When the replica is rebuilding, delete it.<br>1.  The rebuilding replica should be completed and then a new replica should get created on another node.<br>2.  The replica from the evicted node should be removed.<br>3.  Data should be intact.
| 4   | Evict node with multiple replicas of the same volume | **Pre-requisite:**<br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br>1.  Enable soft anti-affinity to True.<br>2.  Create a volume (3 replicas) and make sure two replicas exist on the same node.<br>3.  Write data to it and compute md5sum.<br>4.  Evict replicas from the node where the two replicas exist.<br>5.  Verify the data after the replicas are evicted. | 1.  Two replicas should be rebuilt in any other node except the evicted node.<br>2.  The replica from the evicted node should be removed.<br>3.  Data should be intact
| 5   | Evict node with soft anti-affinity as false | **Pre-requisite:**<br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br>1.  Create a volume (3 replicas), attach it to a pod.<br>2.  Write data to it and compute md5sum.<br>3.  Evict replicas from one node<br>4.  Verify the data after the replica is evited. | 1.  Eviction should stuck.<br>2.  Volume scheduling should fail.<br>3.  There should be logs like <pre>`[longhorn-manager-dkn9c] time=``"2020-09-09T00:23:56Z"` `level=debug msg=``"Creating one more replica for eviction"`<br>`[longhorn-manager-dkn9c] time=``"2020-09-09T00:23:56Z"` `level=error msg=``"There's no available disk for replica vol-1-r-6268393a, size 2147483648"`<br>`[longhorn-manager-dkn9c] time=``"2020-09-09T00:23:56Z"` `level=error msg=``"unable to schedule replica vol-1-r-6268393a of volume vol-1"`</pre>
| 6   | Add node after evicting node with soft anti-affinity as false | **Pre-requisite:**<br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br>1.  Create a volume (3 replicas), attach it to a pod.<br>2.  Write data to it and compute md5sum.<br>3.  Evict replicas from one node<br>4.  Add a worker node to the cluster.| 1.  Eviction should stuck but recover after the additional node is available for scheduling.<br>2.  Volume scheduling should fail initially but should be successful once the additional node is available.
| 7   | Multi operation | **Pre-requisite:**<br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br><br>1.  Create a volume (3 replicas), attach it to a pod.<br>2.  Write data to it and compute md5sum.<br>3.  Enable soft anti-affinity to True.<br>4.  Select two nodes and evict replicas from them. | 1.  Replica eviction should happen one by one from nodes |
| 8   | Replica eviction from disk with soft anti-affinity as True. | **Pre-requisite:**<br><br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br><br>1.  Add an additional disk to a node.<br>2.  Create a volume (3 replicas), attach it to a pod.<br>3.  Write data to it and compute md5sum.<br>4.  Enable soft anti-affinity to True.<br>5.  Evict from the additional disk. | 1.  A replica should be rebuilt in any other node except the evicted node.<br>2.  The replica from the evicted node should be removed.<br>3.  Data should be intact |
| 9   | Replica eviction from disk with soft anti-affinity as False. | **Pre-requisite:**<br><br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br><br>1.  Add an additional disk to a node.<br>2.  Create a volume (3 replicas), attach it to a pod.<br>3.  Write data to it and compute md5sum.<br>4.  Enable soft anti-affinity to True.<br>5.  Evict from the additional disk. | 1.  A replica should be rebuilt in another disk on the same node.<br>2.  The replica from the evicted node should be removed.<br>3.  Data should be intact |
| 10  | Interrupt eviction | **Pre-requisite:**<br><br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br><br>1.  Create a volume (3 replicas), attach it to a pod.<br>2.  Write data to it and compute md5sum.<br>3.  Enable soft anti-affinity to True.<br>4.  Evict replicas from one node<br>5.  Stop eviction. | 1.  The replicas should evict one by one and once the eviction |
| 11  | Evict replica of volume with DR volume | **Pre-requisite:**<br><br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br><br>1.  Create a DR volume (3 replicas)<br>2.  Write data to it and compute md5sum.<br>3.  Enable soft anti-affinity to True.<br>4.  Evict replicas from one node<br>5.  Verify the data after the replica is evited. | 1.  A replica should be rebuilt in any other node except the evicted node.<br>2.  The replica from the evicted node should be removed.<br>3.  Data should be intact. |
| 12  | Evict replica of volume with the restored volume | **Pre-requisite:**<br><br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br><br>1.  Restore a volume (3 replicas) from backup, attach it to a pod.<br>2.  Enable soft anti-affinity to True.<br>3.  Evict replicas from one node<br>4.  Verify the data after the replica is evited. | 1.  A replica should be rebuilt in any other node except the evicted node.<br>2.  The replica from the evicted node should be removed.<br>3.  Data should be intact. |
| 13  | Evict replica of volume with the detached volume | **Pre-requisite:**<br><br>1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane<br><br>**Test Steps:**<br><br>1.  Restore a volume (3 replicas) from backup.<br>2.  Write data to it and compute md5sum.<br>3.  Enable soft anti-affinity to True.<br>4.  Evict replicas from one node<br>5.  Verify the data after the replica is evited. | 1.  The volume should get attached to a node.<br>2.  A replica should be rebuilt in any other node except the evicted node.<br>3.  The replica from the evicted node should be removed.<br>4.  The volume should get detached.<br>5.  Data should be intact. |
| 14  | On upgraded setup | **Pre-requisite:**<br><br>1.  Longhorn v1.0.2 installed in set up of 4 worker nodes and 1 etc/control plane<br>2.  Create a volume, restored volume and DR volume<br><br>**Test Steps:**<br><br>1.  Upgrade the longhorn to master.<br>2.  Create a volume and attach it to a pod.<br>3.  Write data to it and compute md5sum.<br>4.  Evict replicas from one node<br>5.  Verify the data after the replica is evited. | 1.  The replicas of volumes with v1.0.2 engine should also get evicted except the DR volume. |