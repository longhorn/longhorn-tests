---
title: 8. Scheduling
---

Manual Test
-----------

| **Test name** | **Prerequisite** | **Expectation** |
| --- | --- | --- |
| EKS across zone scheduling | **Prerequisite**:<br><br>*   EKS Cluster with 3 nodes across two AWS zones (zone#1, zone#2)<br><br>1.  Create a volume with 2 replicas, and attach it to a node.<br>2.  Delete a replica scheduled to each zone, repeat it few times<br>3.  Scale volume replicas = 3<br>4.  Scale volume replicas to 4 | *   Volume replicas should be scheduled one per AWS zone<br>*   Deleting a replica in a zone should trigger a replica rebuild<br>*   new rebuilding replica should be scheduled to the same zone as the deleted replica<br>*   Scaling volume replicas to 3 will distribute replicas across all nodes<br>*   Scaling volume replicas to 4 will be governed by soft anti-affinity rule, so no guarantee on which node the new replica should be scheduled. |

Automation tests
----------------

|     | **Test name** | **Description** | **Tags** |
| --- | --- | --- | --- |
| 1   | test\_volume\_scheduling\_failure | Test fail to schedule by disable scheduling for all the nodes<br><br>Also test cannot attach a scheduling failed volume<br><br>1.  Disable allowScheduling for all nodes<br>2.  Create a volume.<br>3.  Verify the volume condition Scheduled is false<br>4.  Verify attaching the volume will result in error<br>5.  Enable allowScheduling for all nodes<br>6.  Volume should be automatically scheduled (condition become true)<br>7.  Volume can be attached now | Scheduling |
| 2   | test\_replica\_scheduler\_exceed\_over\_provisioning | Test replica scheduler: exceeding overprovisioning parameter<br><br>1.  Set setting `overprovisioning` to 100<br>2.  Update every disks to set 1G available for scheduling<br>3.  Try to schedule a volume of 2G. Volume scheduled condition should be false | Scheduling: Space |
| 3   | test\_replica\_scheduler\_just\_under\_over\_provisioning | Test replica scheduler: just under overprovisioning parameter<br><br>1.  Set setting `overprovisioning` to 100<br>2.  Get the maximum size of all the disks<br>3.  Create a volume using maximum\_size - 2MiB as the volume size.<br>4.  Volume scheduled condition should be true.<br>5.  Make sure every replica landed on different nodes's default disk. | Scheduling: Space |
| 4   | test\_replica\_scheduler\_large\_volume\_fit\_small\_disk | Test replica scheduler: not schedule a large volume to small disk<br><br>1.  Create a host disk `small_disk` and attach i to the current node.<br>2.  Create a new large volume.<br>3.  Verify the volume wasn't scheduled on the `small_disk` | Scheduling: Space |
| 5   | test\_replica\_scheduler\_no\_disks | Test replica scheduler with no disks available<br><br>1.  Delete all the disks on all the nodes<br>2.  Create a volume.<br>3.  Wait for volume condition `scheduled` to be false | Scheduling: Space |
| 6   | test\_replica\_scheduler\_rebuild\_restore\_is\_too\_big | Test replica scheduler: rebuild/restore can be too big to fit a disk<br><br>1.  Create a small host disk with `SIZE` and add it to the current node.<br>2.  Create a volume with size `SIZE`.<br>3.  Disable all scheduling except for the small disk.<br>4.  Write a data size `SIZE * 0.9` to the volume and make a backup<br>5.  Create a restored volume with 1 replica from backup.<br>    1.  Verify the restored volume cannot be scheduled since the existing data cannot fit in the small disk<br>        <br>6.  Delete a replica of volume.<br>    1.  Verify the volume reports `scheduled = false` due to unable to find a suitable disk for rebuliding replica, since the replica with the existing data cannot fit in the small disk<br>        <br>7.  Enable the scheduling for other disks, disable scheduling for small disk<br>8.  Verify the volume reports `scheduled = true`. And verify the data.<br>9.  Cleanup the volume.<br>10.  Verify the restored volume reports `scheduled = true`.<br>11.  Wait for the restored volume to complete restoration, then check data. | Scheduling: Space<br><br>Backup |
| 7   | test\_replica\_scheduler\_too\_large\_volume\_fit\_any\_disks | Test replica scheduler: volume is too large to fit any disks<br><br>1.  Disable all default disks on all nodes by setting storageReserved to maximum size<br>2.  Create volume.<br>3.  Verify the volume scheduled condition is false.<br>4.  Reduce the storageReserved on all the disks to just enough for one replica.<br>5.  The volume should automatically change scheduled condition to true<br>6.  Attach the volume.<br>7.  Make sure every replica landed on different nodes's default disk | Scheduling: Space |
| 8   | test\_replica\_scheduler\_update\_minimal\_available | Test replica scheduler: update setting `minimal available`<br><br>1.  Set setting `minimal available` to 100% (means no one can schedule)<br>2.  Verify for all disks' schedulable condition to become false.<br>3.  Create a volume. Verify it's unschedulable.<br>4.  Set setting `minimal available` back to default setting<br>5.  Disk should become schedulable now.<br>6.  Volume should be scheduled now.<br>7.  Attach the volume.<br>8.  Make sure every replica landed on different nodes's default disk. | Scheduling: Space |
| 9   | test\_replica\_scheduler\_update\_over\_provisioning | Test replica scheduler: update overprovisioning setting<br><br>1.  Set setting `overprovisioning` to 0. (disable all scheduling)<br>2.  Create a new volume. Verify volume's `scheduled` condition is false.<br>3.  Set setting `over provisioning` to 100%.<br>4.  Verify volume's `scheduled` condition now become true.<br>5.  Attach the volume.<br>6.  Make sure every replica landed on different nodes's default disk. | Scheduling: Space |
| 10  | test\_hard\_anti\_affinity\_detach | Test that volumes with Hard Anti-Affinity are still able to detach and reattach to a node properly, even in degraded state.<br><br>1.  Create a volume and attach to the current node<br>2.  Generate and write `data` to the volume.<br>3.  Set `soft anti-affinity` to false<br>4.  Disable current node's scheduling.<br>5.  Remove the replica on the current node<br>    1.  Verify volume will be in degraded state.<br>        <br>    2.  Verify volume reports condition `scheduled == false`<br>        <br>6.  Detach the volume.<br>7.  Verify that volume only have 2 replicas<br>    1.  Unhealthy replica will be removed upon detach.<br>        <br>8.  Attach the volume again.<br>    1.  Verify volume will be in degraded state.<br>        <br>    2.  Verify volume reports condition `scheduled == false`<br>        <br>    3.  Verify only two of three replicas of volume are healthy.<br>        <br>    4.  Verify the remaining replica doesn't have `replica.HostID`, meaning it's unscheduled<br>        <br>9.  Check volume `data` | Scheduling: Anti-affinity |
| 11  | test\_hard\_anti\_affinity\_live\_rebuild | Test that volumes with Hard Anti-Affinity can build new replicas live once a valid node is available.<br><br>If no nodes without existing replicas are available, the volume should remain in "Degraded" state. However, once one is available, the replica should now be scheduled successfully, with the volume returning to "Healthy" state.<br><br>1.  Create a volume and attach to the current node<br>2.  Generate and write `data` to the volume.<br>3.  Set `soft anti-affinity` to false<br>4.  Disable current node's scheduling.<br>5.  Remove the replica on the current node<br>    1.  Verify volume will be in degraded state.<br>        <br>    2.  Verify volume reports condition `scheduled == false`<br>        <br>6.  Enable the current node's scheduling<br>7.  Wait for volume to start rebuilding and become healthy again<br>8.  Check volume `data` | Scheduling: Anti-affinity |
| 12  | test\_hard\_anti\_affinity\_offline\_rebuild | Test that volumes with Hard Anti-Affinity can build new replicas during the attaching process once a valid node is available.<br><br>Once a new replica has been built as part of the attaching process, the volume should be Healthy again.<br><br>1.  Create a volume and attach to the current node<br>2.  Generate and write `data` to the volume.<br>3.  Set `soft anti-affinity` to false<br>4.  Disable current node's scheduling.<br>5.  Remove the replica on the current node<br>    1.  Verify volume will be in degraded state.<br>        <br>    2.  Verify volume reports condition `scheduled == false`<br>        <br>6.  Detach the volume.<br>7.  Enable current node's scheduling.<br>8.  Attach the volume again.<br>9.  Wait for volume to become healthy with 3 replicas<br>10.  Check volume `data` | Scheduling: Anti-affinity |
| 13  | test\_hard\_anti\_affinity\_scheduling | Test that volumes with Hard Anti-Affinity work as expected.<br><br>With Hard Anti-Affinity, scheduling on nodes with existing replicas should be forbidden, resulting in "Degraded" state.<br><br>1.  Create a volume and attach to the current node<br>2.  Generate and write `data` to the volume.<br>3.  Set `soft anti-affinity` to false<br>4.  Disable current node's scheduling.<br>5.  Remove the replica on the current node<br>    1.  Verify volume will be in degraded state.<br>        <br>    2.  Verify volume reports condition `scheduled == false`<br>        <br>    3.  Verify only two of three replicas of volume are healthy.<br>        <br>    4.  Verify the remaining replica doesn't have `replica.HostID`, meaning it's unscheduled<br>        <br>6.  Check volume `data` | Scheduling: Anti-affinity |
| 14  | test\_soft\_anti\_affinity\_detach | Test that volumes with Soft Anti-Affinity can detach and reattach to a node properly.<br><br>1.  Create a volume and attach to the current node.<br>2.  Generate and write `data` to the volume<br>3.  Set `soft anti-affinity` to true<br>4.  Disable current node's scheduling.<br>5.  Remove the replica on the current node<br>6.  Wait for the new replica to be rebuilt<br>7.  Detach the volume.<br>8.  Verify there are 3 replicas<br>9.  Attach the volume again. Verify there are still 3 replicas<br>10.  Verify the `data` | Scheduling: Anti-affinity |
| 15  | test\_soft\_anti\_affinity\_scheduling | Test that volumes with Soft Anti-Affinity work as expected.<br><br>With Soft Anti-Affinity, a new replica should still be scheduled on a node with an existing replica, which will result in "Healthy" state but limited redundancy.<br><br>1.  Create a volume and attach to the current node<br>2.  Generate and write `data` to the volume.<br>3.  Set `soft anti-affinity` to true<br>4.  Disable current node's scheduling.<br>5.  Remove the replica on the current node<br>6.  Wait for the volume to complete rebuild. Volume should have 3 replicas.<br>7.  Verify `data` | Scheduling: Anti-affinity |
| 16  | test\_tag\_basic | Test that applying Tags to Nodes/Disks and retrieving them work as expected. Ensures that Tags are properly validated when updated.<br><br>1.  Generate tags and apply to the disk and nodes<br>2.  Make sure the tags are applied<br>3.  Try to apply invalid tags to the disk and node. Action will fail. | Scheduling: Tag |
| 17  | test\_tag\_scheduling | Test success scheduling with tags<br><br>Case 1: Don't specify any tags, replica should be scheduled to 3 disks.<br><br>Case 2: Use disk tags to select two nodes for all replicas.<br><br>Case 3: Use node tags to select two nodes for all replicas.<br><br>Case 4: Combine node and disk tags to schedule all replicas on one node. | Scheduling: Tag |
| 18  | test\_tag\_scheduling\_failure | Test that scheduling fails if no Nodes/Disks with the requested Tags are available.<br><br>Case 1: Validate that if specifying nonexist tags in volume, API call will fail.<br><br>Case 2:<br><br>1.  Specify existing but no node or disk can unsatisfied tags.<br>2.  Validate the volume will failed the scheduling | Scheduling: Tag |
| 19  | test\_tag\_scheduling\_on\_update | Test that Replicas get scheduled if a Node/Disk disks updated with the proper Tags.<br><br>1.  Create volume with tags that can not be satisfied<br>2.  Wait for volume to fail scheduling<br>3.  Update the node and disk with extra tags to satisify the volume<br>4.  Verify now volume has been scheduled<br>5.  Attach the volume and check the replicas has been scheduled properly | Scheduling: Tag |
| 20  | test\_zone\_tags | Test anti affinity zone feature<br><br>1.  Add Kubernetes zone labels to the nodes<br>    1.  Only two zones now: zone1 and zone2<br>        <br>2.  Create a volume with two replicas<br>3.  Verify zone1 and zone2 either has one replica.<br>4.  Remove a random replica and wait for volume to finish rebuild<br>5.  Verify zone1 and zone2 either has one replica.<br>6.  Repeat step 4-5 a few times.<br>7.  Update volume to 3 replicas, make sure they're scheduled on 3 nodes<br>8.  Remove a random replica and wait for volume to finish rebuild<br>9.  Make sure replicas are on different nodes<br>10.  Repeat step 8-9 a few times | Scheduling: Zone |

Anti-affinity test
------------------

| **#** | **Test case** | **Steps** | **Expectation** | **Automation test case**|
|-------|---------------|-----------|-----------------|-------------------------|
| 1     | Replica scheduling (soft anti-affinity enabled) | **Prerequisite:**<br>*   **Replica Soft Anti-Affinity** setting is **Enabled**<br>1.  Create a volume<br>2.  Attach volume to a node<br>3.  Increase replica count to exceed the number of Longhorn node count | *   New replicas will be scheduled to node<br>*   Volume Status will be `Healthy`, with limited node redundancy hint icon<br>`Limited node redundancy: at least one healthy replica is running at the same node as another` | test\_soft\_anti\_affinity\_scheduling |
| 2     | Replica scheduling (soft anti-affinity disabled) | **Prerequisite:**<br>*   **Replica Soft Anti-Affinity** setting is **Enabled**<br>1.  Create a volume<br>2.  Attach volume to a node<br>3.  Increase replica count to exceed the number of Longhorn node count<br>4.  Disable **Replica Soft Anti-Affinity** setting<br>5.  Delete a replica<br>6.  Re-Enable **Replica Soft Anti-Affinity** setting | *   Replicas won’t be removed after disabling **Replica Soft Anti-Affinity**<br>*   when **Replica Soft Anti-Affinity** setting is disabled New Replicas will not be scheduled to nodes.<br>*   when **Replica Soft Anti-Affinity** setting is re-enabled, New Replicas can be scheduled to nodes. | test\_hard\_anti\_affinity\_scheduling |


Additional Tests
----------------

|  **#**   | **Scenario** | **Steps** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Add Disk disk1, Disable scheduling for default disk -1 | 1.  By default the disk on a node is 0 default disk in in path - `/var/lib/longhorn/`<br>2.  Add disk1 on the node<br>3.  Disable scheduling for the default disk<br>4.  Create a volume in Longhorn<br>5.  Verify the replicas are scheduled on disk1 |
| 2   | Add Disk disk1, Disable scheduling for default disk -2 | Cluster spec - 3 worker nodes<br><br>1.  Create a volume - 3 replicas in `/var/lib/longhorn/` - default disk<br>2.  Add disk 1 on `/mnt/vol2`on node 1<br>3.  Disable scheduling for the default disk<br>4.  enable scheduling for disk1<br>5.  Update the replicas to count = 4<br>6.  Say a replica is built on Node 2<br>7.  Delete the replica on node 1<br>8.  a new replica is rebuilt on node 1<br>9.  Verify replica is now available in `/mnt/vol2` | Replica when rebuilt on node 1 should be available on disk 1 - `/mnt/vol2` |
| **Disable Scheduling On Cordoned Node** |     |     |     |     |     |
| 3   | Disable Scheduling On Cordoned Node: **True**<br><br>**New volume** | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a new volume with 4 replicas<br>4.  Verify the volume `vol-1` is in detached state with error `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>5.  Add a new worker node W5 to the cluster<br>6.  `vol-1` should become healthy.<br>7.  Attach it to a workload and verify data can be written into the volume | 1.  `vol-1` should be in detached state with error `Scheduling Failure Replica Scheduling Failure`<br>2.  vol-1 should become healthy and should be used in a workload to write data into the volume |
| 4   | Disable Scheduling On Cordoned Node: **True**<br><br>**Existing volume** | 1.  There are 4 worker nodes - custom cluster<br>2.  Create a new volume with 4 replicas<br>3.  Volume vol-1 should be in a healthy detached state<br>4.  Attach it to a workload and verify data can be written into the volume<br>5.  cordon a worker node<br>6.  Use the. volume to a workload<br>7.  All the three replicas will be in running healthy state<br>8.  Delete replica on cordoned worker node<br>9.  Verify the volume `vol-1` is in degraded state with error `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>10.  Add a new worker node W5 to the cluster<br>11.  Verify the repliica failed will be in rebuilding state now<br>12.  `vol-1` should become healthy.<br>13.  Verify the data is consistent |     |
| 5   | Disable Scheduling On Cordoned Node: **False**<br><br>**New volume** | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a new volume with 4 replicas<br>4.  `vol-1` should be in healthy.<br>5.  Verify a replica is created on the cordoned worker node<br>6.  Attach it to a workload and verify data can be written into the volume |     |
| 6   | Disable Scheduling On Cordoned Node: **False**<br><br>**Existing volume** |     |     |
| 7   | Disable Scheduling On Cordoned Node: **True**<br><br>Backup restore | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a backup restore volume from an existing backup.<br>4.  Give in the number of replicas - 4, volume name: `vol-2`<br>5.  Verify the volume `vol-2` is in detached state with error `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>6.  Verify no restoring should happen on the replicas.<br>7.  Add a new worker node W5 to the cluster<br>8.  `vol-2` should start restoring now<br>9.  `vol-2` should be in detached healthy state.<br>10.  attach to a workload and verify the checksum of data with that of the original one |     |
| 8   | Disable Scheduling On Cordoned Node: **False**<br><br>Backup restore | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a backup restore volume from an existing backup.<br>4.  Give in the number of replicas - 4, volume name: `vol-2`<br>5.  Verify volume is in attached state and restoring should happen on the replicas<br>6.  `vol-2` should be in detached healthy state. after restoration is complete<br>7.  attach to a workload and verify the checksum of data with that of the original one |     |
| 7   | Disable Scheduling On Cordoned Node: **True**<br><br>Create DR volume | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a `DRV` from an existing backup.<br>4.  Give in the number of replicas - 4<br>5.  Verify the `DRV` is in detached state with error `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>6.  Verify no restoring should happen on the replicas.<br>7.  Add a new worker node W5 to the cluster<br>8.  `DRV` should start restoring now<br>9.  `DRV` should be in healthy state.<br>10.  Activate the `DRV` and verify it is in detached state<br>11.  attach to a workload and verify the checksum of data with that of the original one |     |
| 8   | Disable Scheduling On Cordoned Node: **False**<br><br>Create DR volume | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a `DRV` from an existing backup.<br>4.  Give in the number of replicas - 4<br>5.  `DRV` should start restoring now<br>6.  `DRV` should be in healthy state.<br>7.  Activate the `DRV` and verify it is in detached state<br>8.  attach to a workload and verify the checksum of data with that of the original one |     |     |
| 9   | Replica node level soft anti affinity: **False**<br><br>**New volume** | 1.  There are 3 worker nodes - custom cluster<br>2.  Create a volume with replicas - 4<br>3.  Volume should be in detached state with error - `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>4.  Add a worker node<br>5.  the volume should be in healthy state<br>6.  User should be able to use the volume on the workload |     |     |
| 10  | Replica node level soft anti affinity: **True**<br><br>**New volume** | 1.  There are 3 worker nodes - custom cluster<br>2.  Create a volume with replicas - 4<br>3.  the volume should be in healthy state. two replicas should be on the same host<br>4.  User should be able to use the volume on the workload |     |     |