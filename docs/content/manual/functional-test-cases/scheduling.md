---
title: 8. Scheduling
---

Manual Test
-----------

| **Test name** | **Prerequisite** | **Expectation** |
| --- | --- | --- |
| EKS across zone scheduling | **Prerequisite**:<br><br>*   EKS Cluster with 3 nodes across two AWS zones (zone#1, zone#2)<br><br>1.  Create a volume with 2 replicas, and attach it to a node.<br>2.  Delete a replica scheduled to each zone, repeat it few times<br>3.  Scale volume replicas = 3<br>4.  Scale volume replicas to 4 | *   Volume replicas should be scheduled one per AWS zone<br>*   Deleting a replica in a zone should trigger a replica rebuild<br>*   new rebuilding replica should be scheduled to the same zone as the deleted replica<br>*   Scaling volume replicas to 3 will distribute replicas across all nodes<br>*   Scaling volume replicas to 4 will be governed by soft anti-affinity rule, so no guarantee on which node the new replica should be scheduled. |


Anti-affinity test
------------------

| **#** | **Test case** | **Steps** | **Expectation** | **Automation test case**|
|-------|---------------|-----------|-----------------|-------------------------|
| 1     | Replica scheduling (soft anti-affinity enabled) | **Prerequisite:**<br>*   **Replica Soft Anti-Affinity** setting is **Enabled**<br>1.  Create a volume<br>2.  Attach volume to a node<br>3.  Increase replica count to exceed the number of Longhorn node count | *   New replicas will be scheduled to node<br>*   Volume Status will be `Healthy`, with limited node redundancy hint icon<br>`Limited node redundancy: at least one healthy replica is running at the same node as another` | test\_soft\_anti\_affinity\_scheduling |
| 2     | Replica scheduling (soft anti-affinity disabled) | **Prerequisite:**<br>*   **Replica Soft Anti-Affinity** setting is **Enabled**<br>1.  Create a volume<br>2.  Attach volume to a node<br>3.  Increase replica count to exceed the number of Longhorn node count<br>4.  Disable **Replica Soft Anti-Affinity** setting<br>5.  Delete a replica<br>6.  Re-Enable **Replica Soft Anti-Affinity** setting | *   Replicas wonâ€™t be removed after disabling **Replica Soft Anti-Affinity**<br>*   when **Replica Soft Anti-Affinity** setting is disabled New Replicas will not be scheduled to nodes.<br>*   when **Replica Soft Anti-Affinity** setting is re-enabled, New Replicas can be scheduled to nodes. | test\_hard\_anti\_affinity\_scheduling |


Additional Tests
----------------

|  **#**   | **Scenario** | **Steps** | **Expected Results** |
| --- | --- | --- | --- |
| 1   | Add Disk disk1, Disable scheduling for default disk -1 | 1.  By default the disk on a node is 0 default disk in in path - `/var/lib/longhorn/`<br>2.  Add disk1 on the node<br>3.  Disable scheduling for the default disk<br>4.  Create a volume in Longhorn<br>5.  Verify the replicas are scheduled on disk1 |
| 2   | Add Disk disk1, Disable scheduling for default disk -2 | Cluster spec - 3 worker nodes<br><br>1.  Create a volume - 3 replicas in `/var/lib/longhorn/` - default disk<br>2.  Add disk 1 on `/mnt/vol2`on node 1<br>3.  Disable scheduling for the default disk<br>4.  enable scheduling for disk1<br>5.  Update the replicas to count = 4<br>6.  Say a replica is built on Node 2<br>7.  Delete the replica on node 1<br>8.  a new replica is rebuilt on node 1<br>9.  Verify replica is now available in `/mnt/vol2` | Replica when rebuilt on node 1 should be available on disk 1 - `/mnt/vol2` |
| **Disable Scheduling On Cordoned Node** |     |     |     |     |     |
| 3   | Disable Scheduling On Cordoned Node: **True**<br><br>**New volume** | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a new volume with 4 replicas<br>4.  Verify the volume `vol-1` is in detached state with error `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>5.  Add a new worker node W5 to the cluster<br>6.  `vol-1` should become healthy.<br>7.  Attach it to a workload and verify data can be written into the volume | 1.  `vol-1` should be in detached state with error `Scheduling Failure Replica Scheduling Failure`<br>2.  vol-1 should become healthy and should be used in a workload to write data into the volume |
| 4   | Disable Scheduling On Cordoned Node: **True**<br><br>**Existing volume** | 1.  There are 4 worker nodes - custom cluster<br>2.  Create a new volume with 4 replicas<br>3.  Volume vol-1 should be in a healthy detached state<br>4.  Attach it to a workload and verify data can be written into the volume<br>5.  cordon a worker node<br>6.  Use the. volume to a workload<br>7.  All the three replicas will be in running healthy state<br>8.  Delete replica on cordoned worker node<br>9.  Verify the volume `vol-1` is in degraded state with error `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>10.  Add a new worker node W5 to the cluster<br>11.  Verify the repliica failed will be in rebuilding state now<br>12.  `vol-1` should become healthy.<br>13.  Verify the data is consistent |     |
| 5   | Disable Scheduling On Cordoned Node: **False**<br><br>**New volume** | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a new volume with 4 replicas<br>4.  `vol-1` should be in healthy.<br>5.  Verify a replica is created on the cordoned worker node<br>6.  Attach it to a workload and verify data can be written into the volume |     |
| 6   | Disable Scheduling On Cordoned Node: **False**<br><br>**Existing volume** |     |     |
| 7   | Disable Scheduling On Cordoned Node: **True**<br><br>Backup restore | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a backup restore volume from an existing backup.<br>4.  Give in the number of replicas - 4, volume name: `vol-2`<br>5.  Verify the volume `vol-2` is in detached state with error `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>6.  Verify no restoring should happen on the replicas.<br>7.  Add a new worker node W5 to the cluster<br>8.  `vol-2` should start restoring now<br>9.  `vol-2` should be in detached healthy state.<br>10.  attach to a workload and verify the checksum of data with that of the original one |     |
| 8   | Disable Scheduling On Cordoned Node: **False**<br><br>Backup restore | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a backup restore volume from an existing backup.<br>4.  Give in the number of replicas - 4, volume name: `vol-2`<br>5.  Verify volume is in attached state and restoring should happen on the replicas<br>6.  `vol-2` should be in detached healthy state. after restoration is complete<br>7.  attach to a workload and verify the checksum of data with that of the original one |     |
| 7   | Disable Scheduling On Cordoned Node: **True**<br><br>Create DR volume | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a `DRV` from an existing backup.<br>4.  Give in the number of replicas - 4<br>5.  Verify the `DRV` is in detached state with error `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>6.  Verify no restoring should happen on the replicas.<br>7.  Add a new worker node W5 to the cluster<br>8.  `DRV` should start restoring now<br>9.  `DRV` should be in healthy state.<br>10.  Activate the `DRV` and verify it is in detached state<br>11.  attach to a workload and verify the checksum of data with that of the original one |     |
| 8   | Disable Scheduling On Cordoned Node: **False**<br><br>Create DR volume | 1.  There are 4 worker nodes - custom cluster<br>2.  Cordon a node W1<br>3.  Create a `DRV` from an existing backup.<br>4.  Give in the number of replicas - 4<br>5.  `DRV` should start restoring now<br>6.  `DRV` should be in healthy state.<br>7.  Activate the `DRV` and verify it is in detached state<br>8.  attach to a workload and verify the checksum of data with that of the original one |     |     |
| 9   | Replica node level soft anti affinity: **False**<br><br>**New volume** | 1.  There are 3 worker nodes - custom cluster<br>2.  Create a volume with replicas - 4<br>3.  Volume should be in detached state with error - `Scheduling Failure Replica Scheduling Failure`with the 4th replica in N/A state<br>4.  Add a worker node<br>5.  the volume should be in healthy state<br>6.  User should be able to use the volume on the workload |     |     |
| 10  | Replica node level soft anti affinity: **True**<br><br>**New volume** | 1.  There are 3 worker nodes - custom cluster<br>2.  Create a volume with replicas - 4<br>3.  the volume should be in healthy state. two replicas should be on the same host<br>4.  User should be able to use the volume on the workload |     |     |